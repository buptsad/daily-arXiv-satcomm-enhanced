{"id": "2512.17128", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.17128", "abs": "https://arxiv.org/abs/2512.17128", "authors": ["Lin Sok", "Martianus Frederic Ezerman", "Ling San"], "title": "Hermitian Hulls of Rational Algebraic Geometry Codes and Applications in Quantum Codes", "comment": "23 pages", "summary": "Interest in the hulls of linear codes has been growing rapidly. More is known when the inner product is Euclidean than Hermitian. A shift to the latter is gaining traction. The focus is on a code whose Hermitian hull dimension and dual distance can be systematically determined. Such a code can serve as an ingredient in designing the parameters of entanglement-assisted quantum error-correcting codes (EAQECCs).\n  We use tools from algebraic function fields of one variable to efficiently determine a good lower bound on the Hermitian hull dimensions of generalized rational algebraic geometry (AG) codes. We identify families of AG codes whose hull dimensions can be well estimated by a lower bound. Given such a code, the idea is to select a set of evaluation points for which the residues of the Weil differential associated with the Hermitian dual code has an easily verifiable property.\n  The approach allows us to construct codes with designed Hermitian hull dimensions based on known results on Reed-Solomon codes and their generalization. Using the Hermitian method on these maximum distance separable (MDS) codes with designed hull dimensions yields two families of MDS EAQECCs. We confirm that the excellent parameters of the quantum codes from these families are new.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u6570\u51e0\u4f55\u7801\u548c\u5e0c\u5c14\u66fc\u5bf9\u79f0\u5185\u79ef\u7684\u8239\u5c3e\u90e8\u7ed3\u6784\uff0c\u7ed9\u51fa\u53ef\u8bbe\u8ba1\u7684 Hermitian \u8239\u5934\u7ef4\u6570\u4e0b\u754c\uff0c\u5e76\u636e\u6b64\u6784\u9020\u65b0\u53c2\u6570\u7684 MDS \u91cf\u5b50\u7ea0\u9519\u7801\uff08EAQECCs\uff09\u3002", "motivation": "\u5728\u7ebf\u6027\u7801\u7684 Hull \u7ef4\u6570\u7814\u7a76\u4e2d\uff0cHermitian \u5185\u79ef\u4e0b\u7684\u7ed3\u6784\u6bd4 Euclidean \u66f4\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4e00\u53d8\u91cf\u4ee3\u6570\u51fd\u6570\u573a\u5de5\u5177\uff0c\u7cfb\u7edf\u5730\u4f30\u8ba1\u4e00\u822c\u5316\u6709\u7406\u4ee3\u6570\u51e0\u4f55\u7801\u7684 Hermitian hull \u7ef4\u6570\uff0c\u5e76\u636e\u6b64\u4e3a\u8bbe\u8ba1 EAQECC \u63d0\u4f9b\u65b0\u7684\u53ef\u884c\u53c2\u6570\u3002", "method": "\u5229\u7528\u4e00\u53d8\u91cf\u4ee3\u6570\u51fd\u6570\u573a\u7684\u6280\u5de7\uff0c\u5bf9\u4e00\u822c\u5316\u6709\u7406\u4ee3\u6570\u51e0\u4f55\u7801\u7684 Hermitian hull \u7ef4\u6570\u7ed9\u51fa\u4e0b\u754c\uff1b\u901a\u8fc7\u9009\u62e9\u8bc4\u4ef7\u70b9\u4f7f\u4e0e Hermitian \u5bf9\u5076\u7801\u76f8\u5173\u7684 Weil \u5fae\u5206\u7684\u7559\u6570\u5177\u6709\u6613\u4e8e\u9a8c\u8bc1\u7684\u6027\u8d28\uff0c\u6765\u4f30\u8ba1 hull\uff1b\u57fa\u4e8e\u5df2\u77e5\u7684 Reed-Solomon \u53ca\u5176\u5e7f\u4e49\u6269\u5c55\u7684\u7ed3\u679c\uff0c\u6784\u9020\u5177\u6709\u8bbe\u8ba1 hull \u7ef4\u6570\u7684\u7801\uff1b\u5728\u8fd9\u4e9b\u5e26\u8bbe\u8ba1 hull \u7684\u6700\u5927\u8ddd\u79bb\u53ef\u5e76\u7684\u7801\uff08MDS\uff09\u4e0a\u5e94\u7528 Hermitian \u65b9\u6cd5\uff0c\u5f97\u5230\u4e24\u65cf\u65b0\u7684 MDS EAQECC\u3002", "result": "\u7ed9\u51fa\u4e24\u65cf\u65b0\u7684\u53c2\u6570\u4f18\u79c0\u7684 MDS EAQECC\uff0c\u5b83\u4eec\u6765\u81ea\u5bf9\u5e26\u8bbe\u8ba1 hull \u7ef4\u6570\u7684 AG \u7801\u7684 Hermitian \u6784\u9020\uff0c\u4e14\u8fd9\u4e9b\u91cf\u5b50\u7801\u7684\u53c2\u6570\u5728\u5df2\u77e5\u6587\u732e\u4e2d\u662f\u65b0\u7684\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5229\u7528 AG \u7801\u5728 Hermitian \u60c5\u51b5\u4e0b\u7684 hull \u7ef4\u6570\u8fdb\u884c\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5bf9 Weil \u5fae\u5206\u7559\u6570\u7684\u63a7\u5236\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684 hull \u4e0b\u754c\uff0c\u8fdb\u800c\u5f97\u5230\u53c2\u6570\u65b0\u9896\u7684 MDS EAQECC\u3002"}}
{"id": "2512.17055", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17055", "abs": "https://arxiv.org/abs/2512.17055", "authors": ["Brian Nelson", "Behrouz Farhang-Boroujeny"], "title": "Packet Detection in a Filter Bank-Based Ultra-Wideband Communication System", "comment": "13 pages, 11 figures. Submitted for review to IEEE Transactions on Signal Processing on December 18, 2025", "summary": "Recently, filter bank multi-carrier spread spectrum (FBMC-SS) technology has been proposed for use in ultra-wideband (UWB) communication systems. It has been noted that, due to the spectral partitioning properties of the filter banks, a UWB signal can be synthesized and processed using a parallel set of signal processors operating at a moderate rate. This transceiver architecture can be used to generate UWB signals, without requiring a high-rate analog-to-digital and/or digital-to-analog converter. In this paper, beginning with a design operating on a single signal processor, we explore the development of a packet detector using the Rao score test. Taking advantage of the FBMC-SS signal structure, an effective detector design based on a cascade channelizer is proposed. We refer to this design as singe-radio band (SRB) detector. Given the typical bandwidth of UWB systems ($\\bf 500$~MHz or wider), the SRB detector has to operate at a fast sampling rate of greater than $\\bf 500$~MHz. This may be undesirable, as low cost analog-to-digital (ADC) and digital-to-analog (DAC) converters are often limited to a sampling rate of $\\bf 200$~MHz or lower. Taking note of this point, the proposed SRB detector is extended to a multi-radio band (MRB) detector, where a set of parallel signal processors operating at a moderate sampling rate are used for a more practical implementation of the detector. Through computer simulations, we show that SRB and MRB detectors have the same performance in typical UWB channels. Finally, we provide results from an over-the-air demonstration of a UWB design occupying $\\bf 1.28$~GHz of bandwidth. We find that reliable detection performance is possible in the harshest environments, at signal-to-noise ratios as low as $\\bf -40$~dB with a preamble length of approximately half the duration of longest preamble length recommended in the IEEE802.15.4 standard.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eFBMC-SS\u7684SRB/MRB\u63a2\u6d4b\u5668\u7528\u4e8eUWB\u7cfb\u7edf\uff0c\u901a\u8fc7Rao score test\u8bbe\u8ba1\u5206\u7ea7\u901a\u9053\u5668\uff0c\u4ee5\u5728\u4e2d\u7b49\u91c7\u6837\u7387\u5b9e\u73b0\u6027\u80fd\u4e0e\u9ad8\u91c7\u6837\u7387\u65b9\u6848\u76f8\u5f53\u7684\u68c0\u6d4b\uff0c\u5e76\u57281.28 GHz\u5e26\u5bbd\u7684OTA\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u6781\u4f4eSNR\u4e0b\u53ef\u9760\u68c0\u6d4b\uff08-40 dB\uff0c\u9884\u7f6e\u534a\u957fIEEE802.15.4\u9884\u7801\uff09\u3002", "motivation": "\u5728\u8d85\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u9700\u8981\u6781\u5bbd\u5e26\u5bbd\u7684\u4fe1\u53f7\u5904\u7406\uff0c\u4f46\u9ad8\u91c7\u6837\u7387ADC/DAC\u6210\u672c\u9ad8\u3001\u5b9e\u73b0\u56f0\u96be\u3002FBMC-SS\u7684\u8c31\u5206\u5272\u7279\u6027\u5141\u8bb8\u4f7f\u7528\u5e76\u884c\u5904\u7406\u7684\u4e2d\u901f\u91c7\u6837\u7387\u6765\u5408\u6210/\u5904\u7406UWB\u4fe1\u53f7\uff0c\u56e0\u6b64\u6709\u673a\u4f1a\u5728\u964d\u4f4e\u91c7\u6837\u7387\u7684\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u4ece\u5355\u4fe1\u53f7\u5904\u7406\u5668\u51fa\u53d1\uff0c\u8bbe\u8ba1\u57fa\u4e8eRao score\u68c0\u9a8c\u7684\u5206\u7ec4\u68c0\u6d4b\u5668\uff1b\u5229\u7528FBMC-SS\u7ed3\u6784\u63d0\u51fa\u7ea7\u8054\u901a\u9053\u5206\u6790\u5668(Model Cascade Channelizer)\u6765\u5b9e\u73b0\u63a2\u6d4b\uff0c\u547d\u540d\u4e3a\u5355\u5c04\u9891\u5e26(SRB)\u63a2\u6d4b\u5668\u3002\u8003\u8651\u5230500 MHz\u53ca\u4ee5\u4e0a\u5e26\u5bbd\u8981\u6c42\uff0cSRB\u9700>500 MHz\u91c7\u6837\u7387\uff0c\u56e0\u800c\u6269\u5c55\u4e3a\u591a\u5c04\u9891\u5e26(MRB)\u63a2\u6d4b\u5668\uff0c\u4f7f\u7528\u5e76\u884c\u5904\u7406\u5668\u5728\u4e2d\u7b49\u91c7\u6837\u7387\u4e0b\u5b9e\u73b0\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\u6bd4\u8f83SRB\u4e0eMRB\u5728\u5178\u578bUWB\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\uff1b\u5e76\u7ed9\u51fa1.28 GHz\u5e26\u5bbd\u7684OTA\u5b9e\u6d4b\u7ed3\u679c\u3002", "result": "SRB\u4e0eMRB\u63a2\u6d4b\u5668\u5728\u5178\u578bUWB\u4fe1\u9053\u4e2d\u5177\u6709\u76f8\u540c\u7684\u68c0\u6d4b\u6027\u80fd\uff1bOTA\u5b9e\u9a8c\u8868\u660e\u5728\u6076\u52a3\u73af\u5883\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u9760\u68c0\u6d4b\uff0c-40 dB SNR\uff0c\u534a\u957f\u5ea6\u7684\u9884\u7801\uff08\u2248IEEE802.15.4\u6807\u51c6\u6700\u957f\u9884\u7801\u7684\u4e00\u534a\uff09\u5373\u53ef\u8fbe\u5230\u53ef\u9760\u68c0\u6d4b\u3002", "conclusion": "\u57fa\u4e8eFBMC-SS\u7684SRB/MRB\u63a2\u6d4b\u6846\u67b6\u5728\u964d\u4f4eADE/ADC/DAC\u91c7\u6837\u7387\u8981\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u5907\u5728\u5b9e\u9645UWB\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u5728OTA\u4e2d\u9a8c\u8bc1\u4e86\u5728\u5bbd\u5e26\u6761\u4ef6\u4e0b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.17256", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.17256", "abs": "https://arxiv.org/abs/2512.17256", "authors": ["Shakir Ali", "Atif Ahmad Khan", "Abhishek Kesarwani", "Susanta Samanta"], "title": "Quasi-recursive MDS Matrices over Galois Rings", "comment": null, "summary": "Let $p$ be a prime and $s,m,n$ be positive integers. This paper studies quasi-recursive MDS matrices over Galois rings $GR(p^{s}, p^{sm})$ and proposes various direct construction methods for such matrices. The construction is based on skew polynomial rings $GR(p^{s}, p^{sm})[X;\u03c3]$, whose rich factorization properties and enlarged class of polynomials are used to define companion matrices generating quasi-recursive MDS matrices. First, two criteria are established for characterizing polynomials that yield recursive MDS matrices, generalizing existing results, and then an additional criterion is derived in terms of the right roots of the associated Wedderburn polynomial. Using these criteria, methods are developed to construct skew polynomials that give rise to quasi-recursive MDS matrices over Galois rings. This framework extends known constructions to the non-commutative setting and significantly enlarges the family of available matrices, with potential applications to efficient diffusion layers in cryptographic primitives. The results are particularly relevant for practical implementations when $s = 1$ and $p = 2$, i.e., over the finite field $\\mathbb{F}_{2^m}$, which is of central interest in real-world cryptographic applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f3d\u7f57\u65af\u73af\u548c\u659c\u591a\u9879\u5f0f\u73af\u7684\u65b0\u76f4\u63a5\u6784\u9020\u65b9\u6cd5\uff0c\u7528\u4ee5\u751f\u6210\u51c6\u9012\u5f52\u7684MDS\u77e9\u9635\uff0c\u5e76\u7ed9\u51fa\u591a\u9879\u5f0f\u5224\u5b9a\u51c6\u5219\u53ca\u5176\u53f3\u6839\u76f8\u5173\u7684Wedderburn\u6761\u4ef6\uff1b\u5728\u975e\u4ea4\u6362\u8bbe\u5b9a\u4e0b\u663e\u8457\u6269\u5c55\u77e9\u9635\u65cf\uff0c\u9002\u7528\u4e8e\u5bc6\u7801\u5b66\u6269\u6563\u5c42\u5e94\u7528\uff0c\u7279\u522b\u662fs=1\u3001p=2\u65f6\u7684\u5b9e\u73b0\u3002", "motivation": "\u6269\u5c55\u73b0\u6709\u9012\u5f52MDS\u77e9\u9635\u7684\u6784\u9020\u81f3\u975e\u4ea4\u6362\u7684Galois\u73af\uff0c\u589e\u52a0\u77e9\u9635\u65cf\u89c4\u6a21\u5e76\u63d0\u5347\u5728\u5bc6\u7801\u5b66\u6269\u6563\u5c42\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5229\u7528 GR(p^s, p^{sm})[X;\u03c3] \u7684\u659c\u591a\u9879\u5f0f\u73af\u53ca\u5176\u4e30\u5bcc\u7684\u5206\u89e3\u6027\u8d28\uff0c\u6784\u9020\u4f34\u968f\u77e9\u9635\u751f\u6210\u51c6\u9012\u5f52MDS\u77e9\u9635\uff1b\u63d0\u51fa\u4e24\u6761\u7528\u4e8e\u5224\u5b9a\u80fd\u4ea7\u751f\u9012\u5f52MDS\u77e9\u9635\u7684\u51c6\u5219\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u57fa\u4e8e\u76f8\u5173 Wedderburn \u591a\u9879\u5f0f\u53f3\u6839\u7684\u9644\u52a0\u51c6\u5219\uff1b\u636e\u6b64\u8bbe\u8ba1\u659c\u591a\u9879\u5f0f\u4ee5\u751f\u6210\u76ee\u6807\u77e9\u9635\uff0c\u62d3\u5c55\u5230\u975e\u4ea4\u6362\u8bbe\u5b9a\u3002", "result": "\u7ed9\u51fa\u5224\u5b9a\u591a\u9879\u5f0f\u7684\u4e24\u4e2a\u51c6\u5219\u53ca\u4e00\u4e2a\u5173\u4e8eWedderburn\u591a\u9879\u5f0f\u53f3\u6839\u7684\u9644\u52a0\u51c6\u5219\uff1b\u5efa\u7acb\u4ece\u659c\u591a\u9879\u5f0f\u5230\u51c6\u9012\u5f52MDS\u77e9\u9635\u7684\u76f4\u63a5\u6784\u9020\u6846\u67b6\uff1b\u5728\u975e\u4ea4\u6362\u8bbe\u7f6e\u4e0b\u663e\u8457\u6269\u5c55\u53ef\u7528\u77e9\u9635\u65cf\uff0c\u63d0\u5347\u5b9e\u9645\u5b9e\u73b0\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5bf9 s=1 \u4e14 p=2\uff08\u5373\u6709\u9650\u57df F_{2^m}\uff09\u7684\u5b9e\u7528\u5b9e\u73b0\u5c24\u4e3a\u76f8\u5173\uff0c\u5e76\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u5bc6\u7801\u5b66\u5e94\u7528\u5177\u6709\u6f5c\u5728\u610f\u4e49\u3002"}}
{"id": "2512.17381", "categories": ["cs.NI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17381", "abs": "https://arxiv.org/abs/2512.17381", "authors": ["Yu-Pin Hsu", "Yi-Hsuan Tseng"], "title": "Timely Information Updating for Mobile Devices Without and With ML Advice", "comment": "23 pages, journal version of arXiv:1901.03137, submitted for possible journal publication", "summary": "This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u4fe1\u606f\u66f4\u65b0\u7cfb\u7edf\u4e2d\u7684\u5728\u7ebf\u66f4\u65b0\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5728\u4ec5\u57fa\u4e8e\u53ef\u83b7\u5f97\u89c2\u6d4b\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5bf9\u65f6\u6548\u6027\u4e0e\u66f4\u65b0\u6210\u672c\u7684\u6700\u4f18\u6743\u8861\uff0c\u4e14\u5728\u5bf9\u624b\u53ef\u540c\u65f6\u64cd\u63a7\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\uff08\u64cd\u4f5c\u65f6\u957f\u3001\u4fe1\u606f\u9648\u65e7\u6027\u3001\u66f4\u65b0\u6210\u672c\u3001\u66f4\u65b0\u673a\u4f1a\u53ef\u7528\u6027\uff09\u7684\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u4f18\u7ade\u4e89\u6bd4\u3002\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5efa\u8bae\u5f15\u5165\u8bbe\u8ba1\uff0c\u63d0\u51faML\u589e\u5f3a\u7b97\u6cd5\uff0c\u5728\u5bf9\u624b\u53ef\u989d\u5916\u6c61\u67d3ML\u5efa\u8bae\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u4e00\u81f4\u6027-\u9c81\u68d2\u6027\u6743\u8861\u7684\u6700\u4f18\u89e3\u3002\u7ade\u4e89\u6bd4\u968f\u66f4\u65b0\u6210\u672c\u533a\u95f4\u7ebf\u6027\u589e\u957f\uff0c\u4f46\u5bf9\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u4e0d\u53d7\u5f71\u54cd\u3002\u6700\u7ec8\u66f4\u65b0\u7b56\u7565\u5bf9ML\u5efa\u8bae\u5448\u73b0\u9608\u503c\u578b\uff1a\u8981\u4e48\u5b8c\u5168\u4fe1\u4efb\u8981\u4e48\u5b8c\u5168\u5ffd\u7565\uff0c\u90e8\u5206\u4fe1\u4efb\u7121\u6cd5\u5728\u4e0d\u4e25\u91cd\u524a\u5f31\u9c81\u68d2\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e00\u81f4\u6027\u3002\u5927\u91cf\u968f\u673a\u73af\u5883\u7684\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u8bba\u3002", "motivation": "\u5728\u79fb\u52a8\u8bbe\u5907\u5bf9\u7269\u7406\u8fc7\u7a0b\u8fdb\u884c\u72b6\u6001\u66f4\u65b0\u5e76\u4f20\u8f93\u7ed9\u63a5\u5165\u70b9\u7684\u573a\u666f\u4e2d\uff0c\u9700\u5728\u4fe1\u606f\u7684\u65f6\u6548\u6027\uff08\u65b0\u9c9c\u5ea6\uff09\u548c\u8bbe\u5907\u66f4\u65b0\u6210\u672c\u4e4b\u95f4\u6743\u8861\uff0c\u5e76\u9762\u5bf9\u591a\u6e90\u4e0d\u786e\u5b9a\u6027\u4e0e\u6f5c\u5728\u5bf9\u624b\u7684\u653b\u51fb\u6027\u5efa\u6a21\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u53ef\u89c2\u6d4b\u4fe1\u606f\u7684\u5728\u7ebf\u66f4\u65b0\u89e6\u53d1\u7b56\u7565\uff0c\u5e76\u5728\u5bf9\u624b\u53ef\u540c\u65f6\u63a7\u5236\u66f4\u65b0\u65f6\u957f\u3001\u4fe1\u606f\u9648\u65e7\u6027\u3001\u66f4\u65b0\u6210\u672c\u548c\u66f4\u65b0\u673a\u4f1a\u7684\u6076\u610f\u73af\u5883\u4e2d\u8fdb\u884c\u7ade\u4e89\u5206\u6790\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u5bf9\u6bd4 adversary \u7684\u6700\u4f18\u7ade\u4e89\u6bd4\uff1b\u5f15\u5165\u5bf9ML\u5efa\u8bae\u7684\u9c81\u68d2\u5904\u7406\uff0c\u8bbe\u8ba1ML\u589e\u5f3a\u7684\u5728\u7ebf\u7b97\u6cd5\uff0c\u63a2\u7d22\u5bf9ML\u5efa\u8bae\u7684\u4f7f\u7528\u9608\u503c\u6027\u7b56\u7565\uff08\u4fe1\u4efb/\u4e0d\u4fe1\u4efb\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5728\u7ebf\u7b97\u6cd5\u5728\u5bf9\u624b adversary \u7684\u60c5\u5f62\u4e0b\u8fbe\u5230\u7406\u8bba\u4e0a\u7684\u6700\u4f18\u7ade\u4e89\u6bd4\uff1bML\u589e\u5f3a\u7b97\u6cd5\u5b9e\u73b0\u4e00\u81f4\u6027-\u9c81\u68d2\u6027\u6743\u8861\u7684\u6700\u4f18\u89e3\uff0c\u5e76\u4e14\u8be5\u7ade\u4e89\u6bd4\u968f\u66f4\u65b0\u6210\u672c\u8303\u56f4\u7ebf\u6027\u589e\u957f\u4e14\u4e0d\u53d7\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\uff1b\u5728\u4eff\u771f\u4e2d\uff0c\u9608\u503c\u578b\u5bf9ML\u5efa\u8bae\u7684\u4f7f\u7528\u88ab\u8bc1\u660e\u80fd\u5728\u63d0\u9ad8\u4e00\u81f4\u6027\u7684\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4e14\u5728\u968f\u673a\u8bbe\u5b9a\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u8bba\u3002", "conclusion": "\u7ed9\u51fa\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u5728\u7ebf\u66f4\u65b0\u8c03\u5ea6\u6846\u67b6\uff0c\u8bc1\u660e\u5bf9ML\u5efa\u8bae\u7684\u9608\u503c\u6027\u4fe1\u4efb\u673a\u5236\u4e0e\u6210\u672c-\u65f6\u6548\u6743\u8861\u7684\u6700\u4f18\u6027\uff0c\u4ee5\u53ca\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5bf9\u6297\u9c81\u68d2\u7684\u5b9e\u73b0\u8def\u5f84\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.17542", "categories": ["cs.IT", "cs.DM"], "pdf": "https://arxiv.org/pdf/2512.17542", "abs": "https://arxiv.org/abs/2512.17542", "authors": ["Andrea Di Giusto", "Elisa Gorla", "Alberto Ravagnani"], "title": "A distance-free approach to generalized weights", "comment": null, "summary": "We propose a unified theory of generalized weights for linear codes endowed with an arbitrary distance. Instead of relying on supports or anticodes, the weights of a code are defined via the intersections of the code with a chosen family of spaces, which we call a test family. The choice of test family determines the properties of the corresponding generalized weights and the characteristics of the code that they capture. In this general framework, we prove that generalized weights are weakly increasing and that certain subsequences are strictly increasing. We also prove a duality result reminiscent of Wei's Duality Theorem. The corresponding properties of generalized Hamming and rank-metric weights follow from our general results by selecting optimal anticodes as a test family. For sum-rank metric codes, we propose a test family that results in generalized weights that are closely connected to -- but not always the same as -- the usual generalized weights. This choice allows us to extend the known duality results for generalized sum-rank weights to some sum-rank-metric codes with a nonzero Hamming component. Finally, we explore a family of generalized weights obtained by intersecting the underlying code with MDS or MRD codes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5e7f\u4e49\u6743\u91cd\u6846\u67b6\uff1a\u901a\u8fc7\u4e00\u4e2a\u6d4b\u8bd5\u65cf\u5c06\u7ebf\u6027\u7801\u4e0e\u82e5\u5e72\u5b50\u7a7a\u95f4\u7684\u4ea4\u96c6\u5b9a\u4e49\u6743\u91cd\uff0c\u80fd\u591f\u4ece\u4e2d\u5bfc\u51fa\u5e7f\u4e49\u6c49\u660e\u3001\u79e9\u5ea6\u3001sum-rank\u7b49\u7684\u6027\u8d28\uff0c\u5e76\u7ed9\u51fa\u5bf9\u5076\u6027\u53ca\u5355\u8c03\u6027\u7b49\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u540c\u8ddd\u79bb\u4e0b\u5bf9\u7ebf\u6027\u7801\u7684\u5e7f\u4e49\u6743\u91cd\u7684\u7edf\u4e00\u5b9a\u4e49\u4e0e\u6027\u8d28\u95ee\u9898\uff0c\u907f\u514d\u4ec5\u4f9d\u8d56\u652f\u6301\u96c6\u6216\u53cd\u7801\uff0c\u63d0\u51fa\u4e00\u4e2a\u53ef\u901a\u8fc7\u9009\u62e9\u6d4b\u8bd5\u65cf\u6765\u63ed\u793a\u6743\u91cd\u6027\u8d28\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u5b9a\u4e49\u6d4b\u8bd5\u65cf\u5e76\u901a\u8fc7\u7801\u4e0e\u8be5\u65cf\u7684\u4ea4\u96c6\u6765\u5b9a\u4e49\u6743\u91cd\uff1b\u5728\u4e00\u822c\u6846\u67b6\u4e0b\u8bc1\u660e\u5e7f\u4e49\u6743\u91cd\u7684\u5f31\u5355\u8c03\u6027\uff0c\u4ee5\u53ca\u90e8\u5206\u5b50\u5e8f\u5217\u7684\u4e25\u683c\u9012\u589e\uff1b\u7ed9\u51fa\u7c7b\u4f3c Wei \u5bf9\u5076\u5b9a\u7406\u7684\u5bf9\u5076\u6027\u7ed3\u8bba\uff1b\u901a\u8fc7\u9009\u53d6\u6700\u4f73\u53cd\u7801\u3001\u4ee5\u53ca\u9488\u5bf9 sum-rank\u3001MDS/MRD \u7684\u7279\u6b8a\u6d4b\u8bd5\u65cf\uff0c\u5c06\u901a\u7528\u7ed3\u679c\u9000\u5316\u4e3a\u5df2\u77e5\u7684\u5e7f\u4e49\u6743\u91cd\u60c5\u5f62\u6216\u8fd1\u4f3c\u60c5\u5f62\uff0c\u5e76\u6269\u5c55\u5bf9\u5305\u542b\u975e\u96f6\u6c49\u660e\u5206\u91cf\u7684\u548c\u79e9\u7801\u7684\u5bf9\u5076\u6027\u3002", "result": "\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u53ca\u5176\u6027\u8d28\uff1a\u5e7f\u4e49\u6743\u91cd\u5728\u4e00\u822c\u6d4b\u8bd5\u65cf\u4e0b\u5177\u5907\u5f31\u5355\u8c03\u6027\uff0c\u67d0\u4e9b\u5b50\u5e8f\u5217\u4e25\u683c\u9012\u589e\uff1b\u5b58\u5728\u4e0e Wei \u7c7b\u4f3c\u7684\u5bf9\u5076\u6027\uff1b\u4e0d\u540c\u6d4b\u8bd5\u65cf\u53ef\u8fd8\u539f\u4e3a\u5e7f\u4e49\u6c49\u660e\u3001\u79e9\u6743\u91cd\u53ca sum-rank \u6743\u91cd\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u7279\u5b9a\u60c5\u5f62\uff08\u5982 MDS/MRD\u3001\u975e\u96f6\u6c49\u660e\u5206\u91cf\u7684\u548c\u79e9\u7801\uff09\u83b7\u5f97\u6269\u5c55\u6027\u7684\u5bf9\u5076\u7ed3\u8bba\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u89c6\u89d2\u6765\u7406\u89e3\u4e0d\u540c\u8ddd\u79bb\u4e0b\u7684\u5e7f\u4e49\u6743\u91cd\uff0c\u901a\u8fc7\u9009\u62e9\u4e0d\u540c\u6d4b\u8bd5\u65cf\u53ef\u4ee5\u5f97\u5230\u5177\u4f53\u60c5\u5f62\u7684\u6743\u91cd\u53ca\u5bf9\u5076\u6027\u8d28\uff0c\u62d3\u5c55\u4e86\u5bf9 sum-rank \u7b49\u65b0\u60c5\u5f62\u7684\u7406\u89e3\uff0c\u5e76\u4e0e\u73b0\u6709\u7ed3\u6784\uff08\u5982\u6700\u4f73\u53cd\u7801\u3001MDS/MRD\uff09\u76f8\u543b\u5408\u6216\u6269\u5c55\u3002"}}
{"id": "2512.17603", "categories": ["cs.IT", "math.NT"], "pdf": "https://arxiv.org/pdf/2512.17603", "abs": "https://arxiv.org/abs/2512.17603", "authors": ["Namhun Koo", "Soonhak Kwon", "Minwoo Ko", "Byunguk Kim"], "title": "Locally-APN Binomials with Low Boomerang Uniformity in Odd Characteristic", "comment": null, "summary": "Recently, several studies have shown that when $q\\equiv3\\pmod{4}$, the function $F_r(x)=x^r+x^{r+\\frac{q-1}{2}}$ defined over $\\mathbb{F}_q$ is locally-APN and has boomerang uniformity at most~$2$. In this paper, we extend these results by showing that if there is at most one $x\\in \\mathbb{F}_q$ with $\u03c7(x)=\u03c7(x+1)=1$ satisfying $(x+1)^r - x^r = b$ for all $b\\in \\mathbb{F}_q^*$ and $\\gcd(r,q-1)\\mid 2$, then $F_r$ is locally-APN with boomerang uniformity at most $2$. Moreover, we study the differential spectra of $F_3$ and $F_{\\frac{2q-1}{3}}$, and the boomerang spectrum of $F_2$ when $p=3$.", "AI": {"tldr": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cF_r(x)=x^r+x^{r+(q-1)/2}\u5728\u6709\u9650\u57dfF_q\u4e0a\u5c40\u90e8APN\u4e14boomerang\u8c31\u4e0d\u8d85\u8fc72\uff1b\u5e76\u7814\u7a76F_3\u3001F_{(2q-1)/3}\u7684\u5fae\u5206\u8c31\uff0c\u4ee5\u53caF_2\u5728p=3\u65f6\u7684boomerang\u8c31\u3002", "motivation": "\u6269\u5c55\u5df2\u6709\u5173\u4e8eq \u2261 3 mod 4 \u4e0b\u5bf9\u5e42\u591a\u9879\u7684\u5c40\u90e8-APN\u6027\u548cboomerang\u8c31\u7684\u8ba4\u8bc6\uff0c\u7ed9\u51fa\u66f4\u5bbd\u7684\u53c2\u6570\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u754c\u9650\uff0c\u5e76\u5206\u6790\u5177\u4f53\u5e42\u7684\u5fae\u5206\u8c31\u4e0eboomerang\u8c31\u4ee5\u6df1\u5316\u5bf9\u8fd9\u7c7b\u51fd\u6570\u5728\u6709\u9650\u57df\u4e2d\u7684\u5dee\u5206\u6027\u8d28\u4e0e\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5bf9\u03c7\u7b26\u53f7\u6761\u4ef6\uff08\u9ad8\u65af\u7b26\u53f7\uff09\u4e0e\u5e42\u591a\u9879\u5dee\u5206\u65b9\u7a0b\u7684\u89e3\u8ba1\u6570\u5206\u6790\uff0c\u7ed3\u5408 gcd(r,q-1) | 2 \u7684\u7ea6\u675f\uff0c\u6269\u5c55\u524d\u4eba\u5de5\u4f5c\u5f97\u5230\u5c40\u90e8APN\u6027\u4e0eboomerang\u8c31\u754c\uff1b\u968f\u540e\u8ba1\u7b97\u5e76\u7ed9\u51faF_3\u3001F_{(2q-1)/3} \u7684\u5fae\u5206\u8c31\uff0c\u4ee5\u53ca\u5728 p=3 \u65f6 F_2 \u7684 boomerang\u8c31\u3002", "result": "\u82e5\u5b58\u5728\u81f3\u591a\u4e00\u4e2ax\u2208F_q\uff0c\u4f7f \u03c7(x)=\u03c7(x+1)=1 \u4e14 (x+1)^r - x^r = b \u5bf9\u6240\u6709 b\u2208F_q^* \u6210\u7acb\uff0c\u4e14 gcd(r,q-1) | 2\uff0c\u5219 F_r \u5728 F_q \u4e0a\u5c40\u90e8APN\uff0cboomerang\u8c31\u4e0d\u8d85\u8fc7 2\uff1b\u5e76\u7ed9\u51fa F_3\u3001F_{(2q-1)/3} \u7684\u5fae\u5206\u8c31\uff0c\u4ee5\u53ca F_2 \u5728 p=3 \u65f6\u7684 boomerang\u8c31\u7684\u5177\u4f53\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u5c06\u524d\u4eba\u7ed3\u679c\u8fdb\u4e00\u6b65\u63a8\u5e7f\u81f3\u66f4\u5e7f\u7684\u53c2\u6570\u8303\u56f4\uff0c\u63ed\u793a\u5728\u7279\u5b9a\u9ad8\u65af\u7b26\u53f7\u6761\u4ef6\u4e0e\u9636\u7684\u7ea6\u675f\u4e0b\uff0c\u4ee3\u6570\u5e42\u591a\u9879\u5728\u6709\u9650\u57df\u7684\u5dee\u5206\u4e0eboomerang\u6027\u8d28\u5177\u6709\u7a33\u5b9a\u754c\u9650\uff0c\u4e3a\u8bbe\u8ba1\u5177\u9c81\u68d2\u5dee\u5206\u7279\u6027\u7684\u65b0\u578b\u7684\u6709\u9650\u57df\u51fd\u6570\u63d0\u4f9b\u6761\u4ef6\u6027\u8bc1\u636e\u3002"}}
{"id": "2512.16928", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16928", "abs": "https://arxiv.org/abs/2512.16928", "authors": ["Kwangjun Ahn", "Noah Amsel", "John Langford"], "title": "Dion2: A Simple Method to Shrink Matrix in Muon", "comment": "https://github.com/microsoft/dion/", "summary": "The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.", "AI": {"tldr": "Dion2\u901a\u8fc7\u7a00\u758f\u66f4\u65b0\u7b56\u7565\u5728Muon\u4f18\u5316\u5668\u4e2d\u663e\u8457\u964d\u4f4e\u6b63\u4ea4\u5316\u6210\u672c\u5e76\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u6b63\u4ea4\u5316\u9636\u6bb5\u5448\u73b0\u8d85\u7ebf\u6027\u6210\u672c\uff0c\u968f\u7740\u89c4\u6a21\u589e\u957f\u4ee3\u4ef7\u4e0a\u5347\u3002\u5c3d\u7ba1\u6709\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u7f29\u5c0f\u8fdb\u5165\u6b63\u4ea4\u5316\u7684\u77e9\u9635\u6765\u7f13\u89e3\uff0c\u4f46\u4ecd\u9700\u8981\u66f4\u7b80\u6d01\u6709\u6548\u7684\u65b9\u6848\u3002", "method": "Dion2\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u9009\u62e9\u4e00\u5b9a\u6bd4\u4f8b\u7684\u884c\u6216\u5217\uff0c\u53ea\u6709\u9009\u4e2d\u7684\u5b50\u96c6\u88ab\u8fdb\u884c\u6b63\u4ea4\u5316\uff0c\u66f4\u65b0\u53d8\u5f97\u7a00\u758f\uff0c\u56e0\u800c\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "\u7a00\u758f\u66f4\u65b0\u964d\u4f4e\u4e86\u603b\u6210\u672c\uff0c\u63d0\u5347\u4e86Muon\u7684\u6a2a\u5411\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "conclusion": "Dion2\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u524d\u671f\u5de5\u4f5c\u66f4\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u77e9\u9635\u7f29\u5c0f\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684Muon\u4f18\u5316\u5668\u3002"}}
{"id": "2512.17057", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17057", "abs": "https://arxiv.org/abs/2512.17057", "authors": ["Lyes Smaili", "Soulaimane Berkane"], "title": "Perception-Limited Smooth Safety Filtering", "comment": "8 pages", "summary": "This paper develops a smooth safety-filtering framework for nonlinear control-affine systems under limited perception. Classical Control Barrier Function (CBF) filters assume global availability of the safety function - its value and gradient must be known everywhere - an assumption incompatible with sensing-limited settings, and the resulting filters often exhibit nonsmooth switching when constraints activate. We propose two complementary perception-aware safety filters applicable to general control-invariant safety sets. The first introduces a smooth perception gate that modulates barrier constraints based on sensing range, yielding a closed-form Lipschitz-safe controller with forward-invariance guarantees. The second replaces the hard CBF constraint with a differentiable penalty term, leading to a smooth unconstrained optimization-based safety filter consistent with CBF principles. For both designs, we establish existence, uniqueness, and forward invariance of the closed-loop trajectories. Numerical results demonstrate that the proposed smooth filters enable the synthesis of higher-order tracking controllers for systems such as drones and second-order ground robots, offering substantially smoother and more robust safety-critical behaviors than classical CBF-based filters.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u611f\u77e5\u611f\u77e5\u53cb\u597d\u3001\u5e73\u6ed1\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u7528\u4e8e\u5728\u611f\u77e5\u6709\u9650\u7684\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f20\u7edfCBF\u5728\u611f\u77e5\u53d7\u9650\u4e0b\u7684\u5207\u6362\u4e0d\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u4ee5\u95ed\u5f0f\u89e3\u548c\u53ef\u5fae\u60e9\u7f5a\u5f62\u5f0f\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\uff0c\u7406\u8bba\u4e0e\u6570\u503c\u5747\u8868\u660e\u4f18\u4e8e\u4f20\u7edfCBF\u3002", "motivation": "\u7531\u4e8e\u611f\u77e5\u53d7\u9650\u5bfc\u81f4\u5168\u5c40\u53ef\u7528\u7684\u5b89\u5168\u51fd\u6570\u4e0d\u53ef\u884c\uff0c\u4f20\u7edfCBF\u5728\u611f\u77e5\u8fb9\u754c\u5904\u4f1a\u4ea7\u751f\u4e0d\u5e73\u6ed1\u5207\u6362\uff0c\u4e9f\u9700\u5728\u4fdd\u8bc1\u5b89\u5168\u4e0e\u53ef\u5b9e\u73b0\u6027\u524d\u63d0\u4e0b\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6ee4\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7684\u611f\u77e5\u611f\u77e5\u5b89\u5168\u8fc7\u6ee4\u5668\uff1a\u7b2c\u4e00\u79cd\u662f\u5e73\u6ed1\u611f\u77e5\u95e8\uff0c\u57fa\u4e8e\u611f\u77e5\u8ddd\u79bb\u8c03\u6574CBF\u7ea6\u675f\uff0c\u7ed9\u51fa\u95ed\u5f0f\u7684Lipschitz\u5b89\u5168\u63a7\u5236\u5f8b\u5e76\u4fdd\u8bc1\u524d\u5411\u4e0d\u53d8\u6027\uff1b\u7b2c\u4e8c\u79cd\u7528\u53ef\u5fae\u60e9\u7f5a\u9879\u66ff\u4ee3\u786cCBF\u7ea6\u675f\uff0c\u5f97\u5230\u4e00\u4e2a\u4e0eCBF\u539f\u7406\u4e00\u81f4\u7684\u5e73\u6ed1\u65e0\u7ea6\u675f\u4f18\u5316\u5b89\u5168\u6ee4\u6ce2\u5668\u3002\u5bf9\u4e24\u79cd\u8bbe\u8ba1\u7ed9\u51fa\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u95ed\u73af\u8f68\u8ff9\u7684\u524d\u5411\u4e0d\u53d8\u6027\u8bc1\u660e\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u5e73\u6ed1\u6ee4\u6ce2\u5668\u80fd\u4e3a\u65e0\u4eba\u673a\u548c\u4e8c\u9636\u5730\u9762\u673a\u5668\u4eba\u7b49\u7cfb\u7edf\u5408\u6210\u66f4\u9ad8\u9636\u7684\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u80fd\u7684\u5e73\u6ed1\u6027\u548c\u9c81\u68d2\u6027\uff0c\u76f8\u8f83\u4e8e\u7ecf\u5178CBF\u6ee4\u6ce2\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u611f\u77e5\u611f\u77e5\u7684\u5b89\u5168\u8fc7\u6ee4\u7b56\u7565\uff0c\u786e\u4fdd\u5728\u611f\u77e5\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5e73\u6ed1\u5b89\u5168\u884c\u4e3a\uff0c\u5e76\u5728\u7406\u8bba\u548c\u6570\u503c\u4e0a\u4f18\u4e8e\u4f20\u7edfCBF\u6ee4\u6ce2\u5668\uff0c\u63a8\u52a8\u9ad8\u9636\u63a7\u5236\u5728\u611f\u77e5\u53d7\u9650\u4e0b\u7684\u5b89\u5168\u6027\u5b9e\u73b0\u3002"}}
{"id": "2512.16957", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.16957", "abs": "https://arxiv.org/abs/2512.16957", "authors": ["Friedrich Doku", "Jonathan Laughton", "Nick Wanninger", "Peter Dinda"], "title": "CAPIO: Safe Kernel-Bypass of Commodity Devices using Capabilities", "comment": null, "summary": "Securing low-latency I/O in commodity systems forces a fundamental trade-off: rely on the kernel's high overhead mediated interface, or bypass it entirely, exposing sensitive hardware resources to userspace and creating new vulnerabilities. This dilemma stems from a hardware granularity mismatch: standard MMUs operate at page boundaries, making it impossible to selectively expose safe device registers without also exposing the sensitive control registers colocated on the same page. Existing solutions to driver isolation enforce an isolation model that cannot protect sub-page device resources.\n  This paper presents CAPIO, the first architecture to leverage hardware capabilities to enforce fine-grained access control on memory-mapped I/O. Unlike prior page-based protections, CAPIO utilizes unforgeable capabilities to create precise, sub-page \"slices\" of device memory. This mechanism enables the kernel to delegate latency-critical hardware access to userspace applications while strictly preventing interaction with co-located privileged registers.\n  We implement CAPIO based on CHERI on the ARM Morello platform and demonstrate a proof-of-concept safe-access driver for a commodity network card which was not originally designed for kernel bypass. We demonstrate that CAPIO achieves the latency improvements of kernel bypass while enforcing byte-level access control of privileged resources.", "AI": {"tldr": "CAPIO \u4ee5\u80fd\u529b\u673a\u7406\u5b9e\u73b0\u5bf9\u5185\u5b58\u6620\u5c04I/O\u7684\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\uff0c\u901a\u8fc7\u5b50\u9875\u5207\u7247\u6765\u5b9e\u73b0\u5bf9\u8bbe\u5907\u5185\u5b58\u7684\u7cbe\u786e\u6388\u6743\uff0c\u4e0e\u5185\u6838\u7ed5\u884c\u76f8\u6bd4\u4fdd\u7559\u5b57\u8282\u7ea7\u8bbf\u95ee\u63a7\u5236\u4e0e\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u5185\u6838\u4f9d\u8d56\u7684\u9ad8\u5f00\u9500\u4e2d\u4ecb\u63a5\u53e3\u4e0e\u6309\u9875\u7c92\u5ea6\u7684\u5bfb\u5740\u6a21\u578b\u5bfc\u81f4\u4e86\u4f4e\u5ef6\u8fdfI/O\u7684\u5b89\u5168\u4e0e\u6027\u80fd\u77db\u76fe\uff1b\u5fc5\u987b\u5728\u4e0d\u66b4\u9732\u5b8c\u6574\u7684\u63a7\u5236\u5bc4\u5b58\u5668\u7684\u524d\u63d0\u4e0b\uff0c\u907f\u514d\u5b50\u9875\u7ea7\u522b\u7684\u8d44\u6e90\u66b4\u9732\u3002", "method": "\u5728 CHERI \u7684 ARM Morello \u5e73\u53f0\u4e0a\u5b9e\u73b0 CAPIO\uff0c\u5229\u7528\u4e0d\u53ef\u4f2a\u9020\u7684\u80fd\u529b\u521b\u5efa\u8bbe\u5907\u5185\u5b58\u7684\u5b50\u9875\u201c\u5207\u7247\u201d\uff0c\u5e76\u5c06\u5ef6\u8fdf\u654f\u611f\u7684\u786c\u4ef6\u8bbf\u95ee\u4e0b\u653e\u7ed9\u7528\u6237\u6001\uff0c\u540c\u65f6\u4e25\u683c\u9694\u79bb\u540c\u9875\u4e0a\u5171\u5b58\u7684\u7279\u6743\u5bc4\u5b58\u5668\uff1b\u5b9e\u73b0\u4e00\u4e2a\u9762\u5411\u5546\u7528\u7f51\u5361\u7684\u5b89\u5168\u8bbf\u95ee\u9a71\u52a8\u4f5c\u4e3a\u539f\u578b\uff0c\u9a8c\u8bc1\u5728\u5185\u6838\u7ed5\u884c\u4e0b\u7684\u5ef6\u8fdf\u6539\u5584\u4e0e\u5b57\u8282\u7ea7\u8bbf\u95ee\u63a7\u5236\u3002", "result": "CAPIO \u5b9e\u73b0\u4e86\u5185\u6838\u7ed5\u884c\u7684\u4f4e\u5ef6\u8fdf\u4f18\u52bf\uff0c\u5e76\u5728\u5b57\u8282\u7ea7\u522b\u4e0a\u5bf9\u7279\u6743\u8d44\u6e90\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\u7684\u8bc1\u660e\u539f\u578b\uff0c\u5c55\u793a\u4e86\u7ec6\u7c92\u5ea6I/O\u6743\u9650\u63a7\u5236\u7684\u53ef\u884c\u6027\u4e0e\u6548\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u786c\u4ef6\u80fd\u529b\u63d0\u4f9b\u7684\u4e0d\u53ef\u4f2a\u9020\u7684\u80fd\u529b\uff0cCAPIO \u8bc1\u660e\u4e86\u7ec6\u7c92\u5ea6\u7684 I/O \u8bbf\u95ee\u7b56\u7565\u5728\u5546\u4e1a\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4e0d\u66b4\u9732\u5b8c\u6574\u8bbe\u5907\u5bc4\u5b58\u5668\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf I/O \u63d0\u4f9b\u4e86\u4e00\u6761\u5207\u5b9e\u8def\u5f84\u3002"}}
{"id": "2512.17628", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.17628", "abs": "https://arxiv.org/abs/2512.17628", "authors": ["Liandong Hu", "Jian Dang", "Zaichen Zhang"], "title": "Iterative Gaussian Approximation for Random Spreading Unsourced Random Access", "comment": null, "summary": "Massive machine-type communications (mMTC) demand robust solutions to support extensive connectivity efficiently. Unsourced random access (URA) has emerged as a promising approach, delivering high spectral and energy efficiency. Among URA code structures, the random spreading (RS) category is a key enabler, providing strong anti-interference capabilities through spectrum spreading gain. Notably, RS-URA approaches theoretical performance limits over the Gaussian multiple access channel in scenarios with few active users. In this paper, we propose an iterative Gaussian approximation decoder designed universally for RS-URA categories. The proposed receiver iterates extrinsic and intrinsic soft information to enhance decoding performance, requiring only a few iterations to converge. Numerical results validate the decoder's effectiveness in terms of performance and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u4e8e RS-URA \u7c7b\u522b\u7684\u8fed\u4ee3\u9ad8\u65af\u8fd1\u4f3c\u89e3\u7801\u5668\uff0c\u9002\u7528\u4e8e RS-URA \u7684\u968f\u673a\u6269\u9891\u7ed3\u6784\uff0c\u80fd\u5728\u5c11\u6570\u6d3b\u52a8\u7528\u6237\u573a\u666f\u4e0b\u63a5\u8fd1\u7406\u8bba\u6781\u9650\uff0c\u4e14\u8fed\u4ee3\u6b21\u6570\u5c11\u3001\u6536\u655b\u5feb\u3002", "motivation": "\u9762\u5411\u5927\u89c4\u6a21\u673a\u5668\u7c7b\u578b\u901a\u4fe1(mMTC)\u5bf9\u5e7f\u6cdb\u8fde\u63a5\u7684\u9c81\u68d2\u548c\u9ad8\u6548\u9700\u6c42\uff0cURA \u5c24\u5176\u662f\u968f\u673a\u6269\u9891\u7684 RS-URA \u4f5c\u4e3a\u964d\u4f4e\u5e72\u6270\u3001\u63d0\u9ad8\u9891\u8c31\u4e0e\u80fd\u91cf\u6548\u7387\u7684\u5173\u952e\u6280\u672f\uff0c\u9700\u63a5\u8fd1\u9ad8\u65af\u591a\u5740\u4fe1\u9053\u6781\u9650\u7684\u89e3\u7801\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u4e8e RS-URA \u7c7b\u522b\u7684\u8fed\u4ee3\u9ad8\u65af\u8fd1\u4f3c\u89e3\u7801\u5668\u3002\u6240\u8ff0\u89e3\u7801\u5668\u901a\u8fc7\u8fed\u4ee3\u5916\u4fe1\u606f\u4e0e\u5185\u4fe1\u606f\uff08extrinsic/intrinsic soft information\uff09\u5b9e\u73b0 soft-output \u89e3\u7801\uff0c\u4e14\u6536\u655b\u53ea\u9700\u5c11\u91cf\u8fed\u4ee3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u89e3\u7801\u5668\u5728\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6548\uff0c\u80fd\u5728\u4e0d\u540c RS-URA \u573a\u666f\u4e2d\u63d0\u5347\u89e3\u7801\u6027\u80fd\u5e76\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u901a\u7528\u8fed\u4ee3\u9ad8\u65af\u8fd1\u4f3c\u89e3\u7801\u6846\u67b6\u5728 RS-URA \u4e2d\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3001\u5feb\u901f\u6536\u655b\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u6d3b\u52a8\u7528\u6237\u6570\u60c5\u5f62\u4e0b\u7684\u89e3\u7801\u80fd\u529b\u3002"}}
{"id": "2512.17133", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17133", "abs": "https://arxiv.org/abs/2512.17133", "authors": ["Talha Akyildiz", "Hessam Mahdavifar"], "title": "Deep Reinforcement Learning-Aided Strategies for Big Data Offloading in Vehicular Networks", "comment": null, "summary": "We consider vehicular networking scenarios where existing vehicle-to-vehicle (V2V) links can be leveraged for an effective uploading of large-size data to the network. In particular, we consider a group of vehicles where one vehicle can be designated as the \\textit{leader} and other \\textit{follower} vehicles can offload their data to the leader vehicle or directly upload it to the base station (or a combination of the two). In our proposed framework, the leader vehicle is responsible for receiving the data from other vehicles and processing it in order to remove the redundancy (deduplication) before uploading it to the base station. We present a mathematical framework of the considered network and formulate two separate optimization problems for minimizing (i) total time and (ii) total energy consumption by vehicles for uploading their data to the base station. We employ deep reinforcement learning (DRL) tools to obtain solutions in a dynamic vehicular network where network parameters (e.g., vehicle locations and channel coefficients) vary over time. Our results demonstrate that the application of DRL is highly beneficial, and data offloading with deduplication can significantly reduce the time and energy consumption. Furthermore, we present comprehensive numerical results to validate our findings and compare them with alternative approaches to show the benefits of the proposed DRL methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.16929", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.16929", "abs": "https://arxiv.org/abs/2512.16929", "authors": ["Pranesh Sathish Kumar"], "title": "BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control", "comment": "12 pages, 8 figures", "summary": "Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.16962", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16962", "abs": "https://arxiv.org/abs/2512.16962", "authors": ["Saksham Sahai Srivastava", "Haoyu He"], "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval", "comment": "14 pages, 1 figure, includes appendix", "summary": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17335", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.17335", "abs": "https://arxiv.org/abs/2512.17335", "authors": ["Xiao Tang", "Zhen Ma", "Bin Li", "Cong Li", "Qinghe Du", "Dusit Niyato", "Zhu Han"], "title": "Active RIS-Aided Anti-Jamming Wireless Communications: A Stackelberg Game Perspective", "comment": "Accepted @ IEEE TCOM", "summary": "The pervasive threat of jamming attacks, particularly from adaptive jammers capable of optimizing their strategies, poses a significant challenge to the security and reliability of wireless communications. This paper addresses this issue by investigating anti-jamming communications empowered by an active reconfigurable intelligent surface. The strategic interaction between the legitimate system and the adaptive jammer is modeled as a Stackelberg game, where the legitimate user, acting as the leader, proactively designs its strategy while anticipating the jammer's optimal response. We prove the existence of the Stackelberg equilibrium and derive it using a backward induction method. Particularly, the jammer's optimal strategy is embedded into the leader's problem, resulting in a bi-level optimization that jointly considers legitimate transmit power, transmit/receive beamformers, and active reflection. We tackle this complex, non-convex problem by using a block coordinate descent framework, wherein subproblems are iteratively solved via convex relaxation and successive convex approximation techniques. Simulation results demonstrate the significant superiority of the proposed active RIS-assisted scheme in enhancing legitimate transmissions and degrading jamming effects compared to baseline schemes across various scenarios. These findings highlight the effectiveness of combining active RIS technology with a strategic game-theoretic framework for anti-jamming communications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e3b\u52a8\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u7684 anti-jamming \u901a\u4fe1\u65b9\u6848\uff0c\u91c7\u7528 Stackelberg \u535a\u5f08\u6a21\u578b\uff0c\u9886\u5bfc\u8005\u4e3a\u5408\u6cd5\u7528\u6237\uff0c\u9884\u6d4b\u5e76\u5d4c\u5165\u5e72\u6270\u8005\u7684\u6700\u4f18\u7b56\u7565\uff1b\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u5206\u5757\u5750\u6807\u4e0b\u964d\u6c42\u89e3\uff0c\u8bc1\u660e\u5b58\u5728 Stackelberg \u5747\u8861\uff0c\u7b97\u6cd5\u5728\u591a\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "motivation": "\u5bf9\u6297\u81ea\u9002\u5e94\u5e72\u6270\uff08jammer\uff09\u5bf9\u65e0\u7ebf\u901a\u4fe1\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u65b0\u578b\u6297\u5e72\u6270\u6280\u672f\u3002\u4e3b\u52a8 RIS \u53ef\u63d0\u4f9b\u53ef\u63a7\u4fe1\u9053\u6539\u9020\uff0c\u7ed3\u5408\u535a\u5f08\u8bba\u53ef\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u6216\u5bf9\u6297\u73af\u5883\u4e0b\u4f18\u5316\u8d44\u6e90\u914d\u7f6e\uff0c\u63d0\u5347\u5408\u6cd5\u4f20\u8f93\u8d28\u91cf\u5e76\u524a\u5f31\u5e72\u6270\u3002", "method": "\u5c06\u5408\u6cd5\u7cfb\u7edf\u4e0e\u81ea\u9002\u5e94\u5e72\u6270\u8005\u7684\u4e92\u52a8\u5efa\u6a21\u4e3a Stackelberg \u535a\u5f08\uff0c\u9886\u5bfc\u8005\uff08\u5408\u6cd5\u7528\u6237\uff09\u5148\u8bbe\u8ba1\u4f20\u8f93\u7b56\u7565\u5e76\u9884\u671f\u5e72\u6270\u8005\u7684\u6700\u4f18\u54cd\u5e94\uff1b\u5e72\u6270\u8005\u6700\u4f18\u7b56\u7565\u88ab\u5d4c\u5165\u9886\u5bfc\u8005\u95ee\u9898\uff0c\u5f62\u6210\u53cc\u5c42\u4f18\u5316\uff0c\u8054\u5408\u4f18\u5316\u5408\u6cd5\u7684\u53d1\u5c04\u529f\u7387\u3001\u53d1\u5c04/\u63a5\u6536\u6ce2\u675f\u3001\u4ee5\u53ca\u4e3b\u52a8\u53cd\u5c04\u53c2\u6570\uff1b\u901a\u8fc7\u5206\u5757\u5750\u6807\u4e0b\u964d\u6c42\u89e3\uff0c\u5b50\u95ee\u9898\u91c7\u7528\u51f8\u677e\u5f1b\u4e0e\u5e8f\u5217\u51f8\u8fd1\u4f3c\u9010\u6b65\u6536\u655b\u3002", "result": "\u8bc1\u660e\u5b58\u5728 Stackelberg \u5747\u8861\uff1b\u7ed9\u51fa\u53ef\u5b9e\u73b0\u7684\u6c42\u89e3\u6d41\u7a0b\uff08BCD+\u51f8\u677e\u5f1b/\u8fd1\u4f3c\uff09\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e3b\u52a8 RIS \u52a9\u529b\u7684\u65b9\u6848\u5728\u63d0\u5347\u5408\u6cd5\u4f20\u8f93\u548c\u964d\u4f4e\u5e72\u6270\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5747\u5177\u4f18\u52bf\u3002", "conclusion": "\u4e3b\u52a8 RIS \u4e0e\u535a\u5f08\u8bba\u6846\u67b6\u7684\u7ed3\u5408\u5728\u6297\u5e72\u6270\u901a\u4fe1\u4e2d\u6709\u6548\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u7684\u8d44\u6e90\u5206\u914d\u548c\u4fe1\u9053\u63a7\u5236\u5b9e\u73b0\u5bf9\u81ea\u9002\u5e94\u5e72\u6270\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002"}}
{"id": "2512.17138", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17138", "abs": "https://arxiv.org/abs/2512.17138", "authors": ["Vinicius P. Campos", "Diego Szczupak", "Tales Santini", "Afonso C. Silva", "Alessandro Foi", "Marcelo A. C. Vieira", "Corey A. Baron"], "title": "BM4D-PC: nonlocal volumetric denoising of principal components of diffusion-weighted MR images", "comment": "Submitted to Magnetic Resonance in Medicine", "summary": "Purpose: Noise in diffusion-weighted MRI (dMRI) is often spatially correlated due to different acquisition and reconstruction strategies, which is not fully accounted for in current denoising strategies. Thus, we propose a novel model-based denoising method for dMRI that effectively accounts for the different noise characteristics of data. Methods: We propose a denoising strategy that incorporates full noise statistics, including the noise power spectral density (PSD), by leveraging the BM4D algorithm. Furthermore, to exploit redundancy across the diffusion MRI dataset, BM4D is applied to principal components (PC) of diffusion-weighted images (DWI) obtained through principal component analysis (PCA) decomposition of the entire DWI dataset, an approach we refer to as BM4D-PC. Importantly, our method also allows for direct estimation of both the noise map and PSD. We evaluated BM4D-PC against four existing state-of-the-art methods using in-silico and in vivo datasets, including high-resolution human and marmoset acquisitions. Results: Overall, BM4D-PC presented the best results for the metrics PSNR, SSIM and RMSE on the in-silico experiments. The in-vivo studies also showed that BM4D-PC dramatically enhanced the image quality of raw DWIs, outperforming existing denoising methods in terms of noise suppression and detail preservation, leading to improved quality of diffusion metrics. Conclusion: The proposed BM4D-PC method demonstrated state-of-the-art denoising results for dMRI using datasets from various acquisition strategies and image resolutions, potentially supporting future advances in neuroscience research.", "AI": {"tldr": "\u63d0\u51fa BM4D-PC \u7684 dMRI \u53bb\u566a\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u5728 PCA \u5206\u89e3\u540e\u7684 diffusion \u6743\u91cd\u56fe\u50cf\u7684\u4e3b\u6210\u5206\u4e0a\u5e94\u7528 BM4D\uff0c\u7ed3\u5408\u5b8c\u6574\u566a\u58f0\u7edf\u8ba1\uff08PSD\uff09\uff0c\u5b9e\u73b0\u5bf9\u7a7a\u95f4\u76f8\u5173\u566a\u58f0\u7684\u6709\u6548\u5efa\u6a21\uff0c\u5e76\u53ef\u76f4\u63a5\u4f30\u8ba1\u566a\u58f0\u56fe\u548c PSD\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u4e8e\u56db\u79cd\u5bf9\u6bd4\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "dMRI \u7684\u566a\u58f0\u7531\u4e8e\u91c7\u96c6\u4e0e\u91cd\u5efa\u7b56\u7565\u7684\u4e0d\u540c\u800c\u5448\u73b0\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u73b0\u6709\u53bb\u566a\u65b9\u6cd5\u672a\u5145\u5206\u5efa\u6a21\u566a\u58f0\u7279\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u5229\u7528\u5b8c\u6574\u566a\u58f0\u7edf\u8ba1\u4fe1\u606f\u7684\u53bb\u566a\u6846\u67b6\uff1b\u4ee5\u63d0\u5347\u53bb\u566a\u6548\u679c\u3001\u4fdd\u7559\u7ec6\u8282\u5e76\u6539\u5584\u6269\u6563\u6307\u6807\u4e3a\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4ee5 BM4D \u4e3a\u6838\u5fc3\u7684\u53bb\u566a\u7b56\u7565\uff0c\u5229\u7528\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\uff08PSD\uff09\u7b49\u5b8c\u6574\u566a\u58f0\u7edf\u8ba1\u4fe1\u606f\uff1b\u5c06 BM4D \u5e94\u7528\u4e8e\u901a\u8fc7\u5bf9\u6574\u4e2a DWI \u6570\u636e\u96c6\u8fdb\u884c PCA \u5206\u89e3\u5f97\u5230\u7684\u4e3b\u6210\u5206\uff0c\u4ece\u800c\u6316\u6398\u8de8\u6269\u6563\u65b9\u5411\u7684 Redundancy\uff08BM4D-PC\uff09\u3002\u65b9\u6cd5\u8fd8\u53ef\u76f4\u63a5\u4f30\u8ba1\u566a\u58f0\u6620\u5c04\u53ca PSD\u3002\u5bf9\u6bd4\u7814\u7a76\u5305\u62ec\u5728\u4f53\u5916\u4eff\u771f\u4e0e\u4f53\u5185\u9ad8\u5206\u8fa8\u7387\u4eba\u7c7b\u548c\u5377\u5c3e\u7334\u6570\u636e\uff0c\u548c\u56db\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e2d\uff0cBM4D-PC \u5728 PSNR\u3001SSIM\u3001RMSE \u4e09\u9879\u6307\u6807\u4e0a\u5747\u8868\u73b0\u6700\u597d\uff1b\u5728\u4f53\u5185\u6570\u636e\u4e2d\uff0cBM4D-PC \u663e\u8457\u63d0\u5347\u539f\u59cb DWI \u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5728\u566a\u58f0\u6291\u5236\u548c\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u5e76\u6539\u5584\u4e86\u6269\u6563\u6307\u6807\u7684\u8d28\u91cf\u3002", "conclusion": "BM4D-PC \u80fd\u5728\u591a\u79cd\u91c7\u96c6\u7b56\u7565\u4e0e\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u524d\u6cbf\u7684 dMRI \u53bb\u566a\u6548\u679c\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6269\u6563\u5206\u6790\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6491\u3002"}}
{"id": "2512.17249", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17249", "abs": "https://arxiv.org/abs/2512.17249", "authors": ["Yuqi Ping", "Junwei Wu", "Bofeng Zheng", "Fan Liu", "Tianhao Liang", "Tingting Zhang"], "title": "Uncertainty-Aware 3D UAV Tracking Using Single-Anchor UWB Measurements", "comment": null, "summary": "In this letter, we present an uncertainty-aware single-anchor Ultra-Wideband (UWB)-based 3D tracking framework. Specifically, a mobile Unmanned Aerial Vehicle (UAV) maintains a desired standoff distance to a moving target using range and 3D bearing measurements from a multi-antenna UWB anchor rigidly mounted on the UAV. To enhance the stability and safety under measurement degradation and motion uncertainty, we jointly design a robust factor-graph-based target localization method and a covariance-aware control Lyapunov function--control barrier function (CLF--CBF) tracking controller. This controller adaptively adjusts distance bounds and safety margins based on the posterior target covariance provided by the factor graph. The proposed system is evaluated through numerical simulations and real-world experiments carried out in a narrow indoor corridor environment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u951aUWB\u76843D\u8ddf\u8e2a\u6846\u67b6\uff0c\u7ed3\u5408\u9c81\u68d2\u56e0\u5b50\u56fe\u5b9a\u4f4d\u4e0e\u534f\u65b9\u5dee\u611f\u77e5\u7684CLF-CBF\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u4ee5\u5728\u6d4b\u91cf\u964d\u7ea7\u548c\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u4e0b\u63d0\u5347\u7a33\u5b9a\u6027\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u4ec5\u4f9d\u8d56\u5355\u951aUWB\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5bf9\u79fb\u52a8\u76ee\u6807\u7684\u53ef\u97603D\u8ddf\u8e2a\uff0c\u9700\u5c06\u4e0d\u786e\u5b9a\u6027\u5f15\u5165\u7684\u5b9a\u4f4d\u4e0e\u63a7\u5236\u8026\u5408\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\uff08\u5982\u5ba4\u5185\u72ed\u7a84\u8d70\u5eca\uff09\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u5728UAV\u4e0a\u5b89\u88c5\u5355\u951aUWB\uff0c\u901a\u8fc7\u8ddd\u79bb\u4e0e\u4e09\u7ef4\u65b9\u4f4d\u89d2\u7b49\u89c2\u6d4b\u6784\u5efa\u9c81\u68d2\u56e0\u5b50\u56fe\u5b9e\u73b0\u76ee\u6807\u5b9a\u4f4d\uff0c\u5e76\u5229\u7528\u5176\u540e\u9a8c\u534f\u65b9\u5dee\u6765\u9a71\u52a8\u534f\u65b9\u5dee\u611f\u77e5\u7684CLF-CBF\u63a7\u5236\u5668\uff0c\u52a8\u6001\u8c03\u6574\u76ee\u6807\u8ddd\u79bb\u7ea6\u675f\u4e0e\u5b89\u5168\u88d5\u5ea6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u4e0e\u771f\u5b9e\u5ba4\u5185\u8d70\u5eca\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d4b\u91cf\u4e0b\u964d\u7ea7\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u8f83\u597d\u8ddf\u8e2a\u6027\u4e0e\u5b89\u5168\u6027\u3002", "conclusion": "\u5c06\u89c2\u6d4b\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u5230\u63a7\u5236\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u8026\u5408\uff0c\u63d0\u4f9b\u4e00\u79cd\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u53ef\u884c\u76843D\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17360", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17360", "abs": "https://arxiv.org/abs/2512.17360", "authors": ["Wanli Xie", "Jiale Zhang", "Ruiqing Cao"], "title": "Grey graphs and its application", "comment": null, "summary": "In multi-attribute decision-making problems where the attribute values are interval grey numbers, a simplified form based on kernels and the degree of greyness is presented. Combining fuzzy graph theory with the kernel and the degree of greyness of interval grey numbers, grey graphs and their corresponding operation rules are presented. This paper presents a new multi-attribute decision-making method based on grey graph theory. We analyzed and evaluated the alternative schemes using grey graph. Lastly, a numerical example was conducted in order to demonstrate the effectiveness and feasibility of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7070\u8272\u56fe\u7406\u8bba\u7684\u591a\u5c5e\u6027\u51b3\u7b56\u65b9\u6cd5\uff0c\u5229\u7528\u533a\u95f4\u7070\u6570\u3001\u6838\u4e0e\u7070\u5ea6\u5ea6\uff0c\u7ed3\u5408\u6a21\u7cca\u56fe\u4e0e\u7070\u8272\u56fe\u8fd0\u7b97\u89c4\u5219\uff0c\u901a\u8fc7\u6570\u503c\u4f8b\u5b50\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u5c5e\u6027\u51b3\u7b56\u95ee\u9898\u4e2d\u5c5e\u6027\u503c\u4e3a\u533a\u95f4\u7070\u6570\u65f6\u7684\u51b3\u7b56\u96be\u9898\uff0c\u5229\u7528\u7070\u8272\u56fe\u548c\u6838/\u7070\u5ea6\u5ea6\u6539\u5584\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u884c\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u6a21\u7cca\u56fe\u7406\u8bba\u4e0e\u533a\u95f4\u7070\u6570\u7ed3\u5408\uff0c\u63d0\u51fa\u57fa\u4e8e\u7070\u8272\u56fe\u7684\u591a\u5c5e\u6027\u51b3\u7b56\u65b9\u6cd5\uff1b\u7ed9\u51fa\u7070\u8272\u56fe\u53ca\u5176\u8fd0\u7b97\u89c4\u7a0b\uff0c\u5e76\u5728\u51b3\u7b56\u5206\u6790\u4e2d\u4f7f\u7528\u6838\u548c\u7070\u5ea6\u5ea6\u6765\u8868\u5f81\u5c5e\u6027\u503c\u4e0e\u5173\u7cfb\uff1b\u901a\u8fc7\u4e00\u4e2a\u6570\u503c\u5b9e\u4f8b\u6f14\u793a\u65b9\u6cd5\u6b65\u9aa4\u3002", "result": "\u7ed9\u51fa\u4e00\u79cd\u7b80\u5316\u7684\u5f62\u5f0f\u3001\u7070\u8272\u56fe\u7684\u64cd\u4f5c\u89c4\u5219\u3001\u53ca\u5bf9\u5907\u9009\u65b9\u6848\u7684\u5206\u6790\u4e0e\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8be5\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u4e0e\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4e3a\u533a\u95f4\u7070\u6570\u7684\u591a\u5c5e\u6027\u51b3\u7b56\u63d0\u4f9b\u53ef\u884c\u7684\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u4e0e\u5e94\u7528\u4ef7\u503c\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u80fd\u5305\u62ec\u6269\u5c55\u3001\u9c81\u68d2\u6027\u5206\u6790\u3001\u5bf9\u6bd4\u7814\u7a76\u7b49\u3002"}}
{"id": "2512.17274", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17274", "abs": "https://arxiv.org/abs/2512.17274", "authors": ["Lin Chen", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Near-Field Position and Orientation Tracking With Hybrid ELAA Architecture", "comment": null, "summary": "This paper investigates near-field (NF) position and orientation tracking of a multi-antenna mobile station (MS) using an extremely large antenna array (ELAA)-equipped base station (BS) with a limited number of radio frequency (RF) chains. Under this hybrid array architecture, the received uplink pilot signal at the BS is first combined by analog phase shifters, producing a low-dimensional observation before digital processing. Such analog compression provides only partial access to the ELAA measurement, making it essential to design an analog combiner that can preserve pose-relevant signal components despite channel uncertainty and unit-modulus hardware constraints. To address this, we propose a predictive analog combining-assisted extended Kalman filter (PAC-EKF) framework, where the analog combiner can leverage the temporal correlation in the MS pose variation to capture the most informative signal components predictively. We then analyze fundamental performance limits via Bayesian Cram\u00e9r-Rao bound and Fisher information matrix, explicitly quantifying how the analog combiner, array size, signal-to-noise ratio, and MS pose influence the pose information contained in the uplink observation. Building on these insights, we develop two methods for designing a low-complexity analog combiner. Numerical results show that the proposed predictive analog combining approach significantly improves tracking accuracy, even with fewer RF chains and lower transmit power.", "AI": {"tldr": "Proposes predictive analog combining with an extended Kalman filter (PAC-EKF) for near-field pose tracking using ELAA at the base station under limited RF chains; analyzes fundamental limits with Bayesian CRB; develops two low-complexity analog combiner designs; shows improved tracking with fewer RF chains and lower transmit power.", "motivation": "Enable accurate near-field position and orientation tracking in ELAA-based systems under hardware constraints (limited RF chains and analog compression) by preserving pose-relevant signal components and leveraging temporal pose correlation.", "method": "Introduce PAC-EKF that uses an analog combiner to form informative low-dimensional observations, model temporal pose dynamics, analyze Fisher information and Bayesian CRB to quantify pose information under unit-modulus analog hardware, and design two low-complexity analog combiners.", "result": "Analytical bounds quantify the impact of analog combiner, array size, SNR, and pose on achievable information; numerical experiments show substantial tracking accuracy gains with predictive analog combining even with few RF chains and reduced transmit power.", "conclusion": "Predictive analog combining with EKF enhances NF pose tracking in ELAA systems under hardware constraints; the proposed designs provide practical, low-complexity solutions that improve information capture and tracking performance."}}
{"id": "2512.16967", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16967", "abs": "https://arxiv.org/abs/2512.16967", "authors": ["Marcelo Cerda Castillo"], "title": "Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes", "comment": "12 pages, 5 tables, 1 figure. Uses publicly available METAR surface observations and TAF forecast data for benchmarking", "summary": "Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.\n  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing", "AI": {"tldr": "\u57fa\u4e8e METAR \u7684\u8f7b\u91cf\u7ea7 XGBoost \u6a21\u578b\u572811\u4e2a\u673a\u573a\u7684\u77ed\u65f6\u5929\u6c14\u9884\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709 TAF\uff0c\u5e76\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u5408\u8fb9\u7f18\u8ba1\u7b97\u3002", "motivation": "\u63d0\u5347\u4f4e\u80fd\u89c1\u5ea6\u4e0e\u964d\u6c34\u7b49\u77ed\u65f6\u9884\u62a5\uff08nowcasting\uff09\u7684\u51c6\u786e\u6027\u4e0e\u65f6\u6548\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u503c\u5929\u6c14\u9884\u62a5\u4e0e TAF \u7684\u8ba1\u7b97\u6210\u672c\u3001\u504f\u597d\u4fdd\u5b88\u6027\u53ca\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ec5\u4ee5 METAR \u4e3a\u8f93\u5165\u7684 XGBoost \u8f7b\u91cf\u6846\u67b6\uff0c\u8f85\u4ee5\u57fa\u4e8e\u70ed\u529b\u5b66\u539f\u7406\u7684\u7269\u7406\u5f15\u5bfc\u7279\u5f81\u5de5\u7a0b\uff1b\u572811\u4e2a\u673a\u573a\u3001\u8986\u76d62000\u20132024\u5e74\u7684\u5386\u53f2\u6570\u636e\u4e0a\u8bc4\u4f30\uff1b\u4e0e\u8fd0\u8425 TAF \u8fdb\u884c\u76f2\u8bc4\u4f30\u6bd4\u8f83\uff1b\u901a\u8fc7 SHAP \u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6a21\u578b\u57283\u5c0f\u65f6\u6218\u672f\u7a97\u53e3\u5185\u5bf9\u4f4e\u80fd\u89c1\u5ea6\u4e0e\u964d\u6c34\u4e8b\u4ef6\u7684\u68c0\u6d4b\u7387\u663e\u8457\u63d0\u5347\uff0c\u53ec\u56de\u7387\u63d0\u5347\u7ea6\u4e3a2.5\u20134.0\u500d\uff0c\u865a\u8b66\u663e\u8457\u4e0b\u964d\uff1bSHAP \u8868\u660e\u6a21\u578b\u5bf9\u5e73\u6d41\u3001\u8f90\u5c04\u4e0e\u8f88\u964d\u7b49\u5c40\u90e8\u7269\u7406\u9a71\u52a8\u8fdb\u884c\u4e86\u9690\u5f0f\u91cd\u5efa\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u3001\u53ef\u89e3\u91ca\u3001\u8de8\u6c14\u5019\u533a\u7684 nowcasting \u65b9\u6848\uff0c\u4ec5\u4f9d\u8d56\u5730\u9762\u89c2\u6d4b\u6570\u636e\uff0c\u5177\u5907\u8fb9\u7f18\u8ba1\u7b97\u53cb\u597d\u6027\u5e76\u80fd\u63d0\u5347\u8fd0\u8425\u6001\u52bf\u611f\u77e5\u3002"}}
{"id": "2512.17045", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17045", "abs": "https://arxiv.org/abs/2512.17045", "authors": ["Alejandro Ranchal-Pedrosa", "Benjamin Marsh", "Lefteris Kokoris-Kogias", "Alberto Sonnino"], "title": "Sedna: Sharding transactions in multiple concurrent proposer blockchains", "comment": null, "summary": "Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).\n  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \\emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.", "AI": {"tldr": "Sedna uses verifiable rateless coding for transaction dissemination in multi-proposer blockchains, enabling private delivery to subsystems of proposers and deterministic decoding that preserves liveness and until-decode privacy, with 2-3x bandwidth efficiency over naive replication and no consensus changes.", "motivation": "Address the practical trilemma in MCP blockchains: censorship resistance, low latency, and cost. Current replication strategies either expose payloads to MEV or suffer from inefficiency and weak censorship guarantees. Sedna aims to reduce MEV exposure and improve efficiency without modifying the consensus protocol.", "method": "Users privately deliver addressed symbol bundles to subsets of proposers using verifiable rateless coding. Once enough symbols are finalized to decode, execution proceeds in a deterministic order. The approach provides liveness and until-decode privacy, with analytics showing near information-theoretic lower bounds on bandwidth; no changes to the underlying consensus are required.", "result": "Sedna achieves a 2-3x improvement in bandwidth efficiency compared to naive replication, approaches the information-theoretic lower bound for bandwidth, and preserves liveness and until-decode privacy. It enables incremental deployment without consensus modifications.", "conclusion": "Sedna offers a practical and deployable solution to the transaction dissemination problem in MCP blockchains, improving efficiency and privacy while maintaining compatibility with existing consensus, thus mitigating the censorship-latency-cost trilemma."}}
{"id": "2512.17283", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17283", "abs": "https://arxiv.org/abs/2512.17283", "authors": ["Lin Chen", "Ahmed Elzanaty", "Mustafa A. Kishk", "Ying-Jun Angela Zhang"], "title": "Near-Field Multi-User Communications via Polar-Domain Beamfocusing: Analytical Framework and Performance Analysis", "comment": null, "summary": "As wireless systems evolve toward higher frequencies and extremely large antenna arrays, near-field (NF) propagation becomes increasingly dominant. Unlike far-field (FF) communication, which relies on a planar-wavefront model and is limited to angular-domain beamsteering, NF propagation exhibits spherical wavefronts that enable beamfocusing in both angle and distance, i.e., the polar domain, offering new opportunities for spatial multiple access. This paper develops an analytical stochastic geometry (SG) framework for a multi-user system assisted by polar-domain beamfocusing, which jointly captures NF propagation characteristics and the spatial randomness of user locations. The intrinsic coupling between angle and distance in the NF antenna pattern renders inter-user interference analysis intractable. To address this challenge, we propose a tractable near-field multi-level antenna pattern (NF-MLAP) approximation, which enables computationally efficient expressions and tight upper bounds for key performance metrics, including coverage probability, spectrum efficiency, and area spectrum efficiency. Analytical and simulation results demonstrate that the proposed framework accurately captures performance trends and reveals fundamental trade-offs between hardware configuration (including the number of antennas and radio frequency chains) and system performance (in terms of spatial resource reuse and interference mitigation).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u573a\u591a\u7ea7\u5929\u7ebf\u6a21\u5f0f\uff08NF-MLAP\uff09\u7684\u8fd1\u573a\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u6781\u57df\u6ce2\u524d\u4e0e\u968f\u673a\u51e0\u4f55\uff0c\u7528\u4ee5\u5206\u6790\u6781\u5927\u5c3a\u5ea6\u9635\u5217\u4e0b\u7684\u8fd1\u573a\u591a\u7528\u6237\u7cfb\u7edf\u3002\u901a\u8fc7\u53ef\u8ba1\u7b97\u7684\u4e0a\u754c\u548c\u8868\u8fbe\u5f0f\uff0c\u5c55\u793a\u8986\u76d6\u3001\u9891\u8c31\u6548\u7387\u548c\u533a\u57df\u9891\u8c31\u6548\u7387\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u5929\u7ebf\u6570\u91cf\u4e0e\u5c04\u9891\u94fe\u8def\u6570\u7b49\u786c\u4ef6\u914d\u7f6e\u4e0e\u7cfb\u7edf\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u9891\u7387\u5347\u9ad8\u548c\u5929\u7ebf\u9635\u5217\u89c4\u6a21\u589e\u5927\uff0c\u8fd1\u573a\u4f20\u64ad\u6210\u4e3a\u4e3b\u5bfc\u7279\u6027\u3002\u8fd1\u573a\u5141\u8bb8\u5728\u89d2\u5ea6\u548c\u8ddd\u79bb\u4e0a\u8fdb\u884c\u6ce2\u675f\u805a\u7126\uff08\u6781\u57df\uff09\uff0c\u5e26\u6765\u65b0\u7684\u7a7a\u95f4\u591a\u5740\u673a\u4f1a\uff0c\u4f46\u89d2-\u8ddd\u79bb\u8026\u5408\u4f7f\u5e72\u6270\u5206\u6790\u590d\u6742\u3002\u9700\u8981\u4e00\u4e2a\u65e2\u80fd\u53cd\u6620\u8fd1\u573a\u4f20\u64ad\u7279\u6027\u53c8\u5177\u5907\u6570\u503c\u53ef\u5206\u6790\u6027\u7684\u6846\u67b6\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u5206\u6790\u6027\u968f\u673a\u51e0\u4f55\u6846\u67b6\uff0c\u7ed3\u5408\u8fd1\u573a\u4f20\u64ad\u7684\u89d2\u2013\u8ddd\u79bb\u8026\u5408\uff1b\u63d0\u51fa\u8fd1\u573a\u591a\u7ea7\u5929\u7ebf\u6a21\u5f0f\uff08NF-MLAP\uff09\u8fd1\u4f3c\u4ee5\u7b80\u5316\u8026\u5408\u5e76\u5f97\u5230\u53ef\u8ba1\u7b97\u7684\u8986\u76d6\u6982\u7387\u3001\u9891\u8c31\u6548\u7387\u548c\u533a\u57df\u9891\u8c31\u6548\u7387\u7684\u8868\u8fbe\u53ca\u4e0a\u754c\uff1b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e0e\u4eff\u771f\u9a8c\u8bc1\u5206\u6790\u6846\u67b6\u7684\u51c6\u786e\u6027\u4e0e\u8d8b\u52bf\u3002", "result": "\u6846\u67b6\u80fd\u591f\u51c6\u786e\u6355\u6349\u6027\u80fd\u8d8b\u52bf\uff0c\u7ed9\u51fa\u786c\u4ef6\u914d\u7f6e\uff08\u5929\u7ebf\u6570\u3001\u5c04\u9891\u94fe\u8def\u6570\uff09\u4e0e\u7cfb\u7edf\u6027\u80fd\uff08\u7a7a\u95f4\u8d44\u6e90\u91cd\u7528\u3001\u5e72\u6270\u6291\u5236\uff09\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u5e76\u7ed9\u51fa\u7d27\u8d34\u5b9e\u9645\u6027\u80fd\u7684\u4e0a\u754c\u3002", "conclusion": "NF-MLAP\u4e0eSG\u6846\u67b6\u4e3a\u8fd1\u573a\u5927\u89c4\u6a21\u5929\u7ebf\u7cfb\u7edf\u7684\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u4e14\u8ba1\u7b97\u53cb\u597d\u7684\u5de5\u5177\uff0c\u4fbf\u4e8e\u786c\u4ef6\u8bbe\u8ba1\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u5f3a\u8c03\u8fd1\u573a\u6548\u5e94\u5728\u591a\u7528\u6237\u7a7a\u95f4\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.17286", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17286", "abs": "https://arxiv.org/abs/2512.17286", "authors": ["Lizhou Liu", "Xiaohui Chen", "Wenyi Zhang"], "title": "OpenPathNet: An Open-Source RF Multipath Data Generator for AI-Driven Wireless Systems", "comment": "AI, dataset, environment-aware communication, multipath propagation, radio frequency (RF) map, ray tracing", "summary": "The convergence of artificial intelligence (AI) and sixth-generation (6G) wireless technologies is driving an urgent need for large-scale, high-fidelity, and reproducible radio frequency (RF) datasets. Existing resources, such as CKMImageNet, primarily provide preprocessed and image-based channel representations, which conceal the fine-grained physical characteristics of signal propagation that are essential for effective AI modeling. To bridge this gap, we present OpenPathNet, an open-source RF multipath data generator accompanied by a publicly released dataset for AI-driven wireless research. Distinct from prior datasets, OpenPathNet offers disaggregated and physically consistent multipath parameters, including per-path gain, time of arrival (ToA), and spatial angles, derived from high-precision ray tracing simulations constructed on real-world environment maps. By adopting a modular, parameterized pipeline, OpenPathNet enables reproducible generation of multipath data and can be readily extended to new environments and configurations, improving scalability and transparency. The released generator and accompanying dataset provide an extensible testbed that holds promise for advancing studies on channel modeling, beam prediction, environment-aware communication, and integrated sensing in AI-enabled 6G systems. The source code and dataset are publicly available at https://github.com/liu-lz/OpenPathNet.", "AI": {"tldr": "OpenPathNet: \u4e00\u4e2a\u5f00\u6e90\u7684\u5c04\u9891\u591a\u5f84\u6570\u636e\u751f\u6210\u5668\u53ca\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u9010\u8def\u5f84\u53c2\u6570\uff08\u589e\u76ca\u3001\u5230\u8fbe\u65f6\u95f4\u3001\u7a7a\u95f4\u89d2\u5ea6\uff09\u53ca\u9ad8\u7cbe\u5ea6\u5c04\u7ebf\u8ffd\u8e2a\uff0c\u652f\u6301\u518d\u73b0\u6027\u3001\u53ef\u6269\u5c55\u6027\uff0c\u4fc3\u8fdbAI\u9a71\u52a8\u76846G\u65e0\u7ebf\u7814\u7a76\u3002", "motivation": "\u5728AI\u4e0e6G\u65e0\u7ebf\u878d\u5408\u80cc\u666f\u4e0b\uff0c\u8feb\u5207\u9700\u8981\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u3001\u53ef\u91cd\u590d\u7684\u5c04\u9891\u6570\u636e\u96c6\u3002\u73b0\u6709\u8d44\u6e90\uff08\u5982CKMImageNet\uff09\u591a\u4e3a\u9884\u5904\u7406\u7684\u56fe\u50cf\u5316\u4fe1\u9053\u8868\u793a\uff0c\u672a\u80fd\u4fdd\u7559\u4fe1\u53f7\u4f20\u64ad\u7684\u7269\u7406\u7ec6\u8282\uff0c\u5236\u7ea6AI\u5efa\u6a21\u4e0e\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u3001\u53c2\u6570\u5316\u7684\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8e\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u5730\u56fe\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u5c04\u7ebf\u8ffd\u8e2a\uff0c\u5f97\u5230\u9010\u8def\u5f84\u7684\u589e\u76ca\u3001\u5230\u8fbe\u65f6\u95f4\u548c\u7a7a\u95f4\u89d2\u7b49\u7269\u7406\u4e00\u81f4\u7684\u591a\u5f84\u53c2\u6570\uff1b\u53ef\u6269\u5c55\u81f3\u65b0\u7684\u73af\u5883\u4e0e\u914d\u7f6e\uff0c\u5177\u5907\u53ef\u91cd\u590d\u6027\u3002", "result": "\u63d0\u4f9b\u5f00\u6e90\u7684\u6570\u636e\u751f\u6210\u5668\u4e0e\u6570\u636e\u96c6\uff0c\u6784\u6210\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\uff1b\u6709\u671b\u5728\u4fe1\u9053\u5efa\u6a21\u3001\u6ce2\u675f\u9884\u6d4b\u3001\u73af\u5883\u611f\u77e5\u901a\u4fe1\u548cAI\u9a71\u52a8\u76846G\u7cfb\u7edf\u7684\u611f\u77e5\u4e0e\u7efc\u5408\u5e94\u7528\u65b9\u9762\u63a8\u8fdb\u7814\u7a76\u3002", "conclusion": "OpenPathNet\u901a\u8fc7\u63d0\u4f9b\u7269\u7406\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u591a\u5f84\u6570\u636e\uff0c\u63a8\u52a8AI\u9a71\u52a8\u65e0\u7ebf\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u5e76\u4fc3\u8fdb\u5b9e\u9a8c\u7684\u53ef\u91cd\u590d\u6027\u4e0e\u53ef\u6bd4\u6027\u3002"}}
{"id": "2512.17034", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17034", "abs": "https://arxiv.org/abs/2512.17034", "authors": ["Chang-Hwan Lee", "Chanseung Lee"], "title": "GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning", "comment": "23 pages. Submitted to Machine Learning", "summary": "Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \\emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.", "AI": {"tldr": "Gradient-Boosted Deep Q-Networks (GB-DQN): an adaptive additive ensemble that incrementally learns residuals to counteract non-stationarity, with theoretical convergence guarantees and empirical robustness improvements over DQN.", "motivation": "Non-stationary environments cause model drift and catastrophic forgetting in deep reinforcement learning. A principled, online adaptation mechanism is needed to maintain performance when dynamics or rewards change.", "method": "GB-DQN constructs an additive ensemble of Q-networks. Each new learner is trained to approximate the Bellman residual of the current ensemble after observed drift, implementing gradient-boosted residual learning in reinforcement learning. Theoretical results show monotonic reduction of the empirical Bellman residual and convergence to the post-drift optimal value function under standard assumptions.", "result": "The approach provably reduces the Bellman residual at each boosting step and converges to the post-drift optimal value function. Empirical experiments across diverse control tasks with controlled dynamics shifts demonstrate faster recovery, improved stability, and greater robustness relative to DQN and common non-stationary baselines.", "conclusion": "GB-DQN provides a principled, scalable method for adapting to non-stationarity in RL by incremental residual learning, yielding increased robustness and quicker recovery after dynamics changes."}}
{"id": "2512.17518", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17518", "abs": "https://arxiv.org/abs/2512.17518", "authors": ["Bar\u0131\u015f Kavas", "Efe C. Balta", "Lars Witte", "Michael R. Tucker", "John Lygeros", "Markus Bambach"], "title": "Layer-to-layer Closed-loop Switched Heating and Cooling Control of the Laser Powder Bed Fusion Process", "comment": null, "summary": "This study investigates the stabilization of interlayer temperature in the laser powder bed fusion process through a novel switched layer-to-layer closed-loop feedback controller. The controller architecture aims to measure the interlayer temperature by a laterally positioned thermal camera and maintain a preset reference temperature by switching between the heating mode through dynamic laser power adjustment and the cooling mode by assigning interlayer dwell time to allow cooling between layers. The switching controller employs a feedback optimization control algorithm for the heating mode to adjust the laser power, and a triggering algorithm that increases the interlayer dwell time until the interlayer temperature reaches the reference value. Additionally, the study compares the performance of the proposed controller in both supported and unsupported overhanging parts to evaluate the effect of support structures on the controller performance as well as the thermal behavior of overhanging parts. Results demonstrate the controller's effectiveness in stabilizing interlayer temperature across varying cross-sectional areas while remaining within the material's stable processing zone. In the heating mode, the controller efficiently stabilizes temperature, even in geometries with significant cross-section variation. The study also identifies trade-offs among process efficiency, energy consumption, and build time. Supported parts exhibit reduced overheating but consume more energy and material, while unsupported parts stabilize interlayer temperature faster but with longer build times due to increased dwell time assignments. The research highlights notable improvements in interlayer temperature control for geometries prone to excessive thermal stresses. Moreover, the introduction of interlayer dwell time offers a practical solution to maintaining thermal stability in complex geometries.", "AI": {"tldr": "A switched layer-to-layer closed-loop feedback controller stabilizes interlayer temperature in laser powder bed fusion using a lateral thermal camera, switching between heating via laser power and cooling via interlayer dwell time, with evaluation on supported vs. unsupported overhangs and trade-offs between efficiency, energy, and build time.", "motivation": "Thermal management is critical in laser powder bed fusion to prevent overheating and thermal stresses; maintaining a stable interlayer temperature across varying geometries is essential for part quality, especially in complex or overhanging features.", "method": "A novel switched layer-to-layer closed-loop controller measures interlayer temperature with a lateral thermal camera, and maintains a reference temperature by: (1) heating mode using a feedback optimization algorithm to adjust laser power; (2) cooling mode by increasing interlayer dwell time to allow cooling until the target is reached. The controller is evaluated on both supported and unsupported overhangs to assess the influence of support structures on thermal behavior and controller performance.", "result": "The controller stabilizes interlayer temperature across varying cross-sectional areas within the material's stable processing zone. Heating mode achieves temperature stabilization even with significant cross-section variation. Trade-offs identified among process efficiency, energy consumption, and build time: supported parts reduce overheating but use more energy/material; unsupported parts stabilize faster but require longer dwell times and build times. The approach improves thermal stability for geometries prone to excessive thermal stresses and introduces interlayer dwell time as a practical tool for complex geometries.", "conclusion": "Interlayer dwell time coupled with a switched heating/cooling controller provides effective thermal stabilization in L-PBF across diverse geometries, with notable improvements for high-thermal-stress regions. However, users must consider trade-offs in energy use and build time, and the presence of supports alters thermal dynamics and controller efficiency."}}
{"id": "2512.17251", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17251", "abs": "https://arxiv.org/abs/2512.17251", "authors": ["Madhava Gaikwad"], "title": "AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: LOCK-LLM Work-shop, NeurIPS 2025", "summary": "Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.", "AI": {"tldr": "AlignDP: a two-tier privacy mechanism that blocks knowledge transfer at the data interface by separating rare and non-rare fields; rare fields use PAC indistinguishability for zero-epsilon local DP, non-rare fields use RAPPOR for unbiased local DP estimates; a global aggregator enforces composition and budget; theoretical bounds and a toy simulation show rare events are hidden while frequent categories are recoverable with small error.", "motivation": "Mitigate pre-leak privacy risks where language models can extract, distill, or be fine-tuned using leaked data. Existing defenses act post-leak; AlignDP aims to neutralize transfer preemptively at the data interface.", "method": "Partition data into rare and non-rare fields. Apply PAC indistinguishability shielding to rare fields (effectively zero-epsilon local DP) and privatize non-rare fields with RAPPOR (unbiased frequency estimates under local DP). Use a global aggregator to enforce privacy budget and composition across fields. The paper analyzes limits of extending PAC to global aggregation, derives bounds for RAPPOR estimates, and conducts a toy simulation to validate feasibility.", "result": "The work provides theoretical bounds for both components and demonstrates in a toy simulation that rare categories remain hidden while frequent categories can be recovered with small error.", "conclusion": "A two-tier AlignDP design can suppress leakage by protecting rare events and perturbing frequent events, with a global budget mechanism; results show potential utility preservation for common categories while maintaining strong protection for rare ones, though there are theoretical and practical trade-offs to explore."}}
{"id": "2512.17051", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17051", "abs": "https://arxiv.org/abs/2512.17051", "authors": ["Haoye Lu", "Yaoliang Yu", "Darren Ho"], "title": "SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples", "comment": null, "summary": "In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6709\u9ed1\u7bb1\u8150\u8680\u751f\u6210\u5668\u7684\u566a\u58f0\u89c2\u6d4b\u4e0b\u8fdb\u884c\u5206\u5e03\u6062\u590d\u7684\u65b9\u6cd5\u3002\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5355\u8fb9\u4fe1\u606f\u71b5\u6700\u4f18\u4f20\u8f93\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a EM \u98ce\u683c\u7b97\u6cd5\u6c42\u89e3\uff1b\u63d0\u51fa\u53ef\u5224\u5b9a\u5728\u4fe1\u606f\u4e22\u5931\u4e0b\u662f\u5426\u53ef\u6062\u590d\u7684\u6d4b\u8bd5\u51c6\u5219\uff1b\u5728\u5c11\u91cf\u5e72\u51c0\u6837\u672c\u5b58\u5728\u65f6\uff0c\u8bf4\u660e\u5206\u5e03\u4ecd\u53ef\u6062\u590d\u7684\u60c5\u5f62\uff1b\u5e76\u5f15\u5165 SFBD-OMNI \u6846\u67b6\uff0c\u5c06\u8150\u8680\u6837\u672c\u5206\u5e03\u6620\u5c04\u5230\u771f\u5b9e\u5206\u5e03\uff1b\u63a8\u5e7f SFBD \u81f3\u4efb\u610f\u6d4b\u91cf\u6a21\u578b\uff1b\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u83b7\u53d6\u5b8c\u5168\u89c2\u6d4b\u6837\u672c\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5f80\u5f80\u5b58\u5728\u8150\u8680/\u566a\u58f0\u89c2\u6d4b\uff1b\u9700\u8981\u5728\u9ed1\u7bb1\u8150\u8680\u6761\u4ef6\u4e0b\u91cd\u5efa\u771f\u5b9e\u5206\u5e03\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u548c\u53ef\u6062\u590d\u6027\u3002", "method": "\u628a\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u8fb9\u71b5\u6700\u4f18\u4f20\u8f93\uff0c\u4f7f\u7528 EM \u98ce\u683c\u7b97\u6cd5\u6c42\u89e3\uff1b\u63d0\u51fa\u5224\u65ad\u662f\u5426\u53ef recover \u7684\u7406\u8bba\u51c6\u5219\uff1b\u7ed9\u51fa\u5728\u4fe1\u606f\u635f\u5931\u4e0b\u7684\u53ef\u6062\u590d\u6027\u5206\u6790\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa SFBD-OMNI \u6865\u63a5\u6846\u67b6\uff0c\u5c06\u8150\u8680\u6837\u672c\u5206\u5e03\u6620\u5c04\u5230\u771f\u5b9e\u5206\u5e03\uff0c\u63a8\u5e7f SFBD \u81f3\u4efb\u610f\u6d4b\u91cf\u6a21\u578b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u6d4b\u91cf\u8bbe\u5b9a\u4e0b\uff0c\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u90fd\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u5206\u5e03\u6062\u590d\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u6d4b\u91cf\u6a21\u578b\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5177\u6709\u7406\u8bba\u5224\u5b9a\u53ef\u6062\u590d\u6027\u7684\u5de5\u5177\uff1b\u672a\u6765\u53ef\u7ed3\u5408\u66f4\u591a\u6d4b\u91cf\u6a21\u578b\u548c\u66f4\u5927\u89c4\u6a21\u6570\u636e\u3002"}}
{"id": "2512.17254", "categories": ["cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17254", "abs": "https://arxiv.org/abs/2512.17254", "authors": ["Baolei Zhang", "Minghong Fang", "Zhuqing Liu", "Biao Yi", "Peizhao Zhou", "Yuan Wang", "Tong Li", "Zheli Liu"], "title": "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning", "comment": "Accepted for publication in IEEE Transactions on Information Forensics and Security", "summary": "Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.", "AI": {"tldr": "ABBR is a practical framework for Byzantine-robust and privacy-preserving federated learning that uses dimensionality reduction to accelerate private filtering, analyzes accuracy loss in low-dimensional vector-wise filtering, and includes an adaptive tuning strategy. It achieves faster runtimes and lower communication overhead while maintaining comparable Byzantine resilience to state-of-the-art baselines.", "motivation": "Existing defenses against backdoor and privacy inference attacks incur high computational and communication overhead, creating a gap between theory and practice; a practical, efficient solution is needed.", "method": "Introduce ABBR framework; apply dimensionality reduction to speed up private computation of complex filtering rules; analyze accuracy loss of vector-wise filtering in low-dimensional space; propose adaptive tuning strategy to mitigate impact of bypassed filtering; implement with state-of-the-art Byzantine-robust aggregation rules; evaluate on public datasets.", "result": "ABBR runs significantly faster with minimal communication overhead and retains nearly the same Byzantine resilience as baseline methods.", "conclusion": "ABBR provides a practical, efficient solution that balances privacy and robustness in federated learning, narrowing the gap between theory and practice."}}
{"id": "2512.17803", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17803", "abs": "https://arxiv.org/abs/2512.17803", "authors": ["Gerard Marias Gonzalez", "Alejandro Pena-Bello", "J\u00e9r\u00e9my Dumoulin", "Nicolas Wyrsch"], "title": "Techno-Economic Case Study of a Rural Local Electricity Community in Switzerland", "comment": "33 pages and 12 figures", "summary": "Local Electricity Communities (communaut\u00e9s \u00e9lectriques locales, CEL) will become operational in Switzerland in 2026, allowing prosumers, consumers, and storage operators within the same municipality and distribution system operator (DSO) area to exchange electricity over the public grid with reduced distribution tariffs. This report examines a rural Swiss case study to explore the techno-economic implications of CELs for both participants and the local DSO. The findings indicate that CELs can enhance the local use of renewable generation, particularly photovoltaics, and offer modest financial gains, with outcomes strongly shaped by community size, composition, and tariff design. Larger and more heterogeneous communities achieve better internal matching of supply and demand, though the overall incentive remains limited because the tariff reduction applies only to distribution charges. The study further shows that internal energy exchange is maximized when local PV generation covers roughly 1-2 times the community load. For DSOs, CELs reduce grid imports (27-46%), resulting in a substantial reduction in distribution tariff revenues (17-36%), necessitating regulatory adaptation. While centralized batteries provide economic value to members, their technical impact on the grid remains modest due to their small, economically optimized capacity. Larger centralized storage is shown to reduce transformer peak power, but risks increasing line loading, suggesting a need for careful sizing and placement.", "AI": {"tldr": "\u745e\u58eb\u519c\u6751\u5730\u533a\u7684\u672c\u5730\u7535\u529b\u793e\u533a\uff08CEL\uff09\u5c06\u4e8e2026\u5e74\u6295\u5165\u8fd0\u884c\uff0c\u7814\u7a76\u8bc4\u4f30\u5176\u5bf9\u53c2\u4e0e\u8005\u4e0e\u672c\u5730\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd0\u8425\u5546\uff08DSO\uff09\u7684\u6280\u672f-\u7ecf\u6d4e\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0cCEL\u53ef\u63d0\u5347\u672c\u5730\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\uff08\u7279\u522b\u662f\u5149\u4f0f\uff09\u4e14\u5e26\u6765\u6709\u9650\u7684\u8d22\u653f\u6536\u76ca\uff0c\u6548\u679c\u9ad8\u5ea6\u53d7\u793e\u533a\u89c4\u6a21\u3001\u6784\u6210\u4e0e\u5173\u7a0e\u8bbe\u8ba1\u5f71\u54cd\u3002\u5185\u90e8\u4ea4\u6613\u5728\u5149\u4f0f\u4ea7\u51fa\u7ea6\u4e3a\u793e\u533a\u8d1f\u83771\u20132\u500d\u65f6\u6700\u4f18\u5316\uff1b\u5bf9DSO\u800c\u8a00\uff0cCELs\u5c06\u964d\u4f4e\u7f51\u5185\u8fdb\u53e327\u201346%\uff0c\u5bfc\u81f4\u5206\u5e03\u5f0f\u7535\u4ef7\u6536\u5165\u4e0b\u964d\u7ea617\u201336%\uff0c\u9700\u76d1\u7ba1\u8c03\u6574\u3002\u96c6\u4e2d\u5f0f\u50a8\u80fd\u5bf9\u6210\u5458\u6709\u7ecf\u6d4e\u4ef7\u503c\u4f46\u5bf9\u7535\u7f51\u7684\u6280\u672f\u5f71\u54cd\u6709\u9650\uff1b\u82e5\u5bb9\u91cf\u8f83\u5927\u5219\u53ef\u964d\u4f4e\u53d8\u538b\u5668\u5cf0\u503c\uff0c\u4f46\u53ef\u80fd\u589e\u52a0\u7ebf\u8def\u8f7d\u8377\uff0c\u9700\u8c28\u614e\u9009\u578b\u4e0e\u5e03\u5c40\u3002", "motivation": "\u8bc4\u4f30\u672c\u5730\u7535\u529b\u793e\u533a\uff08CEL\uff09\u5728\u745e\u58eb\u7684\u7ecf\u6d4e\u4e0e\u6280\u672f\u5f71\u54cd\uff0c\u4ee5\u53ca\u5bf9\u53c2\u4e0e\u8005\u3001DGOs\u4e0e\u76d1\u7ba1\u6846\u67b6\u7684\u542b\u4e49\uff0c\u4ee5\u8f85\u52a9\u653f\u7b56\u5236\u5b9a\u4e0e\u6295\u8d44\u51b3\u7b56\u3002", "method": "\u4ee5\u745e\u58eb\u519c\u6751\u6848\u4f8b\u4e3a\u57fa\u7840\uff0c\u5206\u6790CEL\u5185\u6210\u5458\u4e4b\u95f4\u7684\u7535\u529b\u4ea4\u6362\u3001tariff\u8bbe\u8ba1\u3001\u4ee5\u53ca\u96c6\u4e2d/\u5206\u5e03\u5f0f\u50a8\u80fd\u5bf9\u4ea4\u6613\u7ecf\u6d4e\u6027\u4e0e\u7535\u7f51\u8d1f\u8377\u7684\u5f71\u54cd\uff0c\u91c7\u7528\u5b9a\u91cf\u6a21\u578b\u8bc4\u4f30\u6210\u672c\u3001\u6536\u76ca\u53ca\u7f51\u8d1f\u8377\u53d8\u5316\uff0c\u5e76\u63a2\u8ba8\u5bf9DSO\u6536\u5165\u4e0e\u76d1\u7ba1\u7684\u542b\u4e49\u3002", "result": "CEL\u63d0\u5347\u672c\u5730\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\uff0c\u5c24\u5176\u662f\u5149\u4f0f\uff0c\u4e14\u5e26\u6765\u6709\u9650\u7684\u8d22\u653f\u6536\u76ca\uff0c\u4e14\u9ad8\u5ea6\u53d7\u793e\u533a\u89c4\u6a21\u3001\u6784\u6210\u548c\u5173\u7a0e\u8bbe\u8ba1\u5f71\u54cd\u3002\u5185\u90e8\u4ea4\u6613\u7684\u6700\u5927\u5316\u51fa\u73b0\u5728\u672c\u5730PV\u4ea7\u51fa\u7ea6\u7b49\u4e8e1\u20132\u500d\u793e\u533a\u8d1f\u8377\u65f6\u3002\u5bf9DSO\u800c\u8a00\uff0cCELs\u4f7f\u8fdb\u53e3\u7535\u91cf\u51cf\u5c11\u7ea627\u201346%\uff0c\u4ece\u800c\u5206\u5e03\u5f0f\u5173\u7a0e\u6536\u5165\u4e0b\u964d\u7ea617\u201336%\uff0c\u9700\u8981\u76d1\u7ba1\u9002\u5e94\u3002\u96c6\u4e2d\u5f0f\u50a8\u80fd\u4e3a\u6210\u5458\u5e26\u6765\u7ecf\u6d4e\u4ef7\u503c\uff0c\u4f46\u5bf9\u7535\u7f51\u7684\u6280\u672f\u5f71\u54cd\u6709\u9650\uff1b\u82e5\u5bb9\u91cf\u8f83\u5927\u53ef\u964d\u4f4e\u53d8\u538b\u5668\u5cf0\u503c\uff0c\u4f46\u53ef\u80fd\u589e\u52a0\u7ebf\u8def\u8f7d\u8377\uff0c\u9700\u8c28\u614e\u5bb9\u91cf\u914d\u7f6e\u4e0e\u5e03\u5c40\u3002", "conclusion": "\u9700\u76d1\u7ba1\u5c42\u9762\u505a\u51fa\u8c03\u6574\u4ee5\u5e94\u5bf9DSO\u6536\u5165\u4e0b\u964d\u7b49\u8d22\u653f\u5f71\u54cd\uff1bCELs\u53ef\u4fc3\u8fdb\u672c\u5730\u80fd\u6e90\u81ea\u7ed9\uff0c\u4f46\u5b9e\u9645\u6548\u5e94\u53d6\u51b3\u4e8e\u8bbe\u8ba1\u548c\u89c4\u6a21\uff1b\u50a8\u80fd\u7b56\u7565\u9700\u4e0e\u5730\u533a\u7f51\u8def\u89c4\u6a21\u5339\u914d\uff0c\u907f\u514d\u9020\u6210\u65b0\u7684\u8d1f\u8377\u9519\u914d\u3002"}}
{"id": "2512.17295", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.17295", "abs": "https://arxiv.org/abs/2512.17295", "authors": ["Rayne Holland"], "title": "An Iconic Heavy Hitter Algorithm Made Private", "comment": "17 pages, 30 figures, 1 table", "summary": "Identifying heavy hitters in data streams is a fundamental problem with widespread applications in modern analytics systems. These streams are often derived from sensitive user activity, making update-level privacy guarantees necessary. While recent work has adapted the classical heavy hitter algorithm Misra-Gries to satisfy differential privacy in the streaming model, the privatization of other heavy hitter algorithms with better empirical utility is absent.\n  Under this observation, we present the first differentially private variant of the SpaceSaving algorithm, which, in the non-private setting, is regarded as the state-of-the-art in practice. Our construction post-processes a non-private SpaceSaving summary by injecting asymptotically optimal noise and applying a carefully calibrated selection rule that suppresses unstable labels. This yields strong privacy guarantees while preserving the empirical advantages of SpaceSaving.\n  Second, we introduce a generic method for extracting heavy hitters from any differentially private frequency oracle in the data stream model. The method requires only O(k) additional memory, where k is the number of heavy items, and provides a mechanism for safely releasing item identities from noisy frequency estimates. This yields an efficient, plug-and-play approach for private heavy hitter recovery from linear sketches.\n  Finally, we conduct an experimental evaluation on synthetic and real-world datasets. Across a wide range of privacy parameters and space budgets, our method provides superior utility to the existing differentially private Misra-Gries algorithm. Our results demonstrate that the empirical superiority of SpaceSaving survives privatization and that efficient, practical heavy hitter identification is achievable under strong differential privacy guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5dee\u5206\u9690\u79c1\u7248\u672c\u7684 SpaceSaving \u7b97\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u4ece\u4efb\u4f55\u5dee\u5206\u9690\u79c1\u9891\u7387 Oracle \u4e2d\u63d0\u53d6\u91cd\u5934\u9879\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u7ed9\u51fa\u5b9e\u9a8c\u8bc1\u660e\u5728\u4e25\u683c\u9690\u79c1\u4e0b\u4ecd\u5177\u5907\u4f18\u8d8a\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u6570\u636e\u6d41\u573a\u666f\u4e2d\uff0c\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u66f4\u65b0\u7ea7\u9690\u79c1\u4fdd\u62a4\uff08update-level DP\uff09\u65e5\u76ca\u91cd\u8981\uff1b\u73b0\u6709\u5de5\u4f5c\u591a\u5c06 Misra-Gries \u6539\u9020\u4ee5\u5b9e\u73b0 DP\uff0c\u4f46\u5bf9\u6027\u80fd\u66f4\u597d\u7684 SpaceSaving \u7684\u79c1\u6709\u5316\u7814\u7a76\u5c1a\u7f3a\u4e4f\uff0c\u4e14\u7f3a\u5c11\u901a\u7528\u7684\u79c1\u6709\u91cd\u5934\u9879\u63d0\u53d6\u673a\u5236\u3002", "method": "\u5bf9\u975e\u79c1\u6709 SpaceSaving \u6458\u8981\u8fdb\u884c\u540e\u5904\u7406\uff0c\u901a\u8fc7\u6ce8\u5165\u6e10\u8fd1\u6700\u4f18\u7684\u566a\u58f0\u5e76\u5e94\u7528\u7a33\u5065\u7684\u6807\u7b7e\u6291\u5236\u9009\u62e9\u89c4\u5219\u6765\u5b9e\u73b0\u79c1\u6709\u5316\uff1b\u540c\u65f6\u63d0\u51fa\u4e00\u4e2a\u4ece\u4efb\u610f DP \u9891\u7387\u9884\u8a00\u5668\u4e2d\u63d0\u53d6\u91cd\u5934\u9879\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u9700\u989d\u5916\u4ec5 O(k) \u5185\u5b58\uff0c\u5e76\u5b89\u5168\u5730\u4ece\u5e26\u566a\u7684\u9891\u7387\u4f30\u8ba1\u4e2d\u91ca\u653e\u9879\u76ee\u8eab\u4efd\u3002", "result": "\u5728\u591a\u7ec4\u9690\u79c1\u53c2\u6570\u548c\u7a7a\u95f4\u9884\u7b97\u4e0b\uff0c\u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u7684\u79c1\u6709 Misra-Gries \u7b97\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u5b9e\u9a8c\u8868\u660e SpaceSaving \u7684\u7ecf\u9a8c\u4f18\u52bf\u5728\u79c1\u6709\u5316\u540e\u4ecd\u7136\u5b58\u5728\uff0c\u4e14\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u91cd\u5934\u9879\u8bc6\u522b\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5728\u5f3a DP \u4fdd\u8bc1\u4e0b\u7684\u5b9e\u7528\u91cd\u5934\u9879\u8bc6\u522b\u65b9\u6cd5\uff1a\u901a\u8fc7\u5bf9 SpaceSaving \u79c1\u6709\u5316\u4fdd\u6301\u5176\u5b9e\u7528\u6027\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u53ef\u63d2\u62d4\u5f0f\u7684\u79c1\u6709\u91cd\u5934\u9879\u56de\u6536\u6846\u67b6\uff0c\u4fbf\u4e8e\u5728\u7ebf\u6027\u8349\u56fe\u4e0a\u8fdb\u884c\u79c1\u6709\u5316\u7684\u91cd\u5934\u9879\u63d0\u53d6\u3002"}}
{"id": "2512.17423", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17423", "abs": "https://arxiv.org/abs/2512.17423", "authors": ["Shunbo Jia", "Caizhi Liao"], "title": "SCAR: Semantic Cardiac Adversarial Representation via Spatiotemporal Manifold Optimization in ECG", "comment": "13 pages, 5 figures", "summary": "Deep learning models for Electrocardiogram (ECG) analysis have achieved expert-level performance but remain vulnerable to adversarial attacks. However, applying Universal Adversarial Perturbations (UAP) to ECG signals presents a unique challenge: standard imperceptible noise constraints (e.g., 10 uV) fail to generate effective universal attacks due to the high inter-subject variability of cardiac waveforms. Furthermore, traditional \"invisible\" attacks are easily dismissed by clinicians as technical artifacts, failing to compromise the human-in-the-loop diagnostic pipeline. In this study, we propose SCAR (Semantic Cardiac Adversarial Representation), a novel UAP framework tailored to bypass the clinical \"Human Firewall.\" Unlike traditional approaches, SCAR integrates spatiotemporal smoothing (W=25, approx. 50ms), spectral consistency (<15 Hz), and anatomical amplitude constraints (<0.2 mV) directly into the gradient optimization manifold.\n  Results: We benchmarked SCAR against a rigorous baseline (Standard Universal DeepFool with post-hoc physiological filtering). While the baseline suffers a performance collapse (~16% success rate on transfer tasks), SCAR maintains robust transferability (58.09% on ResNet) and achieves 82.46% success on the source model. Crucially, clinical analysis reveals an emergent targeted behavior: SCAR specifically converges to forging Myocardial Infarction features (90.2% misdiagnosis) by mathematically reconstructing pathological ST-segment elevations. Finally, we demonstrate that SCAR serves a dual purpose: it not only functions as a robust data augmentation strategy for Hybrid Adversarial Training, offering optimal clinical defense, but also provides effective educational samples for training clinicians to recognize low-cost, AI-targeted semantic forgeries.", "AI": {"tldr": "SCAR is a novel universal adversarial perturbation (UAP) framework for ECG analysis that incorporates clinical realism via spatiotemporal smoothing (W=25 ~50ms), spectral consistency (<15 Hz), and anatomical amplitude constraints (<0.2 mV). It achieves strong transferability (58.09% on ResNet) and high source-model success (82.46%), and reveals an emergent targeted misdiagnosis by reconstructing ST-segment elevations. It also serves as a dual-use tool for data augmentation in adversarial training and as educational material for clinicians to recognize AI-targeted semantic forgeries.", "motivation": "Deep ECG models are vulnerable to adversarial perturbations. Standard imperceptible noise constraints fail to generate effective universal attacks due to high inter-subject variability and lack clinical plausibility. There is a need for attacks that are both effective and believable to clinicians to assess human-in-the-loop defense.", "method": "SCAR integrates spatiotemporal smoothing (W=25, ~50 ms), spectral consistency (<15 Hz), and anatomical amplitude constraints (<0.2 mV) directly into the gradient optimization. It is compared against a Standard Universal DeepFool with post-hoc physiological filtering.", "result": "SCAR attains 82.46% success on the source model and 58.09% transferability on ResNet. Baseline universal DeepFool with filtering collapses to ~16% on transfer tasks. Clinically, SCAR converges to forging Myocardial Infarction features with 90.2% misdiagnosis by reconstructing ST-segment elevations. Additionally, SCAR is proposed as a data augmentation method for Hybrid Adversarial Training and as educational samples for clinicians.", "conclusion": "SCAR demonstrates that physiologically constrained UAPs can robustly attack ECG classifiers and reveal clinically plausible adversarial features. It offers dual use for defense via robust training and for clinician education, but raises concerns about clinical safety and the need for robust, detection-aware defenses."}}
{"id": "2512.17058", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17058", "abs": "https://arxiv.org/abs/2512.17058", "authors": ["Vladimir G. Pestov"], "title": "Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III", "comment": "12 pages, latex with ESAIM P&S macros", "summary": "We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:\n  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.\n  The equivalence (2)$\\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\\Rightarrow$(1) was established by C\u00e9rou and Guyader (2006). We prove the implication (1)$\\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).", "AI": {"tldr": "\u5728\u5b8c\u6574\u53ef\u5206\u7684\u5ea6\u91cf\u7a7a\u95f4 X \u4e0a\uff0ck\u6700\u8fd1\u90bb\u5206\u7c7b\u5668\u7684\uff08\u5f31\uff09\u666e\u904d\u4e00\u81f4\u6027\u7b49\u4ef7\u4e8e\u5bf9\u4efb\u610f\u5c40\u90e8\u6709\u9650\u6d4b\u5ea6\u7684\u5f3a Lebesgue\u2013Besicovitch \u5fae\u5206\u6027\u8d28\uff0c\u4ee5\u53ca X \u7684 Nagata \u7ef4\u5ea6\u7684 sigma-\u6709\u9650\u6027\uff1b\u672c\u6587\u8bc1\u660e\u4ece (1) \u63a8\u51fa (3)\uff0c\u5b8c\u6210\u4e09\u8005\u7684\u7b49\u4ef7\u6027\u5e76\u7ea0\u6b63\u7cfb\u5217\u8bba\u6587\u4e2d\u7684\u9519\u8bef\u9648\u8ff0\u3002", "motivation": "\u5c06\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u4e0e\u51e0\u4f55\u6d4b\u5ea6\u6027\u8d28\u76f8\u8054\u7cfb\uff0c\u7ed9\u51fa\u5728\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u5bf9 k-NN \u5168\u5c40\u4e00\u81f4\u6027\u7684\u5b8c\u6574\u523b\u753b\uff0c\u56de\u5e94\u7cfb\u5217\u8bba\u6587\u7684\u731c\u60f3\u3002", "method": "\u57fa\u4e8e\u5df2\u77e5 (2)\u21d4(3) \u4e0e (2)\u21d2(1) \u7684\u7ed3\u679c\uff0c\u53d1\u5c55\u65b0\u7684\u5206\u6790\u4e0e\u6784\u9020\u6027\u8bc1\u660e\uff0c\u63a8\u5bfc (1)\u21d2(3)\uff0c\u5e76\u7ea0\u6b63\u6b64\u524d\u8bba\u6587\u4e2d\u7684\u9519\u8bef\u65ad\u8a00\uff0c\u6574\u5408\u5dee\u5206\u6027\u8d28\u4e0e\u51e0\u4f55\u7ef4\u5ea6\u6761\u4ef6\u7684\u8bc1\u660e\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86 (1)\u2192(3) \u7684\u8574\u542b\u4ee5\u53ca\u5728\u5df2\u77e5\u7ed3\u8bba\u4e0b\u4e09\u8005\u7684\u7b49\u4ef7\u6027\uff1b\u5b8c\u6210\u5bf9\u8be5\u7cfb\u5217\u7814\u7a76\u4e2d\u672a\u51b3\u547d\u9898\u7684\u89e3\u51b3\u3002", "conclusion": "\u4f7f\u4e09\u8005\u6761\u4ef6\u5728\u9002\u7528\u7684\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u5b8c\u5168\u7b49\u4ef7\uff0c\u56de\u5e94\u4e86\u65e9\u671f\u731c\u60f3\u5e76\u4fee\u6b63\u4e86\u76f8\u5173\u6587\u732e\u4e2d\u7684\u9519\u8bef\u9648\u8ff0\uff0c\u5de9\u56fa\u4e86 k-NN \u5168\u5c40\u4e00\u81f4\u6027\u4e0e\u7a7a\u95f4\u51e0\u4f55\u7ef4\u5ea6\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2512.17899", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17899", "abs": "https://arxiv.org/abs/2512.17899", "authors": ["Aditya Gahlawat", "Ahmed Aboudonia", "Sandeep Banik", "Naira Hovakimyan", "Nikolai Matni", "Aaron D. Ames", "Gioele Zardini", "Alberto Speranzon"], "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy", "comment": "18 pages, 5 figures", "summary": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.", "AI": {"tldr": "DRIP\uff1a\u901a\u8fc7\u5206\u5c42\u63a7\u5236\u67b6\u6784\u5c06 TaSIL \u548c L1-DRAC \u6574\u5408\u5728\u4e00\u8d77\uff0c\u5b9e\u73b0\u53ef\u8bc1\u53ef\u63a7\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u7b56\u7565\u8bef\u5dee\u548c\u5916\u90e8\u6270\u52a8\u5bfc\u81f4\u7684\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u4f9b\u5bf9\u6574\u4e2a\u63a7\u5236\u7ba1\u7ebf\u7684\u8bc1\u4e66\u80fd\u529b\u3002", "motivation": "\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u5206\u5e03\u504f\u79fb\u6765\u81ea\u7b56\u7565\u8bef\u5dee\u548c\u5916\u90e8\u6270\u52a8/\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u81ea\u6211\u9a71\u52a8\u63a7\u5236\u7684\u9c81\u68d2\u6027\u964d\u4f4e\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u8bc1\u53ef\u63a7\u7684\u4f53\u7cfb\uff0c\u5c06\u5b66\u4e60\u7ec4\u4ef6\uff08\u5982\u611f\u77e5\uff09\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u51b3\u7b56\u7ed3\u5408\u8d77\u6765\u3002", "method": "\u63d0\u51fa DRIP \u67b6\u6784\uff08\u4e00\u4e2a\u5c42\u7ea7\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408 TaSIL \u7684\u5bf9\u7b56\u7565\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u548c L1-DRAC \u7684\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\uff09\uff0c\u901a\u8fc7\u5c42\u7ea7\u8f93\u5165\u8f93\u51fa\u7ea6\u675f\u5b9e\u73b0\u5bf9\u6574\u4e2a\u63a7\u5236\u6d41\u6c34\u7ebf\u7684\u53ef\u8bc1\u6027\uff0c\u8bbe\u8ba1 layer-centric \u8f93\u5165\u8f93\u51fa\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u5404\u5c42\u7684\u8bc1\u4e66\u3002", "result": "\u63d0\u4f9b\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u4e3a\u6574\u6761\u63a7\u5236\u6d41\u6c34\u7ebf\u63d0\u4f9b\u8bc1\u4e66\uff0c\u6307\u5411\u53ef\u8bc1\u53ef\u63a7\u7684\u81ea\u4e3b\u7cfb\u7edf\u8def\u5f84\uff0c\u5e76\u5c55\u73b0\u5c06\u5b66\u4e60\u7ec4\u4ef6\u4e0e\u8bc1\u8ba4\u7684\u51b3\u7b56\u5236\u5b9a\u8026\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "DRIP \u4e3a\u5b9e\u73b0\u5b8c\u5168\u53ef\u8bc1\u53ef\u63a7\u7684\u81ea\u4e3b\u7ba1\u7ebf\u94fa\u5e73\u9053\u8def\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u7ec4\u4ef6\uff08\u611f\u77e5\uff09\u4e0e\u53ef\u8bc1\u53ef\u63a7\u7684\u51b3\u7b56\u673a\u5236\u7ed3\u5408\uff0c\u4fc3\u8fdb\u9762\u5411\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u9a8c\u8bc1\u6027\u3002"}}
{"id": "2512.17310", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.17310", "abs": "https://arxiv.org/abs/2512.17310", "authors": ["Tianrui Wang", "Anyu Wang", "Tianshuo Cong", "Delong Ran", "Jinyuan Liu", "Xiaoyun Wang"], "title": "Cryptanalysis of Pseudorandom Error-Correcting Codes", "comment": null, "summary": "Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u4f2a\u968f\u673a\u9519\u8bef\u7ea0\u6b63\u7801\uff08PRC\uff09\u8fdb\u884c\u7cfb\u7edf\u5bc6\u7801\u5206\u6790\uff0c\u63d0\u51fa\u4e09\u79cd\u653b\u51fb\u4ee5\u6311\u6218\u5176\u4e0d\u53ef\u68c0\u6d4b\u6027\u4e0e\u9c81\u68d2\u6027\u5047\u8bbe\uff0c\u5e76\u5bf9\u771f\u5b9e\u5927\u6a21\u578b\u6c34\u5370\u8fdb\u884c\u9a8c\u8bc1\u3002\u653b\u51fb\u5728\u591a\u53c2\u6570\u914d\u7f6e\u4e0b\u6709\u6548\uff0c\u80fd\u5728\u7ea62^22\u6b21\u8fd0\u7b97\u91cf\u5185\u68c0\u6d4b\u6c34\u5370\u3002\u63d0\u51fa\u4e09\u79cd\u9632\u5fa1\u4e0e\u6539\u8fdb\u7684\u5bc6\u94a5\u751f\u6210\u7b97\u6cd5\uff0c\u4f46\u5728\u5f53\u524d\u53c2\u6570\u4e0b\u4ecd\u96be\u4ee5\u8fbe\u5230128\u4f4d\u5b89\u5168\uff0c\u53d7\u5927\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\u7ea6\u675f\u5f71\u54cd\u3002", "motivation": "\u586b\u8865\u5bf9PRC\u5b89\u5168\u6027\u5206\u6790\u7684\u7a7a\u767d\uff0c\u8bc4\u4f30\u5728\u5177\u4f53\u53c2\u6570\u53ca\u73b0\u5b9e\u653b\u51fb\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u4e0d\u53ef\u68c0\u6d4b\u6027\u53ca\u6c34\u5370\u5b89\u5168\u6027\uff0c\u4ee5\u63d0\u5347\u6c34\u5370\u65b9\u6848\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u653b\u51fb\uff1a\u4e24\u79cd\u7528\u4e8e\u533a\u5206PRC\u7801\u5b57\u4e0e\u666e\u901a\u5411\u91cf\u3001\u4e00\u6b21\u7528\u4e8e\u7834\u574f\u89e3\u7801\u8fc7\u7a0b\uff1b\u5728\u771f\u5b9e\u7684\u751f\u6210\u6a21\u578b\uff08\u5982 DeepSeek\u3001Stable Diffusion\uff09\u4e0a\u9a8c\u8bc1\u653b\u51fb\u6709\u6548\u6027\uff1b\u63d0\u51fa\u4e09\u9879\u9632\u5fa1\u63aa\u65bd\uff0c\u5305\u62ec\u53c2\u6570\u4f18\u5316\u3001\u5b9e\u73b0\u7ec6\u8282\u6539\u8fdb\u53ca\u91cd\u65b0\u8bbe\u8ba1\u7684\u5bc6\u94a5\u751f\u6210\u7b97\u6cd5\u3002", "result": "\u653b\u51fb\u5728\u6240\u6709\u53c2\u6570\u914d\u7f6e\u4e0b\u524a\u5f31\u6216\u7834\u574f\u4e86PRC\u7684\u5b89\u5168\u6027\uff1b\u80fd\u4ee5\u7ea62^22\u6b21\u8fd0\u7b97\u68c0\u6d4b\u6c34\u5370\u7684\u5b58\u5728\uff1b\u6539\u8fdb\u7684\u5bc6\u94a5\u751f\u6210\u51fd\u6570\u6709\u6548\u9632\u6b62\u4e86\u5f31\u5bc6\u94a5\uff0c\u4f46\u603b\u4f53\u4ecd\u672a\u8fbe\u5230128-bit \u5b89\u5168\uff0c\u7531\u4e8e\u5927\u6a21\u578b\u7684\u8f93\u51fa\u957f\u5ea6\u7b49\u56fa\u6709\u9650\u5236\u3002", "conclusion": "PRC \u65b9\u6848\u5728\u5f53\u524d\u8bbe\u8ba1\u4e0b\u5b58\u5728\u663e\u8457\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u9632\u5fa1\u548c\u66f4\u4e25\u683c\u7684\u53c2\u6570\u9009\u62e9\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u805a\u7126\u4e8e\u63d0\u5347\u5173\u952e\u5b89\u5168\u6027\u3001\u6539\u8fdb\u6c34\u5370\u5d4c\u5165\u4e0e\u5bc6\u94a5\u751f\u6210\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u5b89\u5168\u7b49\u7ea7\u3002"}}
{"id": "2512.17434", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17434", "abs": "https://arxiv.org/abs/2512.17434", "authors": ["Sasmita Dash", "Constantinos Psomas", "Ioannis Krikidis"], "title": "Sub-6 GHz Beam-Reconfigurable Microfluidic Antenna Using Graphene Liquid for 5G Network", "comment": "8 Pages, 4 Figures", "summary": "As wireless communication systems continue to grow rapidly, high-performance antennas become increasingly crucial for expanding coverage, improving capacity, and enhancing transmission quality. In light of this, research has focused considerable attention on liquid antennas due to their unique characteristics, which include small size, flexibility, reconfigurability and transparency. Recently, graphene liquid has been explored for numerous applications due to its low cost, high conductivity, flexibility, and ease of processing. Specifically for antenna applications, graphene liquid performs better than conventional liquid metal. This paper presents a graphene-liquid antenna with beam reconfiguration ability for sub-6 GHz communication system. The graphene-liquid movement within the microfluidic channel is taken into consideration by the reconfiguration mechanism. The antenna achieves beam reconfiguration in 360\u00b0 directions with 6 dBi of gain at 5.5 GHz, featuring a wideband impedance bandwidth of 24%. The antenna main beam is specifically reconfigured into six directions (0\u00b0, 45\u00b0, 135\u00b0, 180\u00b0, 225\u00b0 and 315\u00b0) at 5.5 GHz. Additionally, in all six reconfigurable scenarios at 5.5 GHz, the antenna provides a stable reflection coefficient. Therefore, for the next generation of wireless communication systems, this novel design of graphene-liquid-based reconfigurable sub-6 GHz antennas holds promise.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77f3\u58a8\u70ef\u6db2\u4f53\u7684\u53ef\u91cd\u6784\u5929\u7ebf\uff0c\u57285.5 GHz\u7684\u5b506 GHz\u6bb5\u5b9e\u73b0360\u00b0\u5168\u65b9\u4f4d\u6ce2\u675f\u91cd\u6784\uff0c\u589e\u76ca6 dBi\uff0c\u963b\u6297\u5e26\u5bbd\u7ea624%\uff0c\u4e14\u5728\u516d\u79cd\u91cd\u6784\u60c5\u5f62\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u53cd\u5c04\u7cfb\u6570\u3002", "motivation": "\u4e3a\u63d0\u5347\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u7684\u8986\u76d6\u3001\u5bb9\u91cf\u4e0e\u4f20\u8f93\u8d28\u91cf\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u4e14\u53ef\u91cd\u6784\u7684\u5929\u7ebf\u3002\u6db2\u6001\u5929\u7ebf\u5177\u6709\u5c0f\u578b\u5316\u3001\u67d4\u6027\u3001\u53ef\u91cd\u6784\u548c\u900f\u660e\u7b49\u4f18\u70b9\uff1b\u77f3\u58a8\u70ef\u6db2\u4f53\u5728\u5bfc\u7535\u6027\u3001\u6210\u672c\u548c\u52a0\u5de5\u6027\u65b9\u9762\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff0c\u4e14\u5728\u5929\u7ebf\u5e94\u7528\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6db2\u6001\u91d1\u5c5e\u3002", "method": "\u901a\u8fc7\u5728\u5fae\u6d41\u63a7\u901a\u9053\u4e2d\u5b9e\u73b0\u77f3\u58a8\u70ef\u6db2\u4f53\u7684\u5b9a\u5411\u79fb\u52a8\u6765\u5b9e\u73b0\u5929\u7ebf\u7684\u6ce2\u675f\u91cd\u6784\u3002\u4ee55.5 GHz\u4e3a\u5de5\u4f5c\u70b9\uff0c\u5929\u7ebf\u5b9e\u73b0360\u00b0\u65b9\u5411\u7684\u6ce2\u675f\u91cd\u6784\uff0c\u63d0\u4f9b\u516d\u4e2a\u76ee\u6807\u65b9\u5411\uff080\u00b0,45\u00b0,135\u00b0,180\u00b0,225\u00b0,315\u00b0\uff09\uff0c\u5728\u6240\u6709\u65b9\u5411\u4e0b\u83b7\u5f97\u7a33\u5b9a\u7684\u53cd\u5c04\u7cfb\u6570\u548c6 dBi\u7684\u589e\u76ca\u3002\u5e26\u5bbd\u7ea624%\u3002", "result": "\u57285.5 GHz\u4e0b\u5b9e\u73b0\u516d\u4e2a\u91cd\u65b0\u914d\u7f6e\u573a\u666f\u4e2d\u5747\u5177\u7a33\u5b9a\u7684\u56de\u6ce2\u635f\u8017/\u53cd\u5c04\u7cfb\u6570\uff0c\u4e14\u4e3b\u74e3\u5b9e\u73b0\u516d\u4e2a\u65b9\u5411\u6ce2\u675f\u91cd\u6784\uff0c\u6700\u5927\u589e\u76ca\u4e3a6 dBi\uff0c\u5e26\u5bbd\u7ea624%\u3002", "conclusion": "\u77f3\u58a8\u70ef\u6db2\u4f53\u57fa\u53ef\u91cd\u6784Sub-6 GHz\u5929\u7ebf\u5177\u6709\u826f\u597d\u524d\u666f\uff0c\u53ef\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u7075\u6d3b\u7684\u6ce2\u675f\u63a7\u5236\u80fd\u529b\u548c\u5bbd\u5e26\u7279\u6027\u3002"}}
{"id": "2512.17073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17073", "abs": "https://arxiv.org/abs/2512.17073", "authors": ["Zhenyu Liu", "Yunzhen Liu", "Zehao Fan", "Garrett Gagnon", "Yayue Hou", "Nan Wu", "Yangwook Kang", "Liu Liu"], "title": "Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation", "comment": null, "summary": "Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u5bbd\u9ad8\u6548\u7684\u81ea\u9002\u5e94MoE\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u8865\u507f\u5b9e\u73b0\u8def\u7531\u5f15\u5bfc\u7684\u7cbe\u5ea6\u6062\u590d\uff0c\u5728\u6bcf\u4e2a token \u4ec5\u4f20\u8f93 Top-n\uff08n<k\uff09\u4e2a\u4e13\u5bb6\u7684\u4f4e\u79e9\u56e0\u5b50\u5e76\u5bf9\u5176\u5e94\u7528\u8865\u507f\uff0c\u5176\u4ed6\u4e13\u5bb6\u4fdd\u6301\u4f4e\u4f4d\u5bbd\uff0c\u4ece\u800c\u63d0\u5347\u5e26\u5bbd\u4e0e\u51c6\u786e\u6027/\u541e\u5410\u7684\u6298\u8877\u3002", "motivation": "MoE \u6a21\u578b\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u6269\u5c55\u5bb9\u91cf\uff0c\u4f46\u5bf9\u663e\u5b58\u548c\u5e26\u5bbd\u9020\u6210\u538b\u529b\uff1b\u6309 token \u8def\u7531\u7684\u79bb\u7ebf/\u5728\u7ebf\u4f20\u8f93\u5bfc\u81f4 I/O \u9650\u5236\uff1b\u9759\u6001\u91cf\u5316\u867d\u7136\u964d\u4f4e\u5e26\u5bbd\uff0c\u4f46\u5bb9\u6613\u5728\u4e13\u5bb6\u5f02\u8d28\u6027\u5b58\u5728\u65f6\u663e\u8457\u635f\u5931\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u7684\u4f4e\u79e9\u8865\u507f\u5668\u5bf9\u8def\u7531\u5f15\u5bfc\u9636\u6bb5\u5b9e\u73b0\u7cbe\u5ea6\u6062\u590d\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u5bf9\u6bcf\u4e2a token \u4ec5\u4f20\u8f93 Top-n\uff08n<k\uff09\u4e2a\u4e13\u5bb6\u7684\u4f4e\u79e9\u56e0\u5b50\uff0c\u5e76\u5bf9\u4f20\u8f93\u7684\u56e0\u5b50\u5e94\u7528\u8865\u507f\uff0c\u5176\u4ed6\u4e13\u5bb6\u4fdd\u6301\u4f4e\u4f4d\u5bbd\u8868\u793a\u3002\u8be5\u65b9\u6848\u4e0e GPU \u4e0e GPU-NDP \u7cfb\u7edf\u4e0a\u7684 offloading \u76f8\u7ed3\u5408\u4ee5\u5b9e\u73b0\u9ad8\u6548\u5e26\u5bbd\u5229\u7528\u3002", "result": "\u5728\u5e26\u5bbd-\u7cbe\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u6298\u8877\uff0c\u5e76\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21 MoE \u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e26\u5bbd\u53cb\u597d\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5145\u5206\u5229\u7528\u4f4e\u79e9\u8865\u507f\u4e0e\u6309\u9700\u4e13\u5bb6\u4f20\u8f93\u7684\u7ed3\u5408\u4ee5\u63d0\u9ad8\u541e\u5410\u4e0e\u7cbe\u5ea6\u8868\u73b0\u3002"}}
{"id": "2512.17473", "categories": ["eess.SP", "cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17473", "abs": "https://arxiv.org/abs/2512.17473", "authors": ["Atharva Awari", "Nicolas Gillis", "Arnaud Vandaele"], "title": "Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions", "comment": "14 pages, 6 figures. Code available from https://gitlab.com/Atharva05/admm-for-nmd", "summary": "We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \\in \\mathbb{R}^{m \\times n}$ and a factorization rank $r \\ll \\min(m, n)$, NMD seeks matrices $W \\in \\mathbb{R}^{m \\times r}$ and $H \\in \\mathbb{R}^{r \\times n}$ such that $X \\approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \\max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \\min(b, \\max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.", "AI": {"tldr": "\u57fa\u4e8e ADMM \u7684\u975e\u7ebf\u6027\u77e9\u9635\u5206\u89e3\uff08NMD\uff09\u65b9\u6cd5\u7efc\u8ff0\u4e0e\u5206\u6790", "motivation": "\u89e3\u51b3\u5728\u8f93\u5165\u77e9\u9635 X \u548c\u4f4e\u79e9\u56e0\u5b50 W, H \u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5bf9\u5143\u7d20\u7ea7\u975e\u7ebf\u6027\u53d8\u6362 f(\u00b7) \u7684\u5904\u7406\uff0c\u5b9e\u73b0\u5bf9\u5e7f\u4e49\u975e\u7ebf\u6027\u6570\u636e\u7684\u9ad8\u6548\u8fd1\u4f3c\uff0c\u5e76\u5728\u591a\u79cd\u975e\u7ebf\u6027\u6a21\u578b\u4e0e\u635f\u5931\u51fd\u6570\u4e0b\u4fdd\u6301\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027.", "method": "\u5c06\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7528\u4e8e\u4f18\u5316\u95ee\u9898\uff0c\u5206\u89e3\u4e3a\u5bf9 W\u3001H \u53ca\u76f8\u5173\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u53d8\u91cf\u7684\u8fed\u4ee3\u66f4\u65b0\uff0c\u5904\u7406 X \u2248 f(WH) \u7684\u975e\u7ebf\u6027\u6620\u5c04\u3002\u652f\u6301\u591a\u79cd\u975e\u7ebf\u6027\u51fd\u6570 f\uff08\u5982 ReLU\u3001\u5e73\u65b9\u3001MinMax\uff09\u53ca\u591a\u79cd\u635f\u5931\u5ea6\u91cf\uff08\u6700\u5c0f\u4e8c\u4e58\u3001L1\u3001KL \u6563\u5ea6\uff09\uff0c\u6846\u67b6\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u975e\u7ebf\u6027\u548c\u6307\u6807\uff0c\u9002\u5408\u5927\u89c4\u6a21\u7a00\u758f\u6216\u6982\u7387\u6027\u6570\u636e\u573a\u666f\u3002", "result": "\u5728\u82e5\u5e72\u4ee3\u8868\u6027\u975e\u7ebf\u6027\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u975e\u7ebf\u6027\u53d8\u6362\u548c\u635f\u5931\u51fd\u6570\u7684\u9002\u7528\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f53\u73b0\u51fa\u826f\u597d\u7684\u9002\u914d\u6027\u3001\u7075\u6d3b\u6027\u4e0e\u6f5c\u5728\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u8f83\u9ad8\u7684\u901a\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5904\u7406\u5e7f\u4e49\u7684\u975e\u7ebf\u6027\u77e9\u9635\u5206\u89e3\u95ee\u9898\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u80fd\u96c6\u4e2d\u5728\u6536\u655b\u6027\u5206\u6790\u3001\u5bf9\u6bd4\u7814\u7a76\u3001\u4ee5\u53ca\u5bf9\u66f4\u591a\u975e\u7ebf\u6027\u53d8\u6362\u4e0e\u635f\u5931\u51fd\u6570\u7684\u6574\u5408\u4e0e\u4f18\u5316\u6765\u63d0\u5347\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.17079", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17079", "abs": "https://arxiv.org/abs/2512.17079", "authors": ["Saraswathy Amjith", "Mihika Dusad", "Neha Muramalla", "Shweta Shah"], "title": "Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?", "comment": null, "summary": "Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.", "AI": {"tldr": "Mixed-CoT-RL: \u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u53d7\u63a7\u7684\u9519\u8bef\u63a8\u7406\u75d5\u8ff9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9047\u5230\u9519\u8bef\u524d\u63d0\u4e0b\u7684\u9519\u4f4d\u6062\u590d\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5728\u5e72\u51c0\u9898\u76ee\u4e0a\u7684\u8868\u73b0\u3002\u76f8\u6bd4\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff0c\u6df7\u5408\u8bad\u7ec3\u5728\u5e26\u9519\u8bef\u524d\u63d0\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff1b\u4e14\u5bf9\u63a8\u7406\u9519\u8bef\u7684\u66b4\u9732\u6bd4\u8ba1\u7b97\u9519\u8bef\u5e26\u6765\u66f4\u5927\u6536\u76ca\u3002", "motivation": "\u89e3\u51b3\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u5728\u65e9\u671f\u9519\u8bef\u51fa\u73b0\u65f6\u5bb9\u6613\u653e\u5927\u5e76\u5bfc\u81f4\u9519\u8bef\u7b54\u6848\u7684\u95ee\u9898\u3002\u63d0\u51fa\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u5e26\u5355\u4e00\u63a7\u5236\u9519\u8bef\u7684\u63a8\u7406\u75d5\u8ff9\uff0c\u4ee5\u6559\u4f1a\u6a21\u578b\u68c0\u6d4b\u5e76\u4ece\u9519\u8bef\u4e2d\u6062\u590d\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u666e\u901a\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u5728 MATH-lighteval \u7684\u7ade\u8d5b\u7ea7\u95ee\u9898\u4e0a\uff0c\u751f\u6210\u5e26\u6709\u4e14\u4ec5\u5e26\u4e00\u4e2a\u53d7\u63a7\u9519\u8bef\u7684 CoT \u524d\u7f00\uff08\u8ba1\u7b97\u9519\u8bef\u5982\u7b26\u53f7\u7ffb\u8f6c\u3001\u7f3a\u5931\u9879\uff1b\u63a8\u7406\u9519\u8bef\u5982\u9519\u8bef\u89c4\u5219\u5e94\u7528\u3001\u65e0\u6839\u636e\u7684\u63a8\u7406\u6b65\u9aa4\uff09\uff0c\u5e76\u7528\u4e8c\u5143\u6700\u7ec8\u7b54\u6848\u5956\u52b1\u5bf9 Qwen3-4B \u8fdb\u884c GRPO \u5fae\u8c03\u3002\u6bd4\u8f83\u6df7\u5408-CoT-RL\u3001\u6807\u51c6 RL\uff08\u5e72\u51c0\u9898\u76ee\uff09\u4ee5\u53ca\u672a\u8c03\u4f18\u57fa\u7ebf\u5728\u6709\u65e0\u9519\u8bef\u524d\u63d0\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u5e72\u51c0\u9898\u76ee\u4e0a\uff0c\u6df7\u5408-CoT-RL \u4e0e\u6807\u51c6 RL \u8868\u73b0\u76f8\u5f53\uff0841%\uff09\u3002\u5728\u5e26\u9519\u8bef\u524d\u63d0\u7684\u9898\u76ee\u4e0a\uff0c\u6df7\u5408-CoT-RL \u663e\u8457\u4f18\u4e8e\u6807\u51c6 RL\uff0824% \u5bf9 19%\uff09\u3002\u5355\u7eaf\u9488\u5bf9\u5e72\u51c0\u9898\u76ee\u7684 RL \u5fae\u8c03\u53cd\u800c\u964d\u4f4e\u9c81\u68d2\u6027\uff0819% \u5bf9 20% \u7684\u672a\u8c03\u4f18\u57fa\u7ebf\uff09\u3002\u5728\u9519\u8bef\u7c7b\u578b\u95f4\uff0c\u66b4\u9732\u4e8e\u63a8\u7406\u9519\u8bef\u6bd4\u66b4\u9732\u4e8e\u8ba1\u7b97\u9519\u8bef\u5e26\u6765\u66f4\u5927\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u4e14\u6df7\u5408\u8bad\u7ec3\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u66b4\u9732\u4e8e\u5e26\u9519\u8bef\u7684\u63a8\u7406\u75d5\u8ff9\u7684\u8bad\u7ec3\u53ef\u4ee5\u63d0\u5347\u9519\u8bef\u6062\u590d\u80fd\u529b\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u8def\u5f84\u3002"}}
{"id": "2512.17679", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17679", "abs": "https://arxiv.org/abs/2512.17679", "authors": ["Zhou Lu", "Mohammed El-Hajjar", "Lie-liang Yang"], "title": "Augmented Affine Frequency Division Multiplexing for Both Low PAPR Signaling and Diversity Gain Protection", "comment": null, "summary": "Research results on Affine Frequency Division Multiplexing (AFDM) reveal that it experiences the same Peak-to-Average Power Ratio (PAPR) problem as conventional Orthogonal Frequency-Division Multiplexing (OFDM). On the other side, some references and also our studies demonstrate that AFDM involves an unneeded matrix, which is based on a parameter typically represented by $c_2$, for signalling. Hence, in this paper, an augmented AFDM scheme, referred to as A$^2$FDM, is proposed to mitigate the PAPR problem of AFDM, which is achieved by replacing the $c_2$ matrix in AFDM by a new unitary matrix that performs both sub-block-based Discrete Fourier Transform (DFT) and symbol mapping. Two symbol mapping schemes, namely interleaved mapping and localized mapping, are proposed for implementing A$^2$FDM, yielding the Interleaved A$^2$FDM and Localized A$^2$FDM. The input-output relationships of these schemes are derived and the complexity and the effects of system parameters on the performance of A$^2$FDM along with AFDM systems are analyzed. Furthermore, simulation results are provided to demonstrate and compare comprehensively the performance of the considered schemes in conjunction with different system settings and various operational conditions. Our studies and results demonstrate that, while A$^2$FDM is capable of circumventing the PAPR problem faced by AFDM, it is capable of attaining the achievable diversity gain, when AFDM is operated in its undesirable conditions resulting in the loss of the diversity gain available.", "AI": {"tldr": "\u63d0\u51faA2FDM\u4ee5\u89e3\u51b3AFDM\u7684PAPR\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u66ff\u6362c2\u77e9\u9635\u4e3a\u4e00\u4e2a\u5355\u4f4d\u77e9\u9635\uff0c\u540c\u65f6\u5b9e\u73b0\u5b50\u5757DFT\u4e0e\u7b26\u53f7\u6620\u5c04\uff0c\u63d0\u51fa\u4e24\u79cd\u6620\u5c04\u65b9\u6848\uff08\u4ea4\u9519\u6620\u5c04\u548c\u5c40\u90e8\u6620\u5c04\uff09\uff0c\u5e76\u5bf9\u590d\u6742\u5ea6\u3001\u53c2\u6570\u4e0e\u6027\u80fd\u8fdb\u884c\u5206\u6790\u4e0e\u4eff\u771f\u6bd4\u8f83\u3002\u7ed3\u8bba\u662fA2FDM\u5728\u6291\u5236PAPR\u7684\u540c\u65f6\u53ef\u5728AFDM\u4e0d\u5229\u6761\u4ef6\u4e0b\u6062\u590d\u591a\u6837\u6027\u589e\u76ca\u3002", "motivation": "AFDM\u5b58\u5728PAPR\u95ee\u9898\uff0c\u4e0eOFDM\u7c7b\u4f3c\u7684\u7279\u6027\uff0c\u4ee5\u53caAFDM\u4e2d\u57fa\u4e8ec2\u7684\u5197\u4f59\u4fe1\u53f7\u77e9\u9635\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\uff0c\u9700\u5728\u964d\u4f4ePAPR\u7684\u540c\u65f6\u4fdd\u6301\u6216\u6062\u590d\u7cfb\u7edf\u589e\u76ca\u3002", "method": "\u63d0\u51fa\u589e\u5e7fAFDM(A2FDM)\uff0c\u7528\u4e00\u4e2a\u65b0\u5355\u4f4d\u77e9\u9635\u4ee3\u66ffAFDM\u4e2d\u7684c2\u77e9\u9635\uff0c\u5355\u4f4d\u77e9\u9635\u5b9e\u73b0\u5b50\u5757DFT\u548c\u7b26\u53f7\u6620\u5c04\uff1b\u8bbe\u8ba1\u4e24\u79cd\u6620\u5c04\uff1aInterleaved\u4e0eLocalized\uff1b\u63a8\u5bfc\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff1b\u5206\u6790\u5b9e\u73b0\u590d\u6742\u5ea6\u53ca\u7cfb\u7edf\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u7ed9\u51fa\u4eff\u771f\u4ee5\u5bf9\u6bd4AFDM\u4e0eA2FDM\u5728\u4e0d\u540c\u8bbe\u5b9a\u4e0b\u7684\u8868\u73b0\u3002", "result": "A2FDM\u80fd\u591f\u7f13\u89e3AFDM\u7684PAPR\u95ee\u9898\uff0c\u5e76\u5728AFDM\u5904\u4e8e\u4e0d\u5229\u6761\u4ef6\u5bfc\u81f4\u4e27\u5931\u591a\u6837\u6027\u589e\u76ca\u65f6\uff0c\u4ecd\u80fd\u83b7\u5f97\u53ef\u89c2\u7684\u591a\u6837\u6027\u589e\u76ca\uff1b\u4e24\u79cd\u6620\u5c04\u65b9\u6848\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e0a\u5b58\u5728\u6743\u8861\uff0c\u4eff\u771f\u8986\u76d6\u4e86\u591a\u79cd\u7cfb\u7edf\u8bbe\u7f6e\u4e0e\u8fd0\u884c\u6761\u4ef6\u3002", "conclusion": "A2FDM\u4e3aAFDM\u7684PAPR\u95ee\u9898\u63d0\u4f9b\u6709\u6548\u7f13\u89e3\uff0c\u540c\u65f6\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u7ef4\u6301\u6216\u6062\u590d\u591a\u6837\u6027\u589e\u76ca\uff1b\u6620\u5c04\u7b56\u7565\u53ca\u53c2\u6570\u9009\u62e9\u5bf9\u6700\u7ec8\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u5728\u5177\u4f53\u573a\u666f\u4e2d\u505a\u51fa\u6743\u8861\u3002"}}
{"id": "2512.17091", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17091", "abs": "https://arxiv.org/abs/2512.17091", "authors": ["Toshiaki Hori", "Jonathan DeCastro", "Deepak Gopinath", "Avinash Balachandran", "Guy Rosman"], "title": "Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making", "comment": "23 pages, 8 figures. Under review", "summary": "We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7ed3\u5408\u7684\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7528RL\u884c\u52a8\u6307\u5bfcMPPI\u91c7\u6837\u5e76\u81ea\u9002\u5e94\u805a\u5408\u91c7\u6837\u6765\u6539\u8fdb\u503c\u51fd\u6570\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347\u5728\u591a\u9886\u57df\u7684\u6570\u636e\u6548\u7387\u4e0e\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5728\u590d\u6742\u5206\u5c42\u89c4\u5212\u95ee\u9898\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u63a2\u7d22-\u4f30\u8ba1\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5b66\u4e60\u7684\u6846\u67b6\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u52a8\u4f5c\u7528\u4e8e\u6307\u5bfcMPPI\u91c7\u6837\uff0c\u5229\u7528\u81ea\u9002\u5e94\u805a\u5408\u7684MPPI\u6837\u672c\u6765 Inform \u503c\u51fd\u6570\u4f30\u8ba1\uff1b\u5728\u4ef7\u503c\u4e0d\u786e\u5b9a\u65f6\u589e\u52a0MPPI\u63a2\u7d22\uff0c\u5f62\u6210\u8026\u5408\u7684\u81ea\u9002\u5e94\u89c4\u5212\u5faa\u73af\uff0c\u4ece\u800c\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u9886\u57df\uff08\u5305\u62ec\u8d5b\u8f66\u9a71\u52a8\u3001\u6539\u88c5\u7684Acrobot\u3001\u5e26\u969c\u788d\u7684\u6708\u7403\u7740\u9646\u5668\uff09\u4e2d\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u4e0e\u603b\u4f53\u8868\u73b0\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u53ef\u8fbe\u7ea672%\uff0c\u76f8\u8f83\u975e\u81ea\u9002\u5e94\u91c7\u6837\u7684\u6536\u655b\u901f\u5ea6\u63d0\u5347\u7ea62.1\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u8026\u5408RL-MPC\u6846\u67b6\u901a\u8fc7\u5c06RL\u884c\u52a8\u878d\u5165\u91c7\u6837\u5e76\u4ee5\u81ea\u9002\u5e94\u65b9\u5f0f\u6574\u5408\u6837\u672c\u6765\u63d0\u5347\u89c4\u5212\u9c81\u68d2\u6027\u4e0e\u6570\u636e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5206\u5c42\u89c4\u5212\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2512.17602", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.17602", "abs": "https://arxiv.org/abs/2512.17602", "authors": ["Davide Mancino", "Davide Rezzoli"], "title": "Sandwiched and Silent: Behavioral Adaptation and Private Channel Exploitation in Ethereum MEV", "comment": null, "summary": "How users adapt after being sandwiched remains unclear; this paper provides an empirical quantification. Using transaction level data from November 2024 to February 2025, enriched with mempool visibility and ZeroMEV labels, we track user outcomes after their n-th public sandwich: (i) reactivation, i.e., the resumption of on-chain activity within a 60-day window, and (ii) first-time adoption of private routing. We refer to users who do not reactivate within this window as churned, and to users experiencing multiple attacks (n>1) as undergoing repeated exposure. Our analysis reveals measurable behavioral adaptation: around 40% of victims migrate to private routing within 60 days, rising to 54% with repeated exposures. Churn peaks at 7.5% after the first sandwich but declines to 1-2%, consistent with survivor bias. In Nov-Dec 2024 we confirm 2,932 private sandwich attacks affecting 3,126 private victim transactions, producing \\$409,236 in losses and \\$293,786 in attacker profits. A single bot accounts for nearly two-thirds of private frontruns, and private sandwich activity is heavily concentrated on a small set of DEX pools. These results highlight that private routing does not guarantee protection from MEV extraction: while execution failures push users toward private channels, these remain exploitable and highly concentrated, demanding continuous monitoring and protocol-level defenses.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u4ea4\u6613\u7ea7\u6570\u636e\uff0c\u91cf\u5316\u7528\u6237\u5728\u591a\u6b21\u516c\u5f00\u5bf9\u624b\u653b\u51fb\u540e\u7684\u9002\u5e94\u884c\u4e3a\uff1a\u7ea640%\u572860\u5929\u5185\u8f6c\u5411\u79c1\u6709\u8def\u7531\uff0c\u91cd\u590d\u653b\u51fb\u60c5\u5f62\u4e0b\u589e\u81f354%\uff1b\u9996\u6b21\u653b\u51fb\u540e\u5f03\u7528\u7387\u5cf0\u503c7.5%\uff0c\u751f\u5b58\u504f\u5dee\u5bfc\u81f41\u20132%\u9010\u6b65\u4e0b\u964d\uff1b2024\u5e7411\u201312\u6708\u53d1\u751f2932\u8d77\u79c1\u6709\u5bf9\u724c\u653b\u51fb\uff0c\u5f71\u54cd3126\u7b14\u79c1\u6709\u53d7\u5bb3\u4ea4\u6613\uff0c\u9020\u6210\u635f\u5931\u7ea6$409,236\uff0c\u653b\u51fb\u8005\u5229\u6da6$293,786\uff1b\u5355\u4e00\u673a\u5668\u4eba\u8d26\u6237\u7ea6\u5360\u79c1\u6709\u524d\u7aef\u653b\u51fb\u7684\u4e09\u5206\u4e4b\u4e8c\uff1b\u79c1\u6709\u653b\u51fb\u96c6\u4e2d\u5728\u5c11\u6570DEX\u6c60\uff1b\u79c1\u6709\u8def\u7531\u5e76\u4e0d\u80fd\u5b8c\u5168\u9632\u62a4MEV\uff0c\u4ecd\u5177\u53ef\u5229\u7528\u6027\uff0c\u9700\u6301\u7eed\u76d1\u63a7\u4e0e\u534f\u8bae\u5c42\u9632\u5fa1\u3002", "motivation": "\u7406\u89e3\u7528\u6237\u5bf9MEV\u653b\u51fb\u7684\u53cd\u5e94\u53ca\u79c1\u6709\u8def\u7531\u7684\u6709\u6548\u6027\uff0c\u91cf\u5316\u884c\u4e3a\u9002\u5e94\u4e0e\u7ecf\u6d4e\u5f71\u54cd\u3002", "method": "\u4f7f\u75282024\u5e7411\u6708\u81f32025\u5e742\u6708\u7684\u4ea4\u6613\u7ea7\u6570\u636e\uff0c\u7ed3\u5408mempool\u53ef\u89c1\u6027\u4e0eZeroMEV\u6807\u7b7e\uff0c\u8ddf\u8e2an\u6b21\u516c\u5f00\u5bf9\u724c\u653b\u51fb\u540e\u7684\u7528\u6237\u7ed3\u679c\uff0c\u5b9a\u4e49\u518d\u6fc0\u6d3b\u3001\u5f03\u7528\u3001\u9996\u6b21\u91c7\u7528\u79c1\u6709\u8def\u7531\u7b49", "result": "\u91cf\u5316\u7ed3\u679c\uff1a\u7ea640%\u7684\u53d7\u5bb3\u800560\u5929\u5185\u8f6c\u5411\u79c1\u6709\u8def\u7531\uff0c\u91cd\u590d\u66b4\u9732\u60c5\u5f62\u4e0b\u589e\u81f354%\uff1b\u9996\u6b21\u653b\u51fb\u540e\u5f03\u7528\u7387\u5cf0\u503c7.5%\uff0c\u5728\u5e78\u5b58\u8005\u504f\u8bef\u7684\u5f71\u54cd\u4e0b\u964d\u81f31\u20132%\uff1b2024\u5e7411\u201312\u6708\u62a5\u544a2,932\u8d77\u79c1\u6709 sandwich \u653b\u51fb\uff0c\u5f71\u54cd3,126\u7b14\u79c1\u6709\u53d7\u5bb3\u4ea4\u6613\uff0c\u635f\u5931$409,236\uff0c\u653b\u51fb\u8005\u5229\u6da6$293,786\uff1b\u4e00\u4e2a\u673a\u5668\u4eba\u8d26\u6237\u51e0\u4e4e\u5360\u79c1\u6709\u524d\u7aef\u653b\u51fb\u7684\u4e09\u5206\u4e4b\u4e8c\uff1b\u79c1\u6709\u653b\u51fb\u9ad8\u5ea6\u96c6\u4e2d\u5728\u5c11\u6570DEX\u6c60\u3002", "conclusion": "\u79c1\u6709\u8def\u7531\u5e76\u4e0d\u80fd\u5b8c\u5168\u4fdd\u62a4\uff0c\u5c3d\u7ba1\u6267\u884c\u5931\u8d25\u4fc3\u4f7f\u7528\u6237\u8f6c\u5411\u79c1\u6709\u901a\u9053\uff0c\u79c1\u6709\u653b\u51fb\u4ecd\u5177\u53ef\u5229\u7528\u6027\u4e14\u9ad8\u5ea6\u96c6\u4e2d\uff0c\u9700\u8981\u6301\u7eed\u76d1\u63a7\u4e0e\u534f\u8bae\u5c42\u9632\u62a4\u3002"}}
{"id": "2512.17100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17100", "abs": "https://arxiv.org/abs/2512.17100", "authors": ["Justin Li", "Efe Sencan", "Jasper Zheng Duan", "Vitus J. Leung", "Stephan Tsaur", "Ayse K. Coskun"], "title": "UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data", "comment": "21 pages, 7 figures", "summary": "Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5bf9\u6bd4\u6027\u89e3\u91ca\u6846\u67b6 UniCoMTE\uff0c\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u5668\u7684\u5bf9\u6bd4\u7c7b\u89e3\u91ca\uff0c\u5e76\u5728ECG\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u4e0eLIME/SHAP\u7b49\u65b9\u6cd5\u6bd4\u8f83\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u9ed1\u7bb1\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u4e2d\u7684\u4fe1\u4efb\u4e0e\u53ef\u89e3\u91ca\u6027\uff1b\u9700\u8981\u517c\u5bb9\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u4e14\u76f4\u63a5\u5728\u539f\u59cb\u8f93\u5165\u4e0a\u5de5\u4f5c\u3002", "method": "\u63d0\u51fa UniCoMTE \u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u6837\u672c\u5e76\u8bc4\u4f30\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u6765\u8bc6\u522b\u6700\u5177\u5f71\u54cd\u7684\u65f6\u95f4\u7279\u5f81\uff0c\u5c5e\u4e8e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4f5c\u7528\u4e8e\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u3002\u5bf9ECG\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u89e3\u91ca\u8d28\u91cf\u4e0eLIME\u3001SHAP\u5728\u53ef\u7406\u89e3\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5bf9\u76f8\u4f3c\u6837\u672c\u7684\u6cdb\u5316\u6027\u6d4b\u8bd5\u4ee5\u53ca\u533b\u751f\u8bc4\u5ba1\u7684\u4e34\u5e8a\u6709\u7528\u6027\u95ee\u5377\u6765\u8bc4\u4f30\u5e94\u7528\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u751f\u6210\u7b80\u6d01\u3001\u7a33\u5b9a\u4e14\u8d34\u5408\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u5728\u6e05\u6670\u5ea6\u548c\u9002\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u6a21\u578b\u9884\u6d4b\u4e0e\u6709\u610f\u4e49\u7684\u4fe1\u53f7\u6a21\u5f0f\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5e94\u7528\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": " UniCoMTE \u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u63d0\u5347\u63d0\u4f9b\u4e86\u666e\u9002\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6f5c\u5728\u5e94\u7528\u548c\u4e34\u5e8a\u4ef7\u503c\u3002"}}
{"id": "2512.17613", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.17613", "abs": "https://arxiv.org/abs/2512.17613", "authors": ["Vikas Srivastava", "Debasish Roy", "Sihem Mesnager", "Nibedita Kundu", "Sumit Kumar Debnath", "Sourav Mukhopadhyay"], "title": "A Post-Quantum Secure End-to-End Verifiable E-Voting Protocol Based on Multivariate Polynomials", "comment": null, "summary": "Voting is a primary democratic activity through which voters select representatives or approve policies. Conventional paper ballot elections have several drawbacks that might compromise the fairness, effectiveness, and accessibility of the voting process. Therefore, there is an increasing need to design safer, effective, and easily accessible alternatives. E-Voting is one such solution that uses digital tools to simplify voting. Existing state-of-the-art designs for secure E-Voting are based on number-theoretic hardness assumptions. These designs are no longer secure due to quantum algorithms such as Shor's algorithm. We present the design and analysis of \\textit{first} post-quantum secure end-to-end verifiable E-Voting protocol based on multivariate polynomials to address this issue. The security of our proposed design depends on the hardness of the MQ problem, which is an NP-hard problem. We present a simple yet efficient design involving only standard cryptographic primitives as building blocks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u5143\u591a\u9879\u5f0f\u7684\u540e\u91cf\u5b50\u5b89\u5168\u7aef\u5230\u7aef\u53ef\u9a8c\u8bc1\u7535\u5b50\u6295\u7968\u534f\u8bae\uff0c\u4f9d\u8d56MQ\u95ee\u9898\u7684NP-hard\u6027\uff0c\u91c7\u7528\u6807\u51c6\u5bc6\u7801\u5b66\u539f\u8bed\uff0c\u8bbe\u8ba1\u7b80\u5355\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u7eb8\u8d28\u6295\u7968\u548c\u73b0\u6709\u8bbe\u8ba1\u5728\u91cf\u5b50\u5a01\u80c1\u4e0b\u53ef\u80fd\u4e0d\u518d\u5b89\u5168\uff0c\u9700\u8981\u4e00\u79cd\u540e\u91cf\u5b50\u3001\u7aef\u5230\u7aef\u53ef\u9a8c\u8bc1\u4e14\u5b9e\u73b0\u53cb\u597d\u7684\u7535\u5b50\u6295\u7968\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u5143\u591a\u9879\u5f0f\u7684\u7aef\u5230\u7aef\u53ef\u9a8c\u8bc1E-Voting\u534f\u8bae\uff0c\u5b89\u5168\u6027\u57fa\u4e8eMQ\u95ee\u9898\u7684\u56f0\u96be\u6027\uff0c\u4f7f\u7528\u6807\u51c6\u5bc6\u7801\u6784\u4ef6\uff0c\u5f3a\u8c03\u5b9e\u73b0\u7b80\u6d01\u548c\u6548\u7387\u3002", "result": "\u7ed9\u51fa\u4e00\u4e2a\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u8bbe\u8ba1\uff0c\u5e76\u7ed9\u51fa\u57fa\u4e8eMQ\u96be\u5ea6\u7684\u5b89\u5168\u6027\u5206\u6790\uff0c\u6ee1\u8db3\u7aef\u5230\u7aef\u53ef\u9a8c\u8bc1\u6027\u3002", "conclusion": "\u57fa\u4e8eMQ\u7684\u540e\u91cf\u5b50E-Voting\u53ef\u884c\uff0c\u63d0\u4f9b\u5bf9\u91cf\u5b50\u653b\u51fb\u7684\u9632\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u73b0\u7b80\u4fbf\u6027\u3002"}}
{"id": "2512.17108", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.17108", "abs": "https://arxiv.org/abs/2512.17108", "authors": ["Kunjal Panchal", "Saayan Mitra", "Somdeb Sarkhel", "Haoliang Wang", "Ishita Dasgupta", "Gang Wu", "Hui Guan"], "title": "Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse", "comment": null, "summary": "Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\\leq$ 2.3 Recall@1 in retrieval, $\\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17722", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17722", "abs": "https://arxiv.org/abs/2512.17722", "authors": ["Paola Di Maio"], "title": "Digital and Web Forensics Model Cards, V1", "comment": null, "summary": "This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.", "AI": {"tldr": "A standardized model-card framework for digital and web forensics, featuring a web-based generator and controlled vocabularies for classification, reasoning, bias, and error categorization; introduces a beta tool and invites community feedback; notes systemic risk due to potential mob influence.", "motivation": "To adapt model-card methodologies for the digital forensics domain, enabling transparent, consistent representation of forensic knowledge and processes, and facilitating evaluation, reuse, and adoption.", "method": "Proposes a web-based framework with defined model-card structure and controlled vocabularies, including classification, types of reasoning, bias identification, and error categorization; provides a beta generator tool and seeks community feedback to refine the standard.", "result": "Description of a beta version of the generator tool, along with the model-card structure and controlled vocabularies; a foundation for systematic representation of forensic knowledge and processes.", "conclusion": "A standardized model-card framework for digital and web forensics can enhance transparency and comparability; iterative refinement via community feedback is planned; the abstract also flags systemic risk from external pressures (mob influence) affecting anti-fraud and forensic processes."}}
{"id": "2512.17109", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17109", "abs": "https://arxiv.org/abs/2512.17109", "authors": ["Alireza Moayedikia", "Alicia Troncoso"], "title": "Bridging Training and Merging Through Momentum-Aware Optimization", "comment": "Proper is work in progress", "summary": "Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5c31\u6301\u7eed\u7ef4\u62a4\u5206\u89e3\u7684\u52a8\u91cf\u548c\u66f2\u7387\u7edf\u8ba1\uff0c\u5e76\u7528\u6765\u8fdb\u884c\u51e0\u4f55\u611f\u77e5\u7684\u6a21\u578b\u5408\u5e76\uff0c\u8fbe\u5230\u4e0e\u6700\u65b0\u4f4e\u79e9\u65b9\u6cd5\u76f8\u5f53\u7684\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u83b7\u5f97\u4efb\u52a1\u91cd\u8981\u6027\u8bc4\u5206\u4ee5\u5b9e\u73b0\u66f4\u4f18\u79c0\u7684\u5408\u5e76\uff0c\u5e76\u7ed9\u51fa\u975e\u51f8\u76ee\u6807\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5c06\u66f2\u7387\u4fe1\u606f\u5728\u8bad\u7ec3\u4e2d\u4ea7\u751f\u540e\u4e22\u5f03\uff0c\u968f\u540e\u5728\u6a21\u578b\u5408\u5e76\u65f6\u91cd\u65b0\u8ba1\u7b97\uff0c\u9020\u6210\u91cd\u590d\u8ba1\u7b97\u5e76\u6d6a\u8d39\u8d44\u6e90\uff1b\u9700\u8981\u4e00\u79cd\u80fd\u5728\u8bad\u7ec3\u4e2d\u5c31\u79ef\u7d2f\u5e76\u91cd\u590d\u4f7f\u7528\u7684\u66f2\u7387-\u52a8\u91cf\u7edf\u8ba1\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u56e0\u5b50\u5316\u7684\u52a8\u91cf\u548c\u66f2\u7387\u7edf\u8ba1\uff0c\u5e76\u7528\u4e8e\u51e0\u4f55\u611f\u77e5\u7684\u6a21\u578b\u5408\u5e76\uff1b\u7d2f\u79ef\u4efb\u52a1\u663e\u8457\u6027\u5f97\u5206\uff0c\u652f\u6301\u66f2\u7387\u611f\u77e5\u7684\u53c2\u6570\u9009\u62e9\uff1b\u5728\u4e0d\u9700\u8981\u540e\u5904\u7406Fisher\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\uff1b\u63d0\u51fa\u9488\u5bf9\u975e\u51f8\u76ee\u6807\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u8bef\u5dee\u754c\u9650\u7531\u68af\u5ea6\u5947\u5f02\u503c\u8870\u51cf\u754c\u5b9a\uff1b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u4e0a\uff0c\u66f2\u7387\u611f\u77e5\u7684\u53c2\u6570\u9009\u62e9\u5728\u6240\u6709\u7a00\u758f\u7a0b\u5ea6\u4e0b\u5747\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u5e45\u5ea6\u9009\u62e9\u7684\u57fa\u7ebf\uff1b\u591a\u4efb\u52a1\u5408\u5e76\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1b\u5185\u5b58\u6548\u7387\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff1b\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3001\u5bf9\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u66f4\u597d\u3002", "conclusion": "\u5c06\u4f18\u5316\u8f68\u8ff9\u89c6\u4e3a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u8d44\u4ea7\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u4f7f\u6a21\u578b\u5408\u5e76\u66f4\u52a0\u6709\u636e\u53ef\u5faa\u4e14\u6210\u672c\u66f4\u4f4e\uff1b\u4e3a\u4f4e\u79e9\u4f18\u5316\u4e0e\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5177\u539f\u5219\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2512.17748", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.17748", "abs": "https://arxiv.org/abs/2512.17748", "authors": ["Aurelia Kusumastuti", "Nikolay Tcholtchev", "Philipp L\u00e4mmel", "Sebastian Bock", "Manfred Hauswirth"], "title": "Methods and Tools for Secure Quantum Clouds with a specific Case Study on Homomorphic Encryption", "comment": "21 pages, 12 figures", "summary": "The rise of quantum computing/technology potentially introduces significant security challenges to cloud computing, necessitating quantum-resistant encryption strategies as well as protection schemes and methods for cloud infrastructures offering quantum computing time and services (i.e. quantum clouds). This research explores various options for securing quantum clouds and ensuring privacy, especially focussing on the integration of homomorphic encryption (HE) into Eclipse Qrisp, a high-level quantum computing framework, to enhance the security of quantum cloud platforms. The study addresses the technical feasibility of integrating HE with Qrisp, evaluates performance trade-offs, and assesses the potential impact on future quantum cloud architectures.The successful implementation and Qrisp integration of three post-quantum cryptographic (PQC) algorithms demonstrates the feasibility of integrating HE with quantum computing frameworks. The findings indicate that while the Quantum One-Time Pad (QOTP) offers simplicity and low overhead, other algorithms like Chen and Gentry-Sahai-Waters (GSW) present performance trade-offs in terms of runtime and memory consumption. The study results in an overall set of recommendations for securing quantum clouds, e.g. implementing HE at data storage and processing levels, developing Quantum Key Distribution (QKD), and enforcing stringent access control and authentication mechanisms as well as participating in PQC standardization efforts.", "AI": {"tldr": "\u5c06\u91cf\u5b50\u4e91\u5b89\u5168\u6027\u4f5c\u4e3a\u6838\u5fc3\uff0c\u7814\u7a76\u5c06\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u4e0e Eclipse Qrisp \u96c6\u6210\u4ee5\u4fdd\u62a4\u91cf\u5b50\u4e91\u5e73\u53f0\uff0c\u8bc4\u4f30\u4e09\u79cd\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u4e0e\u6027\u80fd\uff0c\u7ed9\u51fa\u672a\u6765\u91cf\u5b50\u4e91\u7684\u5b89\u5168\u8981\u70b9\u4e0e\u6807\u51c6\u5316\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u666e\u53ca\uff0c\u4f20\u7edf\u52a0\u5bc6\u9762\u4e34\u6f5c\u5728\u5a01\u80c1\uff0c\u4e9f\u9700\u91cf\u5b50\u6297\u6027\u52a0\u5bc6\u548c\u5bf9\u91cf\u5b50\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u4fdd\u62a4\u673a\u5236\uff1b\u5c06 HE \u878d\u5165\u91cf\u5b50\u8ba1\u7b97\u6846\u67b6\u53ef\u4ee5\u5728\u6570\u636e\u5b58\u50a8\u4e0e\u5904\u7406\u9636\u6bb5\u63d0\u4f9b\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u8bc4\u4f30 HE \u4e0e Eclipse Qrisp \u7684\u6280\u672f\u53ef\u884c\u6027\uff0c\u5206\u6790\u4e09\u79cd PQC \u7b97\u6cd5\u5728\u8be5\u96c6\u6210\u4e2d\u7684\u5b9e\u73b0\u3001\u8fd0\u884c\u65f6\u4e0e\u5185\u5b58\u5f00\u9500\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u91cf\u5b50\u4e91\u4f53\u7cfb\u67b6\u6784\u7684\u5f71\u54cd\u8bc4\u4f30\u4e0e\u5b89\u5168\u7b56\u7565\u5efa\u8bae\u3002", "result": "\u5b9e\u73b0\u5e76\u5c06 HE \u4e0e Eclipse Qrisp \u96c6\u6210\u7684\u4e09\u4e2a PQC \u7b97\u6cd5\u53ef\u884c\uff0cQOTP \u5177\u5907\u7b80\u5355\u6027\u4e0e\u4f4e\u5f00\u9500\uff1bChen \u4e0e GSW \u5728\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u5b58\u5728\u6027\u80fd\u6743\u8861\u3002\u7814\u7a76\u7ed9\u51fa\u5bf9\u91cf\u5b50\u4e91\u5b89\u5168\u7684\u7efc\u5408\u5efa\u8bae\uff0c\u5982\u5728\u6570\u636e\u5b58\u50a8\u4e0e\u5904\u7406\u5c42\u5b9e\u73b0 HE\u3001\u53d1\u5c55\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u3001\u52a0\u5f3a\u8bbf\u95ee\u63a7\u5236/\u8ba4\u8bc1\uff0c\u4ee5\u53ca\u53c2\u4e0e PQC \u6807\u51c6\u5316\u5de5\u4f5c\u3002", "conclusion": "\u5c06 HE \u878d\u5165\u91cf\u5b50\u4e91\u5e73\u53f0\u662f\u53ef\u884c\u4e14\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u7684\u65b9\u5411\uff0c\u672a\u6765\u91cf\u5b50\u4e91\u67b6\u6784\u5e94\u5c06\u6570\u636e\u5c42\u7684\u52a0\u5bc6\u3001\u5bc6\u94a5\u7ba1\u7406\u548c\u8bbf\u95ee\u63a7\u5236\u7b49\u4e0e PQC \u6807\u51c6\u5316\u534f\u540c\u63a8\u8fdb\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u5b89\u5168\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\u3002"}}
{"id": "2512.17111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17111", "abs": "https://arxiv.org/abs/2512.17111", "authors": ["Anjali Sarawgi", "Esteban Garces Arias", "Christof Zotter"], "title": "Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts", "comment": "Under review", "summary": "This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.", "AI": {"tldr": "\u9996\u6b21\u4e3a\u65e7\u5c3c\u6cca\u5c14\u8bed\u5f00\u53d1\u7aef\u5230\u7aef\u624b\u5199\u6587\u672c\u8bc6\u522b\u6d41\u6c34\u7ebf\uff0c\u91c7\u7528\u884c\u7ea7\u8f6c\u5f55\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e0e\u6570\u636e\u9a71\u52a8\u7b56\u7565\uff0c\u6700\u4f73CER\u4e3a4.9%\uff0c\u5e76\u5206\u6790\u89e3\u7801\u7b56\u7565\u4e0e token-level \u6df7\u6dc6\uff1b\u6570\u636e\u96c6\u4fdd\u5bc6\u4f46\u516c\u5f00\u8bad\u7ec3\u4ee3\u7801\u3001\u6a21\u578b\u914d\u7f6e\u4e0e\u8bc4\u4f30\u811a\u672c\u4ee5\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u5386\u53f2\u811a\u672c\u7684\u7814\u7a76\u3002", "motivation": "\u5386\u53f2\u4e0a\u91cd\u8981\u4e14\u8d44\u6e90\u7a00\u7f3a\u7684\u65e7\u5c3c\u6cca\u5c14\u8bed\u624b\u5199\u6587\u672c\u7684\u81ea\u52a8\u8bc6\u522b\u9700\u6c42\uff1b\u63d0\u5347\u6570\u5b57\u5316\u53ca\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff1b\u586b\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684HTR\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8e\u884c\u7ea7\u8f6c\u5f55\uff1b\u6bd4\u8f83\u4e0d\u540c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1b\u5f15\u5165\u6570\u636e\u9a71\u52a8\u7684\u6280\u672f\uff08\u5982\u6570\u636e\u589e\u5f3a\u3001\u6570\u636e\u6e05\u6d17\u3001\u6570\u636e\u91c7\u6837\u7b49\u6570\u636e\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff09\u4ee5\u6539\u8fdb\u8bc6\u522b\uff1b\u5b9e\u73b0\u5e76\u8bc4\u4f30\u89e3\u7801\u7b56\u7565\uff1b\u8fdb\u884c token-level \u6df7\u6dc6\u5206\u6790\u3002", "result": "\u6700\u4f73\u6a21\u578bCER 4.9%\uff1b\u5bf9\u89e3\u7801\u7b56\u7565\u548c token-level \u6df7\u6dc6\u6709\u7cfb\u7edf\u5206\u6790\uff1b\u8bc4\u4f30\u811a\u672c\u516c\u5e03\u4ee5\u4fc3\u8fdb\u590d\u73b0\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0 Old Nepali \u7684\u7aef\u5230\u7aef HTR\uff0c\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u5386\u53f2\u811a\u672c\u9886\u57df\u7814\u7a76\uff1b\u5c3d\u7ba1\u8bc4\u4f30\u6570\u636e\u96c6\u4fdd\u5bc6\uff0c\u4f46\u516c\u5f00\u4ee3\u7801\u4e0e\u914d\u7f6e\u63d0\u5347\u900f\u660e\u5ea6\uff1b\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u591a\u5386\u53f2\u8bed\u8a00\u4e0e\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002"}}
{"id": "2512.17375", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.17375", "abs": "https://arxiv.org/abs/2512.17375", "authors": ["Tung-Ling Li", "Yuhao Wu", "Hongliang Liu"], "title": "AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens", "comment": null, "summary": "Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a reward \u6a21\u578b/\u8bc4\u5224\u7cfb\u7edf\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6613\u88ab\u77ed\u5e8f\u5217\u4f4e\u56f0\u60d1\u5ea6\u63a7\u5236\u4ee4\u724c\u6240\u6b3a\u9a97\uff0c\u901a\u8fc7\u5f71\u54cd\u672b\u5c42\u5bf9\u6570\u5dee\u6765\u628a\u6b63\u786e\u7684\u201cNo\u201d\u5224\u4e3a\u201cYes\u201d\uff1b\u63d0\u51fa AdvJudge-Zero \u4ece\u96f6\u5f00\u59cb\u5229\u7528\u4e0b\u4e00\u65f6\u5e8f\u5206\u5e03\u548c\u675f\u641c\u7d22\u53d1\u73b0\u591a\u6837\u5316\u63a7\u5236\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u53d1\u73b0\u6270\u52a8\u96c6\u4e2d\u5728\u4e00\u4e2a\u4f4e\u79e9\u8f6f\u6a21\u5f0f\uff0c\u4e0e\u8bc4\u5224\u62d2\u7edd\u65b9\u5411\u4e0d\u5bf9\u9f50\uff1b\u5b9e\u9a8c\u8bc1\u660e\u5728\u5927\u89c4\u6a21\u5f00\u6e90\u6743\u91cd\u548c\u4e13\u4e1a\u5316\u8bc4\u4f30\u6a21\u578b\u4e0a\uff0c\u63a7\u5236\u4ee4\u724c\u4f1a\u4ea7\u751f\u9ad8\u5047\u9633\u6027\u7387\uff1b\u901a\u8fc7\u4ee5 LoRA \u4e3a\u57fa\u7840\u7684\u5c0f\u6837\u672c\u5bf9\u6297\u8bad\u7ec3\u53ef\u663e\u8457\u964d\u4f4e\u5047\u9633\u6027\uff0c\u540c\u65f6\u5c3d\u91cf\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u63d0\u5347\u540e\u8bad\u7ec3\u7ba1\u7ebf\u5982 RLHF\u3001DPO\u3001RLAIF \u4e2d\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u9632\u6b62\u73b0\u5b9e\u4e16\u754c\u7b56\u7565\u8f93\u51fa\u901a\u8fc7\u63a7\u5236\u4ee4\u724c\u5e72\u6270\u4e8c\u5206\u7c7b\u8bc4\u4f30\uff0c\u786e\u4fdd\u8bc4\u4f30\u8fc7\u7a0b\u7684\u53ef\u9760\u6027\u3002", "method": "AdvJudge-Zero \u901a\u8fc7\u5229\u7528\u6a21\u578b\u7684\u4e0b\u4e00\u65f6\u5e8f\u5206\u5e03\u548c\u675f\u641c\u7d22\u4ece\u96f6\u5f00\u59cb\u63a2\u7d22\u591a\u6837\u5316\u7684\u63a7\u5236\u4ee4\u724c\u5e8f\u5217\uff1b\u5bf9\u8bf1\u5bfc\u7684\u9690\u85cf\u72b6\u6001\u6270\u52a8\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u6270\u52a8\u96c6\u4e2d\u5728\u4e00\u4e2a\u4f4e\u79e9\u7684\u201c\u8f6f\u6a21\u5f0f\u201d\uff0c\u5e76\u4e14\u8be5\u6a21\u5f0f\u4e0e\u8bc4\u5224\u5668\u7684\u62d2\u7edd\u65b9\u5411\u4e0d\u5bf9\u9f50\uff1b\u5728\u5927\u89c4\u6a21\u5f00\u6e90\u6743\u91cd\u548c\u4e13\u4e1a\u5316 judge \u6a21\u578b\u4e0a\u8bc4\u4f30\u5176\u5bf9\u6570\u5b66\u548c\u63a8\u7406\u57fa\u51c6\u7684\u5f71\u54cd\u3002", "result": "\u63a7\u5236\u4ee4\u724c\u5bfc\u81f4\u5f88\u9ad8\u7684\u5047\u9633\u6027\u7387\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u4e0e\u63a8\u7406\u57fa\u51c6\u4e0a\u5bf9\u9519\u8bef\u7b54\u6848\u7684\u8bc4\u4f30\u4e2d\uff1b\u91c7\u7528 LoRA \u57fa\u7840\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u5bf9\u5c11\u91cf\u5e26\u63a7\u5236\u4ee4\u724c\u7684\u589e\u5f3a\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u540e\uff0c\u663e\u8457\u964d\u4f4e\u5047\u9633\u6027\uff0c\u540c\u65f6\u5c3d\u91cf\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002", "conclusion": "\u63ed\u793a\u8bc4\u4f30\u7cfb\u7edf\u7684\u6f5c\u5728\u8106\u5f31\u6027\u53ca\u5176\u73b0\u5b9e\u653b\u51fb\u9762\uff0c\u63d0\u51fa\u57fa\u4e8e\u5bf9\u6297\u6027\u8bad\u7ec3\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u9700\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u4e0b\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e0e\u6269\u5c55\u3002"}}
{"id": "2512.17121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17121", "abs": "https://arxiv.org/abs/2512.17121", "authors": ["Jasmine Vu", "Shivanand Sheshappanavar"], "title": "The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining", "comment": "10 pages, 7 figures, submitted to WACV Pixels to Patients Workshop", "summary": "Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.", "AI": {"tldr": "\u5bf9\u57fa\u4e8e CLIP \u7684\u80f8\u90e8 X \u7ebf\u68c0\u7d22\u5728\u542b\u5426\u5b9a\u8bcd\u4e0e\u4e0d\u542b\u5426\u5b9a\u8bcd\u7684\u63d0\u793a\u4e0b\u8fdb\u884c\u8bc4\u4f30\u4e0e\u5fae\u8c03\u3002\u7ed3\u679c\u663e\u793a\u5728\u5904\u7406\u5426\u5b9a\u8868\u8fbe\u65b9\u9762\u6709\u6240\u63d0\u5347\uff0c\u4f46\u6b63\u5411\u63d0\u793a\u7684\u51c6\u786e\u5ea6\u7565\u6709\u4e0b\u964d\uff1b\u5e76\u901a\u8fc7 token attribution\u3001t-SNE \u6295\u5f71\u548c\u6ce8\u610f\u529b\u5934\u6d88\u878d\u7b49\u5206\u6790\u7814\u7a76\u5fae\u8c03\u5bf9\u6587\u672c\u7f16\u7801\u5668\u5728\u5426\u5b9a\u6027\u4e34\u5e8a\u8bed\u8a00\u4e0a\u7684\u5185\u90e8\u8868\u793a\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3 CLIP \u5728\u533b\u5b66\u8bed\u8a00\u4e2d\u7684\u5426\u5b9a\u8868\u8fbe\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5728\u533b\u7597\u5f71\u50cf\u68c0\u7d22\u3001\u62a5\u544a\u751f\u6210\u4e0e\u8bca\u65ad\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u5728 Stanford AIMI CheXagent \u6a21\u578b\u4e0a\u8bc4\u4f30\u80f8\u90e8 X \u7ebf\u56fe\u50cf\u68c0\u7d22\uff0c\u4f7f\u7528\u5305\u542b\u4e0e\u4e0d\u5305\u542b\u5426\u5b9a\u7684\u63d0\u793a\uff1b\u5e94\u7528\u57fa\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u7684\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff1b\u7ed3\u5408 token attribution\u3001t-SNE \u6295\u5f71\u548c\u6ce8\u610f\u529b\u5934\u6d88\u878d\u7b49\u5206\u6790\uff0c\u7814\u7a76\u5fae\u8c03\u5982\u4f55\u6539\u53d8\u6587\u672c\u7f16\u7801\u5668\u5bf9\u5426\u5b9a\u4e34\u5e8a\u8bed\u8a00\u7684\u8868\u5f81\u3002", "result": "\u5728\u5904\u7406\u5426\u5b9a\u63d0\u793a\u65b9\u9762\u5b9e\u73b0\u6539\u8fdb\uff1b\u6b63\u5411\u63d0\u793a\u8bc4\u4f30\u7684\u51c6\u786e\u5ea6\u7565\u6709\u4e0b\u964d\uff1b\u5e76\u5bf9\u6a21\u578b\u5185\u90e8\u884c\u4e3a\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u63ed\u793a\u5fae\u8c03\u5982\u4f55\u91cd\u5851\u5426\u5b9a\u6027\u8868\u8fbe\u7684\u8868\u793a\u53ca\u5176\u5bf9\u68c0\u7d22\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "conclusion": "\u5bf9 CLIP \u5728\u533b\u5b66\u573a\u666f\u4e2d\u7684\u5426\u5b9a\u5904\u7406\u6709\u66f4\u6e05\u6670\u7684\u8ba4\u8bc6\uff0c\u57fa\u4e8e\u4e34\u5e8a\u8bed\u8a00\u7684\u5fae\u8c03\u6709\u671b\u63d0\u5347\u5176\u5728\u533b\u7597 AI \u8bbe\u5907\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4f46\u9700\u5728\u5426\u5b9a\u4e0e\u80af\u5b9a\u63d0\u793a\u7684\u6027\u80fd\u6743\u8861\u4e2d\u8fdb\u884c\u4f18\u5316\u3002"}}
{"id": "2512.17129", "categories": ["cs.LG", "cs.MA", "cs.RO", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.17129", "abs": "https://arxiv.org/abs/2512.17129", "authors": ["Seong Ho Pahng", "Guoye Guan", "Benjamin Fefferman", "Sahand Hormoz"], "title": "DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations", "comment": null, "summary": "Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.", "AI": {"tldr": "DiffeoMorph learns a morphogenesis protocol for multi-agent systems to morph into a target 3D shape using an SE(3)-equivariant GNN and a Zernike-based, rotation-aligned shape-matching loss; it employs a bilevel optimization with implicit differentiation to align predicted and target shapes, enabling formation of simple to complex morphologies from minimal cues.", "motivation": "Understand how local, distributed agent interactions can yield precise global morphologies and provide a differentiable framework for programmable morphogenesis applicable to biology-inspired systems, robotics, and multi-agent learning.", "method": "Agents update positions and internal states via an attention-based SE(3)-equivariant graph neural network. A new continuous shape-matching loss based on 3D Zernike polynomials compares the predicted shape to the target as a spatial distribution, invariant to agent ordering, number, and rigid transforms. An alignment step rotates the predicted Zernike spectrum to best match the target, enforcing full SO(3) invariance. This creates a bilevel optimization: inner loop optimizes the best-in-class unit quaternion for alignment; outer loop updates the agent model. Gradients are computed through the alignment via implicit differentiation. The approach is benchmarked against standard shape metrics.", "result": "The framework can morph populations into shapes ranging from ellipsoids to more complex morphologies using minimal spatial cues, with empirical evidence that the Zernike-based loss offers advantages over conventional distance metrics for shape comparison.", "conclusion": "DiffeoMorph provides a differentiable, rotation-aware pathway to programmable morphogenesis in distributed agent systems, enabling robust formation of diverse 3D shapes and offering a competitive loss for shape matching over existing metrics."}}
{"id": "2512.17527", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.17527", "abs": "https://arxiv.org/abs/2512.17527", "authors": ["Muhammad Haris Khan"], "title": "SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals", "comment": null, "summary": "Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.", "AI": {"tldr": "SafeBench-Seq\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5143\u6570\u636e\u3001\u53ef\u590d\u73b0\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u7ea7\u98ce\u9669\u7b5b\u67e5\u57fa\u51c6\uff0c\u4f7f\u7528<=40%\u540c\u6e90\u6027\u805a\u7c7b\u5206\u5272\uff0cCPU \u4e0a\u53ef\u8fd0\u884c\uff0c\u63d0\u4f9b\u53ef\u6821\u51c6\u7684\u6982\u7387\u8f93\u51fa\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u57fa\u7840\u6a21\u578b\u5e26\u6765\u751f\u7269\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u7f3a\u4e4f\u7b80\u5355\u3001\u53ef\u590d\u73b0\u3001\u5728\u540c\u6e90\u63a7\u5236\u4e0b\u7684\u5e8f\u5217\u7ea7\u57fa\u7ebf\u7b5b\u67e5\u5de5\u5177\u3002", "method": "\u6784\u5efa SafeBench-Seq\uff1a\u6765\u81ea\u516c\u5f00\u6570\u636e SafeProtein hazards \u4e0e UniProt benigns\uff1b\u7279\u5f81\u9009\u53d6\u5168\u5c40\u7269\u7406\u5316\u5b66\u63cf\u8ff0\u5b50\u548c\u6c28\u57fa\u9178\u7ec4\u6210\uff1b\u5bf9\u8054\u5408\u6570\u636e\u96c6\u8fdb\u884c <=40% \u76f8\u4f3c\u6027\u805a\u7c7b\u4ee5\u5b9e\u73b0\u7c07\u7ea7 Holdout\uff1b\u8bad\u7ec3\u5e76\u6bd4\u8f83\u7ebf\u6027\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u7ebf\u6027 SVM\uff09\u548c\u6811\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u7b49\uff09\u5728\u6821\u51c6\u6982\u7387\uff08CalibratedClassifierCV\uff09\u4e0b\u7684\u8868\u73b0\uff1b\u8bc4\u4f30\u6307\u6807\u5305\u62ec AUROC\u3001AUPRC\u3001TPR@1%FPR\u3001FPR@95%TPR\uff0c\u4ee5\u53ca Brier\u3001ECE\u3001\u53ef\u9760\u6027\u56fe\uff1b\u8fd8\u901a\u8fc7\u6210\u5206\u4fdd\u6301\u7f6e\u6362\u3001\u957f\u5ea6/\u7ec4\u6210\u6d88\u878d\u7b49\u65b9\u6cd5\u63a2\u7a76\u6613\u53d7\u6377\u5f84\u5f71\u54cd\uff1b\u8f93\u51fa\u4e3a\u5143\u6570\u636e\uff08\u805a\u7c7b ID\u3001\u5206\u5272\u6807\u7b7e\u7b49\uff09\uff0cCPU-only\uff0c\u4e14\u4e0d\u5206\u53d1\u6709\u5bb3\u5e8f\u5217\u3002", "result": "\u968f\u673a\u5212\u5206\u9ad8\u4f30\u9c81\u68d2\u6027\uff0c\u805a\u7c7b\u5206\u5272\u66f4\u63a5\u8fd1\u201c\u4ece\u672a\u89c1\u8fc7\u5a01\u80c1\u201d\u7684\u60c5\u5f62\uff1b\u7ebf\u6027\u6a21\u578b\u8f83\u6709\u826f\u597d\u6821\u51c6\uff0c\u6811\u6a21\u578b\u7684 Brier \u548c ECE \u7565\u9ad8\uff1b\u63d0\u4f9b\u4e86\u6982\u7387\u6821\u51c6\u7684\u57fa\u7ebf\uff0c\u65b9\u4fbf\u590d\u73b0\u5b9e\u9a8c\u3002", "conclusion": "SafeBench-Seq \u63d0\u4f9b\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u5143\u6570\u636e\u4ec5\u4f9d\u8d56\u7684\u5e8f\u5217\u7ea7\u57fa\u51c6\uff0c\u4e3a\u540c\u6e90\u63a7\u5236\u7684\u98ce\u9669\u7b5b\u67e5\u63d0\u4f9b\u57fa\u7ebf\uff0c\u5e76\u5f3a\u8c03\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u5b89\u5168\u8bc4\u4f30\u4e2d\u9700\u8981\u8003\u8651\u805a\u7c7b\u5206\u5272\u548c\u6982\u7387\u6821\u51c6\u7b49\u56e0\u7d20\u3002"}}
{"id": "2512.17131", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17131", "abs": "https://arxiv.org/abs/2512.17131", "authors": ["Aaron Defazio", "Konstantin Mishchenko", "Parameswaran Raman", "Hao-Jun Michael Shi", "Lin Xiao"], "title": "Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs", "comment": null, "summary": "We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17198", "abs": "https://arxiv.org/abs/2512.17198", "authors": ["Shao-Ting Chiu", "Ioannis G. Kevrekidis", "Ulisses Braga-Neto"], "title": "BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions", "comment": null, "summary": "We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17203", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.17203", "abs": "https://arxiv.org/abs/2512.17203", "authors": ["Jiwoo Song", "Daning Huang", "John Harlim"], "title": "Learning solution operator of dynamical systems with diffusion maps kernel ridge regression", "comment": null, "summary": "Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17257", "abs": "https://arxiv.org/abs/2512.17257", "authors": ["Iason Kyriakopoulos", "Yannis Theodoridis"], "title": "Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods", "comment": "18 pages, 2 figures, 5 tables", "summary": "With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.", "AI": {"tldr": "\u7cfb\u7edf\u6027\u6bd4\u8f835\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u548c\u7a7a\u95f4\u805a\u5408\u5c42\u7ea7\u4e0a\u7684EV\u5145\u7535\u9700\u6c42\u9884\u6d4b\uff0c\u57fa\u4e8e4\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u7535\u52a8\u8f66\u5145\u7535\u9700\u6c42\u5bf9\u7535\u7f51\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5f25\u8865\u5728\u591a\u65f6\u7a7a\u5c3a\u5ea6\u548c\u591a\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u8bc4\u4f30\u4e94\u79cd\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08\u6db5\u76d6\u4f20\u7edf\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\uff09\u5728\u77ed/\u4e2d/\u957f\u671f\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u53ca\u4ece\u5355\u4e00\u5145\u7535\u7ad9\u5230\u533a\u57df\u548c\u57ce\u5e02\u7ea7\u522b\u7684\u4e0d\u540c\u7a7a\u95f4\u805a\u5408\u3002\u6570\u636e\u6765\u81ea\u56db\u4e2a\u516c\u5f00\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u9010\u4e00\u62a5\u544a\u7ed3\u679c\u3002", "result": "\u5404\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u7a7a\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u8981\u5bf9\u65f6\u7a7a\u5c3a\u5ea6\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u548c\u6df7\u5408\u65b9\u6cd5\u7684\u8003\u8651\uff1b\u5728\u56db\u6570\u636e\u96c6\u4e0a\u7684\u4e00\u81f4\u6027\u589e\u5f3a\u4e86\u7ed3\u8bba\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u9996\u6b21\u5728\u5982\u6b64\u5e7f\u6cdb\u7684\u65f6\u7a7a\u7c92\u5ea6\u548c\u591a\u6570\u636e\u96c6\u8303\u56f4\u5185\u7cfb\u7edf\u8bc4\u4f30EV\u5145\u7535\u9700\u6c42\u9884\u6d4b\uff0c\u63d0\u4f9b\u5bf9\u7814\u7a76\u4e0e\u5e94\u7528\u7684\u5168\u9762\u89c1\u89e3\u3002"}}
{"id": "2512.17270", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17270", "abs": "https://arxiv.org/abs/2512.17270", "authors": ["Yongqi Li", "Hao Lang", "Fei Huang", "Tieyun Qian", "Yongbin Li"], "title": "Understanding Generalization in Role-Playing Models via Information Theory", "comment": null, "summary": "Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u7406\u8bba\u5ea6\u91cfR-EMID\u6765\u8bc4\u4f30\u89d2\u8272\u626e\u6f14\u6a21\u578b\uff08RPM\uff09\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u6cdb\u5316\u9000\u5316\uff0c\u5e76\u7ed9\u51fa\u4e0a\u754c\u9884\u6d4b\u6700\u574f\u60c5\u5f62\uff1b\u540c\u65f6\u901a\u8fc7\u4e00\u4e2a\u5171\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u6539\u8fdb\u5bf9\u8bdd\u751f\u6210\u6982\u7387\u7684\u4f30\u7b97\u4ee5\u8ba1\u7b97R-EMID\uff0c\u5e76\u5728\u5b9e\u8bc1\u4e2d\u53d1\u73b0\u7528\u6237\u6f02\u79fb\u98ce\u9669\u6700\u9ad8\uff0cRL\u662f\u63d0\u5347\u6cdb\u5316\u7684\u6700\u6709\u6548\u65b9\u6cd5\u3002", "motivation": "RPM\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u7528\u6237\u3001\u89d2\u8272\u4e0e\u5bf9\u8bdd\u7b49\u5206\u5e03\u6f02\u79fb\u800c\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff09\u65e0\u6cd5\u5bf9\u8fd9\u79cd\u9000\u5316\u7ed9\u51fa\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u4e0e\u6846\u67b6\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u89e3\u91ca\u7684\u5ea6\u91cf\u4e0e\u53bb\u8026\u5408\u7684\u7406\u8bba\u5206\u6790\u6765\u7406\u89e3\u5404\u7c7b\u6f02\u79fb\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faR-EMID\u4f5c\u4e3a\u4fe1\u606f\u8bba\u89d2\u5ea6\u7684\u5ea6\u91cf\uff0c\u5e76\u63a8\u5bfc\u51fa\u5176\u4e0a\u754c\u4ee5\u9884\u6d4b\u6700\u574f-case\u6cdb\u5316\uff1b\u5efa\u7acb\u4e00\u4e2a\u5171\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u5efa\u6a21\u7528\u6237\u3001\u89d2\u8272\u3001\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u8026\u5408\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u5bf9\u8bdd\u751f\u6210\u6982\u7387\u7684\u4f30\u8ba1\u4ee5\u4fbf\u8ba1\u7b97R-EMID\u3002", "result": "\u5728\u591a\u4e2aRPMs\u7684\u6cdb\u5316\u8bc4\u4f30\u4e2d\uff0cR-EMID\u63ed\u793a\u7528\u6237\u6f02\u79fb\u98ce\u9669\u6700\u5927\uff1b\u4f7f\u7528RL\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347RPM\u7684\u6cdb\u5316\u8868\u73b0\u3002", "conclusion": "R-EMID\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6027\u80fd\u9000\u5316\u5ea6\u91cf\u548c\u4e0a\u754c\uff0c\u63ed\u793a\u4e0d\u540c\u6f02\u79fb\u8d21\u732e\uff1b\u5171\u8fdb\u5316RL\u6846\u67b6\u6709\u6548\u63d0\u5347\u5bf9\u8bdd\u751f\u6210\u6982\u7387\u4f30\u8ba1\u548cRPM\u6cdb\u5316\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u6f02\u79fb\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2512.17273", "categories": ["cs.LG", "math-ph", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.17273", "abs": "https://arxiv.org/abs/2512.17273", "authors": ["Farinaz Mostajeran", "Aruzhan Tleubek", "Salah A Faroughi"], "title": "MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics", "comment": null, "summary": "Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.", "AI": {"tldr": "MINPO \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u975e\u5c40\u90e8\u7b97\u5b50\u53ca\u5176\u9006\u7b97\u5b50\uff0c\u4f7f\u7528\u8bb0\u5fc6\u611f\u77e5\u7f16\u7801\u5668\uff08KAN \u6216 MLP\uff09\u5e76\u901a\u8fc7\u975e\u5c40\u90e8\u4e00\u81f4\u6027\u635f\u5931\u5b9e\u73b0\u89e3\u573a\u91cd\u6784\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u975e\u5c40\u90e8 IDEs\uff08\u5305\u62ec\u5206\u6570\u9636 PDEs\uff09\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u795e\u7ecf\u65b9\u6cd5\u3002", "motivation": "\u8bb8\u591a\u7269\u7406\u7cfb\u7edf\u7531\u975e\u5c40\u90e8\u65f6\u7a7a\u884c\u4e3a\u9a71\u52a8\uff0c\u9700\u6c42\u89e3\u79ef\u5206-\u5fae\u5206\u65b9\u7a0b\uff08IDEs\uff09\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5377\u79ef\u79ef\u5206\u8ba1\u7b97\u4e0a\u6210\u672c\u9ad8\uff0c\u4e14\u73b0\u6709\u795e\u7ecf\u6c42\u89e3\u5668\u96be\u4ee5\u8de8\u8d8a\u591a\u6837\u7684\u975e\u5c40\u90e8\u7ed3\u6784\u5b9e\u73b0\u6cdb\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6db5\u76d6\u957f\u7a0b\u7a7a\u95f4\u76f8\u4e92\u4f5c\u7528\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa Memory-Informed Neural Pseudo-Operator (MINPO)\u3002MINPO \u53ef\u9009\u62e9\u4f7f\u7528 Kolmogorov-Arnold Networks (KANs) \u6216\u591a\u5c42\u611f\u77e5\u673a\uff08MLPs\uff09\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u76f4\u63a5\u5b66\u4e60\u975e\u5c40\u90e8\u7b97\u5b50\u53ca\u5176\u9006\u7b97\u5b50\uff0c\u5e76\u663e\u5f0f\u91cd\u6784\u672a\u77e5\u89e3\u573a\u3002\u4e3a\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u975e\u5c40\u90e8\u4e00\u81f4\u6027\u635f\u5931\uff0c\u589e\u5f3a\u5b66\u4e60\u7684\u7b97\u5b50\u4e0e\u91cd\u6784\u89e3\u7684\u4e00\u81f4\u6027\u3002MINPO \u80fd\u81ea\u7136\u6355\u6349\u5e76\u9ad8\u6548\u6c42\u89e3\u7531\u957f\u7a0b\u975e\u5c40\u90e8\u4f9d\u8d56\u9a71\u52a8\u7684\u5e7f\u8c31 IDEs \u53ca\u5176\u5b50\u96c6\uff08\u542b\u5206\u6570\u9636 PDEs\uff09\u3002", "result": "\u4e0e\u7ecf\u5178\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u57fa\u65b9\u6cd5\uff08\u5982 A-PINN\u3001fPINN\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684 A-PIKAN\u3001fPIKAN\uff09\u5bf9\u6bd4\uff0cMINPO \u5728\u591a\u6837\u5316\u6838\u7c7b\u578b\u3001\u6838\u7ef4\u5ea6\u4ee5\u53ca\u91cd\u590d\u8ba1\u7b97\u6838\u79ef\u5206\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u60c5\u5f62\u4e0b\u4ecd\u5c55\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff1b\u80fd\u591f\u5904\u7406\u4e0d\u540c\u6838\u7ed3\u6784\u548c\u9ad8\u7ef4\u5ea6\u60c5\u5f62\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "MINPO \u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d85\u8d8a\u7279\u5b9a\u95ee\u9898\u8bbe\u5b9a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7531\u975e\u5c40\u90e8\u7b97\u5b50\u652f\u914d\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u66f4\u5e7f\u6cdb\u7684 IDEs \u4e0e\u5b50\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u4e0e\u8bb0\u5fc6\u6548\u5e94\u7684\u5efa\u6a21\u4e0e\u6c42\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.17276", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17276", "abs": "https://arxiv.org/abs/2512.17276", "authors": ["Alireza Moayedikia", "Sara Fin"], "title": "Alzheimer's Disease Brain Network Mining", "comment": null, "summary": "Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.", "AI": {"tldr": "A semi-supervised, multi-view learning framework MATCH-AD combines deep representations, graph-based label propagation, and optimal transport to diagnose Alzheimer\u2019s disease with scarce labels across multimodal neuroimaging data, achieving near-perfect accuracy on ~5k subjects and providing convergence guarantees.", "motivation": "Clinical assessment for AD is expensive and invasive, leading to labels available for only a fraction of neuroimaging data; leverage unlabeled data and multiple modalities to improve diagnosis.", "method": "MATCH-AD integrates deep representation learning, graph-based label propagation, and Wasserstein (optimal transport) distances within a semi-supervised, multi-view framework to propagate diagnostic information and model disease progression across cognitive states.", "result": "On ~5000 subjects from NACC with structural MRI, CSF biomarkers, and clinical variables, MATCH-AD achieves near-perfect diagnostic accuracy with <1/3 labeled, outperforms baselines, kappa indicating almost perfect agreement, and provides theoretical bounds on label propagation error and transport stability.", "conclusion": "Principled semi-supervised learning can unlock diagnostic potential of large partially annotated neuroimaging datasets, reducing annotation burden while maintaining clinical accuracy for deployment."}}
{"id": "2512.17299", "categories": ["cs.LG", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.17299", "abs": "https://arxiv.org/abs/2512.17299", "authors": ["Abdullah M. Zyarah", "Dhireesha Kudithipudi"], "title": "M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge", "comment": null, "summary": "Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.", "AI": {"tldr": "M2RU\u662f\u4e00\u79cd\u6df7\u5408\u4fe1\u53f7\u9012\u5f52\u5355\u5143\u52a0\u901f\u5668\uff0c\u652f\u6301\u7247\u4e0a\u6301\u7eed\u5b66\u4e60\uff0c\u9762\u5411\u8fb9\u7f18\u65f6\u95f4\u5904\u7406\uff0c\u5177\u9ad8\u80fd\u6548\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u9ad8\u80fd\u8017\u548c\u9891\u7e41\u6570\u636e\u79fb\u52a8\u7684\u95ee\u9898\uff0c\u9700\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faM2RU\u6df7\u5408\u4fe1\u53f7\u67b6\u6784\uff0c\u91c7\u7528minion\u9012\u5f52\u5355\u5143\uff0c\u6574\u5408weighted-bit streaming\u3001\u8de8\u77e9\u9635\u5904\u7406\u591a\u6bd4\u7279\u8f93\u5165\uff0c\u4ee5\u53ca\u7ecf\u9a8c\u56de\u653e\u4ee5\u5728\u57df\u6f02\u79fb\u4e0b\u7a33\u5b9a\u5b66\u4e60\uff1b\u5b9e\u73b0\u7247\u4e0a\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5b9e\u73b015 GOPS\u300148.62 mW\u3001312 GOPS/W\uff1b\u5728\u987a\u5e8fMNIST\u548c CIFAR-10\u4efb\u52a1\u4e0a\u7cbe\u5ea6\u4e0e\u8f6f\u4ef6\u57fa\u7ebf\u76f8\u5dee\u4e0d\u8d85\u8fc75%\uff1b\u4e0eCMOS\u6570\u5b57\u8bbe\u8ba1\u76f8\u6bd4\u80fd\u6e90\u6548\u7387\u63d0\u5347\u7ea629X\uff1b\u5728\u6301\u7eed\u5b66\u4e60\u8d1f\u8f7d\u4e0b\u8bbe\u5907\u53ef\u7528\u5bff\u547d\u7ea612.2\u5e74\u3002", "conclusion": "M2RU\u4e3a\u8fb9\u7f18\u7aef\u65f6\u95f4\u667a\u80fd\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9ad8\u80fd\u6548\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u5e73\u53f0\u3002"}}
{"id": "2512.17325", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17325", "abs": "https://arxiv.org/abs/2512.17325", "authors": ["Chaeha Kim"], "title": "Task Schema and Binding: A Double Dissociation Study of In-Context Learning", "comment": "20pages, 2figures", "summary": "We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.", "AI": {"tldr": "ICL\u7531\u4e24\u5927\u5206\u79bb\u673a\u5236\u7ec4\u6210\uff1a\u4efb\u52a1\u6a21\u5f0f\u8bc6\u522b\uff08Task Schema\uff09\u4e0e\u5177\u4f53\u8f93\u5165\u8f93\u51fa\u7ed1\u5b9a\uff08Binding\uff09\uff0c\u5e76\u5728\u8de8\u6a21\u578b\u8865\u4e01\u5b9e\u9a8c\u4e2d\u8bc1\u5b9e\u5176\u53ef\u5206\u79bb\u6027\u4ee5\u53ca\u8de8\u67b6\u6784\u7684\u4e00\u822c\u6027\uff0c\u5e76\u89e3\u91ca\u4e86\u5148\u9a8c\u77e5\u8bc6\u5bf9\u6a21\u5f0f\u4f9d\u8d56\u7684\u5f71\u54cd\u53ca\u5bf9\u63d0\u793a\u8bbe\u8ba1\u7684\u542f\u793a\u3002", "motivation": "\u6311\u6218\u628aICL\u89c6\u4e3a\u5355\u4e00\u673a\u5236\u7684\u89c2\u70b9\uff0c\u63d0\u4f9b\u53cc\u8fc7\u7a0b\u7406\u8bba\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u89e3\u91caICL\u73b0\u8c61\u80cc\u540e\u7684\u56e0\u679c\u673a\u5236\u3002", "method": "\u57289\u4e2a\u6a21\u578b\u30017\u4e2aTransformer\u5bb6\u65cf\u4ee5\u53caMamba\uff08370M-13B\u53c2\u6570\uff09\u4e2d\uff0c\u8fdb\u884c\u6fc0\u6d3bpatching\u5b9e\u9a8c\uff1b\u901a\u8fc7\u5bf9late MLP\u5c42\u8865\u4e01\u4f20\u9012Task Schema\uff0c\u4ee5\u53ca\u5bf9\u6b8b\u5dee\u6d41\u8865\u4e01\u4f20\u9012Binding\uff0c\u6765\u68c0\u9a8c\u4e24\u79cd\u673a\u5236\u7684\u53ef\u4f20\u9012\u6027\u4e0e\u8026\u5408\u6027\u3002", "result": "\u5f97\u5230\u4e09\u70b9\u53d1\u73b0\uff1a1) \u53cc\u91cd\u89e3\u8026\u8bc1\u636e\uff1aTask Schema\u901a\u8fc7\u665a\u671fMLP\u8865\u4e01\u4f20\u9012\u8fbe\u5230100%\uff0cBinding\u901a\u8fc7\u6b8b\u5dee\u6d41\u8865\u4e01\u4f20\u9012\u7ea662%\uff1b2) Prior-Schema\u6743\u8861\uff1a\u5f53\u5b58\u5728\u5148\u9a8c\u77e5\u8bc6\u65f6\uff0cSchema\u4f9d\u8d56\u4e0e\u5148\u9a8c\u77e5\u8bc6\u5448\u8d1f\u76f8\u5173\uff08Spearman rho=-0.596\uff0cp<0.001\uff0cN=28\u4e2a\u4efb\u52a1-\u6a21\u578b\u5bf9\uff09\uff1b3) \u67b6\u6784\u666e\u9002\u6027\uff1a\u8be5\u673a\u5236\u5728\u6240\u6709\u6d4b\u8bd5\u67b6\u6784\u4e0a\u6210\u7acb\uff0c\u5305\u62ec\u975eTransformer\u7684Mamba\u3002\u5e76\u7ed9\u51fa\u5bf9ICL\u4e4b\u201c\u5355\u4e00\u673a\u5236\u201d\u7684\u53cd\u9a73\uff0c\u63d0\u51fa\u4e24\u79cd\u673a\u5236\u5728\u795e\u7ecf\u5c42\u9762\u7684\u56e0\u679c\u5206\u79bb\u3002", "conclusion": "ICL\u5177\u6709\u53cc\u8fc7\u7a0b\u6027\u8d28\uff0cTask Schema\u5728\u7f3a\u4e4f\u5148\u9a8c\u65f6\u53d1\u6325\u4f5c\u7528\uff0cBinding\u5728\u9ad8\u5148\u9a8c\u65f6\u6613\u53d7\u6ce8\u610f\u529b\u9519 routing\u7684\u5e72\u6270\u800c\u975e\u76f4\u63a5\u8f93\u51fa\u7ade\u4e89\uff1b\u8fd9\u4e00\u53d1\u73b0\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u65b9\u5411\uff1a\u901a\u8fc7\u63d0\u5347\u6a21\u5f0f\u8f6c\u79fb\u6548\u7387\u53ef\u51cf\u5c11\u793a\u4f8b\u9700\u6c42\uff0c\u540c\u65f6\u5728\u9ad8\u5148\u9a8c\u60c5\u5883\u4e0b\u901a\u8fc7\u8bbe\u8ba1\u6765\u964d\u4f4e\u7ed1\u5b9a\u5931\u8d25\u7387\uff08\u7ea638%\uff09\uff0c\u63d0\u5347\u751f\u4ea7\u4e2d\u7684ICL\u53ef\u9760\u6027\u3002\u6700\u7ec8\uff0c\u89e3\u91ca\u4e86ICL\u96be\u9898\u7684\u539f\u56e0\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u8de8\u67b6\u6784\u3001\u8de8\u4efb\u52a1\u7684\u901a\u7528ICL\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.17367", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17367", "abs": "https://arxiv.org/abs/2512.17367", "authors": ["Yidong Chai", "Yi Liu", "Mohammadreza Ebrahimi", "Weifeng Li", "Balaji Padmanabhan"], "title": "Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach", "comment": null, "summary": "Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0e\u805a\u5408\u7684\u5bf9\u6297\u9c81\u68d2\u68c0\u6d4b\u6846\u67b6\uff08LLM-SGA\uff09\u53ca\u5176\u5b9e\u73b0\u7684 ARHOCD \u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u591a\u68c0\u6d4b\u5668\u96c6\u6210\u3001\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u6743\u91cd\u5206\u914d\u4e0e\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728 hate speech\u3001rumor\u3001extremist content \u4e09\u7c7b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5bf9\u6587\u672c\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u6027\u6587\u672c\u653b\u51fb\u5f71\u54cd\uff0c\u82e5\u8981\u63d0\u5347\u5bf9\u591a\u6837\u5316\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u53c8\u4e0d\u727a\u7272\u603b\u4f53\u51c6\u786e\u6027\uff0c\u9700\u8bbe\u8ba1\u53ef\u6cdb\u5316\u4e14\u9ad8\u6548\u7684\u68c0\u6d4b\u6846\u67b6\uff1b\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u517c\u987e\u6cdb\u5316\u4e0e\u51c6\u786e\u6027\u3002", "method": "1) \u63d0\u51fa LLM-SGA \u6846\u67b6\uff0c\u8bc6\u522b\u6587\u672c\u5bf9\u6297\u653b\u51fb\u7684\u5173\u952e\u4e0d\u53d8\u91cf\u4ee5\u63d0\u5347\u6cdb\u5316\uff1b2) \u5728\u8be5\u6846\u67b6\u5185\u5b9e\u73b0 ARHOCD\uff0c\u5305\u542b\uff1a (a) \u57fa detector \u7684\u96c6\u5408\u5e76\u7b56\u7565\uff1b (b) \u52a8\u6001\u6743\u91cd\u5206\u914d\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6837\u672c\u53ef\u9884\u6d4b\u6027\u4e0e\u5404\u68c0\u6d4b\u5668\u80fd\u529b\uff0c\u6743\u91cd\u521d\u59cb\u7531\u9886\u57df\u77e5\u8bc6\u8bbe\u5b9a\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u66f4\u65b0\uff1b(c) \u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\uff0c\u8fed\u4ee3\u4f18\u5316\u57fa\u68c0\u6d4b\u5668\u4e0e\u6743\u91cd\u5206\u914d\u5668\u3002", "result": "\u5728 hate speech\u3001rumor\u3001extremist content \u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cARHOCD \u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e0e\u68c0\u6d4b\u5668\u5728\u517c\u987e\u5bf9\u6297\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u7f13\u89e3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u5728\u66f4\u5e7f\u6cdb\u5728\u7ebf\u5185\u5bb9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.17409", "categories": ["cs.LG", "stat.AP", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17409", "abs": "https://arxiv.org/abs/2512.17409", "authors": ["Dishantkumar Sutariya", "Eike Petersen"], "title": "meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis", "comment": null, "summary": "Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u8ba1\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u5728\u533b\u7597\u5f71\u50cf\u6a21\u578b\u4e2d\u5bf9\u6309\u60a3\u8005\u4e0e\u8bb0\u5f55\u5c5e\u6027\u5206\u5c42\u7684\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u4e25\u683c\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u5305\u62ec\u6307\u6807\u9009\u62e9\u3001\u4e0d\u786e\u5b9a\u5ea6\u4f30\u8ba1\u3001\u591a\u91cd\u6bd4\u8f83\u6821\u6b63\uff0c\u4ee5\u53ca\u5bf9\u4ea4\u53c9\u5b50\u7ec4\u7684\u8bc6\u522b\uff1b\u5e76\u901a\u8fc7 ISIC2020 \u548c MIMIC-CXR \u4e24\u4e2a\u6848\u4f8b\u8fdb\u884c\u6f14\u793a\u3002", "motivation": "\u5728\u5206\u6790\u6309\u60a3\u8005\u548c\u8bb0\u5f55\u5c5e\u6027\u5206\u5c42\u7684\u6a21\u578b\u6027\u80fd\u65f6\uff0c\u5c3d\u7ba1\u80fd\u63ed\u793a\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u4f46\u8981\u505a\u5230\u7edf\u8ba1\u5b66\u4e0a\u7684\u4e25\u683c\u6bd4\u8f83\u5e76\u975e\u6613\u4e8b\u3002\u9700\u8981\u5728\u4e0d\u540c\u6837\u672c\u91cf\u548c\u57fa\u7ebf\u53d1\u751f\u7387\u4e0b\u9009\u62e9\u5408\u9002\u7684\u6027\u80fd\u6307\u6807\u3001\u4f30\u8ba1\u6307\u6807\u4e0d\u786e\u5b9a\u6027\u3001\u5bf9\u591a\u91cd\u6bd4\u8f83\u8fdb\u884c\u6821\u6b63\uff0c\u4ee5\u53ca\u5728\u7ec4\u5408\u4e30\u5bcc\u7684\u5b50\u7ec4\u4e2d\u53d1\u73b0\u6700\u5177\u201c\u8da3\u5473\u6027\u201d\u7684\u5b50\u7ec4\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u533b\u7597\u5f71\u50cf\u7684\u7edf\u8ba1\u5de5\u5177\u7bb1\uff0c\u63d0\u4f9b\u9002\u7528\u4e8e\u8de8\u5b50\u7ec4\u6bd4\u8f83\u7684\u6307\u6807\u3001\u7f6e\u4fe1\u533a\u95f4\u3001\u591a\u91cd\u6bd4\u8f83\u6821\u6b63\u548c\u4ea4\u53c9\u5b50\u7ec4\u6316\u6398\u673a\u5236\uff1b\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u8fdb\u884c\u65b9\u6cd5\u793a\u8303\u3002", "result": "\u8be5\u5de5\u5177\u7bb1\u4f7f\u7814\u7a76\u8005\u80fd\u591f\u66f4\u4e25\u683c\u5730\u8bc4\u4f30\u6f5c\u5728\u7684\u5b50\u7ec4\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5728 ISIC2020 \u548c MIMIC-CXR \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u793a\u4f8b\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u5177\u7bb1\u5177\u666e\u904d\u9002\u7528\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u9886\u57df\uff0c\u63d0\u4f9b\u4e86\u4e00\u5957\u6613\u4e8e\u4f7f\u7528\u3001\u53ef\u7528\u4e8e\u53d1\u73b0\u548c\u8bc4\u4f30\u5b50\u7ec4\u6027\u80fd\u5dee\u5f02\u7684\u7edf\u8ba1\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2512.17452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17452", "abs": "https://arxiv.org/abs/2512.17452", "authors": ["Yen-Chieh Huang", "Rui Fang", "Ming-Syan Chen", "Pi-Cheng Hsiu"], "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference", "comment": null, "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .", "AI": {"tldr": "\u901a\u8fc7\u5199\u5165\u95e8\u63a7\u7684 KV \u7f13\u5b58\u7ba1\u7406\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u9ad8\u6548\u5316\uff0c\u63d0\u51fa KV Admission/Selection/Eviction \u4e09\u539f\u8bed\uff0c\u5e76\u5728 Llama \u4e0a\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u548c\u901f\u5ea6\u63d0\u5347\uff0c\u517c\u5bb9 FlashAttention \u4e0e paged-KV\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u53d7\u4e8c\u6b21\u6ce8\u610f\u529b\u548c KV \u7f13\u5b58\u589e\u957f\u7684\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u805a\u7126\u540e\u7eed\u9009\u62e9\u6216\u9a71\u9010\uff0c\u672a\u89e3\u51b3\u6839\u672c\u7684\u5199\u5165\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u5c06 KV \u7f13\u5b58\u7ba1\u7406\u5f62\u5f0f\u5316\u4e3a Admission\u3001Selection\u3001Eviction \u4e09\u4e2a\u539f\u8bed\u3002\u901a\u8fc7 Write-Gated KV \u5728\u8fdb\u5165\u7f13\u5b58\u524d\u9884\u6d4b token \u7684\u6548\u7528\uff0c\u4ec5\u5199\u5165\u9ad8\u6548\u7528\u72b6\u6001\uff0c\u4fdd\u6301\u5168\u5c40\u7f13\u5b58\u7d27\u51d1\u5e76\u4f7f\u7528\u6ed1\u52a8\u672c\u5730\u7f13\u5b58\uff1b\u4e0e FlashAttention \u4e0e paged-KV \u517c\u5bb9\u3002", "result": "\u5728 Llama \u4e0a\u5b9e\u73b0\u540e\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\u7ea6 46-57%\uff0c\u9884\u70ed\uff08prefill\uff09\u63d0\u901f\u7ea6 3.03\u20133.45x\uff0c\u89e3\u7801\u63d0\u901f\u7ea6 1.89\u20132.56x\uff0c\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\uff1b\u4ee3\u7801\u5f00\u653e\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u4f55\u65f6\u5199\u5165\u7f13\u5b58\uff0c\u662f\u5b9e\u73b0\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e00\u4e2a\u539f\u7406\u6027\u4e14\u5b9e\u7528\u7684\u529e\u6cd5\uff0c\u5177\u5907\u4e0e\u73b0\u6709\u7f13\u5b58\u4e0e\u6ce8\u610f\u529b\u5b9e\u73b0\u534f\u540c\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.17569", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.17569", "abs": "https://arxiv.org/abs/2512.17569", "authors": ["Xietao Wang Lin", "Juan Ungredda", "Max Butler", "James Town", "Alma Rahat", "Hemant Singh", "Juergen Branke"], "title": "Bayesian Optimisation: Which Constraints Matter?", "comment": null, "summary": "Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \\emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u89e3\u8026\u7ea6\u675f\u7684 Knowledge Gradient \u8d1d\u53f6\u65af\u4f18\u5316\u53d8\u4f53\uff0c\u80fd\u591f\u53ea\u8bc4\u4f30\u4e0e\u6700\u4f18\u89e3\u76f8\u5173\u7684\u7ea6\u675f\u4ee5\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6602\u8d35\u7684\u5168\u5c40\u9ed1\u7bb1\u4f18\u5316\u4e2d\uff0c\u76ee\u6807\u4e0e\u7ea6\u675f\u5f80\u5f80\u53ef\u89e3\u8026\u8bc4\u4f30\uff1b\u901a\u5e38\u53ea\u6709\u5c11\u6570\u7ea6\u675f\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u8d77\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u80fd\u4e13\u6ce8\u4e8e\u7ed1\u5b9a\u7ea6\u675f\u7684\u83b7\u53d6\u51fd\u6570\u4ee5\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684 Knowledge Gradient \u83b7\u53d6\u51fd\u6570\u53d8\u4f53\uff0c\u9002\u7528\u4e8e\u5e26\u89e3\u8026\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u7684\u5b50\u96c6\u53ef\u72ec\u7acb\u8fdb\u884c\u8bc4\u4f30\u3002\u65b9\u6cd5\u5305\u62ec\u8bc6\u522b\u5e76\u805a\u7126\u4e8e\u53ef\u80fd\u7ed1\u5b9a\u7684\u7ea6\u675f\uff0c\u4ece\u800c\u4ec5\u8bc4\u4f30\u76f8\u5173\u7ea6\u675f\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\uff0c\u5e76\u4e0e\u73b0\u6709\u7ea6\u675f\u83b7\u53d6\u51fd\u6570\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u57fa\u51c6\u5b9e\u9a8c\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u57fa\u51c6\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u5b9e\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u89e3\u8026\u7ea6\u675f\u7684 KG \u53d8\u4f53\u80fd\u63d0\u9ad8\u6602\u8d35\u5168\u5c40\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\uff0c\u5f53\u53ea\u6709\u5c11\u6570\u7ea6\u675f\u7ed1\u5b9a\u65f6\uff0c\u80fd\u663e\u8457\u4f18\u4e8e\u72b6\u6001-of-the-art\u3002"}}
{"id": "2512.17570", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.17570", "abs": "https://arxiv.org/abs/2512.17570", "authors": ["Yikang Yue", "Yishu Yin", "Xuehai Qian"], "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping", "comment": null, "summary": "SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake", "AI": {"tldr": "GreedySnake is an SSD-offloaded training system that uses vertical scheduling to improve LLM training throughput by finishing all micro-batches for a layer before moving to the next, overlapping part of the optimization with the next forward pass to mitigate I/O bottlenecks; it outperforms horizontal scheduling and ZeRO-Infinity, achieving up to ~2x throughput gains on GPT-65B/175B models, with code released.", "motivation": "\u964d\u4f4e\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u9ad8\u541e\u5410\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5b58\u50a8\u5378\u8f7d\u5230SSD\u5e76\u4f18\u5316\u5fae\u6279\u6b21\u8c03\u5ea6\u6765\u7f13\u89e3I/O\u74f6\u9888\u3002", "method": "\u57fa\u4e8e\u68af\u5ea6\u7d2f\u79ef\u7684\u5fae\u6279\u6b21\u8bad\u7ec3\uff0c\u91c7\u7528\u5782\u76f4\u8c03\u5ea6\uff08\u540c\u5c42\u5185\u5b8c\u6210\u6240\u6709\u5fae\u6279\u6b21\u540e\u518d\u8fdb\u5165\u4e0b\u4e00\u5c42\uff09\uff0c\u5e76\u5728\u4e00\u5b9a\u9636\u6bb5\u5c06\u4f18\u5316\u6b65\u9aa4\u4e0e\u4e0b\u4e00\u8f6e\u524d\u5411\u4f20\u64ad\u91cd\u53e0\uff1b\u5229\u7528SSD\u5378\u8f7d\u6765\u51cf\u8f7b\u663e\u5b58\u538b\u529b\u3002", "result": "\u5728A100 GPU\u4e0a\uff0c\u76f8\u6bd4ZeRO-Infinity\uff0cGPT-65B\u57281\u5361\u4e0a\u63d0\u5347\u7ea61.96x\uff0c4\u5361\u4e0a\u7ea61.93x\uff1bGPT-175B\u57281\u5361\u4e0a\u63d0\u5347\u7ea62.53x\u3002\u4ee3\u7801\u5f00\u6e90\uff0c\u5730\u5740\u63d0\u4f9b\u3002", "conclusion": "\u5782\u76f4\u8c03\u5ea6\u4e0e\u90e8\u5206\u4f18\u5316\u91cd\u53e0\u663e\u8457\u7f13\u89e3SSD\u5378\u8f7d\u8bad\u7ec3\u7684I/O\u74f6\u9888\uff0c\u4f7f\u8bad\u7ec3\u541e\u5410\u91cf\u66f4\u63a5\u8fd1\u7406\u8bbaRoofline\u6781\u9650\uff0c\u63d0\u5347\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u6210\u672c\u6548\u7387\u3002"}}
{"id": "2512.17586", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17586", "abs": "https://arxiv.org/abs/2512.17586", "authors": ["Mahesh Keswani", "Raunak Bhattacharyya"], "title": "Learning Safe Autonomous Driving Policies Using Predictive Safety Representations", "comment": "8 pages, 4 figures. Submitted to ICRA 2026", "summary": "Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.", "AI": {"tldr": "SRPL extends SafeRL for autonomous driving by using predictive safety representations to balance reward and safety, showing improved performance on WOMD and NuPlan with robustness gains, though effectiveness depends on policy optimizer and dataset.", "motivation": "In autonomous driving, optimizing performance under strict safety constraints is crucial; traditional methods face a tension between conservatism and exploration. Predictive safety representations aim to forecast future constraint violations to enable safer learning.", "method": "Apply the Safety Representations for Safer Policy Learning (SRPL) framework with a predictive model of future constraint violations; evaluate on Waymo Open Motion Dataset (WOMD) and NuPlan; assess reward-safety tradeoff, robustness to observation noise, and zero-shot cross-dataset generalization; report effect sizes and statistical significance.", "result": "SRPL improves the reward-safety tradeoff, with statistically significant improvements in success rate (effect sizes r=0.65\u20130.86) and cost reduction (r=0.70\u20130.83; p<0.05). Its effectiveness depends on the underlying policy optimizer and dataset distribution. Predictive safety representations enhance robustness to observation noise and improve zero-shot cross-dataset generalization.", "conclusion": "Predictive safety representations are a promising direction to strengthen SafeRL for real-world autonomous driving, offering improved safety-performance tradeoffs and generalization, though their benefits hinge on policy choice and data distribution."}}
{"id": "2512.17592", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17592", "abs": "https://arxiv.org/abs/2512.17592", "authors": ["Arthur Guijt", "Dirk Thierens", "Ellen Kerkhof", "Jan Wiersma", "Tanja Alderliesten", "Peter A. N. Bosman"], "title": "Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models", "comment": "35 pages, 11 figures", "summary": "Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.\n  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.", "AI": {"tldr": "\u5728\u4e0d\u5171\u4eab\u6743\u91cd\u7684\u5f02\u6b65\u534f\u4f5c\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165\u62fc\u63a5\u5c42\u5c06\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u6574\u5408\uff0c\u8fbe\u5230\u5bf9\u591a\u65b9\u6570\u636e\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff1b\u591a\u76ee\u6807\u89c6\u89d2\u63ed\u793a\u5355\u65b9\u6570\u636e\u8bad\u7ec3\u7684\u5c40\u90e8\u4f18\u52bf\u4e0e\u8de8\u65b9\u6027\u80fd\u635f\u5931\u7684\u6743\u8861\uff0c\u62fc\u63a5\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u5e76\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u662f\u533b\u7597\u7b49\u9886\u57df\u7684\u6570\u636e\u5206\u6563\u5bfc\u81f4\u96be\u4ee5\u76f4\u63a5\u5171\u4eab\uff1b\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u89e3\u51b3\u6570\u636e\u5206\u5e03\u95ee\u9898\uff0c\u4f46\u9700\u8981\u53c2\u4e0e\u65b9\u540c\u6b65\u8bad\u7ec3\u548c\u4ea4\u6362\u6a21\u578b\u6743\u91cd\uff0c\u5b58\u5728\u5ef6\u65f6\u548c\u9690\u79c1/\u5b89\u5168\u6311\u6218\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u5728\u975e\u540c\u6b65\u3001\u5f02\u6b65\u534f\u4f5c\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u62fc\u63a5\u5b9e\u73b0\u8de8\u65b9\u6a21\u578b\u7ec4\u5408\u7684\u53ef\u884c\u6027\u4e0e\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u91c7\u7528\u591a\u76ee\u6807\u89c6\u89d2\u5bf9\u6bcf\u4e2a\u53c2\u4e0e\u65b9\u7684\u6570\u636e\u6027\u80fd\u8fdb\u884c\u72ec\u7acb\u8bc4\u4f30\uff1b2) \u5728\u6bcf\u4e2a\u65b9\u72ec\u7acb\u8bad\u7ec3\u6a21\u578b\uff1b3) \u8bbe\u8ba1\u5e76\u63d2\u5165\u4e00\u5bf9\u62fc\u63a5\u5c42\uff0c\u5c06\u5404\u81ea\u7684\u4e2d\u95f4\u8868\u793a\u8fdb\u884c\u878d\u5408\uff0c\u5f62\u6210\u53ef\u5728\u5f02\u6b65\u573a\u666f\u4e0b\u4f7f\u7528\u7684\u7ec4\u5408\u6a21\u578b\uff1b4) \u4e0e\u4ec5\u5728\u5355\u65b9\u6570\u636e\u4e0a\u8bad\u7ec3\u3001\u4ee5\u53ca\u628a\u6570\u636e\u5728\u65b9\u4e4b\u95f4\u7b80\u5355\u5408\u5e76\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff1b5) \u8bc4\u4f30\u62fc\u63a5\u5c42\u5bf9\u4fdd\u6301\u826f\u597d\u6cdb\u5316\u4e0e\u6062\u590d\u8de8\u65b9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u4ec5\u5728\u5355\u65b9\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e0e\u5c06\u5176\u4e0e\u53e6\u4e00\u65b9\u6570\u636e\u5408\u5e76\u540e\u5728\u8be5\u5355\u65b9\u6570\u636e\u4e0a\u8bc4\u4f30\u7684\u6027\u80fd\u76f8\u8fd1\uff0c\u4f46\u5728\u5176\u4ed6\u65b9\u6570\u636e\u4e0a\u7684\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff1b\u4f7f\u7528\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u96c6\u6210\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\uff0c\u4f46\u5bf9\u5404\u65b9\u81ea\u8eab\u6570\u636e\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\uff1b\u901a\u8fc7\u5728\u72ec\u7acb\u6a21\u578b\u7684\u4e2d\u95f4\u8868\u793a\u4e0a\u6dfb\u52a0\u6070\u5f53\u7684\u4e24\u5c42\u62fc\u63a5\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6539\u8fdb\u6cdb\u5316\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u6216\u6062\u590d\u5bf9\u591a\u65b9\u6570\u636e\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u8868\u660e\u5f02\u6b65\u534f\u4f5c\u662f\u53ef\u884c\u4e14\u5177\u6f5c\u529b\u7684\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u5728\u4e0d\u9700\u8981\u5171\u4eab\u6743\u91cd\u7684\u5f02\u6b65\u534f\u4f5c\u573a\u666f\u4e2d\uff0c\u5229\u7528\u4e2d\u95f4\u8868\u793a\u62fc\u63a5\u5c42\u53ef\u6709\u6548\u6574\u5408\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u83b7\u5f97\u63a5\u8fd1\u6216\u53ef\u4e0e\u540c\u6b65\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u8de8\u65b9\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2512.17598", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17598", "abs": "https://arxiv.org/abs/2512.17598", "authors": ["Guner Dilsad Er", "Sebastian Trimpe", "Michael Muehlebach"], "title": "A Systems-Theoretic View on the Convergence of Algorithms under Disturbances", "comment": null, "summary": "Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.", "AI": {"tldr": "\u5728\u5b58\u5728\u6270\u52a8\u3001\u566a\u58f0\u548c\u7cfb\u7edf\u8026\u5408\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u5bfc\u7b97\u6cd5\u6536\u655b\u4e0e\u7a33\u5b9a\u6027\u754c\u9650\uff0c\u5e76\u7ed9\u51fa\u57fa\u4e8e\u5bf9\u5076Lyapunov\u5b9a\u7406\u7684\u6270\u52a8\u5f71\u54cd\u91cf\u5316\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5728\u566a\u58f0\u73af\u5883\u4e0b\u5206\u6790\u7b97\u6cd5\u884c\u4e3a\u7684\u7edf\u4e00\u5de5\u5177\u3002", "motivation": "\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\uff0c\u7b97\u6cd5\u5f80\u5f80\u66b4\u9732\u4e8e\u6270\u52a8\u548c\u4e0e\u5176\u4ed6\u52a8\u529b\u7cfb\u7edf\u8026\u5408\uff0c\u9700\u4ece\u7406\u8bba\u4e0a\u6269\u5c55\u5b64\u7acb\u60c5\u5f62\u7684\u6536\u655b\u6027\u5230\u6709\u6270\u52a8\u60c5\u5f62\u3002", "method": "\u5229\u7528\u5bf9\u5076Lyapunov\u5b9a\u7406\u63a8\u5bfc\u5173\u952e\u4e0d\u7b49\u5f0f\uff0c\u91cf\u5316\u6270\u52a8\u5bf9\u6536\u655b\u7387\u548c\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u7ed3\u679c\u5e94\u7528\u4e8e\u5206\u5e03\u5f0f\u5b66\u4e60\u901a\u4fe1\u7ea6\u675f\u3001\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u7684\u654f\u611f\u6027\u3001\u4ee5\u53ca\u9690\u79c1\u566a\u58f0\u6ce8\u5165\u7b49\u573a\u666f\u3002", "result": "\u7ed9\u51fa\u7a33\u5b9a\u754c\u548c\u6536\u655b\u901f\u7387\u7684\u4e0a/\u4e0b\u754c\uff0c\u4f5c\u4e3a\u8bc4\u4f30\u7b97\u6cd5\u5728\u566a\u58f0\u4e0e\u8026\u5408\u73af\u5883\u4e2d\u7684\u6027\u80fd\u7684\u7edf\u4e00\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u5206\u6790\u5728\u566a\u58f0\u3001\u5e72\u6270\u53ca\u4e0e\u5176\u4ed6\u52a8\u529b\u7cfb\u7edf\u8026\u5408\u7684\u7b97\u6cd5\u65f6\u7684\u7edf\u4e00\u5206\u6790\u5de5\u5177\uff0c\u5e2e\u52a9\u7406\u89e3\u548c\u8bbe\u8ba1\u9c81\u68d2\u7b97\u6cd5\u3002"}}
{"id": "2512.17607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17607", "abs": "https://arxiv.org/abs/2512.17607", "authors": ["Zhaoqian Gao", "Min Yanga"], "title": "More Consistent Accuracy PINN via Alternating Easy-Hard Training", "comment": null, "summary": "Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u786c\u6027\u4e0e\u6613\u4e8e\u8bad\u7ec3\u7684\u4f18\u5148\u5316\u7b56\u7565\u4ea4\u66ff\u7ed3\u5408\u7684\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e PINN \u7684 PDE \u6c42\u89e3\uff0c\u5728\u5177\u6709\u68af\u5ea6\u9661\u5ced\u3001\u975e\u7ebf\u6027\u548c\u9ad8\u7ef4\u5ea6\u7684\u65b9\u7a0b\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff1b\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u5bf9 L2 \u8bef\u5dee\u8fbe\u5230 1e-5~1e-6\u3002", "motivation": "\u73b0\u6709\u7684\u786c\u4f18\u5148\u5316\u4e0e\u6613\u4f18\u5148\u5316\u5728 PINN \u7684\u8bad\u7ec3\u4e2d\u5404\u6709\u4f18\u52a3\uff0c\u4f46\u5bf9\u4e0d\u540c PDE \u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u4e00\u79cd\u5728\u591a\u7c7b\u95ee\u9898\u4e0a\u517c\u5177\u9ad8\u6548\u4e0e\u7a33\u5b9a\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ea4\u66ff\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5c06\u786c\u4f18\u5148\u5316\u4e0e\u6613\u4f18\u5148\u5316\u7ed3\u5408\uff0c\u901a\u8fc7\u4ea4\u66ff\u7684\u8bad\u7ec3\u9636\u6bb5\u6765\u878d\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5f62\u6210\u6df7\u5408\u578b\u7684\u6743\u91cd/\u91c7\u6837\u7b56\u7565\u4ee5\u63d0\u5347\u5bf9\u5c16\u9510\u68af\u5ea6\u3001\u975e\u7ebf\u6027\u548c\u9ad8\u7ef4\u95ee\u9898\u7684\u62df\u5408\u80fd\u529b\u3002", "result": "\u5728\u68af\u5ea6\u9661\u5ced\u3001\u975e\u7ebf\u6027\u5f3a\u548c\u9ad8\u7ef4\u5ea6\u7684 PDE \u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u76f8\u5bf9 L2 \u8bef\u5dee\u5728 1e-5\u20131e-6 \u7684\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u591a\u7c7b\u95ee\u9898\u7684\u66f4\u9ad8\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u53ef\u63d0\u5347 PINNs \u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u7a33\u5065\u7684 PINN \u8bad\u7ec3\u6846\u67b6\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2512.17629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17629", "abs": "https://arxiv.org/abs/2512.17629", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "title": "SCOPE: Sequential Causal Optimization of Process Interventions", "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.", "AI": {"tldr": "\u63d0\u51faSCOPE\uff0c\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5b66\u4e60\u7684PresPM\u5e8f\u5217\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u540e\u63a8\u5bfc\u5b9e\u73b0\u9010\u6b65\u5e72\u9884\u5bf9\u6700\u7ec8KPI\u7684\u5f71\u54cd\uff0c\u80fd\u76f4\u63a5\u4f7f\u7528\u89c2\u6d4b\u6570\u636e\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5e72\u9884\u5e76\u975e\u5b64\u7acb\uff1b\u9700\u5bf9\u591a\u6b21\u5e72\u9884\u5e8f\u5217\u8fdb\u884c\u5bf9\u9f50\u4ee5\u5171\u540c\u4f18\u5316\u7ed3\u679c\u3002\u73b0\u6709PresPM\u591a\u805a\u7126\u5355\u6b21\u5e72\u9884\uff0c\u6216\u5c06\u591a\u6b21\u5e72\u9884\u72ec\u7acb\u5904\u7406\uff0c\u5ffd\u89c6\u65f6\u95f4\u4f9d\u8d56\uff1b\u800c\u5c06\u5176\u5efa\u6a21\u4e3aRL\u5f80\u5f80\u9700\u6a21\u62df\u6216\u6570\u636e\u589e\u5f3a\uff0c\u5bfc\u81f4\u73b0\u5b9e\u5dee\u8ddd\u548c\u504f\u5dee\u3002", "method": "SCOPE\u901a\u8fc7\u5411\u540e\u63a8\u5bfc(backward induction)\u4f30\u8ba1\u6bcf\u4e2a\u5019\u9009\u5e72\u9884\u52a8\u4f5c\u7684\u6548\u679c\uff0c\u5c06\u5176\u5f71\u54cd\u4ece\u6700\u7ec8\u51b3\u7b56\u70b9\u5411\u524d\u4f20\u64ad\u5230\u521d\u59cb\u70b9\uff1b\u5229\u7528\u56e0\u679c\u5b66\u4e60\u5668\uff0c\u4f7f\u5f97\u53ef\u76f4\u63a5\u5229\u7528\u89c2\u6d4b\u6570\u636e\u800c\u4e0d\u9700\u8981\u4e3aRL\u6784\u5efa\u8fc7\u7a0b\u8fd1\u4f3c\u3002", "result": "\u5728\u4e00\u4e2a\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u57fa\u4e8e\u73b0\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u65b0\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cSCOPE\u5728\u4f18\u5316KPI\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684PresPM\u65b9\u6cd5\uff1b\u65b0\u5f15\u5165\u7684\u534a\u5408\u6210\u57fa\u51c6\u57fa\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5177\u5907\u53ef\u590d\u7528\u6027\u3002", "conclusion": "SCOPE\u5b9e\u73b0\u4e86\u5bf9\u5e8f\u5217\u5e72\u9884\u7684\u5bf9\u9f50\u5efa\u8bae\uff0c\u89e3\u51b3\u5e72\u9884\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\u95ee\u9898\uff0c\u5e76\u53ef\u76f4\u63a5\u5229\u7528\u89c2\u6d4b\u6570\u636e\uff1b\u540c\u65f6\u63d0\u4f9b\u53ef\u590d\u7528\u7684\u534a\u5408\u6210\u57fa\u51c6\uff0c\u4fc3\u8fdb\u540e\u7eed\u5bf9\u5e8f\u5217PresPM\u7684\u7814\u7a76\u3002"}}
{"id": "2512.17636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17636", "abs": "https://arxiv.org/abs/2512.17636", "authors": ["Mingyu Su", "Jian Guan", "Yuxian Gu", "Minlie Huang", "Hongning Wang"], "title": "Trust-Region Adaptive Policy Optimization", "comment": null, "summary": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.", "AI": {"tldr": "TRAPO\u662f\u4e00\u79cd\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\u4ea4\u66ff\u8fdb\u884cSFT\u4e0eRL\u7684\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e13\u5bb6\u524d\u7f00\u4e0a\u4f18\u5316SFT\u635f\u5931\u3001\u5728\u6a21\u578b\u81ea\u6211\u5b8c\u6210\u4e0a\u4f18\u5316RL\u635f\u5931\u6765\u7edf\u4e00\u5916\u90e8\u76d1\u7763\u4e0e\u81ea\u6211\u63a2\u7d22\uff1b\u5e76\u5f15\u5165TrSFT\u4e0e\u81ea\u9002\u5e94\u524d\u7f00\u9009\u62e9\u673a\u5236\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4f20\u7edfSFT\u3001RL\u53caSFT-RL\u7b49\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e24\u9636\u6bb5\u7ba1\u7ebf\uff08\u5148SFT\u540eRL\uff09\u4e2d\u5b58\u5728\u7684\u521a\u6027\u6a21\u4eff\u5bfc\u81f4\u7684\u63a2\u7d22\u53d7\u9650\u4e0e\u9057\u5fd8\u95ee\u9898\uff0c\u63a8\u52a8RL\u5728\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e2d\u7684\u6f5c\u529b\u53d1\u6325\u3002", "method": "TRAPO\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u4e2d\u4ea4\u66ff\u4f18\u5316\uff1a\u5bf9\u4e13\u5bb6\u524d\u7f00\u5e94\u7528SFT\u635f\u5931\uff0c\u5bf9\u6a21\u578b\u81ea\u6211\u5b8c\u6210\u5e94\u7528RL\u635f\u5931\uff1b\u5f15\u5165Trust-Region SFT\uff08TrSFT\uff09\u5728\u524d\u5411KL\u7ea6\u675f\u7684\u53ef\u4fe1\u57df\u5185\u6700\u5c0f\u5316\uff0c\u8d85\u51fa\u90e8\u5206\u964d\u4f4e\u4f18\u5316\u5f3a\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u5411\u9006KL\u7684\u504f\u597d\uff0c\u83b7\u5f97\u5bf9RL\u53cb\u597d\u7684\u6a21\u5f0f\u805a\u7126\u66f4\u65b0\uff1b\u540c\u65f6\u8bbe\u8ba1\u81ea\u9002\u5e94\u524d\u7f00\u9009\u62e9\u673a\u5236\uff0c\u57fa\u4e8e\u6d4b\u91cf\u7684\u6548\u7528\u5206\u914d\u4e13\u5bb6\u6307\u5bfc\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cTRAPO\u5bf9\u6bd4\u6807\u51c6SFT\u3001RL\u53caSFT-then-RL\u7ba1\u7ebf\u4ee5\u53ca\u6700\u65b0\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u786e\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u63a8\u7406\u589e\u5f3a\u578bLLM\u8bad\u7ec3\u8303\u5f0f\u3002", "conclusion": "TRAPO\u901a\u8fc7\u5c06\u4fe1\u4efb\u57df\u7ea6\u675f\u7684SFT\u548c\u81ea\u6211\u63a2\u7d22\u7684RL\u6709\u673a\u7ed3\u5408\uff0c\u5e76\u8f85\u4ee5\u81ea\u9002\u5e94\u524d\u7f00\u5206\u914d\uff0c\u7a33\u5b9a\u4e14\u6709\u6548\u5730\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\uff0c\u4e3a\u63a8\u7406\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.17678", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17678", "abs": "https://arxiv.org/abs/2512.17678", "authors": ["Daphn\u00e9 Chopard", "Jorge da Silva Gon\u00e7alves", "Irene Cannistraci", "Thomas M. Sutter", "Julia E. Vogt"], "title": "You Only Train Once: Differentiable Subset Selection for Omics Data", "comment": null, "summary": "Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.", "AI": {"tldr": "YOTO proposes an end-to-end, differentiable framework that jointly selects discrete gene subsets and performs prediction on single-cell RNA-seq data, using a closed feedback loop with sparsity constraints and multi-task learning, achieving superior performance and compact gene subsets.", "motivation": "The goal is to obtain compact, informative gene subsets for biomarker discovery, interpretability, and cost-effective profiling. Existing methods are often multi-stage or rely on post hoc feature attribution, leading to weak coupling between selection and prediction.", "method": "An end-to-end differentiable architecture that jointly selects discrete gene subsets and predicts outcomes. Sparsity is enforced so only selected genes contribute at inference. A closed feedback loop lets prediction guide gene selection and vice versa. Multi-task learning enables shared representations and leveraging partially labeled data, promoting cross-task generalization.", "result": "Evaluated on two representative single-cell RNA-seq datasets, YOTO consistently outperforms state-of-the-art baselines, yielding sparse, meaningful gene subsets and improved predictive performance that generalizes across tasks without extra training steps.", "conclusion": "Sparse, end-to-end, multi-task gene subset selection enhances predictive accuracy and interpretability, reduces reliance on downstream classifiers, and facilitates biomarker discovery in single-cell analysis."}}
{"id": "2512.17720", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17720", "abs": "https://arxiv.org/abs/2512.17720", "authors": ["Joanna Sliwa", "Frank Schneider", "Philipp Hennig", "Jose Miguel Hernandez-Lobato"], "title": "Mitigating Forgetting in Low Rank Adaptation", "comment": null, "summary": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.", "AI": {"tldr": "LaLoRA: \u5728LoRA\u6743\u91cd\u4e0a\u5e94\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u7684\u6743\u91cd\u7a7a\u95f4\u6b63\u5219\u5316\uff0c\u4ee5\u6291\u5236\u5728\u9ad8\u66f2\u7387\u65b9\u5411\u4e0a\u7684\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u51cf\u5c11\u9057\u5fd8\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u5f3a\u5ea6\u76f4\u63a5\u63a7\u5236\u5b66\u4e60-\u9057\u5fd8\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u5728\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u65f6\uff0c\u6a21\u578b\u5728\u65b0\u9886\u57df\u4e0a\u5feb\u901f\u9002\u5e94\u7684\u540c\u65f6\u6613\u53d1\u751f\u5bf9\u539f\u6709\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002", "method": "\u5bf9LoRA\u6743\u91cd\u4ec5\u5e94\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u53c2\u6570\u7684\u91cd\u8981\u6027/\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5728\u9ad8\u66f2\u7387\u65b9\u5411\u5bf9\u66f4\u65b0\u8fdb\u884c\u7ea6\u675f\uff0c\u4ece\u800c\u5728\u4fdd\u6301LoRA\u8f7b\u91cf\u5316\u7684\u524d\u63d0\u4e0b\u4fdd\u62a4\u539f\u6709\u77e5\u8bc6\u3002\u8fd8\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u66f2\u7387\u8fd1\u4f3c\u3001\u5206\u6790\u7528\u4e8e\u8fd1\u4f3c\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5bf9Llama\u6a21\u578b\u8fdb\u884c\u6570\u5b66\u63a8\u7406\u5fae\u8c03\u65f6\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u5b66\u4e60-\u9057\u5fd8\u6743\u8861\uff0c\u4e14\u8be5\u5e73\u8861\u80fd\u901a\u8fc7\u6b63\u5219\u5316\u5f3a\u5ea6\u76f4\u63a5\u63a7\u5236\u3002\u5bf9\u53c2\u6570\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3001\u66f2\u7387\u8fd1\u4f3c\u4ee5\u53ca\u6570\u636e\u9009\u62e9\u7684\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u5e76\u7ef4\u6301\u4e86\u8f7b\u91cf\u5316\u7279\u6027\u3002", "conclusion": "\u628a\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u5e94\u7528\u5230LoRA\u6743\u91cd\u4e0a\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6743\u91cd\u7a7a\u95f4\u6b63\u5219\u5316\u601d\u8def\uff0c\u80fd\u591f\u5728\u4fdd\u6301LoRA\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u63d0\u9ad8\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u5bf9\u66f2\u7387\u8fd1\u4f3c\u548c\u6570\u636e\u9009\u62e9\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u65b9\u5411\u4ee5\u63d0\u5347\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.17762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17762", "abs": "https://arxiv.org/abs/2512.17762", "authors": ["Luca Miglior", "Matteo Tolloso", "Alessio Gravina", "Davide Bacciu"], "title": "Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation", "comment": null, "summary": "Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.", "AI": {"tldr": "\u63d0\u51fa\u4e86 ECHO \u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30 GNN \u5728\u6781\u957f\u8ddd\u79bb\u4f20\u64ad\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u542b\u4e09\u9879\u5408\u6210\u4efb\u52a1\u4e0e\u4e24\u7ec4\u5316\u5b66\u6570\u636e\u96c6\uff0c\u63ed\u793a\u73b0\u6709GNN\u5728\u957f\u7a0b\u4fe1\u606f\u4f20\u9012\u4e0a\u7684\u4e0d\u8db3\u5e76\u7ed9\u51fa\u6f5c\u5728\u7684\u8bbe\u8ba1\u65b9\u5411\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u6355\u6349\u56fe\u7ed3\u6784\u4e2d\u8fdc\u8ddd\u79bb\u4fe1\u606f\u4f20\u64ad\u662f GNN \u7814\u7a76\u7684\u6838\u5fc3\u6311\u6218\u4e4b\u4e00\uff0c\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u4e25\u683c\u6d4b\u8bd5\u6a21\u578b\u5728\u6781\u957f\u4fe1\u606f\u4f20\u9012\u4e2d\u7684\u8868\u73b0\u3002\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u957f\u7a0b\u4f20\u64ad\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\u6765\u63a8\u52a8\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0 ECHO \u57fa\u51c6\uff1a(1) \u4e09\u4e2a\u5408\u6210\u56fe\u4efb\u52a1\uff08\u5355\u6e90\u6700\u77ed\u8def\u5f84\u3001\u8282\u70b9\u79bb\u5fc3\u6027\u3001\u56fe\u76f4\u5f84\uff09\uff0c\u5728\u7ed3\u6784\u590d\u6742\u3001\u4fe1\u606f bottleneck \u660e\u663e\u7684\u62d3\u6251\u4e0a\u6784\u9020\u4efb\u52a1\uff1b(2) \u4e24\u7ec4\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08ECHO-Charge\u3001ECHO-Energy\uff09\uff0c\u57fa\u4e8e\u5316\u5b66\u9886\u57df\uff0c\u6807\u7b7e\u6765\u81ea\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\uff0c\u5929\u7136\u4f9d\u8d56\u957f\u7a0b\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\uff1b(3) \u5bf9\u6d41\u884c\u7684 GNN \u67b6\u6784\u8fdb\u884c\u5e7f\u6cdb\u57fa\u51c6\u8bc4\u6d4b\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u73b0\u6709 GNN \u5728\u5904\u7406\u6781\u957f\u7a0b\u4f20\u64ad\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8bf4\u660e\u201c\u771f\u6b63\u7684\u201d\u957f\u7a0b\u4f20\u64ad\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u5b9e\u73b0\uff1b\u67d0\u4e9b\u8bbe\u8ba1\u9009\u62e9\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u74f6\u9888\uff0c\u6307\u5411\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "ECHO \u4e3a\u8bc4\u4f30\u957f\u7a0b\u4fe1\u606f\u4f20\u64ad\u5efa\u7acb\u4e86\u65b0\u7684\u6807\u51c6\uff0c\u5e76\u4e14\u53ef\u63a8\u52a8 AI for science \u9886\u57df\u5728\u957f\u8ddd\u79bb\u56fe\u4fe1\u606f\u4f20\u9012\u65b9\u9762\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2512.17788", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17788", "abs": "https://arxiv.org/abs/2512.17788", "authors": ["Wei Tang", "Yin-Fang Yang", "Weijia Zhang", "Min-Ling Zhang"], "title": "Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning", "comment": null, "summary": "Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u63d2\u62d4\u7684\u53ef\u6821\u51c6\u6d88\u89e3\u635f\u5931 (CDL)\uff0c\u7528\u4e8e\u591a\u5b9e\u4f8b\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff08MIPL\uff09\u548c\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff08PLL\uff09\uff0c\u901a\u8fc7\u4e24\u79cd\u5b9e\u4f8b\u5316\u65b9\u5f0f\u5b9e\u73b0\u5bf9\u5019\u9009\u6807\u7b7e\u96c6\u53ca\u5019\u9009+\u975e\u5019\u9009\u6807\u7b7e\u96c6\u7684\u6982\u7387\u6821\u51c6\uff0c\u7406\u8bba\u5206\u6790\u652f\u6301\u5176\u4e0b\u754c\u4e0e\u6b63\u5219\u5316\u6027\u8d28\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u5206\u7c7b\u4e0e\u6821\u51c6\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6d88\u89e3\u635f\u5931\uff0c\u4e14\u53ef\u65e0\u7f1d\u63d2\u5165\u73b0\u6709\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684 MIPL \u5728\u6807\u7b7e\u548c\u5b9e\u4f8b\u7a7a\u95f4\u4e2d\u5747\u5904\u4e8e\u5f31\u76d1\u7763\u72b6\u6001\uff0c\u5f80\u5f80\u5bfc\u81f4\u9884\u6d4b\u7684\u6821\u51c6\u6027\u4e0d\u8db3\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002\u9700\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u4e0e\u6982\u7387\u6821\u51c6\u3002", "method": "\u63d0\u51fa CDL\uff0c\u5177\u6709\u4e24\u79cd instantiations\uff1a1) \u57fa\u4e8e\u5019\u9009\u6807\u7b7e\u96c6\u6982\u7387\u8fdb\u884c\u6821\u51c6\uff1b2) \u540c\u65f6\u6574\u5408\u5019\u9009\u4e0e\u975e\u5019\u9009\u6807\u7b7e\u96c6\u7684\u6982\u7387\u8fdb\u884c\u6821\u51c6\u3002CDL \u53ef\u4f5c\u4e3a\u201cplug-and-play\u201d\u7ec4\u4ef6\u5d4c\u5165\u73b0\u6709\u7684 MIPL/PLL \u6846\u67b6\uff0c\u7406\u8bba\u4e0a\u7ed9\u51fa CDL \u7684\u4e0b\u754c\u4e0e\u6b63\u5219\u5316\u6027\u8d28\uff0c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u6d88\u89e3\u635f\u5931\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cCDL \u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u4e0e\u6982\u7387\u6821\u51c6\u80fd\u529b\uff0c\u76f8\u8f83\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "CDL \u80fd\u540c\u65f6\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u4e0e\u6821\u51c6\u6027\uff0c\u5177\u5907\u7406\u8bba\u652f\u6491\u4e0e\u826f\u597d\u5b9e\u8bc1\u6548\u679c\uff0c\u4e14\u5177\u5907\u826f\u597d\u7684\u53ef\u5d4c\u5165\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u6709 MIPL/PLL \u6846\u67b6\u3002"}}
{"id": "2512.17820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17820", "abs": "https://arxiv.org/abs/2512.17820", "authors": ["Liam Collins", "Bhuvesh Kumar", "Clark Mingxuan Ju", "Tong Zhao", "Donald Loveland", "Leonardo Neves", "Neil Shah"], "title": "Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation", "comment": null, "summary": "Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.", "AI": {"tldr": "\u7814\u7a76ID\u5d4c\u5165\u4e0e\u6587\u672c\u6a21\u6001\u5728\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u4e92\u8865\u6027\u3002\u53d1\u73b0\u4e24\u79cd\u7279\u5f81\u5b58\u5728\u4e92\u8865\u4fe1\u53f7\uff0c\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u7279\u5f81\u90fd\u80fd\u63d0\u5347\u65f6\u4e5f\u80fd\u53d7\u76ca\u4e8e\u53e6\u4e00\u4e2a\u7279\u5f81\u7684\u52a0\u5165\u3002\u63d0\u51fa\u901a\u8fc7\u72ec\u7acb\u6a21\u578b\u8bad\u7ec3\u4ee5\u4fdd\u6301\u4e92\u8865\u6027\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u96c6\u6210\u7b56\u7565\u5229\u7528\u4e92\u8865\u6027\uff0c\u6548\u679c\u4f18\u4e8e\u82e5\u5e72\u57fa\u7ebf\uff0c\u8868\u660e\u540c\u65f6\u4f7f\u7528ID\u548c\u6587\u672c\u7279\u5f81\u4e14\u4e0d\u9700\u8981\u590d\u6742\u7684\u878d\u5408\u67b6\u6784\u5373\u53ef\u8fbe\u5230SOTA\u3002", "motivation": "\u5f53\u524dSR\u9886\u57df\u8981\u4e48\u7528\u6587\u672c/\u6a21\u6001\u5d4c\u5165\u5b8c\u5168\u66ff\u4ee3ID\u5d4c\u5165\uff0c\u8981\u4e48\u4e0eID+\u6a21\u6001\u7279\u5f81\u5171\u5b58\u4f46\u4f7f\u7528\u590d\u6742\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u4e0e\u5bf9\u9f50\u7ed3\u6784\uff0c\u7f3a\u4e4f\u5bf9ID\u4e0e\u6a21\u6001\u7279\u5f81\u4e92\u8865\u6027\u7684\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e24\u7c7b\u7279\u5f81\u7684\u4e92\u8865\u6027\u53ca\u5176\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5206\u6790ID-\u6587\u672cSR\u6a21\u578b\u4e4b\u95f4\u7684\u4fe1\u53f7\u4e92\u8865\u6027\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u7684SR\u65b9\u6848\uff1a\u5206\u522b\u8bad\u7ec3\u72ec\u7acb\u7684ID\u6a21\u578b\u4e0e\u6587\u672c\u6a21\u578b\u4ee5\u4fdd\u6301\u4e92\u8865\u6027\uff0c\u518d\u901a\u8fc7\u7b80\u5355\u7684\u96c6\u6210\u7b56\u7565\u878d\u5408\u4e24\u8005\u7684\u8f93\u51fa\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7b80\u5355\u7684\u72ec\u7acb\u8bad\u7ec3+\u96c6\u6210\u65b9\u6cd5\u5728\u591a\u7ec4\u57fa\u7ebf\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u9a8c\u8bc1\u4e86ID\u4e0e\u6587\u672c\u7279\u5f81\u7684\u4e92\u8865\u6027\uff0c\u540c\u65f6\u8868\u660e\u8981\u8fbe\u5230SOTA\u5e76\u975e\u5fc5\u987b\u91c7\u53d6\u590d\u6742\u7684\u878d\u5408\u67b6\u6784\u3002", "conclusion": "ID\u4e0e\u6587\u672c\u7279\u5f81\u5bf9\u5e8f\u5217\u63a8\u8350\u5177\u6709\u4e92\u8865\u6027\uff0c\u4e14\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3\u5e76\u7b80\u5355\u96c6\u6210\u5373\u53ef\u5145\u5206\u5229\u7528\u8fd9\u4e00\u4e92\u8865\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u597d\u7684\u63a8\u8350\u6027\u80fd\uff0c\u6311\u6218\u4e86\u9700\u8981\u590d\u6742\u878d\u5408\u67b6\u6784\u7684\u89c2\u70b9\u3002"}}
{"id": "2512.17878", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17878", "abs": "https://arxiv.org/abs/2512.17878", "authors": ["Herlock Rahimi"], "title": "Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow", "comment": "26 pages, 1 figure", "summary": "Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.", "AI": {"tldr": "WFR-based diffusion samplers augment traditional diffusion with mass reweighting via Wasserstein\u2013Fisher\u2013Rao geometry, implemented through weighted SDEs and Feynman\u2013Kac corrections; aims to improve exploration in nonconvex/multimodal targets.", "motivation": "Overdamped/underdamped diffusion samplers mix poorly for nonconvex or multimodal targets, limiting sampling efficiency in high-dimensional generative modeling. The work investigates WFR geometry to better explore such landscapes by coupling transport with reaction dynamics and mass reweighting.", "method": "Introduce explicit correction terms to reweight mass in diffusion samplers under Wasserstein\u2013Fisher\u2013Rao framework; implement via weighted stochastic differential equations and Feynman\u2013Kac representation; analyze geometric and operator-theoretic structure.", "result": "Preliminary, rigorous characterization of WFR-based sampling dynamics; derives formulation and properties linking reweighting to diffusion in WFR geometry; sets groundwork for theoretical/algorithmic development.", "conclusion": "WFR-based reweighting provides a principled extension to diffusion samplers, aligning transport and reaction dynamics; further work needed to develop concrete algorithms and to quantify gains in sampling efficiency for nonconvex targets."}}
{"id": "2512.17884", "categories": ["cs.LG", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17884", "abs": "https://arxiv.org/abs/2512.17884", "authors": ["Xinyue Yu", "Hayden Schaeffer"], "title": "Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space", "comment": null, "summary": "Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \\log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7684\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81-\u6709\u9650\u5143\u91cd\u5efa\u6620\u5c04\uff08RRFF-FEM\uff09\u7684\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u6765\u81ea\u591a\u53d8\u91cf\u5b66\u751f\u5206\u5e03\u7684\u968f\u673a\u7279\u5f81\u548c\u5e26\u6709\u9891\u7387\u52a0\u6743\u7684Tikhonov\u6b63\u5219\u5316\u6765\u6291\u5236\u9ad8\u9891\u566a\u58f0\uff0c\u5728\u8bad\u7ec3\u6837\u672c\u91cfm\u4e0e\u7279\u5f81\u6570N\u6ee1\u8db3N\u2248m log m\u65f6\uff0c\u7cfb\u7edf\u6761\u4ef6\u826f\u597d\uff0c\u7406\u8bba\u4e0a\u7ed9\u51fa\u8bef\u5dee\u4e0e\u6cdb\u5316\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u566a\u58f0\u9c81\u68d2\u3001\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\uff0c\u4e0e\u672a\u6b63\u5219\u5316\u7684\u968f\u673a\u7279\u5f81\u3001\u6838\u65b9\u6cd5\u548c\u795e\u7ecf\u7b97\u5b50\u5728\u51c6\u786e\u6027\u4e0a\u7ade\u4e89\u3002", "motivation": "\u5728\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u7b97\u5b50\u5b66\u4e60\u9700\u8981\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u5bf9\u566a\u58f0\u9c81\u68d2\u7684\u8fd1\u4f3c\u3002\u6838\u65b9\u6cd5\u867d\u7406\u8bba\u53ef\u63a7\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u654f\u611f\u3002\u9700\u8981\u4e00\u79cd\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3001\u4e14\u8bad\u7ec3\u9ad8\u6548\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u5143Student\u5206\u5e03\u7684\u968f\u673a\u7279\u5f81\uff0c\u4ee5\u53ca\u5e26\u6709\u9891\u7387\u6743\u91cd\u7684Tikhonov\u6b63\u5219\u5316\u4ee5\u6291\u5236\u9ad8\u9891\u566a\u58f0\uff0c\u7ed3\u5408\u6709\u9650\u5143\u91cd\u5efa\u6620\u5c04\uff08FEM\uff09\u6765\u83b7\u53d6\u7b97\u5b50\u8f93\u51fa\u3002\u7406\u8bba\u4e0a\u7ed9\u51fa\u968f\u673a\u7279\u5f81\u77e9\u9635\u7684\u6781\u7aef\u5947\u5f02\u503c\u7684\u9ad8\u6982\u7387\u754c\u5b9a\uff1b\u82e5\u7279\u5f81\u6570N\u4e0e\u8bad\u7ec3\u6837\u672c\u6570m\u4e4b\u6bd4\u6ee1\u8db3N\u2248m log m\uff0c\u5219\u7cfb\u7edf\u6761\u4ef6\u826f\u597d\uff0c\u8fdb\u800c\u7ed9\u51fa\u4f30\u8ba1\u4e0e\u6cdb\u5316\u4fdd\u8bc1\u3002\u901a\u8fc7\u5bf9\u4e00\u7cfb\u5217PDE\u57fa\u51c6\u95ee\u9898\uff08\u5bf9\u6d41\u3001 Burgers\u3001 Darcy\u3001 Helmholtz\u3001 Navier\u2013Stokes\u3001\u7ed3\u6784\u529b\u5b66\uff09\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u9c81\u68d2\u6027\u3001\u8bad\u7ec3\u65f6\u95f4\u7684\u6539\u8fdb\u4ee5\u53ca\u4e0e\u6838/\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\u3002", "result": "\u7ed9\u51fa\u4e86\u5728\u9ad8\u6982\u7387\u610f\u4e49\u4e0b\u968f\u673a\u7279\u5f81\u77e9\u9635\u7684\u6781\u503c\u5947\u5f02\u503c\u754c\u9650\uff0c\u5e76\u8bc1\u660e\u5728N\u2248m log m\u4e0b\u7cfb\u7edf\u53ef\u826f\u597d\u6761\u4ef6\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u8bef\u5dee\u548c\u6cdb\u5316\u754c\u9762\u7684\u7406\u8bba\u4fdd\u969c\u3002\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\uff1aRRFF\u53caRRFF-FEM\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8bad\u7ec3\u65f6\u95f4\u76f8\u8f83\u672a\u6b63\u5219\u5316\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u5728\u4e0e\u6838\u65b9\u6cd5\u548c\u795e\u7ecf\u7b97\u5b50\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "RRFF-FEM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e5f\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\uff0c\u662f\u6838\u65b9\u6cd5\u4e0e\u795e\u7ecf\u7b97\u5b50\u5728PDE\u7b97\u5b50\u5b66\u4e60\u9886\u57df\u7684\u6709\u529b\u7ade\u4e89\u8005\uff0c\u5c24\u5176\u5728\u9700\u8981\u5904\u7406\u5927\u578b\u8bad\u7ec3\u96c6\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
