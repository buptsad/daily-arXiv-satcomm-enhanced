<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 4]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 37]
- [eess.SY](#eess.SY) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Near-field Target Localization: Effect of Hardware Impairments](https://arxiv.org/abs/2512.21480)
*Jiapeng Li,Changsheng You,Chao Zhou,Yong Zeng,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出了一种面向极大型阵列（XL阵列）的三阶段 HI 感知近场定位框架，先通过压缩感知检测错配天线，再进行相位标定，最后利用标定后的全阵列实现高精度定位，并给出基于错配的 Cramer-Rao 下界（MCRB）分析与数值结果，显著提升在短距离和高故障概率情形下的定位性能。


<details>
  <summary>Details</summary>
Motivation: XL 阵列在现实中存在硬件缺陷（HIs），会引入未知的相位与幅度误差；现有基于 BCD 的联合估计在目标与阵列距离很近时易产生较大定位误差。需要一个鲁棒的三阶段方法来检测故障、标定相位并提升定位精度。

Method: 1) 通过压缩感知方法检测故障天线，并基于粗略定位提升检测精度；2) 设计专用相位标定方法，纠正检测到的故障天线所引起的相位误差；3) 在经过相位标定的完整 XL 阵列上提出高效的近场定位算法以精确估计目标位置；同时使用错配 Cramer-Rao 下界（MCRB）来量化硬件缺陷带来的性能损失。

Result: 数值结果表明，与多种基准方案相比，所提方法在定位误差方面有显著降低，尤其在目标距离较短和故障概率较高的情形下效果更明显。

Conclusion: 提出的三阶段 HI 感知近场定位方法能够有效缓解硬件缺陷对 XL 阵列定位的影响，且 MCRB 分析支持了对性能损失的量化评估，具备良好的实际应用前景。

Abstract: The prior works on near-field target localization have mostly assumed ideal hardware models and thus suffer from two limitations in practice. First, extremely large-scale arrays (XL-arrays) usually face a variety of hardware impairments (HIs) that may introduce unknown phase and/or amplitude errors. Second, the existing block coordinate descent (BCD)-based methods for joint estimation of the HI indicator, channel gain, angle, and range may induce considerable target localization error when the target is very close to the XL-array. To address these issues, we propose in this paper a new three-phase HI-aware near-field localization method, by efficiently detecting faulty antennas and estimating the positions of targets. Specifically, we first determine faulty antennas by using compressed sensing (CS) methods and improve detection accuracy based on coarse target localization. Then, a dedicated phase calibration method is designed to correct phase errors induced by detected faulty antennas. Subsequently, an efficient near-field localization method is devised to accurately estimate the positions of targets based on the full XL-array with phase calibration. Additionally, we resort to the misspecified Cramer-Rao bound (MCRB) to quantify the performance loss caused by HIs. Last, numerical results demonstrate that our proposed method significantly reduces the localization errors as compared to various benchmark schemes, especially for the case with a short target range and/or a high fault probability.

</details>


### [2] [Pinching Antenna-aided NOMA Systems with Internal Eavesdropping](https://arxiv.org/abs/2512.21601)
*Haolian Chi,Kunrui Cao,Zhou Su,Lei Zhou,Panagiotis D. Diamantoulakis,Yuanwei Liu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出一种基于 pinching 天线（PA）的 NOMA 安全传输框架，通过调控 PA 的耦合长度和两种功率模型实现对内部窃听的物理层安全优化，给出 SOP 的闭式解并提出灵活的功率分配策略。


<details>
  <summary>Details</summary>
Motivation: NOMA 在提高频谱效率和多用户接入方面有显著优势，但在功域的 SIC 内部窃听风险成为关键安全挑战。将可调节的 PA 融入 NOMA，可通过对无源/辐射特性的控制提升信道管理与安全性；同时扩展 PA 的设计自由度（包括耦合长度）以实现更灵活的保护机制。

Method: 建立 PA 辐射功率模型（等功率模型与比例功率模型）并将其应用于 PA 辅助的 NOMA 系统；通过优化 PA 辐射功率来提升 secrecy outage probability（SOP）性能，推导 SOP 的闭式表达；把 PA 灵活性扩展到耦合长度的控制，给出基于两种模型的灵活功率分配策略；对比分析内部窃听情形与正常传输。

Result: 得到 SOP 的闭式表达式，表明在 PA 辅助的 NOMA 系统中可显著降低由于内部窃听导致的安全损失；基于等功率和比例功率两种模型提出的灵活功率策略可在不同场景下实现对覆盖范围与安全性的权衡与优化。

Conclusion: PA-aided NOMA 为改善物理层安全提供了有效路径，耦合长度调控及多种功率模型的引入提高了设计自由度与鲁棒性；研究结果支持在未来实际系统中通过功率分配与 PA 参数调控来抑制内部窃听并优化用户活动半径与覆盖。

Abstract: As a novel member of flexible antennas, the pinching antenna (PA) is realized by integrating small dielectric particles on a waveguide, offering unique regulatory capabilities on constructing line-of-sight (LoS) links and enhancing transceiver channels, reducing path loss and signal blockage. Meanwhile, non-orthogonal multiple access (NOMA) has become a potential technology of next-generation communications due to its remarkable advantages in spectrum efficiency and user access capability. The integration of PA and NOMA enables synergistic leveraging of PA's channel regulation capability and NOMA's multi-user multiplexing advantage, forming a complementary technical framework to deliver high-performance communication solutions. However, the use of successive interference cancellation (SIC) introduces significant security risks to power-domain NOMA systems when internal eavesdropping is present. To this end, this paper investigates the physical layer security of a PA-aided NOMA system where a nearby user is considered as an internal eavesdropper. We enhance the security of the NOMA system through optimizing the radiated power of PAs and analyze the secrecy performance by deriving the closed-form expressions for the secrecy outage probability (SOP). Furthermore, we extend the characterization of PA flexibility beyond deployment and scale adjustment to include flexible regulation of PA coupling length. Based on two conventional PA power models, i.e., the equal power model and the proportional power model, we propose a flexible power strategy to achieve secure transmission. The results highlight the potential of the PA-aided NOMA system in mitigating internal eavesdropping risks, and provide an effective strategy for optimizing power allocation and cell range of user activity.

</details>


### [3] [Phase-Coherent D-MIMO ISAC: Multi-Target Estimation and Spectral Efficiency Trade-Offs](https://arxiv.org/abs/2512.21953)
*Venkatesh Tentu,Henk Wymeersch,Musa Furkan Keskin,Sauradeep Dey,Tommy Svensson*

Main category: eess.SP

TL;DR: 提出一种分布式D-MIMO ISAC系统的两阶段感知框架和自适应AP模式选择策略，实现通信与感知的权衡与协同提升。


<details>
  <summary>Details</summary>
Motivation: 在分布式多输入多输出ISAC系统中，多个AP协同服务UE并共同探测估计静态目标，需要高精度的多目标估计以及在通信和感知之间取得平衡的AP选择与资源分配策略。

Method: 提出两阶段感知框架：先进行非相干ML估计以获得初步多目标信息，再进行相干ML估计以提升精度。并提出自适应AP模式选择：以通信为核心的下行频谱效率（SE）最大化策略，以及以感知为核心的几何上分散的接收AP策略以提升感知覆盖。通过功率分配、阵列尺寸等参数分析来实现SE与感知的折中。

Result: 仿真结果验证了SE与感知之间的权衡关系，适当的通信/感知功率分配和更大阵列孔径可缓解性能下降，在毫米波尺度实现高SE和厘米级/毫米级感知精度。并且AP选择策略揭示了一个最佳接收AP数量，使感知覆盖最大化的同时SE损失不显著。

Conclusion: 所提框架能在分布式ISAC系统中同时提供高下行SE与毫米级的感知精度，且自适应AP选择能在不显著损害SE的前提下增强感知覆盖，并给出最优AP数量。

Abstract: We investigate distributed multiple-input multiple-output (D-MIMO) integrated sensing and communication (ISAC) systems, in which multiple phase-synchronized access points (APs) jointly serve user equipments (UEs) while cooperatively detecting and estimating multiple static targets. To achieve high-accuracy multi-target estimation, we propose a two-stage sensing framework combining non-coherent and coherent maximum-likelihood (ML) estimation. In parallel, adaptive AP mode-selection strategies are introduced to balance communication and sensing performance: a communication-centric scheme that maximizes downlink spectral efficiency (SE) and a sensing-centric scheme that selects geometrically diverse receive APs to enhance sensing coverage. Simulation results confirm the SE-sensing trade-off, where appropriate power allocation between communication and sensing and larger array apertures alleviate performance degradation, achieving high SE with millimeter-level sensing precision. We further demonstrate that the proposed AP-selection strategy reveals an optimal number of receive APs that maximizes sensing coverage without significantly sacrificing SE.

</details>


### [4] [Hybrid Deep Reinforcement Learning for Joint Resource Allocation in Multi-Active RIS-Aided Uplink Communications](https://arxiv.org/abs/2512.22107)
*Mohamed Shalma,Engy Aly Maher,Ahmed El-Mahdy*

Main category: eess.SP

TL;DR: 提出一种基于混合深度强化学习的资源分配框架，在多用户上行系统中借助多台主动RIS进行优化，目标在于同时优化用户发射功率、主动RIS配置和基站波束成形以最大化最小用户速率；对波束成形给出闭式解，并比较SAC、DDPG、TD3三种DRL算法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着6G对高频段、覆盖能力和系统容量的需求增加，主动RIS成为提升能量效率和覆盖的关键技术。资源分配问题在高维、非凸环境下难以直接求解，因此需要高效的学习与优化框架来在动态多用户场景中自适应配置。

Method: 提出一个混合DRL框架：对基站波束成形给出闭式解以降低优化维度，同时使用多种DRL算法（SAC、DDPG、TD3）来学习剩余的资源分配策略（包括用户发射功率和主动RIS配置），以求最大化最小用户速率。

Result: 仿真结果表明：SAC在学习速率与收敛速度方面优于DDPG和TD3，达到更低的计算成本；闭式波束成形的存在显著提升了最小速率。

Conclusion: 将闭式波束成形与SAC相结合的混合DRL框架在多用户上行系统中利用主动RIS实现高效资源分配，对6G场景具有潜在应用价值。

Abstract: Active Reconfigurable Intelligent Surfaces (RIS) are a promising technology for 6G wireless networks. This paper investigates a novel hybrid deep reinforcement learning (DRL) framework for resource allocation in a multi-user uplink system assisted by multiple active RISs. The objective is to maximize the minimum user rate by jointly optimizing user transmit powers, active RIS configurations, and base station (BS) beamforming. We derive a closed-form solution for optimal beamforming and employ DRL algorithms: Soft actor-critic (SAC), deep deterministic policy gradient (DDPG), and twin delayed DDPG (TD3) to solve the high-dimensional, non-convex power and RIS optimization problem. Simulation results demonstrate that SAC achieves superior performance with high learning rate leading to faster convergence and lower computational cost compared to DDPG and TD3. Furthermore, the closed-form of optimally beamforming enhances the minimum rate effectively.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [5] [Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme](https://arxiv.org/abs/2512.21396)
*Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: 提出可重构的 LOCO 编码用于 TDMR，并通过离线/在线学习来决定何时在不同编码方案之间切换，以最大化容量或最小化解码复杂度，问题转化为线性规划，实验验证全局最优性与有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据革命时代，设备在不同生命周期阶段需要不同级别的保护。TDMR 需要针对设备状态进行可重构的编码以延长寿命。LOCO 编码天然支持重构，并可针对不同阶段进行切换。

Method: 提出离线学习与在线学习框架，通过设备状态数据拟合比特误码率（BER）相对于TD密度的多项式关系；基于该关系构建优化问题，目标是在最大化存储容量和/或最小化解码复杂度之间做权衡；约束来自 LOCO 编码集合及实现；问题可线性化为线性规划并给出全局最优性分析。

Result: 理论与实验分析表明在该问题结构下解为全局最优；通过 TDMR 系统的实验验证所提方法在性能与可行性方面的提升。

Conclusion: 可重构 LOCO 编码结合离线/在线学习提供一种有效的自适应切换策略，以提升 TDMR 设备的容量与解码效率、延长设备寿命，且求解可通过线性规划高效实现。

Abstract: In the age of data revolution, a modern storage~or transmission system typically requires different levels of protection. For example, the coding technique used to fortify data in a modern storage system when the device is fresh cannot be the same as that used when the device ages. Therefore, providing reconfigurable coding schemes and devising an effective way to perform this reconfiguration are key to extending the device lifetime. We focus on constrained coding schemes for the emerging two-dimensional magnetic recording (TDMR) technology. Recently, we have designed efficient lexicographically-ordered constrained (LOCO) coding schemes for various stages of the TDMR device lifetime, focusing on the elimination of isolation patterns, and demonstrated remarkable gains by using them. LOCO codes are naturally reconfigurable, and we exploit this feature in our work. Reconfiguration based on predetermined time stamps, which is what the industry adopts, neglects the actual device status. Instead, we propose offline and online learning methods to perform this task based on the device status. In offline learning, training data is assumed to be available throughout the time span of interest, while in online learning, we only use training data at specific time intervals to make consequential decisions. We fit the training data to polynomial equations that give the bit error rate in terms of TD density, then design an optimization problem in order to reach the optimal reconfiguration decisions to switch from a coding scheme to another. The objective is to maximize the storage capacity and/or minimize the decoding complexity. The problem reduces to a linear programming problem. We show that our solution is the global optimal based on problem characteristics, and we offer various experimental results that demonstrate the effectiveness of our approach in TDMR systems.

</details>


### [6] [Near-Field Communication with Massive Movable Antennas: An Electrostatic Equilibrium Perspective](https://arxiv.org/abs/2512.21660)
*Shicong Liu,Xianghao Yu,Shenghui Song,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 提出了一种面向近场大规模天线的天线布放新框架，将布放问题在角域中重构为加权Fekete问题，并将最优性条件暴露为静电平衡问题。通过基于常微分方程的求解框架与两步特征值分解实现高效求解，且在天线尺寸趋于无穷时给出闭式解。仿真表明该方法能有效利用近场信道的空间自由度，提升谱效率并对参数失配具有鲁棒性，同时给出近似理论最优的渐近解。


<details>
  <summary>Details</summary>
Motivation: 在近场大规模天线通信中，需充分利用空间自由度以提升光谱效率，但现有天线布放策略在可扩展性方面受限，且常忽略近场效应。该工作旨在在近场大规模MIMO系统中实现对天线位置的高效布放以最大化空间自由度的利用。

Method: 将天线布放问题从角域入手重构为加权Fekete问题；推导最优性条件并揭示其等效于一个静电平衡问题；提出基于常微分方程的求解框架来高效求解平衡点；通过两步特征值分解得到最优天线位置；在天线尺寸趋于无穷时给出渐近闭式解；数值仿真验证了方法在利用空间自由度、提升谱效率和对系统参数失配的鲁棒性方面的有效性。

Result: 结果表明，该布放框架能够高效利用近场通道的空间自由度，获得显著的谱效率提升，并对参数失配具有鲁棒性；渐近解与理论最优在广泛的实际场景下高度接近。

Conclusion: 提出了一种可扩展且近场感知的天线布放方法，结合了角域重构、静电等效、ODE求解和EVD两步降维，提供了理论与计算双重工具，同时给出在天线尺寸无限大时的闭式渐近解，为近场大规模MIMO系统的布放优化提供了新的思路与实践路径。

Abstract: Recent advancements in large-scale position-reconfigurable antennas have opened up new dimensions to effectively utilize the spatial degrees of freedom (DoFs) of wireless channels. However, the deployment of existing antenna placement schemes is primarily hindered by their limited scalability and frequently overlooked near-field effects in large-scale antenna systems. In this paper, we propose a novel antenna placement approach tailored for near-field massive multiple-input multiple-output systems, which effectively exploits the spatial DoFs to enhance spectral efficiency. For that purpose, we first reformulate the antenna placement problem in the angular domain, resulting in a weighted Fekete problem. We then derive the optimality condition and reveal that the {optimal} antenna placement is in principle an electrostatic equilibrium problem. To further reduce the computational complexity of numerical optimization, we propose an ordinary differential equation (ODE)-based framework to efficiently solve the equilibrium problem. In particular, the optimal antenna positions are characterized by the roots of the polynomial solutions to specific ODEs in the normalized angular domain. By simply adopting a two-step eigenvalue decomposition (EVD) approach, the optimal antenna positions can be efficiently obtained. Furthermore, we perform an asymptotic analysis when the antenna size tends to infinity, which yields a closed-form solution. Simulation results demonstrate that the proposed scheme efficiently harnesses the spatial DoFs of near-field channels with prominent gains in spectral efficiency and maintains robustness against system parameter mismatches. In addition, the derived asymptotic closed-form {solution} closely approaches the theoretical optimum across a wide range of practical scenarios.

</details>


### [7] [On the Ergodic Capacity for SIM-Aided Holographic MIMO Communications](https://arxiv.org/abs/2512.22068)
*Anastasios Papazafeiropoulos,Ioannis Bartsiokas,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 提出了一个针对HMIMO系统+SIM的新的经验闭式下界，用于在Rayleigh fading下的ergodic capacity；对有限天线和SIM单元的系统适用，且在全SNR范围内紧密；并给出低SNR分析及参数影响的直观结论。


<details>
  <summary>Details</summary>
Motivation: 在 HMIMO/SIM 场景中，容量分析面临高维复杂度，迫切需要可解析、紧凑的界来辅助设计和理解。现有研究多依赖仿真或近似，缺乏对有限维系统的闭式下界及低-SNR行为的洞察。

Method: 基于Rayleigh衰落假设，建立对数行列式的下界，利用随机矩阵理论对有限天线和SIM单元的系统进行分析，得到一个闭式表达。随后在低SNR区域展开并提取关键系统参数（天线数、SIM元件数、路径损耗等）对容量的影响。

Result: 给出一个对Ergodic Capacity的严格的闭式下界，适用于任意有限的天线数和SIM元件数；在全SNR范围内显示出与真实容量的紧致性；低SNR分析给出定量的参数影响趋势。

Conclusion: 提出的下界不仅有理论意义，也可用于系统设计和参数选取，尤其在低SNR场景。未来工作可扩展到其他衰落模型、考虑CSI估计误差、硬件非理想性等。

Abstract: We derive a novel closed-form lower bound on the ergodic capacity of holographic multiple-input multiple-output (HMIMO) systems enhanced by stacked intelligent metasurfaces (SIMs) under Rayleigh fading conditions. The proposed expression is valid for systems with a finite number of antennas and SIM elements and exhibits tightness throughout the whole signal-to-noise ratio (SNR) range. Furthermore, we conduct a comprehensive low-SNR analysis, offering meaningful observations on how key system parameters influence the capacity performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [8] [Composition Theorems for f-Differential Privacy](https://arxiv.org/abs/2512.21358)
*Natasha Fernandes,Annabelle McIver,Parastoo Sadeghi*

Main category: cs.CR

TL;DR: fDP 基于统计假设检验的理论基础与量化信息流的信道模型等价，通过伽罗瓦(Galois)连接在两个部分有序集合之间建立联系，从而获得对 fDP 的新组合定理，提升对复杂隐私设计的分析。


<details>
  <summary>Details</summary>
Motivation: 通过将 fDP 与量化信息流（QIF）结合，利用信道模型和假设检验的视角，提升对隐私损失的理解，并改进对复杂隐私设计的组合分析。

Method: 建立 fDP 与 QIF 信道模型之间的等价性，借助两个部分有序集合之间的伽罗瓦连接来证明这一等价性。

Result: 证明了 fDP 与 QIF 信道模型之间的等价性；由此推导出新的一般性组合定理，提升对复杂隐私设计的分析能力。

Conclusion: 跨领域的等价关系为隐私机制的分析与设计提供了新的理论工具，增强对组合性质的理解与应用。

Abstract: "f differential privacy" (fDP) is a recent definition for privacy privacy which can offer improved predictions of "privacy loss". It has been used to analyse specific privacy mechanisms, such as the popular Gaussian mechanism. In this paper we show how fDP's foundation in statistical hypothesis testing implies equivalence to the channel model of Quantitative Information Flow. We demonstrate this equivalence by a Galois connection between two partially ordered sets. This equivalence enables novel general composition theorems for fDP, supporting improved analysis for complex privacy designs.

</details>


### [9] [Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide](https://arxiv.org/abs/2512.21362)
*Behnam Farnaghinejad,Antonio Porsia,Annachiara Ruospo,Alessandro Savino,Stefano Di Carlo,Ernesto Sanchez*

Main category: cs.CR

TL;DR: 通过RTL级功耗分析揭示CVA6核心在AES实现上的明显侧信道泄漏，能够进行密钥恢复，强调在设计早期进行RTL级安全评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 在现代RISC-V处理器中，除了功能正确性，还需对抗侧信道攻击。该工作针对CVA6核心，评估其在软件实现的AES中的侧信道脆弱性，以证实RTL阶段的安全设计不足之处。

Method: 在RTL级功耗分析框架VeriSide下，对CVA6核心进行软件实现的AES加密的相关性功耗分析（CPA），以尝试从功耗信号中恢复密钥，评估泄漏程度。

Result: CPA分析揭示显著的泄漏，使得能从功耗数据中恢复AES密钥，证实CVA6在所测试情形下的安全性不足。该结果强调在芯片设计的早期RTL阶段进行安全评估的重要性。

Conclusion: 早期RTL安全评估对于面向安全的RISC-V设计至关重要，需在后续实现中引入对侧信道的防护机制与设计改进。

Abstract: Security in modern RISC-V processors demands more than functional correctness: It requires resilience to side-channel attacks. This paper evaluates the vulnerability of the side channel of the CVA6 RISC-V core by analyzing software-based AES encryption uses an RTL-level power profiling framework called VeriSide. This work represents that this design's Correlation Power Analysis (CPA) reveals significant leakage, enabling key recovery. These findings underscore the importance of early-stage RTL assessments in shaping future secure RISC-V designs.

</details>


### [10] [Key Length-Oriented Classification of Lightweight Cryptographic Algorithms for IoT Security](https://arxiv.org/abs/2512.21368)
*Arsalan Vahi*

Main category: cs.CR

TL;DR: 针对物联网中对称轻量密码的安全评估进行综述，强调密钥长度对安全性的决定性作用，提出基于应用特征和密钥长度的两类分类体系。


<details>
  <summary>Details</summary>
Motivation: IoT 设备资源受限，轻量加密算法的安全性评估需从应用环境出发，而不仅是实现和性能；需要关注对称轻量密码在现实世界中的安全强度，确保在实时和资源受限场景中的应用。

Method: 通过对现有文献的安全评估视角分析，聚焦对称轻量密码的安全强度；提出两类分类法：基于 IoT 应用特征的分类，以及基于密钥长度的安全等级评估。对比分析密钥长度对安全性的重要性。

Result: 发现密钥长度是影响轻量密码安全性的关键参数；键长<128比特的密码在保护敏感数据时往往不安全甚至不可行；并提出以密钥长度作为评估安全等级的核心指标的初步框架。

Conclusion: 安全评估应重视密钥长度等参数，结合应用特征建立分级的安全评估体系，为 IoT 场景下的轻量对称密码选择提供指导。

Abstract: The successful deployment of the Internet of Things (IoT) applications relies heavily on their robust security, and lightweight cryptography is considered an emerging solution in this context. While existing surveys have been examining lightweight cryptographic techniques from the perspective of hardware and software implementations or performance evaluation, there is a significant gap in addressing different security aspects specific to the IoT environment. This study aims to bridge this gap. This research presents a thorough survey focused on the security evaluation of symmetric lightweight ciphers commonly used in IoT systems. The objective of this study is to provide a holistic understanding of lightweight ciphers, emphasizing their security strength, which is an essential consideration for real-time and resource-constrained applications. Furthermore, we propose two taxonomies: one for classifying IoT applications based on their inherent characteristics, and another for evaluating security levels based on key size. Our findings indicate that key size is a critical parameter in the security of lightweight ciphers. Ciphers employing keys shorter than 128 bits are considered less secure or even insecure for protecting sensitive data

</details>


### [11] [A Systematic Review of Technical Defenses Against Software-Based Cheating in Online Multiplayer Games](https://arxiv.org/abs/2512.21377)
*Adwa Alangari,Ohoud Alharbi*

Main category: cs.CR

TL;DR: 对在线游戏反作弊技术的系统性综述，按服务器端检测、客户端防篡改、内核级反作弊驱动及硬件加速信任执行环境等四类进行评估，揭示各自的检测有效性、性能开销、隐私及可扩展性之间的权衡，并强调持续的对抗演化与对抗设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解当前软件作弊防护技术的全貌及其优劣，以指导更鲁棒、可扩展且对抗性强的反作弊设计。

Method: 系统性文献综述，收集并分类现有防作弊方法，依据检测有效性、性能开销、隐私影响、可扩展性等标准对各类方法进行评估，并讨论它们之间的权衡与未来方向。

Result: 提出四类防作弊方法的关键权衡：内核级方案检测能力强但隐私与稳定性风险高；服务器端方法入侵性低、但可视信息有限；客户端防篡改与硬件TEEs在隐私与可见性之间提供不同折中。总结出需要多层防御、对抗性设计与持续演化以应对作弊者的对抗升级。

Conclusion: 构建鲁棒的反作弊系统需综合多层防御，兼顾隐私、稳定性与可扩展性；应关注对抗性设计、跨层协同，以及对作弊者的持续进化进行跟进。

Abstract: This systematic literature review surveys technical defenses against software-based cheating in online multiplayer games. Categorizing existing approach-es into server-side detection, client-side anti-tamper, kernel-level anti-cheat drivers, and hardware-assisted TEEs. Each category is evaluated in terms of detection effectiveness, perfor-mance overhead, privacy im-pact, and scalability. The analy-sis highlights key trade-offs, particularly between the high visibility of kernel-level solutions and their privacy and stability risks, versus the low intrusive-ness but limited insight of server-side methods. Overall, the re-view emphasizes the ongoing arms race with cheaters and the need for robust, adversary-resistant anti-cheat designs.

</details>


### [12] [GoldenFuzz: Generative Golden Reference Hardware Fuzzing](https://arxiv.org/abs/2512.21524)
*Lichao Wu,Mohamadreza Rostami,Huimin Li,Nikhilesh Singh,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoldenFuzz 提出了一种两阶段的硬件模糊测试框架，使用 ISA 兼容的黄金参考模型 GRM 作为 DUT 的数字孪生，在快速 refinement 的同时提升覆盖率和漏洞发现效率，适用于多款 RISC-V 核心。


<details>
  <summary>Details</summary>
Motivation: 现代硬件系统越来越复杂，存在大量潜在缺陷和安全漏洞；现有硬件模糊测试器在语义感知、测试 refinement 和慢速设备仿真带来的开销方面存在不足。

Method: 两阶段模糊：先在快速且 ISA 兼容的 GRM 上进行模糊以实现低成本的测试 refinement；通过拼接精心选择的指令块来迭代构造测试用例，关注指令间与指令内的质量平衡；结合高覆盖与低覆盖样本的反馈机制引导状态空间探索；在 RocketChip、BOOM、CVA6 三个 RISC-V 处理器上评估。

Result: 与现有模糊器相比，GoldenFuzz 在覆盖率、测试用例长度和计算开销方面显著优于对手；发现并覆盖了所有已知漏洞，另外发现五个新漏洞，其中四个严重等级（CVSS v3）大于7；还在商用 BA51-H 核扩展中发现两处未知漏洞。

Conclusion: 通过将测试 refinement 与覆盖/漏洞探索部分解耦，并以 GRM 作为数字孪生，GoldenFuzz 实现了快速、有效的硬件模糊测试；在多款 RISC-V 核心上验证了其实用性，具有进一步提升硬件漏洞发现效率的潜力。

Abstract: Modern hardware systems, driven by demands for high performance and application-specific functionality, have grown increasingly complex, introducing large surfaces for bugs and security-critical vulnerabilities. Fuzzing has emerged as a scalable solution for discovering such flaws. Yet, existing hardware fuzzers suffer from limited semantic awareness, inefficient test refinement, and high computational overhead due to reliance on slow device simulation.
  In this paper, we present GoldenFuzz, a novel two-stage hardware fuzzing framework that partially decouples test case refinement from coverage and vulnerability exploration. GoldenFuzz leverages a fast, ISA-compliant Golden Reference Model (GRM) as a ``digital twin'' of the Device Under Test (DUT). It fuzzes the GRM first, enabling rapid, low-cost test case refinement, accelerating deep architectural exploration and vulnerability discovery on DUT. During the fuzzing pipeline, GoldenFuzz iteratively constructs test cases by concatenating carefully chosen instruction blocks that balance the subtle inter- and intra-instructions quality. A feedback-driven mechanism leveraging insights from both high- and low-coverage samples further enhances GoldenFuzz's capability in hardware state exploration. Our evaluation of three RISC-V processors, RocketChip, BOOM, and CVA6, demonstrates that GoldenFuzz significantly outperforms existing fuzzers in achieving the highest coverage with minimal test case length and computational overhead. GoldenFuzz uncovers all known vulnerabilities and discovers five new ones, four of which are classified as highly severe with CVSS v3 severity scores exceeding seven out of ten. It also identifies two previously unknown vulnerabilities in the commercial BA51-H core extension.

</details>


### [13] [Enhancing Distributed Authorization With Lagrange Interpolation And Attribute-Based Encryption](https://arxiv.org/abs/2512.21525)
*Keshav Sinha,Sumitra,Richa Kumari,Akashdeep Bhardwaj,Shawon Rahman*

Main category: cs.CR

TL;DR: 提出基于服务器端的多方执行框架以降低数据访问的计算开销，通过两条路径实现安全数据访问：1) 使用基于逆向函数的流密码对数据加密；2) 使用Shamir秘密分享进行对称密钥分发，并通过二阶Lagrange插值重构密钥，评估加解密时间、吞吐量、开销与安全性，面向组织内的大规模安全数据共享。


<details>
  <summary>Details</summary>
Motivation: 随着数据访问对 confidentiality 与授权的需求日益增加，传统的访问控制列表会带来显著的服务器计算开销与延迟，需要一种在保密性与效率之间取得平衡的方案，尤其是在大规模数据共享场景。

Method: 提出两种并行的处理路径以实现服务器端的多方执行：第一，使用Involution Function Based Stream Cipher对文件数据进行加密；第二，采用Shamir秘密分享将对称密钥分发给各用户，解密时通过二阶Lagrange插值从隐藏点重构密钥。该框架旨在降低服务器的计算开销与提高响应效率。

Result: 通过对加解密时间、吞吐量、计算开销以及安全性分析等指标的评估，验证所提方案在降低服务器开销方面具有潜在收益，且对安全性有相应分析。

Conclusion: 提出一种可用于组织内部大规模安全数据共享的多方执行机制，未来工作方向包括在更大规模环境中的扩展与优化。

Abstract: In todays security landscape, every user wants to access large amounts of data with confidentiality and authorization. To maintain confidentiality, various researchers have proposed several techniques. However, to access secure data, researchers use access control lists to grant authentication and provide authorization. The above several steps will increase the server's computation overhead and response time. To cope with these two problems, we proposed multiparty execution on the server. In this paper, we introduce two different approaches. The first approach is encryption, utilizing the Involution Function Based Stream Cipher to encrypt the file data. The second approach is key distribution, using the Shamir secret sharing scheme to divide and distribute the symmetric key to every user. The decryption process required key reconstruction, which used second order Lagrange interpolation to reconstruct the secret keys from the hidden points. The process will reduce the server's computational overhead. The results are evaluated based on the encryption and decryption time, throughput, computational overhead, and security analysis. In the future, the proposed mechanism will be used to share large-scale, secure data within the organization.

</details>


### [14] [Verifiable Passkey: The Decentralized Authentication Standard](https://arxiv.org/abs/2512.21663)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman*

Main category: cs.CR

TL;DR: 提出可验证凭证的“Verifiable Passkey”以解决传统无密码认证在存储限制和跨平台隐私跟踪方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的无密码认证（如 FIDO2 Passkeys）需要在集中服务器（RP）保存用户凭据，且受限于硬件密钥（如 TPM/安全密钥）的存储容量，导致用户需为每个账号创建新Passkey；并且通过单点登录（SSO）的联邦认证可能让身份提供方(IdP)跟踪用户跨服务的行为，带来隐私风险。

Method: 提出一种新标准“Verifiable Passkey”，使用户能够使用为可验证凭证（Verifiable Credential, VC）发行方创建的Passkeys，在任意平台上使用，防止隐私泄露或用户被跟踪。

Result: 本文给出一个概念性/标准化方案，提出Verifiable Passkey的设计思路和工作原理，但未给出具体实现细节、系统实现或实验评估。

Conclusion: Verifiable Passkey提供了一种潜在的隐私保护与跨平台可互操作的无密码认证路径，缓解存储容量和SSO造成的隐私风险，有望推动无密码生态的可扩展性。

Abstract: Passwordless authentication has revolutionized the way we authenticate across various websites and services. FIDO2 Passkeys, is one of the most-widely adopted standards of passwordless authentication that promises phishing-resistance. However, like any other authentication system, passkeys require the user details to be saved on a centralized server, also known as Relying Party (RP) Server. This has led users to create a new passkey for every new online account. While this just works for a limited number of online accounts, the limited storage space of secure storage modules like TPM or a physical security key limits the number of passkeys a user can have. For example, Yubico Yubikey 5 (firmware 5.0 - 5.6) offers to store only 25 passkeys, while firmware 5.7+ allows to store upto 100 [1]. To overcome this problem, one of the widely adopted approaches is to use Federated Authentication with Single Sign On (SSO). This allows the user to create a passkey for the Identity Provider (IdP) and use the IdP to authenticate to all service providers. This proves to be a significant privacy risk since the IdP can potentially track users across different services. To overcome these limitations, this paper introduces a novel standard 'Verifiable Passkey' that allows the user to use Passkeys created for a Verifiable Credential issuer across any platform without risking privacy or user tracking.

</details>


### [15] [Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2512.21681)
*Tian Li,Bo Lin,Shangwen Wang,Yusong Tan*

Main category: cs.CR

TL;DR: 本研究提出 VenomRACG，针对检索增强的代码生成中的检索器后门进行系统性分析，发现可注入仅占知识库极小比例的恶意代码就能显著提高被攻击的代码排名，且现有防御难以检测，揭示软件开发生态系统的供应链安全风险。


<details>
  <summary>Details</summary>
Motivation: RACG（检索增强的代码生成）在提升大型语言模型软件开发能力方面日益普及，但检索组件的安全性研究严重不足，潜在的后门攻击具有现实可行性与危害性。

Method: 提出 VenomRACG 作为一种强悍且隐蔽的攻击样本，使被污染的代码在统计上与良性代码难以区分；通过实验评估其对检索排名和下游模型生成的影响，并用多种潜在防御（包括潜在空间分析和逐字标记检测）进行对比。

Result: 实验表明：当知识库规模仅占比约 0.05% 时，攻击者可使后门检索器将易受攻击的代码在前五的命中率达到 51.29%；以 GPT-4o 等模型在目标场景中生成易受攻击代码的比例高于 40%；且总体模型性能受影响极小。此结果表明检索器后门是现实且严重的供应链安全威胁，现有防御不足以检测或防御。

Conclusion: 检索器后门对软件开发生态构成现实威胁，亟需更强健的安全策略与防御机制以抵御此类隐匿攻击。

Abstract: Retrieval-Augmented Code Generation (RACG) is increasingly adopted to enhance Large Language Models for software development, yet its security implications remain dangerously underexplored. This paper conducts the first systematic exploration of a critical and stealthy threat: backdoor attacks targeting the retriever component, which represents a significant supply-chain vulnerability. It is infeasible to assess this threat realistically, as existing attack methods are either too ineffective to pose a real danger or are easily detected by state-of-the-art defense mechanisms spanning both latent-space analysis and token-level inspection, which achieve consistently high detection rates. To overcome this barrier and enable a realistic analysis, we first developed VenomRACG, a new class of potent and stealthy attack that serves as a vehicle for our investigation. Its design makes poisoned samples statistically indistinguishable from benign code, allowing the attack to consistently maintain low detectability across all evaluated defense mechanisms. Armed with this capability, our exploration reveals a severe vulnerability: by injecting vulnerable code equivalent to only 0.05% of the entire knowledge base size, an attacker can successfully manipulate the backdoored retriever to rank the vulnerable code in its top-5 results in 51.29% of cases. This translates to severe downstream harm, causing models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while leaving the system's general performance intact. Our findings establish that retriever backdooring is not a theoretical concern but a practical threat to the software development ecosystem that current defenses are blind to, highlighting the urgent need for robust security measures.

</details>


### [16] [Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding](https://arxiv.org/abs/2512.21698)
*A V Uday Kiran Kandala*

Main category: cs.CR

TL;DR: 一个统一的栅格域隐写框架GPC，将异构数据（文本、图像、音频、视频）直接嵌入渲染后文本glyph的像素空间，通过对内部墨点的最小扰动像素计数表达载荷，解码通过再栅格化并统计像素差来恢复。


<details>
  <summary>Details</summary>
Motivation: 在字体栅格化后进行隐写，避免对语言结构的依赖，利用可重复的栅格化过程，使文本成为隐蔽的多模态数据载体；要求低计算开销、鲁棒且可解码性强。

Method: 以每个字形作为 covert 编码单元，通过最小强度增量扰动内部墨点的基数来表达载荷。扰动极小且不可感知，形成可解码的信号。解码通过重新栅格化Cover文本，减去规范字形栅格，再通过像素计数分析恢复载荷。为多模态输入，将图像强度、音频特征、视频帧值归一化并映射到受限整数序列，分布到字形。

Result: 在文本到文本嵌入上进行了示例，并推广到多模态输入的框架。修改对人眼几乎不可见，且由于确定性栅格行为，解码过程稳定、轻量。

Conclusion: GPC 框架提供了一种通用的可视化隐蔽媒介，用于在文本字形的栅格空间嵌入多模态数据，具有低计算成本和确定性解码的优势，同时对字体渲染的鲁棒性与潜在的检测风险也提出了关注点。

Abstract: This work introduces a unified raster domain steganographic framework, termed as the Glyph Perturbation Cardinality (GPC) framework, capable of embedding heterogeneous data such as text, images, audio, and video directly into the pixel space of rendered textual glyphs. Unlike linguistic or structural text based steganography, the proposed method operates exclusively after font rasterization, modifying only the bitmap produced by a deterministic text rendering pipeline. Each glyph functions as a covert encoding unit, where a payload value is expressed through the cardinality of minimally perturbed interior ink pixels. These minimal intensity increments remain visually imperceptible while forming a stable and decodable signal. The framework is demonstrated for text to text embedding and generalized to multimodal inputs by normalizing image intensities, audio derived scalar features, and video frame values into bounded integer sequences distributed across glyphs. Decoding is achieved by re-rasterizing the cover text, subtracting canonical glyph rasters, and recovering payload values via pixel count analysis. The approach is computationally lightweight, and grounded in deterministic raster behavior, enabling ordinary text to serve as a visually covert medium for multimodal data embedding.

</details>


### [17] [Assessing the Effectiveness of Membership Inference on Generative Music](https://arxiv.org/abs/2512.21762)
*Kurtis Chow,Omar Samiullah,Vinesh Sridhar,Hewen Zhang*

Main category: cs.CR

TL;DR: 对 MuseGAN 的成员推断攻击进行初步评估，结果显示音乐数据对现有 MIAs 相对鲁棒，攻击效果有限。


<details>
  <summary>Details</summary>
Motivation: 在生成式 AI 快速发展背景下，隐私保护与版权合规成为关键难题。成员推断攻击可能用于识别模型训练数据，或为版权方提供取证工具，因此研究其在生成音乐领域的可行性具有现实意义。

Method: 对现有的成员推断攻击方法在生成音乐模型 MuseGAN 上进行评估，分析这些攻击在识别训练数据成员方面的有效性。

Result: 与其他数据类型相比，音乐数据对已知的成员推断攻击较为鲁棒，攻击对训练数据成员的识别能力有限。

Conclusion: 这是一项初步研究，未来需进一步探索不同攻击策略、音乐表示和数据规模对 MIAs 的影响，并评估可能的防御措施。

Abstract: Generative AI systems are quickly improving, now able to produce believable output in several modalities including images, text, and audio. However, this fast development has prompted increased scrutiny concerning user privacy and the use of copyrighted works in training. A recent attack on machine-learning models called membership inference lies at the crossroads of these two concerns. The attack is given as input a set of records and a trained model and seeks to identify which of those records may have been used to train the model. On one hand, this attack can be used to identify user data used to train a model, which may violate their privacy especially in sensitive applications such as models trained on medical data. On the other hand, this attack can be used by rights-holders as evidence that a company used their works without permission to train a model.
  Remarkably, it appears that no work has studied the effect of membership inference attacks (MIA) on generative music. Given that the music industry is worth billions of dollars and artists would stand to gain from being able to determine if their works were being used without permission, we believe this is a pressing issue to study. As such, in this work we begin a preliminary study into whether MIAs are effective on generative music. We study the effect of several existing attacks on MuseGAN, a popular and influential generative music model. Similar to prior work on generative audio MIAs, our findings suggest that music data is fairly resilient to known membership inference techniques.

</details>


### [18] [Securing Cross-Domain Internet of Drones: An RFF-PUF Allied Authenticated Key Exchange Protocol With Over-the-Air Enrollment](https://arxiv.org/abs/2512.21827)
*Xuanyu Chen,Yue Zheng,Junqing Zhang,Guanxiong Shen,Chip-Hong Chang*

Main category: cs.CR

TL;DR: 提出一种结合射频指纹与物理不可克隆函数的轻量级互认证协议，面向物联网无人机（IoD）在D2D/D2G场景的跨域安全通信，实现OTA注册、无密钥存储的临时密钥生成，并通过形式化与安全分析证明鲁棒性，且在安全性、计算、通信与存储开销方面优于现有方案。


<details>
  <summary>Details</summary>
Motivation: IoD环境对跨域通信的安全性、资源受限性和动态部署提出高效、轻量级的认证与密钥交换需求。现有解决方案在计算、存储、第三方依赖及严格注册条件等方面存在显著局限性，难以支持动态跨域部署。

Method: 将射频指纹（RFF）用于OTA注册以实现设备识别；将物理不可克隆函数（PUF）作为互信根，支撑D2D与D2G之间的互认证；利用PUF的即时密钥生成能力与一次性密钥加密（OTP）实现无密钥存储的临时密钥；通过Informal安全分析和ProVerif形式化验证协议鲁棒性，并在计算、通信、存储等指标上与现有IoD认证方案进行对比。

Result: 证明表明协议具备对常见攻击的鲁棒性，且在安全性、计算开销、通信开销与存储需求方面优于现有IoD认证方案。

Conclusion: 将RFF与PUF有机结合，支持在动态跨域IoD场景下的高效、无密钥存储的安全认证，支持OTA注册并提升跨域部署的可行性与安全性。

Abstract: The Internet of Drones (IoD) is an emerging and crucial paradigm enabling advanced applications that require seamless, secure communication across heterogeneous and untrusted domains. In such environments, access control and the transmission of sensitive data pose significant security challenges for IoD systems, necessitating the design of lightweight mutual authentication and key exchange protocols. Existing solutions are often hampered by high computation overhead, reliance on third parties, the requirement for secret storage in resource-constrained drones, and the need for a strictly controlled enrollment environment. These limitations make them impractical for dynamic cross-domain deployment. To address these limitations, we propose a lightweight mutual authentication mechanism that integrates Radio Frequency Fingerprint (RFF) and Physical Unclonable Function (PUF) technologies for secure drone-to-drone (D2D) and drone-to-ground station server (D2G) communication. RFF-based device identification is used to achieve over-the-air (OTA) enrollment, while the PUF serves as the root of trust for establishing mutual authentication among communication parties. Additionally, the on-the-fly key generation capability of the PUF is co-designed with One-Time-Pad (OTP) encryption to realize ephemeral keying and eliminate the need for storing secrets within drones. Both informal security analysis and ProVerif-based formal security verification comprehensively demonstrate the resilience of our protocol against common security attacks. The proposed protocol also outperforms existing IoD authentication schemes in terms of security features, as well as computation, communication, and storage overhead.

</details>


### [19] [Abstraction of Trusted Execution Environments as the Missing Layer for Broad Confidential Computing Adoption: A Systematization of Knowledge](https://arxiv.org/abs/2512.22090)
*Quentin Michaud,Sara Ramezanian,Dhouha Ayed,Olivier Levillain,Joaquin Garcia-Alfaro*

Main category: cs.CR

TL;DR: 对TEE及其抽象层进行系统化分析，给出设计选项的分类、抽象层知识体系、改进机会，并指出WebAssembly在混合生态中的潜力及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: TEE生态多样且碎片化，抽象层的统一有助于开发者更高效地利用 confidential computing；需要对现有生态进行系统化比较，以指引设计与研究方向。

Method: 基于代表性TEE技术的文献回顾，建立以设计选项为维度的抽象层体系；描述底层技术、抽象层实现及特性；汇总利弊、机会与挑战；重点分析WebAssembly的覆盖能力。

Result: 提出一个分类框架，映射各设计选项的抽象层及其实现，揭示改进空间，强调WebAssembly具有最广的功能集和潜在的整合能力。

Conclusion: 未来研究方向包括推动更多抽象层的演进以适配不同TEE设计、提升互操作性、加强与 confidential computing 生态的集成，并继续深入探索WebAssembly等技术在TEE中的应用。

Abstract: Trusted Execution Environments (TEEs) protect sensitive code and data from the operating system, hypervisor, or other untrusted software. Different solutions exist, each proposing different features. Abstraction layers aim to unify the ecosystem, allowing application developers and system administrators to leverage confidential computing as broadly and efficiently as possible. We start with an overview of representative available TEE technologies. We describe and summarize each TEE ecosystem, classifying them in different categories depending on their main design choices. Then, we propose a systematization of knowledge focusing on different abstraction layers around each design choice. We describe the underlying technologies of each design, as well as the inner workings and features of each abstraction layer. Our study reveals opportunities for improving existing abstraction layer solutions. It also highlights WebAssembly, a promising approach that supports the largest set of features. We close with a discussion on future directions for research, such as how future abstraction layers may evolve and integrate with the confidential computing ecosystem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [A Reinforcement Learning Approach to Synthetic Data Generation](https://arxiv.org/abs/2512.21395)
*Natalia Espinosa-Dice,Nicholas J. Jackson,Chao Yan,Aaron Lee,Bradley A. Malin*

Main category: cs.LG

TL;DR: 提出 RLSyn，将数据生成建模为强化学习策略，利用 PPO 与鉴别器奖励实现数据高效且稳定的合成数据生成，在小样本下对比 GAN 与扩散模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 在小样本和隐私保护约束下，现有生成模型对数据量和训练稳定性要求高，需探索可数据高效的替代方法。

Method: 将数据生成器建模为患者记录的随机策略，使用近端策略优化（PPO）并结合来自鉴别器的奖励，形成 RLSyn 框架；在 AI-READI 和 MIMIC-IV-数据集上进行与 GAN/扩散模型的对比评估。

Result: RL-Syn 在 MIMIC-IV 上与扩散模型相当，优于 GAN；在小数据集 AI-READI 上优于两类对比方法；在隐私、实用性、保真度等多维评估中表现良好。

Conclusion: 将强化学习作为合成生物医学数据的原理性、有效替代方案，尤其在数据稀缺情境下具有显著优势。

Abstract: Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.

</details>


### [21] [kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning](https://arxiv.org/abs/2512.21409)
*Giacomo Turri,Grégoire Pacreau,Giacomo Meanti,Timothée Devergne,Daniel Ordonez,Erfan Mirzaei,Bruno Belucci,Karim Lounici,Vladimir Kostic,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: kooplearn is a machine-learning library that implements linear, kernel, and deep-learning estimators of dynamical operators and their spectral decompositions. kooplearn can model both discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators. By learning these operators, users can analyze dynamical systems via spectral methods, derive data-driven reduced-order models, and forecast future states and observables. kooplearn's interface is compliant with the scikit-learn API, facilitating its integration into existing machine learning and data science workflows. Additionally, kooplearn includes curated benchmark datasets to support experimentation, reproducibility, and the fair comparison of learning algorithms. The software is available at https://github.com/Machine-Learning-Dynamical-Systems/kooplearn.

</details>


### [22] [Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers](https://arxiv.org/abs/2512.21801)
*Krishna Chaitanya Sunkara,Rambabu Konakanchi*

Main category: cs.LG

TL;DR: 提出一种基于物联网的智能液体冷却系统泄漏监测与预测框架，结合 LSTM 预测泄漏概率以及随机森林进行即时检测，在合成数据上取得高精度，显示出在数据中心节能与运维方面的潜力。


<details>
  <summary>Details</summary>
Motivation: GPU 密集型数据中心的极端热负荷驱动液体冷却需求，但泄漏会导致非计划停机与修复延迟，造成能源浪费。本研究旨在通过可扩展的 IoT 监控与机器学习对泄漏进行早期预测与快速检测，从而减少能耗与故障影响。

Method: 使用 LSTM 进行概率性泄漏预测、随机森林用于即时检测；在符合 ASHRAE 2021 标准的合成数据上进行评估；系统架构包括 MQTT 流数据、InfluxDB 存储、Streamlit 可视化；并评估对 47-rack 机房的能耗影响。

Result: 关键结果包括：泄漏检测准确率 96.5%，在 90% 置信水平下，预测准确率 87%，预测区间为±30 分钟；湿度、压力、流量对预测信号强，温度因热惯性影响反应较弱；可实现 2-4 小时的泄漏预测，突发事件在 1 分钟内识别；对 47-rack 机房，年度能源浪费可降低约 1500 kWh；方法在当前为合成数据验证，但具备未来部署的可行性。

Conclusion: 证明了基于 IoT 的液冷泄漏监控与预测在数据中心节能与可靠性方面的可行性，未来可在真实运营环境中进一步验证与优化。

Abstract: AI data centers which are GPU centric, have adopted liquid cooling to handle extreme heat loads, but coolant leaks result in substantial energy loss through unplanned shutdowns and extended repair periods. We present a proof-of-concept smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting with Random Forest classifiers for instant detection. Testing on synthetic data aligned with ASHRAE 2021 standards, our approach achieves 96.5% detection accuracy and 87% forecasting accuracy at 90% probability within plus or minus 30-minute windows. Analysis demonstrates that humidity, pressure, and flow rate deliver strong predictive signals, while temperature exhibits minimal immediate response due to thermal inertia in server hardware. The system employs MQTT streaming, InfluxDB storage, and Streamlit dashboards, forecasting leaks 2-4 hours ahead while identifying sudden events within 1 minute. For a typical 47-rack facility, this approach could prevent roughly 1,500 kWh annual energy waste through proactive maintenance rather than reactive emergency procedures. While validation remains synthetic-only, results establish feasibility for future operational deployment in sustainable data center operations.

</details>


### [23] [DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction](https://arxiv.org/abs/2512.21433)
*Khondoker Mirazul Mumenin,Robert Underwood,Dong Dai,Jinzhen Wang,Sheng Di,Zarija Lukić,Franck Cappello*

Main category: cs.LG

TL;DR: A general deep-surrogate framework DeepCQ for predicting lossy compression quality across compressors/metrics/datasets; using a two-stage design separating heavy feature extraction from lightweight metric prediction, plus a mixture-of-experts for time-evolving data; achieves ~<10% error and outperforms existing methods, reducing I/O and computation.


<details>
  <summary>Details</summary>
Motivation: Rapid growth of scientific data makes post-compression quality assessment computationally expensive; a general, efficient surrogate is needed to predict data quality without full metric computation.

Method: Two-stage design: decouples feature extraction (computationally heavy) from metric prediction (lightweight). Builds a general surrogate model applicable to different error-bounded lossy compressors, quality metrics, and input datasets. Incorporates a mixture-of-experts to improve robustness across simulation timesteps with varying data distributions.

Result: Validated on four real-world scientific applications. Achieves prediction errors generally under 10% across most settings and significantly outperforms existing methods.

Conclusion: DeepCQ enables scientists to make informed compression decisions based on preferred data quality, substantially reducing I/O and computational overhead in scientific data analysis.

Abstract: Error-bounded lossy compression techniques have become vital for scientific data management and analytics, given the ever-increasing volume of data generated by modern scientific simulations and instruments. Nevertheless, assessing data quality post-compression remains computationally expensive due to the intensive nature of metric calculations. In this work, we present a general-purpose deep-surrogate framework for lossy compression quality prediction (DeepCQ), with the following key contributions: 1) We develop a surrogate model for compression quality prediction that is generalizable to different error-bounded lossy compressors, quality metrics, and input datasets; 2) We adopt a novel two-stage design that decouples the computationally expensive feature-extraction stage from the light-weight metrics prediction, enabling efficient training and modular inference; 3) We optimize the model performance on time-evolving data using a mixture-of-experts design. Such a design enhances the robustness when predicting across simulation timesteps, especially when the training and test data exhibit significant variation. We validate the effectiveness of DeepCQ on four real-world scientific applications. Our results highlight the framework's exceptional predictive accuracy, with prediction errors generally under 10\% across most settings, significantly outperforming existing methods. Our framework empowers scientific users to make informed decisions about data compression based on their preferred data quality, thereby significantly reducing I/O and computational overhead in scientific data analysis.

</details>


### [24] [dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.21446)
*Shirui Chen,Jiantao Jiao,Lillian J. Ratliff,Banghua Zhu*

Main category: cs.LG

TL;DR: dUltra是一种基于策略梯度的在策略学习框架，通过引入无掩策略规划头来学习MDLM的逐token取消位点，从而实现高效并行解码。相比现有蒸馏与启发式基线，在数学推理和代码生成任务上在准确性与效率之间取得更优的权衡，向Diffusion supremacy靠拢。


<details>
  <summary>Details</summary>
Motivation: 现有Mask diffusion语言模型在并行生成方面潜力巨大，但单次前传解码通常只能输出较少的标记；蒸馏型加速器往往在策略上离线并且受限于基模型样本质量，导致难以高效发挥潜力。需要一种在策略性、在策略学习基础上的方法，以提高并行解码的速度并保持或提升质量。

Method: 提出dUltra：基于Group Relative Policy Optimization的在策略强化学习框架。引入无掩计划头，针对每个token在独立伯努利分布下预测取消掩码的概率。联合优化基础扩散式语言模型和无掩序列规划器，奖励信号由可验证奖励、蒸馏奖励以及未掩的步数构成，促使更少步骤实现高质量输出。

Result: 在数学推理和代码生成任务上，dUltra相较于状态-of-the-art的启发式与蒸馏基线，显著提升了准确性与效率的权衡，接近甚至推进了“diffusion supremacy”对自回归模型的竞争力。

Conclusion: dUltra证明了在策略强化学习框架下对MDLM进行在策略优化和无掩位点规划的可行性与潜力，提升了并行解码的速率与质量并为未来更广泛任务的应用奠定基础。

Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.

</details>


### [25] [An Information Theoretic Perspective on Agentic System Design](https://arxiv.org/abs/2512.21720)
*Shizhe He,Avanika Narayan,Ishan S. Khare,Scott W. Linderman,Christopher Ré,Dan Biderman*

Main category: cs.LG

TL;DR: 提出将压缩-预测器LM体系视为信息论问题，借助噪声信道下的互信息估计量，任务无关地评估压缩质量。实证表明：增大压缩器规模比扩大神经预测器更有效地提升性能与信息传递效率；在五个数据集和三种模型族中，压缩器越大，单位token传递的信息越多。将结论应用于Deep Research系统时，局部3B参数的压缩器即可在26%的API成本下接近 frontier-LM的99%性能。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩-预测器设计往往缺乏体系性指导，依赖任务特定的参数搜索来评估压缩与预测的贡献。需要一个任务无关、信息论驱动的框架来量化压缩质量及其对下游性能的影响。

Method: 把压缩LM视为噪声信道，提出一个简单的互信息估计量来量化上下文与其压缩之间的互信息；在五个数据集和三种模型族上进行广泛的经验分析，比较不同大小的压缩器与预测器，评估每个token承载的信息量（信息传递效率）；将该框架应用于Deep Research系统以评估在本地压缩器与云端预测器的组合下的成本与性能。

Result: 结果显示：更大压缩器不仅更准确，而且每token传递的信息量更多，使得信息传递更高效。以7B的Qwen-2.5压缩器相比1.5B版本，在准确性、文本压缩率和单位token信息量方面均显著优于小模型；跨数据集，扩展压缩器规模比扩展预测器规模更有效。将原理应用到Deep Research系统，局部3B参数的压缩器可在26% API成本的条件下恢复接近 Frontier-LM的99%性能。

Conclusion: 信息论框架为压缩-预测器LM设计提供解释性指导：应优先扩大压缩器规模以提升信息传递效率与下游性能，且在本地化部署中，较小的压缩器即可实现高效且成本更低的系统。将压缩器放在前端、预测器在云端的组合具有显著的成本与能效优势。

Abstract: Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\times$ more accurate, $4.6\times$ more concise, and conveys $5.5\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of API costs.

</details>


### [26] [An Equivariance Toolbox for Learning Dynamics](https://arxiv.org/abs/2512.21447)
*Yongyi Yang,Liu Ziyin*

Main category: cs.LG

TL;DR: 提出一个通用的等变性工具箱，用以推导学习动态中的一阶和二阶约束，将Noether型分析从梯度约束扩展到Hessian约束，从对称性推广到广义等变性，并覆盖连续到离散变换。


<details>
  <summary>Details</summary>
Motivation: 尽管对称性和等变性是理解深度学习理论结果的关键，但现有分析多为特定问题且多聚焦于一阶结论（如守恒律和隐式偏置），对二阶结构的含义知之甚少，需要揭示在变换结构下的曲率、梯度与Hessian的关系，以及损失景观的几何特征。

Method: 建立一个通用的等变性工具箱，给出学习动力学的一阶与二阶耦合约束。该框架在三方面扩展Noether-type分析：从梯度约束到Hessian约束、从对称性到一般等变性、从连续变换到离散变换。第一阶内涵通过一个统一的等式同时涵盖守恒律与隐式偏置关系。

Result: 提供关于曲率的结构性预测：哪些方向是平坦或尖锐、梯度如何对齐到Hessian特征子空间，以及损失景观几何如何体现底层变换结构。通过若干应用，既能回收已知结果，也能给出新的表征，将变换结构与现代优化几何的观测联系起来。

Conclusion: 该框架实现了一阶与二阶结构的统一理解，将对称性/等变性与优化几何联系起来，扩展了经典Noether-type分析，并在理论与经验之间架起桥梁。

Abstract: Many theoretical results in deep learning can be traced to symmetry or equivariance of neural networks under parameter transformations. However, existing analyses are typically problem-specific and focus on first-order consequences such as conservation laws, while the implications for second-order structure remain less understood. We develop a general equivariance toolbox that yields coupled first- and second-order constraints on learning dynamics. The framework extends classical Noether-type analyses in three directions: from gradient constraints to Hessian constraints, from symmetry to general equivariance, and from continuous to discrete transformations. At the first order, our framework unifies conservation laws and implicit-bias relations as special cases of a single identity. At the second order, it provides structural predictions about curvature: which directions are flat or sharp, how the gradient aligns with Hessian eigenspaces, and how the loss landscape geometry reflects the underlying transformation structure. We illustrate the framework through several applications, recovering known results while also deriving new characterizations that connect transformation structure to modern empirical observations about optimization geometry.

</details>


### [27] [RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models](https://arxiv.org/abs/2512.21572)
*Anthony Bolton,Wuyang Zhou,Zehua Chen,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: 以 Schrödinger Bridge 为基础的 RefineBridge 对 TSFM 的预测进行上下文条件化的随机传输，能在不同预测时 horizon 上提升金融时间序列预测的准确性，优于传统 LoRA 调整。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列的非平稳性、重尾分布及高频噪声使 transformer 基础模型难以直接泛化；LoRA 等参数高效微调方法在金融数据上效果有限，需通过引入一个与模型目标互补的 refined 机制提升预测质量。

Method: 在 TSFM 预测结果作为生成先验、实测目标作为目标的框架下，基于可处理的 Schrödinger Bridge 架构学习上下文条件化的随机传输映射，把先验逐步引导接近真实目标；该过程可迭代进行，从较劣先验向目标收敛，提升预测。

Result: 在多个金融基准数据集上的仿真实验表明 RefineBridge 在不同预测时 horizon 上都能持续提升现有最先进 TSFM 的性能。

Conclusion: RefineBridge 提供了一种原理性强的预测修正机制，能有效补充 TSFM，提升金融时间序列预测性能，对非平稳性和噪声具有鲁棒性，具备广泛应用潜力。

Abstract: Financial time series forecasting is particularly challenging for transformer-based time series foundation models (TSFMs) due to non-stationarity, heavy-tailed distributions, and high-frequency noise present in data. Low-rank adaptation (LoRA) has become a popular parameter-efficient method for adapting pre-trained TSFMs to downstream data domains. However, it still underperforms in financial data, as it preserves the network architecture and training objective of TSFMs rather than complementing the foundation model. To further enhance TSFMs, we propose a novel refinement module, RefineBridge, built upon a tractable Schrödinger Bridge (SB) generative framework. Given the forecasts of TSFM as generative prior and the observed ground truths as targets, RefineBridge learns context-conditioned stochastic transport maps to improve TSFM predictions, iteratively approaching the ground-truth target from even a low-quality prior. Simulations on multiple financial benchmarks demonstrate that RefineBridge consistently improves the performance of state-of-the-art TSFMs across different prediction horizons.

</details>


### [28] [MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding](https://arxiv.org/abs/2512.21506)
*Aiwei Zhang,Arvind Pillai,Andrew Campbell,Nicholas C. Jacobson*

Main category: cs.LG

TL;DR: MotionTeller 将分钟级 actigraphy 与冻结的解码器大语言模型结合，通过一个投影模块在语言模型的标记空间中生成自由文本的日常行为摘要；在 NHANES 数据集上显著优于基线，实现高语义保真与词汇准确性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感数据需要自然语言摘要，直接从原始信号生成易读文本描述，提升临床评估与个性化干预的可读性；现有方法多为模板或提示方法，缺乏端到端的生成能力。

Method: 提出 MotionTeller：包含预训练的 actigraphy 编码器、一个轻量投影模块，将行为嵌入投射到冻结的解码器型大语言模型的 token 空间；形成一个可自回归生成框架。数据方面构建了 54,383 对 (actigraphy, text) 的数据集，源自 NHANES；使用交叉熵损失训练，且仅对语言 token 进行监督。

Result: 在语义保真度和词汇准确性方面表现优异：BERTScore-F1 0.924，ROUGE-1 0.722；相比提示基线 ROUGE-1 提升约 7%；训练过程稳定，平均损失在 15 次 epoch 收敛至 0.38；定性分析显示能捕捉昼夜节律结构与行为转变，PCA 表明嵌入空间的簇对齐增强。

Conclusion: MotionTeller 提供一个可扩展、可解释的系统，将穿戴传感数据转化为流畅、以人为中心的文本描述，为行为监测、临床评审和个性化健康干预开辟新途径。

Abstract: As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.

</details>


### [29] [Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering](https://arxiv.org/abs/2512.21510)
*Wenyuan Yang,Jie Xu,Hongqing He,Jiangzhang Gan,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 提出 TreeEIC，通过缺失模式树将数据分组并在各组内进行多视角聚类，随后通过决策集合进行聚类结果集成，并通过知识蒸馏将集合知识传回视图特定模型，从而在高度不一致的缺失模式下实现更强 IMVC 性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多视图数据往往具有高度不一致的缺失模式，导致可用的多视图对无法充分利用，现有的 incomplete multi-view clustering (IMVC) 方法在这方面存在性能瓶颈。

Method: 提出缺失模式树（missing-pattern tree）模型将数据按不同缺失模式分组成若干决策集，在每个集合内进行多视图聚类；引入多视图决策集成模块以基于不确定性权重聚合各决策集的聚类结果，抑制不可靠的聚类决策；以及一个集成到个体视图的知识蒸馏模块，将 ensemble 知识传递给视图特定的聚类模型，促进跨视图一致性和簇内判别的共同优化。

Result: 在多个基准数据集上进行的大量实验表明，TreeEIC实现了最先进的 IMVC 性能，并在高度不一致的缺失模式下表现出更强的鲁棒性。

Conclusion: 通过对缺失模式的系统化利用，结合集成决策与跨视图蒸馏，TreeEIC 能在不完整的多视图数据场景中实现更高的聚类效果与稳定性。

Abstract: Real-world multi-view data usually exhibits highly inconsistent missing patterns which challenges the effectiveness of incomplete multi-view clustering (IMVC). Although existing IMVC methods have made progress from both imputation-based and imputation-free routes, they have overlooked the pair under-utilization issue, i.e., inconsistent missing patterns make the incomplete but available multi-view pairs unable to be fully utilized, thereby limiting the model performance. To address this, we propose a novel missing-pattern tree based IMVC framework entitled TreeEIC. Specifically, to achieve full exploitation of available multi-view pairs, TreeEIC first defines the missing-pattern tree model to group data into multiple decision sets according to different missing patterns, and then performs multi-view clustering within each set. Furthermore, a multi-view decision ensemble module is proposed to aggregate clustering results from all decision sets, which infers uncertainty-based weights to suppress unreliable clustering decisions and produce robust decisions. Finally, an ensemble-to-individual knowledge distillation module transfers the ensemble knowledge to view-specific clustering models, which enables ensemble and individual modules to promote each other by optimizing cross-view consistency and inter-cluster discrimination losses. Extensive experiments on multiple benchmark datasets demonstrate that our TreeEIC achieves state-of-the-art IMVC performance and exhibits superior robustness under highly inconsistent missing patterns.

</details>


### [30] [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)
*Hongqing He,Jie Xu,Wenyuan Yang,Yonghua Zhu,Guoqiu Wen,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 提出一个统一的全局-局部图引导对比学习框架，用于处理不完整与嘈杂的多视图聚类问题。通过全局视图亲和图构造新的样本对来挖掘互补信息（解决 rare-paired 问题），以及利用局部邻居为对比学习加权以缓解 mis-paired 问题；该方法无需填充缺失值，实验在不完整和含噪声的多视图数据上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多视图数据常常存在缺失视图或视图错配，导致对比学习在多视图聚类中的互补信息难以充分挖掘，且容易被错误的样本对引导。现有CL-based MVC在不完整/嘈杂数据上鲁棒性不足，亟需一种无填充的、能兼顾全局与局部信息的统一框架来增强聚类效果。

Method: 提出全球图引导的对比学习，以全局视图亲和图为基础重新构造样本对，从全局层面挖掘互补信息；并结合局部图加权对比学习，利用局部邻居生成样本对权重，动态增强或削弱某些对比项以对抗错配。该框架为统一的全局-局部图引导对比学习且无填充缺失值。

Result: 在不完整和含噪声的多视图数据Setting下，实验显示该方法在多项指标上超越最新方法，证明了对 Rare-paired 与 Mis-paired 问题的有效缓解和鲁棒性提升。

Conclusion: 所提出的全局-局部图引导的对比学习框架有效解决了 rare-paired 与 mis-paired 问题，且对不完整/嘈杂数据具备良好鲁棒性，具有广泛的嵌入式MVC任务应用潜力。

Abstract: Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.

</details>


### [31] [First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions](https://arxiv.org/abs/2512.21521)
*Egor Shulgin,Grigory Malinovsky,Sarit Khirirat,Peter Richtárik*

Main category: cs.LG

TL;DR: 提出了 Fed-α-NormEC/ Fed-α-NormE 框架，在差分隐私保护下实现联邦学习，支持多本地更新、部分客户端参与、服务器与客户端步长分离，理论给出收敛性与 DP 保证，且通过私有深度学习任务的实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私联邦学习方法往往依赖不现实的假设（如梯度有界、数据异质性强等），且忽略实际场景中的多轮本地更新与部分客户端参与等特征，导致理论与实践脱节。因此，需要在常规假设下，兼顾实际 FL 特性，提供同时具备收敛性与隐私保护的框架。

Method: 提出 Fed-α-NormEC/Fed-α-NormE 框架，在本地执行多轮本地更新（全量与增量梯度步）、实现服务端与客户端步长分离、支持部分客户端参与；通过子采样实现隐私放大从而达到差分隐私保护；给出在标准假设下的收敛性分析与 DP 证明，并通过对私有深度学习任务的实验验证理论结论。

Result: 给出理论层面的收敛性与 DP 保证；在私有深度学习任务上进行实验，展示了在差分隐私约束下的可行性和较好性能。

Conclusion: 该工作首次在符合常规假设的前提下，提出同时支持多本地更新与部分客户端参与的差分隐私联邦学习框架，具备严格的理论收敛与 DP 证据，显著提升实际部署的可行性与隐私保护效果。

Abstract: Federated Learning (FL) enables collaborative training on decentralized data. Differential privacy (DP) is crucial for FL, but current private methods often rely on unrealistic assumptions (e.g., bounded gradients or heterogeneity), hindering practical application. Existing works that relax these assumptions typically neglect practical FL features, including multiple local updates and partial client participation. We introduce Fed-$α$-NormEC, the first differentially private FL framework providing provable convergence and DP guarantees under standard assumptions while fully supporting these practical features. Fed-$α$-NormE integrates local updates (full and incremental gradient steps), separate server and client stepsizes, and, crucially, partial client participation, which is essential for real-world deployment and vital for privacy amplification. Our theoretical guarantees are corroborated by experiments on private deep learning tasks.

</details>


### [32] [Generative Actor Critic](https://arxiv.org/abs/2512.21527)
*Aoyang Qin,Deqian Kong,Wei Wang,Ying Nian Wu,Song-Chun Zhu,Sirui Xie*

Main category: cs.LG

TL;DR: Generative Actor Critic (GAC) decouples policy evaluation and policy improvement by learning a joint distribution p(τ, y) over trajectories and returns, and performs latent-plan-based inference for exploitation and exploration, achieving strong offline performance and significantly better offline-to-online transfer on Gym-MuJoCo and Maze2D, even without step-wise rewards.


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在离线预训练模型通过在线体验进行微调时容易受限，难以充分利用离线数据进行有效的策略改进。为实现更灵活的离线到在线转移，本文提出通过生成模型来统一表征轨迹与回报的联合分布，并将策略改进转化为对该模型的推理任务，从而解耦策略评估与策略改进的过程。

Method: 提出一个具有连续潜在计划向量的潜变量模型，学习联合分布 p(τ, y)。在推理层面，利用该模型进行两类推理：1) 开发性利用（exploitation）：优化潜在计划向量以最大化期望回报；2) 探索性推理（exploration）：在动态调整的目标回报条件下采样潜在计划向量。通过离线数据进行模型训练，并在 Gym-MuJoCo 与 Maze2D 上进行实验，比较离线到在线的性能提升。

Result: 实验结果显示，GAC 在离线性能上表现出色，并相较于现有方法显著提升离线到在线的转移能力，即使在没有逐步奖励信号的情况下也能实现良好表现。

Conclusion: GAC提供了一种新颖的解耦决策框架，通过对联合轨迹-回报分布的生成建模与以潜在计划为核心的推理，实现更灵活的策略改进与更稳健的离线到在线性能提升，适用于多种离线强化学习任务，未来可扩展至更复杂的规划与推理场景。

Abstract: Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.

</details>


### [33] [AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2512.21544)
*Xinru Wen,Weizhong Lin,Xuan Xiao*

Main category: cs.LG

TL;DR: 两阶段AVP-Fusion框架：通过自适应特征融合和对比学习提升抗病毒肽识别与亚类预测，达到SOTA表现并可在数据稀缺场景进行传递学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉复杂的序列依赖，且在难以分类的样本上鲁棒性不足，亟需一种可解释且高通量的AVP筛选工具。

Method: 提出两阶段深度学习框架AVP-Fusion：第一阶段通过10种描述符构建全景特征空间，采用自适应门控机制动态调节局部卷积特征与全局BiLSTM依赖的权重；第二阶段采用以Online Hard Example Mining为驱动的对比学习，结合BLOSUM62增强的数据扩增以强化边界；并在迁移学习框架下实现六个病毒科与八种具体病毒的亚类预测，适用于小样本情形。

Result: 在基准数据集Set 1上实现准确率0.9531、MCC 0.9064，显著优于现有方法；第二阶段通过迁移学习实现对六个病毒科与八种具体病毒的精准亚类预测，即使在样本有限的情况下也具备良好表现。

Conclusion: AVP-Fusion为高通量抗病毒药物筛选提供稳健且可解释的工具，具备优越的识别与转移学习能力。

Abstract: Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model's decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.

</details>


### [34] [Discovering Sparse Recovery Algorithms Using Neural Architecture Search](https://arxiv.org/abs/2512.21563)
*Patrick Yubeaton,Sarthak Gupta,M. Salman Asif,Chinmay Hegde*

Main category: cs.LG

TL;DR: 通过元学习/神经架构搜索（NAS）在大规模搜索空间中自动发现并再现信号处理中的迭代算法，如ISTA/FISTA，并具备对不同数据分布和其他算法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在信号处理的逆问题中，设计有效算法极具挑战且依赖经验直觉。利用元学习与NAS自动化发现算法可提升效率、创造性和可扩展性。

Method: 提出一个元学习框架，利用NAS在含有超过5万变量的搜索空间中 rediscover ISTA/FISTA 的关键要素，并展示对多种数据分布和算法的适用性。

Result: 框架能够在给定数据分布下 rediscover 这两类算法的关键组成部分，且具备对其他数据分布和算法的推广能力。

Conclusion: 基于NAS的元学习框架对自动发现迭代算法具有潜力，能扩展到更多信号处理任务及其他领域的算法发现。

Abstract: The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.

</details>


### [35] [A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)](https://arxiv.org/abs/2512.21610)
*Jagaran Chakma,Zhiguang Zhou,Jyoti Chakma,Cao YuSen*

Main category: cs.LG

TL;DR: 基于数据驱动的多目标机器学习框架用于预测UHPC的力学性能、流动性和孔隙度，采用两阶段XGBoost建模并辅以特征清洗和SHAP解释，辅以GUI界面，显著提升预测精度并降低试验次数。


<details>
  <summary>Details</summary>
Motivation: UHPC的力学性能、流动性和孔隙度之间高度相关且对配方敏感，需要在尽可能少的实验基础上实现准确预测。现有算法多、缺乏多目标一致性，且易受多重共线性与异常值影响，因此需要一个高效、可解释且实用的预测框架。

Method: 评估21种机器学习算法，选出5个高性能模型；采用超参数搜索（随机搜索）和K折交叉验证，XGBoost表现最佳。模型分两阶段：阶段1在原始数据上训练XGBoost。阶段2对数据进行清洗：去除多重共线性特征、用Isolation Forest检测异常值、用SHAP识别重要特征，并以清洗后的数据再次训练XGBoost作为最终模型。并开发GUI以支持材料设计师。

Result: 最终模型在所有输出中均实现高预测准确性；相较于阶段1，阶段2模型具有更稳健的表现；GUI提升用户的可用性和工作流效率；整体降低UHPC配方试验需求。

Conclusion: 将数据清洗、特征选择、可解释性分析与再训练集成到一体的多阶段预测框架，有效提升UHPC配方设计的预测能力和设计效率，具有较强的工程落地价值。

Abstract: This study presents a data-driven, multi-objective approach to predict the mechanical performance, flow ability, and porosity of Ultra-High-Performance Concrete (UHPC). Out of 21 machine learning algorithms tested, five high-performing models are selected, with XGBoost showing the best accuracy after hyperparameter tuning using Random Search and K-Fold Cross-Validation. The framework follows a two-stage process: the initial XGBoost model is built using raw data, and once selected as the final model, the dataset is cleaned by (1) removing multicollinear features, (2) identifying outliers with Isolation Forest, and (3) selecting important features using SHAP analysis. The refined dataset as model 2 is then used to retrain XGBoost, which achieves high prediction accuracy across all outputs. A graphical user interface (GUI) is also developed to support material designers. Overall, the proposed framework significantly improves the prediction accuracy and minimizes the need for extensive experimental testing in UHPC mix design.

</details>


### [36] [MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations](https://arxiv.org/abs/2512.21633)
*Qiuqi Li,Yiting Liu,Jin Zhao,Wencan Zhu*

Main category: cs.LG

TL;DR: A scalable Neural Galerkin framework that integrates Meta-Auto-Decoder (MAD) and space-time decoupling to improve generalization, stability, and efficiency for parametric PDEs, enabling long-horizon, physically consistent predictions with reduced computation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of standard neural PDE solvers (PINNs, Deep Galerkin) which struggle with generalization to unseen parameters and efficient long-time integration due to reliance on full space-time approximations and high computational costs.

Method: Enhance Neural Galerkin Method (NGM) by (1) embedding the Meta-Auto-Decoder (MAD) paradigm for rapid adaptation across parameter configurations, (2) enforcing space-time decoupling to stabilize and speed up time integration, and (3) employing randomized sparse updates to cut computational cost while preserving accuracy, yielding a scalable, physics-consistent solver for parameterized evolution equations.

Result: Numerical experiments on benchmark problems show that the proposed method achieves competitive accuracy, robustness, and adaptability compared to existing approaches, with improved efficiency and stable long-horizon predictions under varying parameters.

Conclusion: The MAD-augmented Neural Galerkin framework provides a scalable, physically consistent, and efficient solver for parameterized PDEs, enabling accurate long-time predictions with substantially reduced computational overhead and strong generalization to unseen parameter regimes.

Abstract: Parametric partial differential equations (PDEs) are fundamental for modeling a wide range of physical and engineering systems influenced by uncertain or varying parameters. Traditional neural network-based solvers, such as Physics-Informed Neural Networks (PINNs) and Deep Galerkin Methods, often face challenges in generalization and long-time prediction efficiency due to their dependence on full space-time approximations. To address these issues, we propose a novel and scalable framework that significantly enhances the Neural Galerkin Method (NGM) by incorporating the Meta-Auto-Decoder (MAD) paradigm. Our approach leverages space-time decoupling to enable more stable and efficient time integration, while meta-learning-driven adaptation allows rapid generalization to unseen parameter configurations with minimal retraining. Furthermore, randomized sparse updates effectively reduce computational costs without compromising accuracy. Together, these advancements enable our method to achieve physically consistent, long-horizon predictions for complex parameterized evolution equations with significantly lower computational overhead. Numerical experiments on benchmark problems demonstrate that our methods performs comparatively well in terms of accuracy, robustness, and adaptability.

</details>


### [37] [Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms](https://arxiv.org/abs/2512.21638)
*Jagaran Chakma,Zhiguang Zhou,Badhan Chakma*

Main category: cs.LG

TL;DR: 本研究比较三类ML模型（ET-XGB、RF-LGBM、Transformer-XGB）用于预测钢-聚丙烯纤维增强高性能混凝土的三种力学性能：抗压强度、抗弯强度与抗拉强度。ET-XGB整体最准确，RF-LGBM对FS最稳定，Transformer-XGB在不确定性方面相对较高；SHAP分析揭示关键影响因子。


<details>
  <summary>Details</summary>
Motivation: 提升HPC力学性能预测的准确性、可解释性与泛化性，便于混合比设计优化与结构性能评估。

Method: 将ET-XGB、RF-LGBM、Transformer-XGB作为三大模型族，在CS、FS、TS上进行k折交叉验证、超参数优化、SHAP解释与不确定性分析。数据来自公开发表的实验研究。

Result: ET-XGB在测试集获得CS R^2≈0.994、FS R^2≈0.944、TS R^2≈0.978，CS与TS的不确定性较低（约13–16%与≈30.4%）。RF-LGBM对FS预测最稳定，R^2≈0.977，FS的不确定性约在5–33%之间。Transformer-XGB在TS(R^2≈0.978)与FS(R^2≈0.967)均具高预测力，但整体不确定性最高，泛化能力相对较弱。SHAP分析显示纤维长径比（AR1、AR2）、硅微粉（Sfu）及钢纤维含量（SF）是主要正向影响因素；水含量（W）与水浆比（w/b）对强度有负向作用。

Conclusion: 结果表明，机器学习模型能够提供准确、可解释且具良好泛化性的HPC力学属性预测，为混合比优化和结构性能评估提供有力工具。

Abstract: This research develops and evaluates machine learning models to predict the mechanical properties of steel-polypropylene fiber-reinforced high-performance concrete (HPC). Three model families were investigated: Extra Trees with XGBoost (ET-XGB), Random Forest with LightGBM (RF-LGBM), and Transformer with XGBoost (Transformer-XGB). The target properties included compressive strength (CS), flexural strength (FS), and tensile strength (TS), based on an extensive dataset compiled from published experimental studies. Model training involved k-fold cross-validation, hyperparameter optimization, Shapley additive explanations (SHAP), and uncertainty analysis to ensure both robustness and interpretability. Among the tested approaches, the ET-XGB model achieved the highest overall accuracy, with testing R^2 values of 0.994 for CS, 0.944 for FS, and 0.978 for TS and exhibited lowest uncertainty for CS and TS (approximately 13-16% and 30.4%, respectively). The RF-LGBM model provided the most stable and reliable predictions for FS (R^2 0.977), yielding the lowest uncertainty for FS (approximately 5-33%). The Transformer-XGB model demonstrated strong predictive capability (R^2 0.978 for TS and 0.967 for FS) but consistently showed the highest uncertainty, indicating reduced generalization reliability. SHAP analysis further indicated that fiber aspect ratios (AR1 and AR2), silica fume (Sfu), and steel fiber content (SF) were the most influential predictors of strength, whereas water content (W) and the water-binder ratio (w/b) consistently had negative effects. The findings confirm that machine learning models can provide accurate, interpretable, and generalizable predictions of HPC mechanical properties. These models offer valuable tools for optimizing concrete mix design and enhancing structural performance evaluation in engineering applications.

</details>


### [38] [Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search](https://arxiv.org/abs/2512.21648)
*Maximilian Weichart*

Main category: cs.LG

TL;DR: 提出 Inverse-RPO，系统性地从无先验的 UCB 推导出带先验的 UCT；以方差感知的 UCB-V 为例得到两种新颖的带先验的树策略，性能优于 PUCT，且实现开销低；并扩展 mctx 库，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决现有先验化 UCT 缺乏严格原理基础的问题，将强大但理论不明确的 PUCT 振兴成可从第一性原理推导的框架，并提升长期规划下的探索效率。

Method: 提出逆向正则化策略优化（Inverse-RPO）框架，使任意先验无关的 UCB 可以导出对应的带先验的 UCT。将该方法应用于方差感知的 UCB-V，得到两种带方差估计的先验化树策略。实现并扩展 mctx 库以支持这些策略。

Result: 在多项基准测试中，所得到的方差感知带先验 UCTs 超越 PUCT；无额外计算成本；代码开源。

Conclusion: Inverse-RPO 提供一个普适、原理性的来源来设计先验化 UCT；方差感知先验化可进一步提升 MCTS 的探索效率，并为未来的先验化 UCT 研究提供平台。

Abstract: Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.

</details>


### [39] [Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation](https://arxiv.org/abs/2512.21650)
*Xiao Liu,Junchen Jin,Yanjie Zhao,Zhixuan Xing*

Main category: cs.LG

TL;DR: Proposes Causal-HM, a unified multimodal unsupervised anomaly detection framework that explicitly models Process-to-Result causal relations via a Sensor-Guided CHM Modulation and a Causal-Hierarchical Architecture, achieving state-of-the-art I-AUROC of 90.7% on Weld-4M across four modalities.


<details>
  <summary>Details</summary>
Motivation: Addresses causal blindness in multimodal UAD by leveraging physical generative logic (Process to Result) and tackling the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals in smart manufacturing, particularly robotic welding.

Method: Two main innovations: (1) Sensor-Guided CHM Modulation that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction; (2) Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency. The framework operates across four modalities: real-time video, audio, sensor data, and post-weld images.

Result: Empirical evaluation on a newly constructed Weld-4M benchmark across four modalities shows state-of-the-art I-AUROC of 90.7%. Code will be released after paper acceptance.

Conclusion: Modeling the physical Process-to-Result dependency with sensor-guided modulation and a causal hierarchy yields improved anomaly detection performance, validated on Weld-4M; promises broader applicability to multimodal UAD in smart manufacturing.

Abstract: Multimodal Unsupervised Anomaly Detection (UAD) is critical for quality assurance in smart manufacturing, particularly in complex processes like robotic welding. However, existing methods often suffer from causal blindness, treating process modalities (e.g., real-time video, audio, and sensors) and result modalities (e.g., post-weld images) as equal feature sources, thereby ignoring the inherent physical generative logic. Furthermore, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals frequently leads to critical process context being drowned out. In this paper, we propose Causal-HM, a unified multimodal UAD framework that explicitly models the physical Process to Result dependency. Specifically, our framework incorporates two key innovations: a Sensor-Guided CHM Modulation mechanism that utilizes low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction , and a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies that violate physical consistency. Extensive experiments on our newly constructed Weld-4M benchmark across four modalities demonstrate that Causal-HM achieves a state-of-the-art (SOTA) I-AUROC of 90.7%. Code will be released after the paper is accepted.

</details>


### [40] [Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2512.21651)
*Dung Anh Hoang,Cuong Pham,Cuong Nguyen,Trung le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.

</details>


### [41] [Dictionary-Transform Generative Adversarial Networks](https://arxiv.org/abs/2512.21677)
*Angshul Majumdar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative adversarial networks (GANs) are widely used for distribution learning, yet their classical formulations remain theoretically fragile, with ill-posed objectives, unstable training dynamics, and limited interpretability. In this work, we introduce \emph{Dictionary-Transform Generative Adversarial Networks} (DT-GAN), a fully model-based adversarial framework in which the generator is a sparse synthesis dictionary and the discriminator is an analysis transform acting as an energy model. By restricting both players to linear operators with explicit constraints, DT-GAN departs fundamentally from neural GAN architectures and admits rigorous theoretical analysis.
  We show that the DT-GAN adversarial game is well posed and admits at least one Nash equilibrium. Under a sparse generative model, equilibrium solutions are provably identifiable up to standard permutation and sign ambiguities and exhibit a precise geometric alignment between synthesis and analysis operators. We further establish finite-sample stability and consistency of empirical equilibria, demonstrating that DT-GAN training converges reliably under standard sampling assumptions and remains robust in heavy-tailed regimes.
  Experiments on mixture-structured synthetic data validate the theoretical predictions, showing that DT-GAN consistently recovers underlying structure and exhibits stable behavior under identical optimization budgets where a standard GAN degrades. DT-GAN is not proposed as a universal replacement for neural GANs, but as a principled adversarial alternative for data distributions that admit sparse synthesis structure. The results demonstrate that adversarial learning can be made interpretable, stable, and provably correct when grounded in classical sparse modeling.

</details>


### [42] [RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting](https://arxiv.org/abs/2512.21685)
*Haochen Lv,Yan Lin,Shengnan Guo,Xiaowei Mao,Hong Nie,Letian Gong,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate traffic flow forecasting is crucial for intelligent transportation services such as navigation and ride-hailing. In such applications, uncertainty estimation in forecasting is important because it helps evaluate traffic risk levels, assess forecast reliability, and provide timely warnings. As a result, probabilistic traffic flow forecasting (PTFF) has gained significant attention, as it produces both point forecasts and uncertainty estimates. However, existing PTFF approaches still face two key challenges: (1) how to uncover and model the causes of traffic flow uncertainty for reliable forecasting, and (2) how to capture the spatiotemporal correlations of uncertainty for accurate prediction.
  To address these challenges, we propose RIPCN, a Road Impedance Principal Component Network that integrates domain-specific transportation theory with spatiotemporal principal component learning for PTFF. RIPCN introduces a dynamic impedance evolution network that captures directional traffic transfer patterns driven by road congestion level and flow variability, revealing the direct causes of uncertainty and enhancing both reliability and interpretability. In addition, a principal component network is designed to forecast the dominant eigenvectors of future flow covariance, enabling the model to capture spatiotemporal uncertainty correlations. This design allows for accurate and efficient uncertainty estimation while also improving point prediction performance. Experimental results on real-world datasets show that our approach outperforms existing probabilistic forecasting methods.

</details>


### [43] [Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning](https://arxiv.org/abs/2512.21743)
*Hengyi Wu,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: 提出一种基于熵感知的连续学习方法，通过对各层的熵动态反馈进行调控，平衡稳定性与可塑性，降低灾难性遗忘并促进泛化，适用于回放/正则化框架，实验显示优于最先进基线。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中，不同层对任务的不确定性水平不同，统一对待导致高熵层欠拟合、低熵层过拟合。需要一种层级自适应的机制来缓解此不平衡。

Method: 引入基于每层熵的动态反馈机制，对高熵层降低熵以缓解欠拟合，对过于自信的层提高熵以缓解过拟合；通过调节梯度/正则化实现，使模型收敛到更宽的局部最小值；方法可无缝嵌入到回放或正则化型方法中。

Result: 在多种数据集上，相较于现有持续学习基线，取得显著性能提升。

Conclusion: 提出的熵感知调控框架通用且可与现有持续学习方法结合，提升泛化与稳定性，缓解灾难性遗忘。

Abstract: Continual learning aims to acquire new tasks while preserving performance on previously learned ones, but most methods struggle with catastrophic forgetting. Existing approaches typically treat all layers uniformly, often trading stability for plasticity or vice versa. However, different layers naturally exhibit varying levels of uncertainty (entropy) when classifying tasks. High-entropy layers tend to underfit by failing to capture task-specific patterns, while low-entropy layers risk overfitting by becoming overly confident and specialized. To address this imbalance, we propose an entropy-aware continual learning method that employs a dynamic feedback mechanism to regulate each layer based on its entropy. Specifically, our approach reduces entropy in high-entropy layers to mitigate underfitting and increases entropy in overly confident layers to alleviate overfitting. This adaptive regulation encourages the model to converge to wider local minima, which have been shown to improve generalization. Our method is general and can be seamlessly integrated with both replay- and regularization-based approaches. Experiments on various datasets demonstrate substantial performance gains over state-of-the-art continual learning baselines.

</details>


### [44] [A Comedy of Estimators: On KL Regularization in RL Training of LLMs](https://arxiv.org/abs/2512.21852)
*Vedant Shah,Johan Obando-Ceron,Vineet Jain,Brian Bartoldson,Bhavya Kailkhura,Sarthak Mittal,Glen Berseth,Pablo Samuel Castro,Yoshua Bengio,Nikolay Malkin,Moksh Jain,Siddarth Venkatraman,Aaron Courville*

Main category: cs.LG

TL;DR: 系统性分析在强化学习（RL）中对大语言模型（LLMs）进行训练时的 KL 估计器配置对梯度偏差、训练稳定性及下游性能的影响。结果显示使用无偏梯度的估计器配置在内外域任务上通常表现更好；在离策略（off-policy）场景中，KL 正则有助于提升异步训练的稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前关于将 KL 作为 RL 目标正则项来训练 LLMs 的实践尚缺乏系统性分析；尽管大量开源实现使用 KL 估计器，但对不同估计器配置如何影响梯度及最终性能的研究不足，且存在实现和目标之间的梯度不一致问题。

Method: 对多种 KL 估计器配置的梯度进行了理论分析，并在三种模型（Qwen2.5-7B、Llama-3.1-8B-Instruct、Qwen3-4B-Instruct-2507）上进行RL微调，比较在同域与跨域任务中的效果，同时考察离策略下的 KL 正则对训练稳定性的影响。

Result: 发现带有梯度偏差的估计器配置可能引发训练不稳定；而能提供无偏梯度的估计器配置在内域与外域任务上表现更佳。在离策略设置中，KL 正则有助于通过异步训练提升稳定性。

Conclusion: KL 估计器的选择与配置对梯度正确性及下游性能至关重要；在对内对外域任务中，优选无偏梯度配置；对离策略场景，KL 正则有稳定性收益，提示在实际 RL-LM 训练中应谨慎设计 KL 的估计与正则化策略。

Abstract: The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

</details>


### [45] [Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation](https://arxiv.org/abs/2512.21866)
*Yiming Qian,Thorsten Neumann,Xueyining Huang,David Hardoon,Fei Gao,Yong Liu,Siow Mong Rick Goh*

Main category: cs.LG

TL;DR: Explainable, privacy-preserving dataset distillation for collaborative fraud detection using axis-aligned leaf regions of a random forest. Synthetic data generated by sampling within regions yields a compact, auditable surrogate; preserves local feature interactions, supports global rule statistics, per-case rationales, and quantified uncertainty; achieves strong privacy, reduces data volume, and enables multi-institution collaboration with competitive performance.


<details>
  <summary>Details</summary>
Motivation: To enable trustworthy fraud analytics across multiple institutions under privacy and regulatory constraints, by producing an interpretable surrogate dataset that preserves useful patterns while protecting original records.

Method: Train a random forest, convert it into transparent axis-aligned rule regions (leaf hyperrectangles). Generate synthetic transactions by uniformly sampling within each region. Use aggregated rule statistics (e.g., support, lift) for global explanations and assign each case to its generating region for per-case rationales with calibrated uncertainty via tree-vote disagreement. Evaluate on IEEE-CIS fraud dataset; compare distilled datasets against original data; assess privacy against membership inference; identify benefits of sharing across institutions and effect of removing high-uncertainty points.

Result: Distilled datasets reduce data volume by 85–93% (often to <15% of original) while maintaining competitive precision and micro-F1 with modest AUC drop. Cross-institution data sharing improves precision, recall, and AUC. Real and synthesized structures remain highly similar (≈93% by NN cosine). Membership-inference attacks perform at chance (~0.50). Removing high-uncertainty points boosts AUC (up to 0.687) and improves calibration. AUC shows weak dependence on distillation ratio (≈0.641–0.645 for 6%–60%).

Conclusion: Tree-region distillation yields trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audits.

Abstract: We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.
  On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).
  Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit.

</details>


### [46] [GQ-VAE: A gated quantized VAE for learning variable length tokens](https://arxiv.org/abs/2512.21913)
*Theo Datta,Kayla Huang,Sham Kakade,David Brandfonbrener*

Main category: cs.LG

TL;DR: 提出门控量化变分自编码器（GQ-VAE）作为现有分词器的可直接替代，能够独立预训练并编码可变长度离散符号，在不修改大模型架构的前提下提升压缩和语言建模性能，接近并在某些情形优于BPE，相较VQ-VAE有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法多为固定长度或基于字节的编码（如BPE、BPE变体），学习型分词器往往增加模型复杂度且难以大规模落地；需要更高效且易于部署的替代方案。

Method: 提出GQ-VAE架构，在预训练阶段作为分词器以独立形式存在；通过门控机制对变长离散符号进行编码，并在VQ-VAE基础上提升压缩与建模能力，目标是实现可插拔的分词替代。

Result: 相对标准VQ-VAE，GQ-VAE在压缩效率和语言建模性能上均有提升，接近BPE的压缩率与LM表现；在BPE词汇更小、压缩等效的情况下，GQ-VAE还能提升下游语言模型学习效果。

Conclusion: 文章结尾讨论了未来的多条研究方向，并给出代码实现链接，显示GQ-VAE作为替代分词器的潜力。

Abstract: While most frontier models still use deterministic frequency-based tokenization algorithms such as byte-pair encoding (BPE), there has been significant recent work to design learned neural tokenizers. However, these schemes generally add to underlying language model complexity and force large changes to architecture, making them hard to implement at large scales. To overcome these challenges, we propose the gated quantized variational autoencoder (GQ-VAE), a novel architecture that can be independently pre-trained to serve as a drop-in replacement for existing tokenizers. The key innovation of the architecture is to learn to encode variable-length discrete tokens. GQ-VAE improves compression and language modeling performance over a standard VQ-VAE tokenizer, and approaches the compression rate and language modeling performance of BPE. Interestingly, if we use BPE with a smaller vocabulary, such that the compression is equivalent between GQ-VAE and BPE, we find that GQ-VAE improves downstream language model learning. We conclude with a discussion of several exciting avenues for future work. Code can be found at https://github.com/Theo-Datta-115/gq-vae.

</details>


### [47] [Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs](https://arxiv.org/abs/2512.21915)
*Yafeng Tang,Xiaoou Ding,Jianzhuo Du,Zishuo Yan,Zhuang Ma,Zheng Liang,Zekai Qian,Hongzhi Wang*

Main category: cs.LG

TL;DR: DATE is a diversity-aware tabular data generator that partitions heterogeneous data, uses LLMs with decision-tree feedback to produce labeled data per subset, and employs a Multi-Arm Bandit to balance diversity and quality, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Real-world tabular data are heterogeneous with diverse distributions, making it hard for a single model to learn well. A framework that can generate diverse yet high-quality data across distributions can improve downstream learning and LLM reasoning.

Method: 1) Partition the original heterogeneous data into multiple diverse subsets to prepare high-quality and distributionally distinct examples for in-context learning. 2) Use Large Language Models (LLMs) with decision-tree reasoning as feedback to explore distribution diversity and generate high-quality labeled data for each subset. 3) Address the diversity-quality trade-off by replacing greedy validation-best selection with a Multi-Arm Bandit–based sampling algorithm.

Result: DATE outperforms state-of-the-art GAN-based and LLM-based methods on tabular classification and regression benchmarks, achieving an average 23.75% reduction in error rate with only 100 generated samples. Data generated by DATE also improves Direct Preference Optimization (DPO) accuracy and enhances LLM reasoning on the target data. The authors provide open-source code.

Conclusion: DATE provides a practical, diversity-aware approach to synthesizing tabular data, enabling improved performance for downstream tasks and reasoning capabilities, with an accessible implementation.

Abstract: Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.

</details>


### [48] [Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model](https://arxiv.org/abs/2512.21917)
*Nathan Kallus*

Main category: cs.LG

TL;DR: Robust policy alignment to preferences under an unknown link between observed preferences and unseen rewards, using an f-divergence constrained framework that induces a semiparametric single-index model. Focus shifts from estimating the index to learning policies directly, with several link-robust learners and finite-sample error bounds.


<details>
  <summary>Details</summary>
Motivation: Mis-specifying the link between observed preferences and latent rewards can bias inferred rewards and misalign policies. There is a need for policy learning that remains valid under an arbitrary, unknown link function.

Method: Under an f-divergence constrained reward maximization, realizability implies a semiparametric single-index model where a scalar index (driven by the policy) captures dependence on demonstrations, while the remainder of the preference distribution is an unrestricted function of that index. The work develops policy learners by profiling the link, orthogonalizing the link, and using link-agnostic bipartite ranking objectives. Finite-sample policy error bounds are derived in terms of the index class complexity. Practical implementations leverage first-order optimization for neural networks and batched data.

Result: Finite-sample policy error bounds are established, showing robustness to unknown preference noise distribution and scale. The methods enable direct policy optimization without explicitly fitting rewards, while tolerating unidentifiable and nonparametric indices.

Conclusion: Policy learning frameworks can achieve robust alignment to preferences without assuming a known link function. The proposed learners provide practical, scalable approaches that preserve direct optimization of policies and remain resilient to preference noise and distributional assumptions.

Abstract: Aligning large language models to preference data is commonly implemented by assuming a known link function between the distribution of observed preferences and the unobserved rewards (e.g., a logistic link as in Bradley-Terry). If the link is wrong, however, inferred rewards can be biased and policies be misaligned. We study policy alignment to preferences under an unknown and unrestricted link. We consider an $f$-divergence-constrained reward maximization problem and show that realizability of the solution in a policy class implies a semiparametric single-index binary choice model, where a scalar-valued index determined by a policy captures the dependence on demonstrations and the rest of the preference distribution is an unrestricted function thereof. Rather than focus on estimation of identifiable finite-dimensional structural parameters in the index as in econometrics, we focus on policy learning, focusing on error to the optimal policy and allowing unidentifiable and nonparametric indices. We develop a variety of policy learners based on profiling the link function, orthogonalizing the link function, and using link-agnostic bipartite ranking objectives. We analyze these and provide finite-sample policy error bounds that depend on generic functional complexity measures of the index class. We further consider practical implementations using first-order optimization suited to neural networks and batched data. The resulting methods are robust to unknown preference noise distribution and scale, while preserving the direct optimization of policies without explicitly fitting rewards.

</details>


### [49] [Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms](https://arxiv.org/abs/2512.21925)
*Kongchang Zhou,Tingyu Zhang,Wei Chen,Fang Kong*

Main category: cs.LG

TL;DR: 提出了一种混合 CMAB-T 框架，将离线数据与在线交互结合起来，提出混合 CUCB 算法，以利用离线数据引导探索、在线交互纠正分布偏差，从而在理论上获得更优的遗憾界限并在实验中显著优于纯在线或纯离线的方法。


<details>
  <summary>Details</summary>
Motivation: 在线学习和离线数据驱动学习各自存在局限：在线方法代价高、适应慢，离线方法受数据质量和探索能力限制。需要一个能同时利用离线数据和在线互动的框架来弥补彼此的不足。

Method: 提出混合 CUCB 算法，在 CMAB-T 设置下使用离线数据来引导探索，并通过在线互动来覆盖离线数据的分布偏差或覆盖不足的问题；给出理论遗憾界限，证明当离线数据质量高时显著优于纯在线，在数据受限或偏差时能有效纠正离线数据的偏置；并给出实验结果验证其优越性。

Result: 给出混合 CMAB-T 的理论遗憾保证，表明在离线数据质量较好时比纯在线方法有明显提升；在数据有限或不对齐时能够有效纠正离线方法的偏差；实验结果显示算法具有一致的优势。

Conclusion: 混合离线与在线数据的 CMAB-T 框架有效融合两种范式的优点，能够加速收敛并提升性能，具有实际应用潜力与进一步的研究空间。

Abstract: The problem of combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) has been extensively studied. Prior work primarily focuses on either the online setting where an agent learns about the unknown environment through iterative interactions, or the offline setting where a policy is learned solely from logged data. However, each of these paradigms has inherent limitations: online algorithms suffer from high interaction costs and slow adaptation, while offline methods are constrained by dataset quality and lack of exploration capabilities. To address these complementary weaknesses, we propose hybrid CMAB-T, a new framework that integrates offline data with online interaction in a principled manner. Our proposed hybrid CUCB algorithm leverages offline data to guide exploration and accelerate convergence, while strategically incorporating online interactions to mitigate the insufficient coverage or distributional bias of the offline dataset. We provide theoretical guarantees on the algorithm's regret, demonstrating that hybrid CUCB significantly outperforms purely online approaches when high-quality offline data is available, and effectively corrects the bias inherent in offline-only methods when the data is limited or misaligned. Empirical results further demonstrate the consistent advantage of our algorithm.

</details>


### [50] [Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing](https://arxiv.org/abs/2512.22024)
*Wesley S. Leite,Rodrigo C. de Lamare,Yuriy Zakharov,Wei Liu,Martin Haardt*

Main category: cs.LG

TL;DR: 提出变量窗口大小( VWS ) 的平滑框架，用于稀疏线性阵列的基于共阵列的DOA估计，提出 VWS-CA-MUSIC 和 VWS-CA-rMUSIC，通过压缩平滑孔径并用无扰低秩项替换部分扰动的一阶外积，在保持信号子空间的同时增大信号与噪声子空间的分离，给出可辨识性界限。与固定窗口方法相比，实验显示性能更好且计算更省。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏几何阵列中 DOA 估计的鲁棒性与分辨率不足的问题；通过可调的平滑窗口来提高信号与噪声子空间的分离，同时保持信号子空间的跨度，以实现更好的估计。

Method: 提出变量窗口大小（VWS）平滑框架，并在此框架下给出 VWS-CA-MUSIC 和 VWS-CA-rMUSIC 两种算法。通过压缩平滑孔径，将部分被扰动的秩一外积替换为无扰低秩附加项，从而增强分离度并保持信号子空间张成。推导出可辨识性的界限，限制压缩参数的取值以保证辨识性。对稀疏几何阵列进行了仿真，比较了与固定窗口共阵列 MUSIC 的性能。

Result: VWS 框架在稀疏几何阵列的 DOA 估计中显著提升了性能并降低了计算复杂度，相较于固定窗口的共阵列 MUSIC，显示出更好的分辨率与鲁棒性，且实现更低的计算成本。

Conclusion: 该工作提出的一般化平滑框架（VWS）及其在 CA-MUSIC 与 CA-rMUSIC 的具体实现，提供了可辨识性界限并在仿真实验中证明了性能与复杂度的改善，具有较强的理论-实践结合价值。

Abstract: In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspaces, while preserving the signal subspace span. We also derive the bounds that guarantees identifiability, by limiting the values that can be assumed by the compression parameter. Simulations with sparse geometries reveal significant performance improvements and complexity savings relative to the fixed-window coarray MUSIC method.

</details>


### [51] [LibContinual: A Comprehensive Library towards Realistic Continual Learning](https://arxiv.org/abs/2512.22029)
*Wenbin Li,Shangge Liu,Borui Kang,Yiyang Chen,KaXuan Lew,Yang Chen,Yinghuan Shi,Lei Wang,Yang Gao,Jiebo Luo*

Main category: cs.LG

TL;DR: LibContinual 提供一个可重复的 Continual Learning 基础库，整合了5大类别下的19种代表算法，建立统一评测环境；通过对现实世界约束下的在线学习、统一内存预算和类别随机化设置的分析，揭示了主流方法在现实场景下的性能下降，强调资源感知与语义鲁棒性的必要性，并提供可复用的代码库以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前 CL 研究呈现高度碎片化的格局，缺乏统一框架、实现不一致、评测协议多样，导致公平比较和可重复性困难。需要一个统一、可重复且贴近现实场景的评测与实现平台来推动研究向现实应用转化。

Method: 构建高内聚低耦合的模块化架构 LibContinual，整合 19 种代表性算法，覆盖五大方法类别，提供标准化执行环境。基于该框架开展三项隐含评估假设的系统性分析：1) 离线数据可获取性、2) 不受限制的内存资源、3) 任务内语义同质性。提出严格在线 CL 设置、统一的内存预算协议，以及类别随机化设置，并在现实约束下评估方法表现。

Result: 在严格的在线限制、统一的内存预算以及类别随机化等现实约束下，许多主流 CL 方法的性能显著下降，凸显资源感知和语义鲁棒性在现实场景中的重要性。LibContinual 为未来研究提供了标准化基线与工具箱。

Conclusion: LibContinual 作为现实导向的持续学习研究基石工具箱，促进方法的公平评测与可重复性，并推动开发更具资源意识和语义稳健性的 CL 策略。同时，源代码可在 GitHub 获取以支持广泛的复现实验。

Abstract: A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.

</details>


### [52] [From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation](https://arxiv.org/abs/2512.22031)
*Nagham Osman,Vittorio Lembo,Giovanni Bottegoni,Laura Toni*

Main category: cs.LG

TL;DR: 生成模型可用于生成类似“命中”（hit-like）的分子以支持药物发现的一个阶段，而非替代整个流程；基于自回归和扩散模型的对比评估，以及多阶段筛选框架，证明在若干靶标（如 GSK-3β）中可以生成有效、可合成且生物活性被验证的化合物，同时指出评估指标和可用训练数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 药物发现中命中识别既关键又成本高企；尽管虚拟筛选进步，仍耗时耗费。本研究尝试将生成模型用于命中化合物的生成这一特定环节，以探讨其在工作流中的直接应用潜力。

Method: 比较两种自回归模型和一种扩散模型在多数据集和训练设置下的生成能力；构建一个面向命中样本的评估框架，结合物理化学性质、结构以及生物活性相关指标，形成多阶段过滤筛选管线；通过对输出进行标准指标和针对靶标的对接分数评估，选取可合成并活性证实的化合物进行实验验证。

Result: 模型能生成有效、多样且具有生物相关性的化合物；在多个靶标上表现良好，并合成了若干 GSK-3β 命中并在体外证实活性；同时指出现有评估指标和训练数据的局限性。

Conclusion: 生成模型对命中样本生成具有潜在帮助作用，可作为药物发现流程的辅助环节，但仍需解决评估手段和数据不足的问题，未来可进一步集成至现有工作流以提升效率。

Abstract: Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$β$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.

</details>


### [53] [Scaling Adversarial Training via Data Selection](https://arxiv.org/abs/2512.22069)
*Youran Ye,Dejin Wang,Ajinkya Bhandare*

Main category: cs.LG

TL;DR: Selective Adversarial Training: perturb only a subset of critical samples during training using margin-based or gradient-matching sampling, achieving robustness comparable to full PGD with up to 50% less adversarial computation on MNIST and CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: Conventional PGD adversarial training incurs high computational cost because all samples in each minibatch undergo identical inner-loop optimization regardless of their contribution to robustness; there is a need for scalable adversarial training by focusing on critical samples.

Method: Introduce two selection criteria to pick a subset of samples per minibatch: (1) margin-based sampling prioritizes samples near the decision boundary, and (2) gradient-matching sampling selects samples whose gradients align with the batch's dominant optimization direction. Adversarial examples are generated only for the selected subset, while the rest are trained with a clean mixed objective.

Result: Experiments on MNIST and CIFAR-10 show robustness comparable to or exceeding full PGD adversarial training, with adversarial computation reduced by up to 50%.

Conclusion: Informed, selective sampling is sufficient to achieve scalable adversarial robustness without sacrificing performance margins, enabling significant reductions in adversarial training cost.

Abstract: Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose \emph{Selective Adversarial Training}, which perturbs only a subset of critical samples in each minibatch. Specifically, we introduce two principled selection criteria: (1) margin-based sampling, which prioritizes samples near the decision boundary, and (2) gradient-matching sampling, which selects samples whose gradients align with the dominant batch optimization direction. Adversarial examples are generated only for the selected subset, while the remaining samples are trained cleanly using a mixed objective. Experiments on MNIST and CIFAR-10 show that the proposed methods achieve robustness comparable to, or even exceeding, full PGD adversarial training, while reducing adversarial computation by up to $50\%$, demonstrating that informed sample selection is sufficient for scalable adversarial robustness.

</details>


### [54] [Unifying Learning Dynamics and Generalization in Transformers Scaling Law](https://arxiv.org/abs/2512.22088)
*Chiwun Yang*

Main category: cs.LG

TL;DR: 提出一个统一框架，解释大型语言模型的扩容规律：通过将变换器学习动力学建模为常微分方程并近似为核行为，给出对随机梯度下降训练的理论分析，揭示在资源随数据扩展时的两阶段收敛：初期指数衰减，达到阈值后进入统计阶段，泛化误差遵循 Θ(C^{-1/6}) 的幂律；并给出模型规模、训练时间和数据集规模的独立扩展规律。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示扩容规律的理论根基，弥补现有经验性规律的不足；从 toy 模型跳出，提供对多层变换器在真实数据分布下的 SGD 训练的严谨分析，量化资源与泛化之间的关系。

Method: 将变换器学习动力学形式化为一个常微分方程系统，并将其近似为核行为；对多层变换器在序列到序列数据、任意数据分布下进行 SGD 训练的理论分析；推导关于一般化误差对资源的上界、以及在资源分配阈值前后的两阶段收敛；进一步导出模型尺寸、训练时间和数据集规模的独立缩放规律。

Result: 给出对超额风险的理论上界，并揭示一个显著的相变：初始优化阶段超额风险随着计算成本的增加呈指数衰减；超过某一资源阈值后进入统计阶段，一般化误差遵循幂律衰减 Θ(C^{-1/6})；并从单独角度给出模型大小、训练时间、数据集大小对一般化上界的影响规律。

Conclusion: 该理论框架将资源分配与泛化行为统一起来，澄清了资源变量如何各自约束和影响大模型的泛化上界，为理解和预测 LLM 的扩容规律提供了理论支撑。

Abstract: The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Θ(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.

</details>


### [55] [A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting](https://arxiv.org/abs/2512.22101)
*Shuyu Gan,Renxiang Wang,James Mooney,Dongyeop Kang*

Main category: cs.LG

TL;DR: A2P-Vis 提出两阶段多智能体管线，Data Analyzer 与 Presenter，自动将原始数据转化为高质量数据可视化报告，涵盖多样化方向、绘图代码生成、布局筛选、洞见评分，并输出可发表的叙事性报告。


<details>
  <summary>Details</summary>
Motivation: 端到端数据科学管线的两大短板在于难以生成洞察力丰富且多样化的可视化证据，以及难以将其整合成连贯、专业的报告，亟需一个质量受控的分析与叙事框架来提升自动化数据分析的实用性。

Method: 提出 Data Analyzer 与 Presenter 的两段式多智能体架构：Data Analyzer 负责数据剖析、提出多样化可视化方向、生成并执行绘图代码、用可读性检查器筛除低质量图，并挖掘候选洞见对其深度、正确性、特异性、深度与可操作性等维度进行自动评分；Presenter 对主题排序、基于图表的洞见撰写叙事、撰写过渡句、并润色文档以提升清晰度和一致性，最终输出可发表的报告。

Result: 系统输出经过筛选的材料（图表 + 经核验的洞见）以及可读的叙事文本，减少手工 glue 工作，提升自动化数据分析的实用性。

Conclusion: 将高质量分析者与叙事者耦合，A2P-Vis 实现端到端的共分析流程，提升自动数据分析对从业者的实际有用性；完整数据集报告可在链接中获取。

Abstract: Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.

</details>


### [56] [Explainable Multimodal Regression via Information Decomposition](https://arxiv.org/abs/2512.22102)
*Zhaozhao Ma,Shujian Yu*

Main category: cs.LG

TL;DR: A PID-based multimodal regression framework that decomposes modality information into unique, redundant, and synergistic components using Gaussianity of latent distributions, with a closed-form regularizer, achieving state-of-the-art accuracy and interpretability on six real-world datasets including brain age prediction; code available.


<details>
  <summary>Details</summary>
Motivation: Interpretability and quantitative attribution in multimodal fusion: existing methods struggle to disentangle how individual modalities contribute and interact. PID provides a principled decomposition of information into unique, redundant, and synergistic parts but is underdetermined; this work introduces inductive bias to enable tractable PID terms and actionable regularizers.

Method: Apply Partial Information Decomposition (PID) to modality-specific latent representations and the transformed response. Enforce Gaussianity in the joint distribution of latent representations and transformed response (inverse normal transform) to derive analytical PID terms. Derive a closed-form conditional independence regularizer to promote isolation of unique information within each modality. Validate across six datasets, including large-scale brain age prediction, with comparisons to state-of-the-art fusion methods.

Result: Outperforms state-of-the-art methods in predictive accuracy and interpretability; enables informed modality selection for efficient inference. Demonstrates practical usefulness on diverse real-world tasks and provides implementation at the linked repository.

Conclusion: Proposes a principled, interpretable multimodal regression framework grounded in PID, enabling dissection of modality contributions and interactions. Combines Gaussianity-based tractable PID with a novel regularizer, yielding improved performance and actionable insights for modality selection; code released.

Abstract: Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [57] [Inter-seasonal and multi-objective optimization of a sustainable hydrogen supply chain in Corsica integrating water availability constraints](https://arxiv.org/abs/2512.21339)
*T. Moustapha Mai,C. Azzaro-Pantel,M. Chin Choi,M. Hajajji,C. Cristofari*

Main category: eess.SY

TL;DR: 将岛屿交通的氢气供给链问题建模为多目标优化问题，利用 MILP 和改进 TOPSIS 进行决策，评估分散式氢气供应对成本、排放与风险的影响。结果指向以分散化氢气供应为主、运输最小化的结构，单位氢气成本与排放在给定区间内波动。


<details>
  <summary>Details</summary>
Motivation: 岛屿地区长期依赖化石燃料进口，亟需低碳、可持续的能源载体以提升能源安全与环境绩效。绿氢被视为能源转型的关键组成部分，需结合水资源、可再生能源、旅游与地理约束等岛屿特性进行优化。

Method: 建立多时段、多目标的混合整数线性规划（MILP）模型以优化氢气供应链的系统成本、温室气体排放和风险指标；将岛屿特有因素（水资源、可再生能源、旅游流、地理约束）纳入模型约束与参数；并采用基于改良 TOPSIS 的多目标决策工具来识别最优解决方案。

Result: 得到分散化的氢气供应链结构，显著降低运输需求；单位氢气成本（LCOH）约为 6.54（单位未给出）/kg；GHG 排放在 1.32–1.75 kg CO2e/kg H2 的区间内波动；旅游需求对能源需求的影响显著，水资源成为制约关键因素。

Conclusion: 提出一种面向岛屿场景的创新性氢气供应链优化方法，强调分散化部署与水资源管理的重要性，以及旅游对能源需求的显著作用，为岛屿能源系统的低碳化路径提供决策支撑。

Abstract: This study investigates the potential of hydrogen as a sustainable energy carrier for mobility applications in island territories, which are traditionally dependent on fossil fuel imports. Green hydrogen is identified as a key component of the energy transition. A Mixed Integer Linear Programming (MILP) model with a multi-period, multi-objective framework is used to optimize the hydrogen supply chain based on system costs, greenhouse gas (GHG) emissions, and a risk index. The model incorporates critical island-specific factors such as water resource availability, renewable energy sources, tourism flow, and geographic constraints. A multi-criteria decision making tool based on a modified version of TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) aids the identification of optimal solutions. Results suggest a decentralized Hydrogen Supply Chains (HSC) structure with minimized transport. The levelized cost of hydrogen (LCOH) is estimated at 6.54 ___/kg, and GHG emissions range from 1.32 to 1.75 kgCO 2 e/kg H 2. This study highlights the impact of tourism on energy demand and the crucial role of water resources, offering a novel approach to optimizing island-specific HSC.

</details>


### [58] [EcoNet: Multiagent Planning and Control Of Household Energy Resources Using Active Inference](https://arxiv.org/abs/2512.21343)
*John C. Boik,Kobus Esterhuysen,Jacqueline B. Hynes,Axel Constant,Ines Hipolito,Mahault Albarracin,Alex B. Kiefer,Karl Friston*

Main category: eess.SY

TL;DR: EcoNet是一种在不确定条件下通过贝叶斯主动推断实现家庭与邻里能源管理的框架，旨在多目标情境下优化能源设备调度与资源分配，且通过仿真评估潜在收益与权衡。


<details>
  <summary>Details</summary>
Motivation: 解决家庭能源管理中的目标冲突与不确定性问题；在成本、排放、舒适等多重目标之间寻找平衡；应对天气和光伏发电等预测的不确定性，需一个统一的概率推断框架来进行决策与协调。

Method: 提出EcoNet，基于贝叶斯方法与主动推断（active inference）的框架，用于家庭与社区级能源管理与协同。系统将不确定性建模为后验推断，考虑条件目标与偏好，并通过仿真实现行动规划与资源调度。

Result: 给出仿真结果并进行讨论，评估在不同情景下的管理性能、协调性和权衡，显示在存在不确定性时的潜在改进与收益。

Conclusion: EcoNet提供一个面向不确定性和多目标偏好的概率框架，整合家庭与邻里能源管理的理论与方法，促进更高效的能源管理与系统协同。

Abstract: Advances in automated systems afford new opportunities for intelligent management of energy at household, local area, and utility scales. Home Energy Management Systems (HEMS) can play a role by optimizing the schedule and use of household energy devices and resources. One challenge is that the goals of a household can be complex and conflicting. For example, a household might wish to reduce energy costs and grid-associated greenhouse gas emissions, yet keep room temperatures comfortable. Another challenge is that an intelligent HEMS agent must make decisions under uncertainty. An agent must plan actions into the future, but weather and solar generation forecasts, for example, provide inherently uncertain estimates of future conditions. This paper introduces EcoNet, a Bayesian approach to household and neighborhood energy management that is based on active inference. The aim is to improve energy management and coordination, while accommodating uncertainties and taking into account potentially conditional and conflicting goals and preferences. Simulation results are presented and discussed.

</details>


### [59] [Multi-Day Scheduling for Electric Vehicle Routing: A Novel Model and Comparison Of Metaheuristics](https://arxiv.org/abs/2512.21346)
*Dominik Köster,Florian Porkert,Klaus Volbert*

Main category: eess.SY

TL;DR: 提出一个多日电动车路径问题的混合整数规划模型，考虑电池容量和时间窗，供跨平台导航使用，使用TS、ALNS、ACO等元启发式算法求解，并通过生成的 ensemble 进行性能分析，与Google OR-Tools的精确解进行比较。


<details>
  <summary>Details</summary>
Motivation: 电动车的续航限制和充电时间，以及不同类型充电桩的差异，要求在日程时间窗约束下进行高效路线规划，减小充电带来的不便。

Method: 提出多日MIP模型，涵盖电池容量、时间窗等约束；用Tabu Search、Adaptive Large Neighborhood Search、Ant Colony Optimization等元启发式算法求解；通过生成的 ensembles 分析各方法在现实中的表现，并与 Google OR-Tools 的精确解进行对比。

Result: 对三种元启发式方法的性能进行了比较分析，通过 ensembles 估算在现实情境中的行为，并与 OR-Tools 的精确结果进行对比。

Conclusion: 研究表明所提出的 EVRP-多日模型在跨平台导航场景中是可行的，并且通过集成分析可以获得对现实问题的更可信的性能评估；与精确求解基准相比，元启发式方法提供了有效的近似解，具备实际应用潜力。

Abstract: The increasing use of electric vehicles (EVs) requires efficient route planning solutions that take into account the limited range of EVs and the associated charging times, as well as the different types of charging stations. In this work, we model and solve an electric vehicle routing problem (EVRP) designed for a cross-platform navigation system for individual transport. The aim is to provide users with an efficient route for their daily appointments and to reduce possible inconveniences caused by charging their EV. Based on these assumptions, we propose a multi-day model in the form of a mixed integer programming (MIP) problem that takes into account the vehicle's battery capacity and the time windows of user's appointments.
  The model is solved using various established metaheuristics, including tabu search (TS), adaptive large neighborhood search (ALNS), and ant colony optimization (ACO). Furthermore, the performance of the individual approaches is analyzed using generated ensembles to estimate their behavior in reality and is compared with the exact results of the Google OR-Tools solver.

</details>


### [60] [An Equivalent and Unified Virtual Battery Modeling Framework for Flexibility Characterization of Building HVAC Systems](https://arxiv.org/abs/2512.21363)
*Qi Zhu,Yu Yang,Liang Yu,Qing-Shan Jia,Costas J. Spanos,Xiaohong Guan*

Main category: eess.SY

TL;DR: 提出一个统一的虚拟电池（VB）框架，用于表征单区与多区建筑HVAC系统的运行灵活性，并通过降维聚合实现低复杂度的需求响应（DR）应用。


<details>
  <summary>Details</summary>
Motivation: HVAC系统在建筑能耗中占比高，且具有显著的操作灵活性，可用于提供电网服务。然而由于建筑热动力学复杂、系统运行约束和舒适性约束等因素，难以表征其灵活性，因此需要一个统一、可操作的框架来量化和利用这部分灵活性。

Method: 1) 识别一个物理意义明确的状态以表示在舒适性约束下的建筑热状况；2) 为单区HVAC建立VB模型以表征其运行灵活性；3) 将VB框架扩展到多区系统，建立逐区的VB模型；4) 提出一套将多区VB模型聚合为低阶、低复杂度聚合VB模型的方法；5) 通过需求响应情景验证VB模型的有效性；6) 将DR策略分解到各区控制输入，兼顾舒适性并实现接近最优的运行成本。

Result: VB模型能够较好地表征建筑HVAC系统的运行灵活性，并在DR应用中发挥作用；通过聚合得到的低阶模型显著降低计算复杂性，同时DR策略可分解至区级控制以维持热舒适性并实现接近最优的运行成本。

Conclusion: 提出的VB建模框架对单区和多区建筑HVAC的运行灵活性具有有效表征力，能够支撑高效的DR参与并降低模型和计算复杂度。

Abstract: The heating, ventilation and air-conditioning (HVAC) system dominates building's energy consumption and meanwhile exhibits substantial operational flexibility that can be exploited for providing grid services. However, the goal is largely hindered by the difficulty to characterize the system's operating flexibility due to the complex building thermal dynamics, system operating limits and human comfort constraints. To address this challenge, this paper develops an unified virtual battery (VB) modeling framework for characterizing the operating flexibility of both single-zone and multi-zone building HVAC systems, enabling flexible buildings to function like virtual batteries. Specifically, a physically meaningful representation state is first identified to represent building thermal conditions under thermal comfort constraints and a VB model is then established for characterizing the operating flexibility of single-zone HVAC systems. We subsequently extend the VB modeling framework to multi-zone HVAC systems and establish a set of zone-level VB models to characterize the building's zonal operating flexibility. We further develop a systematic method to aggregate the VB models into a low-order and low-complexity aggregated VB model, significantly reducing model and computational complexity. We demonstrate the VB model through demand response (DR) applications and conclude that the VB model can well capture the operating flexibility of building HVAC systems and enable effective DR participation. The DR strategies obtained from the VB model can be efficiently decomposed to zone-level control inputs for maintaining human thermal comfort while achieving near-optimal operation cost.

</details>


### [61] [Adaptive Real-Time Scheduling Algorithms for Embedded Systems](https://arxiv.org/abs/2512.21364)
*Abdelmadjid Benmachich,Khadija Rais,Hamda Slimi*

Main category: eess.SY

TL;DR: A concise survey of adaptive real-time scheduling for embedded systems, focusing on feedback-based control scheduling, inter-task interdependence (symbiotic scheduling), predictive methods, and DVFS-driven power management.


<details>
  <summary>Details</summary>
Motivation: Embedded real-time systems increasingly operate in dynamic and uncertain environments where workloads and resources can change at runtime, making static scheduling inadequate. The need is for adaptive yet predictable, safe systems.

Method: A brief survey of key models and mechanisms: feedback control-based scheduling, interdependence between tasks (e.g., symbiotic scheduling of periodic tasks), predictive methods, and power management via DVFS, highlighting how these approaches adapt workload, resources, and system updates.

Result: Provides a succinct overview of core mechanisms and the trade-offs among adaptivity and predictability, typical evaluation metrics, and ongoing problems, especially under safety-critical constraints.

Conclusion: The paper offers an accessible introduction for researchers and practitioners, summarizing key adaptive real-time scheduling mechanisms, their trade-offs, evaluation metrics, and open challenges in safety-critical embedded systems.

Abstract: Embedded systems are becoming more in demand to work in dynamic and uncertain environments, and being confined to the strong requirements of real-time. Conventional static scheduling models usually cannot cope with runtime modification in workload, resource availability, or system updates. This brief survey covers the area of feedback-based control (e.g., Feedback Control Scheduling) and interdependence between tasks (e.g., Symbiotic Scheduling of Periodic Tasks) models. It also borders on predictive methods and power management, combining methods based on Dynamic Voltage and Frequency Scaling (DVFS). In this paper, key mechanisms are briefly summarized, influencing trade-offs relating to adaptivity/predictability, typical metrics of evaluation, and ongoing problems, especially in situations where safety is a critical factor, giving a succinct and easy-to-understand introduction to researchers and practitioners who have to cope with the changing environment of adaptive real-time systems.

</details>


### [62] [Towards Learning-Based Formula 1 Race Strategies](https://arxiv.org/abs/2512.21570)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Mohammad M. Moradi,Christopher H. Onder*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents two complementary frameworks to optimize Formula 1 race strategies, jointly accounting for energy allocation, tire wear and pit stop timing. First, the race scenario is modeled using lap time maps and a dynamic tire wear model capturing the main trade-offs arising during a race. Then, we solve the problem by means of a mixed-integer nonlinear program that handles the integer nature of the pit stop decisions. The same race scenario is embedded into a reinforcement learning environment, on which an agent is trained. Providing fast inference at runtime, this method is suited to improve human decision-making during real races. The learned policy's suboptimality is assessed with respect to the optimal solution, both in a nominal scenario and with an unforeseen disturbance. In both cases, the agent achieves approximately 5s of suboptimality on 1.5h of race time, mainly attributable to the different energy allocation strategy. This work lays the foundations for learning-based race strategies and provides a benchmark for future developments.

</details>


### [63] [Economic and Reliability Value of Improved Offshore Wind Forecasting in Bulk Power Grid Operation: A Case Study of The New York Power Grid](https://arxiv.org/abs/2512.21754)
*Khaled Bin Walid,Feng Ye,Jiaxiang Ji,Ahmed Aziz Ezzat,Travis Miles,Yazhou Leo Jiang*

Main category: eess.SY

TL;DR: 改进的海上风电预测结合空间相关性更强的备用聚合，显著降低成本并提升LOLP。


<details>
  <summary>Details</summary>
Motivation: 海上风电的出力波动对电网成本与可靠性造成挑战；现有的备用评估忽视空间相关性，容易过度备用，需更准确的预测和风险聚合方法。

Method: 建立基于机器学习的区域化海上风电预测模型；将改进预测纳入与 NYISO 实践对齐的动态备用采购框架；使用空间相关性进行风险基的备用聚合；开发运营资源充足性框架以量化预测误差和电网条件的不确定性；以纽约州为案例对比改进预测与NWP模型，评估不同情景下的系统性结果。

Result: 2035 情景下，改进预测相较于经过验证的NWP模型使备用采购成本降低5.53%；在此基础上引入风险分组聚合，总生产成本再降低7.21%；在可靠性方面，LOLP下降约19%。

Conclusion: 改进风电预测在经济性和可靠性方面对区域电网运营具有显著潜力，且风险聚合策略有效提升资源配置效率，对区域互联和未来扩展具有积极意义。

Abstract: This study investigates the economic and reliability benefits of improved offshore wind forecasting for grid operations along the U.S. East Coast. We introduce and evaluate a state-of-the-art, machine-learning-based offshore wind forecasting model tailored for this region by integrating its improved forecasts into a dynamic reserve procurement framework aligned with New York Independent System Operator (NYISO) practices to evaluate their economic value. To determine system-wide reserve needs, plant-specific reserves are aggregated. However, conventional methods overlook spatial correlation across sites, often leading to over procurement. To address this, we propose a risk-based reserve aggregation technique that leverages spatial diversification. Additionally, we evaluate the reliability improvements enabled by the enhanced offshore wind forecast. To evaluate the operational impact, we propose an operational resource adequacy framework that captures uncertainty from forecast errors and grid conditions. Using this framework, we quantify key reliability metrics under different offshore wind forecast scenarios. Using New York State as a case study, we find that the improved forecast enables more accurate reserve estimation, reducing procurement costs by 5.53% in 2035 scenario compared to a well-validated numerical weather prediction model. Applying the risk-based aggregation further reduces total production costs by 7.21%. From a reliability perspective, the improved forecasts lower the system Loss of Load Probability (LOLP) by approximately 19% in the 2035 scenario, highlighting its potential to enhance system reliability during real-time grid operations.

</details>


### [64] [Optimal Placement of Data Centers to Support Power Distribution Networks Using Intelligent Algorithms with Economic Indicators](https://arxiv.org/abs/2512.21987)
*Amin Hajihasani,Mahmoud Modaresi*

Main category: eess.SY

TL;DR: A mixed-integer nonlinear optimization framework for techno-economic siting of data centers with on-site distributed generation (DG) in distribution networks. It maps candidate data-center sites to network buses, sizes DG, and balances losses, voltage quality, and investment cost via a genetic-algorithm-based, multi-scenario framework. Case study on IEEE 33-bus shows improved losses and voltage at a moderate cost.


<details>
  <summary>Details</summary>
Motivation: Data centers consume electricity rapidly and, when connected to weak distribution networks, cause large voltage drops and feeder losses. Integrating data-center siting with on-site renewable generation and power electronics aims to reduce losses and improve voltage profiles while considering economics.

Method: Formulate a mixed integer nonlinear optimization that jointly selects the connection bus and DG capacity, with an objective combining normalized active power losses, a voltage deviation index, and land/DG investment costs. Use an intelligent genetic algorithm within a multi-scenario decision framework with adaptive weight tuning; generate stakeholder scenarios prioritizing losses, voltage quality, or techno-economic balance, converging to an optimal bus.

Result: IEEE 33-bus radial system case study identifies bus 14 with 1.10 MW DG, reducing total losses from 202.67 kW to 129.37 kW and raising the minimum bus voltage to 0.933 p.u., at a moderate investment of 1.33 MUSD.

Conclusion: The framework provides an interpretable pathway to incorporate economic indicators into distribution-aware data-center siting and demonstrates effective trade-offs among losses, voltage quality, and investment.

Abstract: Data centers are among the fastest growing electricity consumers and can impose severe voltage drops and feeder losses when connected to weak distribution networks. This paper formulates a techno economic siting problem in which each candidate data center site is mapped to a bus of the distribution network and is assumed to deploy on site renewable generation and power electronic interfaces, resulting in a controllable net active power injection equivalent to distributed generation. A mixed integer nonlinear optimization model is developed to jointly select the connection bus and size the DG capacity while respecting network operating limits. The objective combines three normalized terms including active power losses, a voltage deviation index capturing profile quality, and investment cost derived from location dependent land price and unit DG cost. To address the discrete continuous search space, an intelligent genetic algorithm is embedded in a multi scenario decision framework with adaptive weight tuning. Three stakeholder scenarios prioritize losses, voltage quality, or techno economic balance, and additional balanced scenarios are generated automatically until the optimal bus decision converges. A case study on the IEEE 33 bus radial system demonstrates the effectiveness of the approach. The converged design selects bus 14 with 1.10 MW DG, reducing total losses from 202.67 kW to 129.37 kW while improving the minimum bus voltage to 0.933 per unit at a moderate investment cost of 1.33 MUSD. The proposed framework provides an interpretable pathway to integrate economic indicators into distribution aware data center siting.

</details>
