{"id": "2512.22126", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.22126", "abs": "https://arxiv.org/abs/2512.22126", "authors": ["Svyatoslav Covanov", "Cedric Pradalier"], "title": "Validation methodology on real data of reversible Kalman Filter for state estimation with Manifold", "comment": null, "summary": "This work extends a previous study that introduced an algorithm for state estimation on manifolds within the framework of the Kalman filter. Its objective is to address the limitations of the earlier approach. The reversible Kalman filter was designed to provide a methodology for evaluating the accuracy of existing Kalman filter variants with arbitrary precision on synthetic data. It has favorable numerical properties on synthetic data, achieving arbitrary precision without relying on the small-velocity assumption and depending only on sensor noise. However, its application to real data encountered difficulties related to measurement noise, which was mitigated using a heuristic. In particular, the heuristic involved an event detection step switching between reversible Kalman filter and classical Kalman variant at chosen moments. In the present work, we propose a study of this detection step and propose a methodology to prove at which moment the reversible Kalman approach improves on classical multiplicative variant. In particular, we propose a metric allowing one to discriminate situations in real-world scenarios where it behaves better than classical approach.", "AI": {"tldr": "\u6269\u5c55\u5148\u524d\u5728\u6d41\u5f62\u4e0a\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e00\u4e2a\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5224\u5b9a\u4f55\u65f6\u201c\u53ef\u9006\u5361\u5c14\u66fc\u6ee4\u6ce2\u201d\u4f18\u4e8e\u7ecf\u5178\u53d8\u4f53\u7684\u68c0\u6d4b\u6b65\u9aa4\u5206\u6790\u4e0e\u5ea6\u91cf\u3002", "motivation": "\u89e3\u51b3\u5148\u524d\u5de5\u4f5c\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5bf9\u6d4b\u91cf\u566a\u58f0\u7684\u654f\u611f\u6027\u53ca\u4ec5\u4f9d\u8d56\u4f20\u611f\u5668\u566a\u58f0\u7684\u7406\u8bba\u5047\u8bbe\u6240\u5e26\u6765\u7684\u5c40\u9650\u6027\uff1b\u5e0c\u671b\u7ed9\u51fa\u4e00\u4e2a\u5728\u4f55\u65f6\u9009\u62e9\u53ef\u9006KF\u7684\u5224\u5b9a\u6846\u67b6\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u68c0\u6d4b\u6b65\u9aa4\uff0c\u63d0\u51fa\u4e00\u79cd\u53ef\u8bc1\u660e\u4f55\u65f6\u53ef\u9006KF\u4f18\u4e8e\u7ecf\u5178\u53d8\u4f53\u7684\u601d\u8def\uff1b\u8bbe\u8ba1\u4e00\u4e2a\u5ea6\u91cf\uff0c\u7528\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u533a\u5206\u4e24\u8005\u7684\u4f18\u52a3\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u6027\u8d28\u3002", "result": "\u63d0\u51fa\u7528\u4e8e\u533a\u5206\u53ef\u9006KF\u76f8\u5bf9\u4e8e\u7ecf\u5178\u4e58\u6cd5\u5f62\u5f0f\u5728\u4f55\u65f6\u5177\u6709\u6539\u8fdb\u7684\u65f6\u523b\u7684\u5ea6\u91cf\uff0c\u4ee5\u53ca\u56f4\u7ed5\u68c0\u6d4b\u6b65\u9aa4\u7684\u7406\u8bba\u5206\u6790\u8def\u5f84\u3002", "conclusion": "\u4e3a\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5e94\u7528\u53ef\u9006KF\u63d0\u4f9b\u4e86\u5224\u5b9a\u51c6\u5219\u4e0e\u8bc4\u4f30\u5de5\u5177\uff0c\u6307\u660e\u5728\u7279\u5b9a\u60c5\u5f62\u4e0b\u53ef\u9006KF\u80fd\u5e26\u6765\u6539\u8fdb\uff0c\u5e76\u7ed9\u51fa\u5b9e\u73b0\u8be5\u6539\u8fdb\u7684\u6761\u4ef6\u3002"}}
{"id": "2512.22668", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22668", "abs": "https://arxiv.org/abs/2512.22668", "authors": ["Arya Rashidinejad Meibodi", "Mahbod Gholamali Sinaki", "Khalil Alipour"], "title": "Optimal Regulation of Nonlinear Input-Affine Systems via an Integral Reinforcement Learning-Based State-Dependent Riccati Equation Approach", "comment": "Presented at the 13th RSI International Conference on Robotics and Mechatronics (ICRoM 2025), Dec. 16-18, 2025, Tehran, Iran", "summary": "The State-Dependent Riccati Equation (SDRE) technique generalizes the classical algebraic Riccati formulation to nonlinear systems by designing an input to the system that optimally(suboptimally) regulates system states toward the origin while simultaneously optimizing a quadratic performance index. In the SDRE technique, we solve the State-Dependent Riccati Equation to determine the control for regulating a nonlinear input-affine system. Since an analytic solution to SDRE is not straightforward, one method is to linearize the system at every state, solve the corresponding Algebraic Riccati Equation (ARE), and apply optimal control until the next state of the system. Completing this task with high frequency gives a result like the original SDRE technique. Both approaches require a complete model; therefore, here we propose a method that solves ARE in every state of the system using a partially model-free approach that learns optimal control in every state of the system, without explicit knowledge of the drift dynamics, based on Integral Reinforcement Learning (IRL). To show the effectiveness of our proposed approach, we apply it to the second-order nonlinear system in simulation and compare its performance with the classical SDRE method, which relies on the system's model and solves the ARE at each state. Our simulation results demonstrate that, with sufficient iterations, the IRL-based approach achieves approximately the same performance as the conventional SDRE method, demonstrating its capability as a reliable alternative for nonlinear system control that does not require an explicit environmental model. Index Terms-Algebraic Riccati Equation (ARE), Integral Reinforcement Learning (IRL), Nonlinear Input-Affine Systems, Optimal Regulation, State-Dependent Riccati Equation (SDRE)", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79ef\u5206\u5f3a\u5316\u5b66\u4e60\u7684\u90e8\u5206\u6a21\u578b\u81ea\u7531\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u975e\u7ebf\u6027\u8f93\u5165\u76f8\u5173\u7cfb\u7edf\u4e2d\u8fd1\u4f3c\u6c42\u89e3\u72b6\u6001\u76f8\u5173\u9ece\u5361\u63d0\u65b9\u7a0b\uff08SDRE\uff09\u4e0b\u7684\u63a7\u5236\u5668\uff0c\u4e0e\u7ecf\u5178SDRE\u76f8\u6bd4\u5728\u4e8c\u9636\u975e\u7ebf\u6027\u7cfb\u7edf\u4eff\u771f\u4e2d\u53ef\u5b9e\u73b0\u76f8\u8fd1\u6027\u80fd\uff0c\u4e14\u4e0d\u9700\u8981\u663e\u5f0f\u6f02\u79fb\u52a8\u529b\u5b66\u6a21\u578b\u3002", "motivation": "SDRE\u4f9d\u8d56\u5b8c\u6574\u7cfb\u7edf\u6a21\u578b\u5e76\u5728\u6bcf\u4e2a\u72b6\u6001\u6c42\u89e3ARE\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u5bf9\u6a21\u578b\u8bef\u5dee\u654f\u611f\u3002\u9700\u8981\u4e00\u79cd\u5728\u5c3d\u91cf\u5c11\u7684\u6a21\u578b\u4fe1\u606f\u4e0b\u4e5f\u80fd\u5b66\u4e60\u8fd1\u4f3c\u6700\u4f18\u63a7\u5236\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5e94\u7528\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u5728\u7cfb\u7edf\u7684\u6bcf\u4e00\u72b6\u6001\u4e0b\u89e3ARE\u7684\u8fd1\u4f3c\u89e3\u7684\u540c\u65f6\uff0c\u4f7f\u7528Integral Reinforcement Learning\uff08IRL\uff09\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60\uff0c\u4ece\u800c\u83b7\u5f97\u5728\u5404\u72b6\u6001\u4e0b\u7684\u6700\u4f18\u63a7\u5236\uff0c\u800c\u4e0d\u663e\u5f0f\u77e5\u9053\u6f02\u79fb\u9879\u7684\u52a8\u529b\u5b66\uff1b\u5728\u4e8c\u9636\u975e\u7ebf\u6027\u8f93\u5165\u8026\u5408\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4eff\u771f\u5e76\u4e0e\u7ecf\u5178SDRE\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8db3\u591f\u8fed\u4ee3\u540e\uff0cIRL\u57fa\u65b9\u6cd5\u7684\u63a7\u5236\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfSDRE\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u663e\u5f0f\u73af\u5883\u6a21\u578b\u65f6\u7684\u53ef\u884c\u6027\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5IRL-based\u65b9\u6cd5\u4e3a\u7f3a\u5c11\u663e\u5f0f\u6a21\u578b\u4fe1\u606f\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u8fd1\u4f3cSDRE\u89e3\u7684\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u9700\u8981\u90e8\u5206\u6a21\u578b\u81ea\u7531\u4e14\u5e0c\u671b\u4fdd\u6301\u8f83\u9ad8\u63a7\u5236\u6027\u80fd\u7684\u573a\u666f\u3002"}}
{"id": "2512.22128", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22128", "abs": "https://arxiv.org/abs/2512.22128", "authors": ["Yongyu Wang"], "title": "Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u56fe\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u526a\u9664\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u6709\u8d1f\u9762\u5f71\u54cd\u7684\u8fb9\uff0c\u63d0\u5347 GNN \u5728\u9ad8\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u5178\u578b GNN \u67b6\u6784\u4e0a\u5b9e\u73b0\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u663e\u8457\u589e\u5f3a\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "GNNs \u5728\u7ed3\u6784\u6216\u7279\u5f81\u5b58\u5728\u6270\u52a8\u65f6\u6613\u88ab\u653e\u5927\uff0c\u5bfc\u81f4\u5bf9\u6297\u653b\u51fb\u4e0e\u566a\u58f0\u8fde\u63a5\u7684\u8106\u5f31\u6027\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5e76\u79fb\u9664\u5bf9\u9c81\u68d2\u6027\u6709\u5bb3\u7684\u56fe\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u526a\u679d\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u5f97\u5230\u7684\u9c81\u68d2\u6027\u5206\u6570\u6765\u8bc6\u522b\u5e76\u526a\u9664\u6700\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u53ef\u9760\u6027\u7684\u8fb9\u6216\u7ed3\u6784\u7ec4\u4ef6\u3002\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u4ee3\u8868\u6027 GNN \u67b6\u6784\u4e0a\u5b9e\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u57fa\u4e8e\u9c81\u68d2\u6027\u5206\u6570\u7684\u8fb9\u526a\u679d\uff0c\u5f97\u5230\u66f4\u5e72\u51c0\u3001\u66f4\u5177\u9c81\u68d2\u6027\u7684\u56fe\u8868\u793a\uff1b\u5728\u9ad8\u6270\u52a8\u573a\u666f\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86 GNN \u7684\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "\u4ee5\u9c81\u68d2\u6027\u4e3a\u5bfc\u5411\u7684\u526a\u679d\u6846\u67b6\u53ef\u6709\u6548\u63d0\u5347 GNN \u7684\u5bf9\u6297\u9c81\u68d2\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2512.22533", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.22533", "abs": "https://arxiv.org/abs/2512.22533", "authors": ["Aparna V C", "Shashank Shekhar", "Sheetal Kalyani"], "title": "RIS, Active RIS or RDARS: A Comparative Insight Through the Lens of Energy Efficiency", "comment": null, "summary": "Multiplicative fading is a major limitation of reconfigurable intelligent surfaces (RIS), restricting their effective coverage in both existing sub-6GHz systems and future mmWave networks. Although active RIS architectures mitigate this issue, they require high power consumption and introduce practical challenges due to the need for integrated amplifiers. Recently, reconfigurable distributed antenna and reflecting surfaces (RDARS) have been proposed to alleviate multiplicative fading through connected modes. In this work, we compare RIS, active RIS, and RDARS in terms of coverage and energy efficiency (EE) in both sub-6GHz and mmWave bands, and we investigate the impact of placement and the number of elements of reconfigurable surface (RS) on EE and coverage. The simulation results show that RDARS offers a highly energy-efficient alternative of enhancing coverage in sub-6GHz systems, while active RIS is significantly more energy-efficient in mmWave systems. Additionally, for a lower number of RS elements and for near UEs, RIS remains considerably more energy-efficient than both active RIS and RDARS.", "AI": {"tldr": "RDARS\u5728sub-6GHz\u4e0b\u53ef\u4ee5\u9ad8\u6548\u63d0\u5347\u8986\u76d6\u5e76\u964d\u4f4e\u80fd\u8017\uff1b\u5728\u6beb\u7c73\u6ce2(mmWave)\u4e0b\uff0c\u4e3b\u52a8RIS\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1b\u5f53RS\u5355\u5143\u8f83\u5c11\u4e14\u63a5\u8fd1UE\u65f6\uff0cRIS\u5bf9\u80fd\u6548\u66f4\u5177\u4f18\u52bf\uff0c\u4f18\u4e8e\u4e24\u79cd\u5176\u4ed6\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u5728sub-6GHz\u4e0emmWave\u4e24\u79cd\u9891\u6bb5\u4e0b\uff0cRIS\u3001\u4e3b\u52a8RIS\u4e0eRDARS\u5728\u8986\u76d6\u4e0e\u80fd\u91cf\u6548\u7387\uff08EE\uff09\u4e0a\u7684\u5bf9\u6bd4\uff0c\u5e76\u8003\u91cfRS\u7684\u653e\u7f6e\u4e0e\u5355\u5143\u6570\u5bf9EE\u548c\u8986\u76d6\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u7ed3\u5408\u4eff\u771f\uff0c\u8bc4\u4f30\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u8986\u76d6\u4e0eEE\uff0c\u5305\u62ecRIS\u3001\u4e3b\u52a8RIS\u3001RDARS\u5728sub-6GHz\u4e0emmWave\u4e24\u79cd\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d8\u91cf\u5305\u62ecRS\u5355\u5143\u6570\u3001\u653e\u7f6e\u4f4d\u7f6e\u53ca\u4e0eUE\u7684\u8ddd\u79bb\u3002", "result": "\u5728sub-6GHz\u573a\u666f\u4e0b\uff0cRDARS\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u80fd\u6548\u63d0\u5347\u4e0e\u8986\u76d6\u589e\u5f3a\uff1b\u5728mmWave\u573a\u666f\u4e2d\uff0c\u4e3b\u52a8RIS\u7684\u80fd\u6548\u663e\u8457\u4f18\u4e8eRDARS\u4e0eRIS\uff1b\u4e14\u5f53RS\u5355\u5143\u6570\u8f83\u5c11\u4e14UE\u9760\u8fd1\u65f6\uff0cRIS\u7684\u80fd\u6548\u4ecd\u663e\u8457\u4f18\u4e8e\u4e3b\u52a8RIS\u4e0eRDARS\u3002", "conclusion": "\u5bf9\u4e8esub-6GHz\uff0cRDARS\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u4f4e\u80fd\u8017\u8986\u76d6\u589e\u5f3a\u65b9\u6848\uff1b\u5bf9\u4e8emmWave\uff0c\u4e3b\u52a8RIS\u66f4\u5177\u80fd\u6548\u4f18\u52bf\uff1b\u5728RS\u5355\u5143\u6570\u8f83\u5c11\u6216UE\u8ddd\u79bb\u8f83\u8fd1\u65f6\uff0c\u4f20\u7edfRIS\u4ecd\u5177\u80fd\u6548\u4f18\u52bf\uff0c\u663e\u793a\u51fa\u4e0d\u540c\u9891\u6bb5\u3001\u90e8\u7f72\u573a\u666f\u548c\u5355\u5143\u5c3a\u5ea6\u4e0b\u7684\u6743\u8861\u3002"}}
{"id": "2512.22143", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22143", "abs": "https://arxiv.org/abs/2512.22143", "authors": ["Gaofeng Dong", "Kang Yang", "Mani Srivastava"], "title": "UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing", "comment": "14 pages, 10 figures", "summary": "Existing Wi-Fi sensing systems rely on injecting high-rate probing packets to extract channel state information (CSI), leading to communication degradation and poor deployability. Although Integrated Sensing and Communication (ISAC) is a promising direction, existing solutions still rely on auxiliary packet injection because they exploit only CSI from data frames. We present UniFi, the first Wi-Fi-based ISAC framework that fully eliminates intrusive packet injection by directly exploiting irregularly sampled CSI from diverse communication packets across multiple frequency bands. UniFi integrates a CSI sanitization pipeline to harmonize heterogeneous packets and remove burst-induced redundancy, together with a time-aware attention model that learns directly from non-uniform CSI sequences without resampling. We further introduce CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic. Extensive evaluations on this dataset and four public benchmarks show that UniFi achieves state-of-the-art accuracy with a compact model size, while fully preserving communication throughput.", "AI": {"tldr": "\u63d0\u51fa UniFi\uff1a\u4e00\u79cd\u57fa\u4e8eWi-Fi\u7684 ISAC \u6846\u67b6\uff0c\u5229\u7528\u6765\u81ea\u591a\u9891\u6bb5\u3001\u5f02\u6b65\u91c7\u6837\u7684 CSI\uff0c\u5b8c\u5168\u907f\u514d\u6ce8\u5165\u63a2\u6d4b\u5305\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u4e0e\u5c0f\u6a21\u578b\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u541e\u5410\u3002", "motivation": "\u73b0\u6709 Wi-Fi \u611f\u77e5\u7cfb\u7edf\u4f9d\u8d56\u9ad8\u901f\u63a2\u6d4b\u5305\u6ce8\u5165\u6765\u63d0\u53d6 CSI\uff0c\u5bfc\u81f4\u901a\u4fe1\u6027\u80fd\u4e0b\u964d\u548c\u90e8\u7f72\u56f0\u96be\uff1bISAC \u65b9\u5411\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u591a\u4e3a\u5229\u7528\u6570\u636e\u5e27 CSI\uff0c\u4ecd\u9700\u6ce8\u5165\u8f85\u52a9\u5305\u3002\u9700\u4e00\u4e2a\u975e\u4fb5\u5165\u5f0f\u3001\u517c\u5bb9\u73b0\u6709\u901a\u4fe1\u6d41\u7684\u611f\u77e5\u6846\u67b6\u3002", "method": "\u63d0\u51fa CSI \u53bb\u566a\u7ba1\u9053\uff08sanitization\uff09\u4ee5\u7edf\u4e00\u5f02\u6784\u6570\u636e\u5305\u3001\u6d88\u9664\u7a81\u53d1\u5197\u4f59\uff1b\u5f15\u5165\u65f6\u5e8f\u81ea\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u975e\u5747\u5300 CSI \u5e8f\u5217\u5b66\u4e60\uff0c\u4e0d\u518d\u91cd\u65b0\u53d6\u6837\uff1b\u8de8\u9891\u5e26\u5229\u7528\u6765\u81ea\u591a\u9891\u6bb5\u6570\u636e\u7684 CSI\uff1b\u6784\u5efa CommCSI-HAR \u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u53cc\u9891\u5e26\u7684\u975e\u5747\u5300\u91c7\u6837 CSI\uff1b\u5728 CommCSI-HAR \u4e0e\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u7684\u611f\u77e5\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6a21\u578b\u89c4\u6a21\u7d27\u51d1\uff1b\u5b8c\u5168\u4e0d\u5f71\u54cd\u539f\u59cb\u901a\u4fe1\u541e\u5410\uff0c\u4e0e\u73b0\u6709\u6ce8\u5165\u578b\u65b9\u6cd5\u76f8\u6bd4\u5177\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u5c55\u793a\u4e86\u65e0\u9700\u4fb5\u5165\u5f0f\u63a2\u6d4b\u5305\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd RAMS \u7684 Wi-Fi ISAC \u6846\u67b6\uff0c\u4e14\u63d0\u4f9b\u4e86\u652f\u6301\u5f02\u6b65\u3001\u8de8\u9891\u6bb5 CSI \u7684\u6570\u636e\u96c6\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.22680", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22680", "abs": "https://arxiv.org/abs/2512.22680", "authors": ["Abderaouf Bahi", "Amel Ourici", "Chaima Lagraa", "Siham Lameche", "Soundess Halimi", "Inoussa Mouiche", "Ylias Sabri", "Waseem Haider", "Mohamed Trari"], "title": "From Electrochemical Energy Storage to Next-Generation Intelligent Battery Technologies for Electric Vehicles: A Survey", "comment": "This work was supervised by leading professor in the field (Pr. Mohamed Trari, Pr. Waseem Haider, Pr. Ylias Sabri)", "summary": "This study provides a comprehensive overview of recent advances in electrochemical energy storage, including Na+ -ion, metal-ion, and metal-air batteries, alongside innovations in electrode engineering, electrolytes, and solid-electrolyte interphase control. It also explores the integration of machine learning, digital twins, large language models and predictive analytics to enable intelligent battery management systems, enhancing performance, safety, and operational longevity. Key challenges, research gaps, and future prospects are addressed, highlighting opportunities presented by hybrid chemistry, scalable manufacturing, sustainability, and AI-driven optimization. This survey aims to provide researchers, engineers, and industry profesionnals with a comprehensive understanding of next-generation battery technologies for the evolving electric vehicles sector.", "AI": {"tldr": "\u5bf9\u6700\u8fd1\u5728\u7535\u5316\u5b66\u50a8\u80fd\u9886\u57df\u7684\u8fdb\u5c55\u8fdb\u884c\u7efc\u8ff0\uff0c\u6db5\u76d6Na+-\u79bb\u5b50\u3001\u91d1\u5c5e\u79bb\u5b50\u53ca\u91d1\u5c5e\u7a7a\u6c14\u7535\u6c60\u3001\u7535\u6781\u5de5\u7a0b\u3001\u7535\u89e3\u8d28\u3001\u56fa\u6001\u7535\u89e3\u8d28\u754c\u9762\u63a7\u5236\uff0c\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u3001\u6570\u5b57\u5b6a\u751f\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u7535\u6c60\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u8ba8\u8bba\u6311\u6218\u3001\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u524d\u666f\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u8f66\u5bf9\u9ad8\u6027\u80fd\u3001\u957f\u5bff\u547d\u4e14\u5b89\u5168\u7684\u50a8\u80fd\u7cfb\u7edf\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5f53\u524d\u6280\u672f\u3001\u6750\u6599\u4e0eAI\u9a71\u52a8\u4f18\u5316\u7684\u8fdb\u5c55\u4e0e\u6311\u6218\u3002", "method": "\u5bf9\u8fd1\u5e74\u6587\u732e\u8fdb\u884c\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6c47\u603b\u7535\u6c60\u4f53\u7cfb\u3001\u6750\u6599\u3001\u5de5\u827a\u3001\u4ee5\u53caAI\u6280\u672f\u5728\u7535\u6c60\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u5206\u7c7b\u6846\u67b6\u4e0e\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u7535\u6c60\u6280\u672f\u7684\u5173\u952e\u8fdb\u5c55\u3001\u53ef\u884c\u8def\u5f84\u548c\u5b9e\u73b0\u6311\u6218\uff1b\u63d0\u51fa\u6df7\u5408\u5316\u5b66\u3001\u89c4\u6a21\u5316\u5236\u9020\u3001\u53ef\u6301\u7eed\u6027\u53caAI\u4f18\u5316\u7b49\u7814\u7a76\u65b9\u5411\uff1b\u5f3a\u8c03AI\u9a71\u52a8\u7684\u4f18\u5316\u5bf9\u6027\u80fd\u3001\u5b89\u5168\u548c\u5bff\u547d\u7684\u6f5c\u5728\u63d0\u5347\u3002", "conclusion": "\u4e3a\u7814\u7a76\u8005\u3001\u5de5\u7a0b\u5e08\u548c\u884c\u4e1a\u4e13\u4e1a\u4eba\u5458\u63d0\u4f9b\u9762\u5411\u672a\u6765\u7684\u4e0b\u4e00\u4ee3\u7535\u6c60\u6280\u672f\u7684\u7efc\u5408\u7406\u89e3\uff0c\u4fc3\u8fdb\u7535\u52a8\u6c7d\u8f66\u9886\u57df\u7684\u6280\u672f\u8f6c\u5316\u4e0e\u4ea7\u4e1a\u5316\u3002"}}
{"id": "2512.22150", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22150", "abs": "https://arxiv.org/abs/2512.22150", "authors": ["Hans Jarett J. Ong", "Brian Godwin S. Lim", "Dominic Dayta", "Renzo Roel P. Tan", "Kazushi Ikeda"], "title": "Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders", "comment": null, "summary": "Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.", "AI": {"tldr": "LANCA \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6027\u566a\u58f0\u6a21\u578b\u7684\u56e0\u679c\u81ea\u7f16\u7801\u5668\uff0c\u7528\u786e\u5b9a\u6027WAE+ANM\u5c42\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u6f5c\u5728\u56e0\u679c\u56e0\u7d20\u7684\u5206\u89e3\uff0c\u5e76\u5c06\u6b8b\u5dee\u72ec\u7acb\u6027\u8f6c\u6362\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u4ece\u800c\u4f7f\u6f5c\u5728\u56e0\u5b50\u5728\u7ec4\u4ef6\u5c42\u9762\u88ab\u8bc6\u522b\uff08\u5728\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u4f18\u4e8e\u6700\u65b0\u65b9\u6cd5\uff0c\u4e14\u5bf9\u80cc\u666f\u6df7\u6dc6\u5177\u6709\u9c81\u68d2\u6027\uff09\u3002", "motivation": "\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u9762\u4e34\u7684\u53ef identifiability \u95ee\u9898\uff1a\u4ec5\u9760\u7edf\u8ba1\u72ec\u7acb\u6027\u5f80\u5f80\u65e0\u6cd5\u63ed\u793a\u6f5c\u5728\u7684\u56e0\u679c\u53d8\u91cf\uff0c\u7f3a\u4e4f\u76d1\u7763\u6216\u5f3a\u504f\u597d\u65f6\u5f88\u96be\u5b9e\u73b0\u56e0\u679c\u5206\u89e3\u3002ANM \u63d0\u4f9b\u5f3a\u7684\u5148\u9a8c\u504f\u7f6e\uff0c\u4f46\u4ec5\u5728\u4e00\u822c\u6df7\u5408\u4e0b\u672a\u5fc5\u5b9e\u73b0\u552f\u4e00\u53ef\u8bc6\u522b\u6027\uff1b\u9700\u8981\u65b9\u6cd5\u4ee5\u63d0\u5347\u6f5c\u5728\u56e0\u679c\u5206\u89e3\u7684\u53ef\u8bc6\u522b\u6027\u3002", "method": "\u63d0\u51fa LANCA\uff1a\u5728\u786e\u5b9a\u6027 WAE \u6846\u67b6\u4e0a\u5d4c\u5165\u53ef\u5fae\u5206\u7684 ANM \u5c42\uff0c\u5c06\u6b8b\u5dee\u72ec\u7acb\u6027\u4f5c\u4e3a\u663e\u5f0f\u4f18\u5316\u76ee\u6807\uff1b\u7406\u8bba\u4e0a\u8bc1\u660e ANM \u7ea6\u675f\u5728\u4e00\u822c\u6df7\u5408\u60c5\u5f62\u4e0b\u4e0d\u4fdd\u8bc1\u552f\u4e00\u53ef\u8bc6\u522b\u6027\uff0c\u4f46\u80fd\u5c06\u81ea\u7531\u53d8\u6362\u4ece\u4efb\u610f\u5fae\u5206\u540c\u80da\u7f29\u5c0f\u5230\u4eff\u5c04\u53d8\u6362\u7c7b\uff0c\u4ece\u800c\u89e3\u51b3\u7ec4\u4ef6\u5c42\u9762\u7684\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u5b9e\u8df5\u4e2d\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u7f16\u7801\u907f\u514d VAE \u7684\u968f\u673a\u6027\u63a9\u76d6\u7ed3\u6784\u6b8b\u5dee\u3002", "result": "\u5728\u5408\u6210\u7269\u7406\u4efb\u52a1\uff08\u6446\u52a8 Pendulum\u3001\u6d41\u4f53 Flow\uff09\u4ee5\u53ca\u771f\u5b9e\u611f\u73af\u5883\uff08CANDLE\uff09\u4e0a\uff0cLANCA \u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u5bf9\u590d\u6742\u80cc\u666f\u573a\u666f\u5f15\u5165\u7684\u4f2a\u76f8\u5173\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06 ANM \u4f5c\u4e3a\u5f3a\u8bf1\u5bfc\u504f\u7f6e\uff0cLANCA \u5b9e\u73b0\u4e86\u6f5c\u5728\u56e0\u679c\u56e0\u7d20\u7684\u66f4\u5f3a\u8bc6\u522b\u80fd\u529b\u5e76\u63d0\u9ad8\u5bf9\u566a\u58f0\u4e0e\u80cc\u666f\u6df7\u6dc6\u7684\u9c81\u68d2\u6027\uff1b\u4f46\u53ef\u8bc6\u522b\u6027\u4ecd\u9650\u5b9a\u5728\u4eff\u5c04\u53d8\u6362\u7c7b\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5728\u66f4\u4e00\u822c\u8bbe\u5b9a\u4e2d\u7684\u9650\u5ea6\u3002"}}
{"id": "2512.22301", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22301", "abs": "https://arxiv.org/abs/2512.22301", "authors": ["Aayush Mainali", "Sirjan Ghimire"], "title": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography", "comment": null, "summary": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u573a\u666f\u5316\u7684\u7edf\u8ba1\u98ce\u9669\u6a21\u578b\uff0c\u7528\u4ee5\u8bc4\u4f30\u65f6\u5e8f\u4fa7\u4fe1\u9053\u5728\u683c\u57fa\u5bc6\u94a5\u4ea4\u6362\u4e2d\u7684\u53ef\u533a\u5206\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6cc4\u6f0f\u6a21\u578b\u548c\u6267\u884c\u573a\u666f\u4e0b\u5bf9 Kyber\u3001Saber\u3001Frodo \u7b49\u5bb6\u65cf\u8fdb\u884c\u53ef\u6bd4\u5206\u6790\u3002", "motivation": "\u65f6\u5e8f\u4fa7\u4fe1\u9053\u662f\u5b9e\u73b0\u5c42\u7684\u4e00\u5927\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u80cc\u666f\u4e0b\uff0c\u590d\u6742\u7b97\u672f\u4e0e\u63a7\u5236\u6d41\u53ef\u80fd\u5bfc\u81f4\u79d8\u5bc6\u76f8\u5173\u7684\u65f6\u5e8f\u6ce2\u52a8\u3002\u73b0\u5b9e\u6d4b\u91cf\u8fd8\u53d7\u73af\u5883\u566a\u58f0\uff08\u8c03\u5ea6\u3001\u4e89\u7528\u3001\u5c3e\u90e8\u5ef6\u8fdf\u7b49\uff09\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u53ef\u5728\u8bbe\u8ba1\u65e9\u671f\u7528\u4e8e\u8de8\u573a\u666f\u6bd4\u8f83\u7684\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5728\u7a7a\u95f2\u3001\u6296\u52a8\u548c\u8d1f\u8f7d\u7b49\u573a\u666f\u4e2d\u5408\u6210\u4e24\u7ec4\u79d8\u5bc6\u7c7b\u522b\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u8986\u76d6\u591a\u79cd\u6cc4\u6f0f\u6a21\u578b\u3002\u4f7f\u7528 Welch \u7684 t \u68c0\u9a8c\u3001KS \u8ddd\u79bb\u3001Cliff \u7684 delta\u3001\u4e92\u4fe1\u606f\u548c\u5206\u5e03\u91cd\u53e0\u5ea6\u7b49\u91cf\u5316\u6cc4\u6f0f\uff0c\u5e76\u4ee5 TLRI \u7c7b\u6bd4\u7684\u65b9\u5f0f\u5c06\u7ed3\u679c\u6574\u5408\u6210\u4e00\u4e2a\u53ef\u7528\u4e8e\u573a\u666f\u6392\u5e8f\u7684\u4e00\u81f4\u5206\u6570\u3002", "result": "\u5728\u4ee3\u8868\u6027\u683c\u57fa\u65b9\u6848 Kyber\u3001Saber\u3001Frodo \u4e2d\uff0c\u7a7a\u95f2\u6761\u4ef6\u4e0b\u7684\u53ef\u533a\u5206\u6027\u901a\u5e38\u6700\u597d\uff1b\u6296\u52a8\u4e0e\u8d1f\u8f7d\u6761\u4ef6\u901a\u8fc7\u65b9\u5dee\u589e\u52a0\u548c\u91cd\u53e0\u5ea6\u589e\u5927\u800c\u964d\u4f4e\u53ef\u533a\u5206\u6027\u3002\u7f13\u5b58\u7d22\u5f15\u6cc4\u6f0f\u4e0e\u5206\u652f\u6837\u5f0f\u6cc4\u6f0f\u5f80\u5f80\u7ed9\u51fa\u66f4\u9ad8\u7684\u98ce\u9669\u4fe1\u53f7\uff1b\u5728\u76f8\u4f3c\u6cc4\u6f0f\u5047\u8bbe\u4e0b\uff0c\u66f4\u5feb\u7684\u65b9\u6848\u53ef\u80fd\u8fbe\u5230\u66f4\u9ad8\u7684\u5cf0\u503c\u98ce\u9669\u3002\u8fd9\u4e3a\u5728\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u3001\u5728\u4e0d\u4f9d\u8d56\u5177\u4f53\u5e73\u53f0\u9a8c\u8bc1\u524d\uff0c\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u8de8\u65b9\u6848\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u5feb\u901f\u6bd4\u8f83\u683c\u57fa KEM \u5bb6\u65cf\u5728\u65f6\u5e8f\u6cc4\u6f0f\u4e0a\u7684\u98ce\u9669\u7684\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u573a\u666f\u5316\u7684\u7edf\u8ba1\u91cf\u5316\u4e0e\u591a\u6a21\u578b\u6574\u5408\uff0c\u5e2e\u52a9\u9009\u578b\u4e0e\u8bc4\u4f30\u3002"}}
{"id": "2512.22146", "categories": ["eess.SP", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22146", "abs": "https://arxiv.org/abs/2512.22146", "authors": ["Hanbeot Park", "Yunjeong Cho", "Hunhee Kim"], "title": "EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG", "comment": "20 pages, 7 figures, 4 tables", "summary": "Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eEEG\u7684\u5f00\u73af\u8bed\u97f3\u91cd\u5efa\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4ece\u975e\u4fb5\u5165\u6027EEG\u751f\u6210mel\u9891\u8c31\uff0c\u8fdb\u800c\u5408\u6210\u8bed\u97f3\u5e76\u89e3\u7801\u6587\u672c\uff1b\u5bf9 spoken \u4e0e imagined \u4e24\u79cd\u8bed\u97f3\u5747\u6709\u6548\uff0c\u4e14\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u4e0e\u8bed\u8a00\u6a21\u578b\u7ea0\u9519\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3EEG\u4fe1\u53f7\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u9650\u5236\u3001\u566a\u58f0\u5e72\u6270\u4ee5\u53ca\u7f3a\u4e4f\u5bf9 imagined speech \u7684\u65f6\u5e8f\u5bf9\u9f50\u7b49\u6311\u6218\uff0c\u76f4\u63a5\u4eceEEG\u5b9e\u73b0\u8bed\u97f3\u91cd\u5efa\u3002", "method": "\u5efa\u7acb\u4e00\u4e2asubject-specific generator\uff0c\u5148\u4eceEEG\u751f\u6210mel-spectrogram\uff1b\u518d\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u58f0\u7801\u5668(vocoder)\u5408\u6210\u8bed\u97f3\uff0c\u4ee5\u53caASR\u6a21\u5757\u89e3\u7801\u6587\u672c\uff1b\u5206\u522b\u4e3a spoken \u4e0e imagined \u8bad\u7ec3\u72ec\u7acb\u751f\u6210\u5668\uff0c\u4f7f\u7528\u4ee5 spoken \u4e3a\u57fa\u7840\u7684\u8fc1\u79fb\u5b66\u4e60\u9002\u5e94 imagined\uff1b\u53ef\u9009\u7684\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u4fee\u6b63\u6a21\u5757\u4ee5\u7ea0\u9519\u800c\u4e0d\u7834\u574f\u8bed\u4e49\u3002\u8bc4\u4f30\u57282\u79d2\u4e0e4\u79d2\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528PCC\u3001RMSE\u3001MCD\u3001CER\u3001WER\u7b49\u6307\u6807\u3002", "result": "\u5728 spoken \u4e0e imagined \u4e0b\uff0c\u89c2\u6d4b\u5230\u7a33\u5b9a\u7684\u58f0\u5b66\u91cd\u5efa\u4e0e\u53ef\u6bd4\u7684\u6587\u672c\u51c6\u786e\u7387\uff1b\u968f\u7740\u65f6\u957f\u589e\u52a0\uff0c\u58f0\u5b66\u76f8\u4f3c\u5ea6\u4e0b\u964d\u4f46\u6587\u672c\u7ea7\u89e3\u7801\u57fa\u672c\u7ef4\u6301\uff0c\u53e5\u672b\u7684\u5355\u8bcd\u4f4d\u7f6e\u9519\u8bef\u7565\u589e\uff1b\u8bed\u8a00\u6a21\u578b\u7ea0\u9519\u663e\u8457\u964d\u4f4eCER\u4e0eWER\u4e14\u4e0d\u5f15\u5165\u8bed\u4e49\u626d\u66f2\u3002", "conclusion": "\u8bc1\u660e\u4e86\u76f4\u63a5\u3001\u5f00\u653e\u5f0f\u7684EEG-to-Voice\u91cd\u5efa\u5728 spoken \u548c imagined \u8bed\u97f3\u4e0a\u53ef\u884c\u4e14\u65e0\u9700\u663e\u5f0f\u65f6\u5e8f\u5bf9\u9f50\u3002"}}
{"id": "2512.22786", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22786", "abs": "https://arxiv.org/abs/2512.22786", "authors": ["\u00d6zhan Bing\u00f6l"], "title": "A Time-Barrier Lyapunov Condition for Predefined-Time Stability", "comment": "4 pages, 0 figures", "summary": "Predefined-time stability enables convergence within a user-specified time independent of initial conditions. Existing results are predominantly based on autonomous Lyapunov inequalities, where the predefined-time is realized through integral bounds on state-dependent decay and therefore acts as an upper bound rather than a structurally enforced deadline. This paper introduces a time-barrier predefined-time stability concept in which convergence is enforced through a nonautonomous Lyapunov mechanism that intrinsically restricts the remaining available time. A sufficient Lyapunov-based condition is established, guaranteeing convergence before the predefined deadline via divergence of a time-dependent barrier. It is further shown that this mechanism cannot be reproduced by classical autonomous predefined-time stability formulations, thereby constituting a distinct stability notion. The proposed approach provides a concise and transparent means of enforcing hard convergence deadlines in nonlinear systems.", "AI": {"tldr": "\u63d0\u51fa\u5e26\u65f6\u95f4\u5c4f\u969c\u7684\u9884\u5b9a\u65f6\u7a33\u5b9a\u6027\u6982\u5ff5\u53ca\u5176\u5145\u5206\u6761\u4ef6\uff0c\u5f3a\u8c03\u901a\u8fc7\u975e\u81ea\u6cbbLyapunov\u673a\u5236\u5728\u9884\u8bbe\u622a\u6b62\u65f6\u95f4\u524d\u5b9e\u73b0\u6536\u655b\uff0c\u4e0e\u81ea\u6cbb\u5f62\u5f0f\u4e0d\u540c\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u81ea\u6cbbLyapunov\u4e0d\u7b49\u5f0f\u7684\u9884\u5b9a\u65f6\u7a33\u5b9a\u6027\u4ec5\u80fd\u63d0\u4f9b\u6536\u655b\u4e0a\u754c\u7684\u95ee\u9898\uff0c\u5bfb\u6c42\u786c\u622a\u6b62\u65f6\u95f4\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u975e\u81ea\u6cbbLyapunov\u7ed3\u6784\u7684\u65f6\u95f4\u76f8\u5173\u5c4f\u969c\u9879\uff0c\u8bc1\u660e\u968f\u7740\u65f6\u95f4\u5c4f\u969c\u7684\u53d1\u6563\uff0c\u7cfb\u7edf\u5728\u9884\u5b9a\u622a\u6b62\u65f6\u95f4\u524d\u6536\u655b\uff1b\u7ed9\u51fa\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\u4ee5\u4fdd\u8bc1\u8be5\u6027\u8d28\u3002", "result": "\u7ed9\u51fa\u4e00\u79cd\u65e0\u6cd5\u88ab\u7ecf\u5178\u81ea\u6cbb\u9884\u5b9a\u65f6\u7a33\u5b9a\u6027\u516c\u5f0f\u91cd\u73b0\u7684\u65b0\u7684\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u4e14\u63d0\u4f9b\u7b80\u660e\u7684\u5b9e\u73b0\u9014\u5f84\u3002", "conclusion": "\u65f6\u95f4\u5c4f\u969c\u9884\u5b9a\u65f6\u7a33\u5b9a\u6027\u4e3a\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5f3a\u5236\u5b9e\u73b0\u786c\u6536\u655b\u622a\u6b62\u65f6\u95f4\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u3001\u900f\u660e\u7684\u6846\u67b6\u3002"}}
{"id": "2512.22553", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.22553", "abs": "https://arxiv.org/abs/2512.22553", "authors": ["Shermin Sultana Setu", "Mst. Amena Akter Pinky", "Md. Abdul Awal", "Sheekar Banerjee", "Ishtiak Al Mamoon"], "title": "A Novel Approach for a Smart IoMT-Based BAN for an Old Home Healthcare Monitoring System Using Starlink", "comment": "IEEE Conference on Biomedical Engineering, Computer and Information Technology for Health 2025", "summary": "The rapid evolution of the Internet of Medical Things (IoMT) technology has become a transformative force in modern healthcare, particularly in elderly patient management. The current elderly care system faces significant challenges, including insufficient long-term care resources and poor communication between healthcare providers. To address this limitation, this study introduces a novel Starlink-assisted IOMT-based elderly healthcare model designed to improve remote patient monitoring and communication reliability. This proposal system focused on a monitoring system of key biomedical parameters such as electrocardiogram (ECG), body temperature, heart attack indicators, and a fall detection alert system. Performance is evaluated using the network simulator (NS-3) to assess its effectiveness in remote and underserved regions. Physiological data collected from patients are transmitted through a local communication hub and forwarded over a Low Earth Orbit (LEO) satellite link to a medical center. Based on Quality of Service (QoS) technology that combines Flow Queuing (FQ) with Controlled Delay (CoDel) with Differentiated Services Code Point (DSCP) marking. This approach prioritizes critical health data for faster transmission while allocating lower priority to non-urgent information. This architecture is entirely wireless, allowing continuous monitoring, real-time alerts, and secure data storage for medical analysis. The simulation results demonstrate that the proposed Starlink-enabled IOMT system outperforms existing solutions in terms of throughput, latency, and reliability. The findings highlight Starlink's potential as a robust and high-performance telehealth communication tool. In addition, this study provides a scalable and reproducible framework for future satellite-assisted healthcare systems that prioritize quality of service (QoS) performance and improved elderly patient care.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u661f\u94fe\u536b\u661f\u7684\u7269\u8054\u7f51\u533b\u7597\uff08IoMT\uff09\u517b\u8001\u62a4\u7406\u7cfb\u7edf\uff0c\u5229\u7528QoS\u7b56\u7565\uff08FQ-CoDel + DSCP\uff09\u5bf9\u5173\u952e\u5065\u5eb7\u6570\u636e\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u961f\u548c\u4f20\u8f93\uff0c\u5728NS-3\u4eff\u771f\u4e2d\u8bc4\u4f30\uff0c\u663e\u793a\u5728\u504f\u8fdc\u5730\u533a\u7684\u541e\u5410\u91cf\u3001\u65f6\u5ef6\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u5177\u5907\u53ef\u6269\u5c55\u4e0e\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u517b\u8001\u62a4\u7406\u4f53\u7cfb\u8d44\u6e90\u4e0d\u8db3\u3001\u533b\u751f\u95f4\u6c9f\u901a\u534f\u4f5c\u4e0d\u7545\u7684\u95ee\u9898\uff0c\u5229\u7528\u661f\u94fe\u536b\u661f\u5b9e\u73b0\u5bf9\u8001\u5e74\u60a3\u8005\u7684\u8fdc\u7a0b\u76d1\u63a7\u4e0e\u7a33\u5b9a\u901a\u4fe1\uff0c\u63d0\u9ad8\u6162\u6027\u75be\u75c5\u7ba1\u7406\u548c\u7d27\u6025\u4e8b\u4ef6\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u5728\u672c\u5730\u901a\u4fe1\u67a2\u7ebd\u6536\u96c6ECG\u3001\u4f53\u6e29\u3001\u5fc3\u6897\u6307\u6807\u4e0e\u8dcc\u5012\u68c0\u6d4b\u7b49\u751f\u7406\u6570\u636e\uff0c\u901a\u8fc7LEO\u536b\u661f\u94fe\u8def\u4f20\u8f93\u81f3\u533b\u7597\u4e2d\u5fc3\uff1b\u4ee5NS-3\u5bf9\u7cfb\u7edf\u5728\u9065\u8fdc/\u6b20\u670d\u52a1\u5730\u533a\u7684\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u5408Flow Queuing\uff08FQ\uff09\u4e0eCoDel\u7684\u6392\u961f\u63a7\u5236\u3001\u5e76\u4f7f\u7528DSCP\u6807\u8bb0\u5b9e\u73b0\u5173\u952e\u6570\u636e\u7684QoS\u4f18\u5148\u7ea7\uff1b\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u5168\u65e0\u7ebf\u3001\u5177\u5907\u5b9e\u65f6\u544a\u8b66\u4e0e\u5b89\u5168\u6570\u636e\u5b58\u50a8\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0Starlink\u534f\u540cIoMT\u7cfb\u7edf\u5728\u541e\u5410\u91cf\u3001\u65f6\u5ef6\u4e0e\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u793aStarlink\u5728\u8fdc\u7a0b\u533b\u7597\u901a\u4fe1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u536b\u661f\u8f85\u52a9\u533b\u7597\u6846\u67b6\uff0c\u5f3a\u8c03\u4ee5QoS\u4e3a\u6838\u5fc3\u7684\u5173\u952e\u5065\u5eb7\u6570\u636e\u4f20\u8f93\u4f18\u5148\u7ea7\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u517b\u8001\u62a4\u7406\u9886\u57df\u7684\u536b\u661f\u901a\u4fe1\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.22691", "categories": ["cs.IT", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.22691", "abs": "https://arxiv.org/abs/2512.22691", "authors": ["Haiyang Wang", "Luca Barletta", "Alex Dytso"], "title": "An Improved Lower Bound on Cardinality of Support of the Amplitude-Constrained AWGN Channel", "comment": null, "summary": "We study the amplitude-constrained additive white Gaussian noise channel. It is well known that the capacity-achieving input distribution for this channel is discrete and supported on finitely many points. The best known bounds show that the support size of the capacity-achieving distribution is lower-bounded by a term of order $A$ and upper-bounded by a term of order $A^2$, where $A$ denotes the amplitude constraint. It was conjectured in [1] that the linear scaling is optimal. In this work, we establish a new lower bound of order $A\\sqrt{\\log A}$, improving the known bound and ruling out the conjectured linear scaling.\n  To obtain this result, we quantify the fact that the capacity-achieving output distribution is close to the uniform distribution in the interior of the amplitude constraint. Next, we introduce a wrapping operation that maps the problem to a compact domain and develop a theory of best approximation of the uniform distribution by finite Gaussian mixtures. These approximation bounds are then combined with stability properties of capacity-achieving distributions to yield the final support-size lower bound.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5728\u5e45\u5ea6\u7ea6\u675f\u7684\u9ad8\u65af\u4fe1\u9053\u4e2d\uff0c\u5bb9\u91cf\u8fbe\u5230\u8f93\u5165\u5206\u5e03\u7684\u6700\u5c0f\u652f\u6301\u5927\u5c0f\u4e0b\u754c\u4e3a O(A sqrt(log A))\uff0c\u5426\u5b9a\u7ebf\u6027\u589e\u957f\u7684\u731c\u60f3\uff1b\u5e76\u63d0\u51fa\u5c06\u95ee\u9898\u6620\u5c04\u5230\u7d27\u81f4\u57df\u7684 wrap \u64cd\u4f5c\u53ca\u7528\u6709\u9650\u9ad8\u65af\u6df7\u5408\u7269\u903c\u8fd1\u5747\u5300\u5206\u5e03\u7684\u6700\u4f18\u8fd1\u4f3c\u7406\u8bba\uff0c\u5f62\u6210\u4e00\u4e2a\u5206\u6790\u6846\u67b6\u3002", "motivation": "\u7406\u89e3\u5e45\u5ea6\u53d7\u9650 AWGN \u4fe1\u9053\u7684\u5bb9\u91cf\u4e0e\u6700\u4f18\u8f93\u5165\u5206\u5e03\u7684\u7ed3\u6784\u3002\u4ee5\u5f80\u4e0b\u754c\u5728\u7ebf\u6027\u5230\u4e8c\u6b21\u4e4b\u95f4\u6ce2\u52a8\uff0c\u4e14\u5b58\u5728\u7ebf\u6027\u589e\u957f\u7684\u731c\u60f3\uff1b\u9700\u8981\u66f4\u4e25\u683c\u7684\u589e\u957f\u9636\u4ee5\u63ed\u793a\u652f\u6301\u5927\u5c0f\u7684\u771f\u5b9e\u89c4\u6a21\u3002", "method": "\u91cf\u5316\u5bb9\u91cf\u8fbe\u5230\u8f93\u51fa\u5206\u5e03\u5728\u5e45\u5ea6\u7ea6\u675f\u5185\u90e8\u63a5\u8fd1\u5747\u5300\u7684\u7279\u6027\uff1b\u5f15\u5165 wrapping \u5c06\u95ee\u9898\u6620\u5c04\u5230\u7d27\u81f4\u57df\uff1b\u5efa\u7acb\u7528\u6709\u9650\u9ad8\u65af\u6df7\u5408\u7269\u8fd1\u4f3c\u5747\u5300\u5206\u5e03\u7684\u6700\u4f73\u8fd1\u4f3c\u7406\u8bba\uff1b\u7ed3\u5408\u5bb9\u91cf\u8fbe\u5230\u5206\u5e03\u7684\u7a33\u5b9a\u6027\u6027\u8d28\uff0c\u5bfc\u51fa\u652f\u6301\u5927\u5c0f\u4e0b\u754c\u3002", "result": "\u5f97\u5230\u65b0\u7684\u4e0b\u754c\uff1a\u5bb9\u91cf\u8fbe\u5230\u5206\u5e03\u7684\u652f\u6301\u5927\u5c0f\u81f3\u5c11\u4e3a O(A sqrt(log A))\uff0c\u6bd4\u5148\u524d\u7ebf\u6027\u4e0b\u754c\u6709\u663e\u8457\u63d0\u5347\uff1b\u901a\u8fc7\u628a\u8f93\u51fa\u8fd1\u4f3c\u4e3a\u5747\u5300\u5e76\u7528\u9ad8\u65af\u6df7\u5408\u7269\u5b9e\u73b0\u903c\u8fd1\uff0c\u7ed9\u51fa\u53ef\u89e3\u6790\u7684\u4e0b\u754c\u8fc7\u7a0b\u3002", "conclusion": "\u7ebf\u6027\u589e\u957f\u5e76\u975e\u6700\u4f18\u4e0b\u754c\uff0c\u5b9e\u9645\u589e\u957f\u81f3\u5c11\u662f A sqrt(log A)\uff1bwrap \u64cd\u4f5c\u4e0e\u6709\u9650\u9ad8\u65af\u6df7\u5408\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u5206\u6790\u53d7\u9650\u901a\u9053\u79bb\u6563\u5316\u7ed3\u6784\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.22306", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22306", "abs": "https://arxiv.org/abs/2512.22306", "authors": ["Chinmay Pushkar", "Sanchit Kabra", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection", "comment": "Under Review", "summary": "Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from \"count bias\" and \"selection bias\" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe \"under-counting\" (Recall dropping to less than 0.30) in complex Python files.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u591a\u6f0f\u6d1e\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u6f0f\u6d1e\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u8ba1\u6570\u504f\u5dee\u548c\u9009\u62e9\u504f\u5dee\u663e\u8457\u5f71\u54cd\u591a\u6807\u7b7e\u6f0f\u6d1e\u68c0\u6d4b\u3002", "motivation": "\u73b0\u5b9e\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u591a\u6f0f\u6d1e\u5e76\u5b58\u4e14\u76f8\u4e92\u4f5c\u7528\uff0c\u73b0\u6709\u57fa\u51c6\u591a\u805a\u7126\u5355\u6f0f\u6d1e\u6216\u51fd\u6570\u7ea7\u522b\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u4ee3\u7801\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u591a\u6807\u7b7e\u4e0e\u957f\u4e0a\u4e0b\u6587\u6311\u6218\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8986\u76d6\u591a\u8bed\u8a00\u3001\u591a\u6f0f\u6d1e\u5bc6\u5ea6\u7684\u57fa\u51c6\u6765\u5b9a\u91cf\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa40,000\u4e2a\u6587\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6C\u3001C++\u3001Python\u3001JavaScript\u56db\u79cd\u8bed\u8a00\uff1b\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u6837\u672c\uff087.5k-10k tokens\uff09\u4e2d\u7cfb\u7edf\u6027\u6ce8\u5165\u6f0f\u6d1e\u6570\u91cf\uff081\u30013\u30015\u30019\uff09\uff0c\u6570\u636e\u6e90\u6765\u81eaCodeParrot\uff1b\u8bc4\u4f30\u4e94\u79cd\u4e3b\u6d41LLM\uff08\u5305\u62ecGPT-4o-mini\u3001Llama-3.3-70B\u3001Qwen-2.5\u7cfb\u7b49\uff09\u3002", "result": "\u968f\u7740\u6f0f\u6d1e\u5bc6\u5ea6\u63d0\u5347\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u5728\u5355\u6f0f\u6d1eC\u4efb\u52a1\u4e0a\uff0cLlama-3.3-70B\u8fd1\u4e4e\u5b8c\u7f8eF1\u7ea60.97\uff0c\u4f46\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u964d\u5e45\u53ef\u8fbe\u7ea640%\uff1bPython\u548cJavaScript\u5448\u73b0\u4e0eC/C++\u4e0d\u540c\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u51fa\u73b0\u4e25\u91cd\u7684\u201c\u6b20\u8ba1\u6570\u201d\u73b0\u8c61\uff0cRecall\u964d\u81f3<0.30\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u8de8\u8bed\u8a00\u591a\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u548c\u7ea0\u6b63\u591a\u6807\u7b7e\u4efb\u52a1\u4e2d\u7684\u8ba1\u6570\u504f\u5dee/\u9009\u62e9\u504f\u5dee\uff0c\u4ee5\u53ca\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u7684\u7279\u5b9a\u7b56\u7565\u4e0e\u6570\u636e\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u5347\u5728\u771f\u5b9e\u4ee3\u7801\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.22793", "categories": ["eess.SY", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.22793", "abs": "https://arxiv.org/abs/2512.22793", "authors": ["Minh Bui", "Simon Monckton", "Mo Chen"], "title": "Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach", "comment": "Paper version accepted to the Journal of Guidance, Control, and Dynamics (JGCD)", "summary": "Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u4e09\u7ef4\u8ffd\u6355\u907f\u8ba9\uff08RA\uff09\u6e38\u620f\u5206\u89e3\u4e3a\u6c34\u5e73\u5b50\u535a\u5f08\u4e0e\u5782\u76f4\u5b50\u535a\u5f08\u7684\u964d\u7ef4\u6846\u67b6\uff0c\u7ed3\u5408Hamilton-Jacobi\uff08HJ\uff09 reachability\u89e3\u51b3\u5b50\u535a\u5f08\uff0c\u5e76\u5f15\u5165\u8003\u8651 Defender \u52a0\u901f\u5ea6\u7684\u4e8c\u9636\u52a8\u529b\u5b66\uff1b\u901a\u8fc7\u57fa\u4e8eHJ\u7684\u8ddf\u8e2a\u63a7\u5236\u5bf9\u539f\u59cb\u6e38\u620f\u8fdb\u884c\u91cd\u5efa\uff0c\u7ed9\u51fa\u6355\u83b7\u6027\u4fdd\u8bc1\u7684\u6761\u4ef6\uff0c\u7406\u8bba\u4e0e\u6570\u503c/ Gazebo \u4eff\u771f\u9a8c\u8bc1\u663e\u793a\u4fdd\u6301\u6700\u4f18\u6027\u4e0e\u5168\u5c40\u6355\u83b7\u80fd\u529b\uff0c\u9996\u6b21\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5bf9\u56db\u65cb\u7ffc\u7684\u6210\u529f\u6355\u83b7\uff08\u636e\u79f0\uff09\u3002", "motivation": "\u5728\u5b89\u5168\u4e0e\u9632\u52a1\u573a\u666f\u4e2d\uff0cRA \u6e38\u620f\u5728\u5305\u542b\u969c\u788d\u548c\u5bf9\u624b\u7684\u975e\u7ebf\u6027\u4e09\u7ef4\u52a8\u529b\u5b66\u4e0b\u6c42\u89e3\u5177\u6709\u6311\u6218\u6027\uff1b\u4f20\u7edfHJ reachability\u96be\u4ee5\u76f4\u63a5\u6269\u5c55\u5230\u4e09\u7ef4\uff0c\u5176\u4ed6\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5bf9\u4e09\u7ef4\u590d\u6742\u52a8\u529b\u5b66\u7684\u901a\u7528\u6027\uff0c\u8981\u4e48\u65e0\u6cd5\u540c\u65f6\u786e\u4fdd\u6700\u4f18\u6027\u4e0e\u53ef\u8bc1\u660e\u7684\u6355\u83b7\u6027\uff1b\u56e0\u6b64\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4e09\u7ef4\u964d\u7ef4\u6846\u67b6\u4e0e\u7406\u8bba\u4fdd\u969c\u3002", "method": "\u5c06\u4e09\u7ef4 RA \u95ee\u9898\u5206\u89e3\u4e3a\u6c34\u5e73\u5b50\u535a\u5f08\u548c\u5782\u76f4\u5b50\u535a\u5f08\uff0c\u5206\u522b\u5e94\u7528 HJ reachability \u8fdb\u884c\u6c42\u89e3\uff1b\u8003\u8651\u9632\u5b88\u8005\u7684\u4e8c\u9636\u52a8\u529b\u5b66\uff08\u52a0\u901f\u5ea6\uff09\u4ee5\u66f4\u771f\u5b9e\u5730\u523b\u753b defender\uff1b\u7528\u57fa\u4e8e HJ \u7684\u8ddf\u8e2a\u63a7\u5236\u5728\u5b50\u535a\u5f08\u4e2d\u91cd\u5efa\u539f\u59cb RA \u95ee\u9898\u7684\u89e3\uff0c\u786e\u4fdd\u6355\u83b7\u5e76\u5728\u6b64\u540e\u80fd\u591f\u8ddf\u8e2a\u653b\u51fb\u8005\uff1b\u7ed9\u51fa\u80fd\u4fdd\u6301\u6355\u83b7\u4fdd\u8bc1\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u4eff\u771f\u548c Gazebo \u7269\u7406\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5206\u89e3\u4fdd\u7559\u4e86\u539f\u59cb\u95ee\u9898\u7684\u6700\u4f18\u6027\u4e0e\u4fdd\u8bc1\u6027\uff1b\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u7b56\u7565\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u4fdd\u6301\u6709\u6548\u6027\u5e76\u8fd1\u4f3c\u6700\u4f18\uff1bGazebo \u9a8c\u8bc1\u9996\u6b21\u5728\u4e09\u7ef4\u7a7a\u95f4\u5b9e\u73b0\u5bf9\u56db\u65cb\u7ffc\u7684\u6210\u529f\u6355\u83b7\uff08\u5728\u76f8\u5173\u9886\u57df\u58f0\u79f0\u9996\u6b21\u5b9e\u73b0\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u4e09\u7ef4 RA \u95ee\u9898\u6c42\u89e3\u8def\u5f84\uff0c\u901a\u8fc7\u964d\u7ef4\u4e0e HJ reachability \u7ed3\u5408\u7684\u6846\u67b6\u5b9e\u73b0\u5bf9\u975e\u5e73\u6ed1\u52a8\u529b\u5b66\u7684\u9c81\u68d2\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u6761\u4ef6\u4e0e\u4eff\u771f\u9a8c\u8bc1\u4e86\u6355\u83b7\u4e0e\u8ddf\u8e2a\u7684\u53ef\u8bc1\u660e\u6027\u4e0e\u53ef\u5b9e\u73b0\u6027\u3002\u672a\u6765\u5de5\u4f5c\u53ef\u6269\u5c55\u81f3\u591a\u667a\u80fd\u4f53\u3001\u9c81\u68d2\u6027\u5206\u6790\u3001\u4ee5\u53ca\u5b9e\u65f6\u5b9e\u73b0\u7b49\u65b9\u5411\u3002"}}
{"id": "2512.22731", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.22731", "abs": "https://arxiv.org/abs/2512.22731", "authors": ["Roberto C. G. Porto", "Rodrigo C. de Lamare"], "title": "Iterative Channel Estimation, Detection and Decoding for Multi-Antenna Systems with RIS", "comment": "15 pages, 12 figures", "summary": "This work proposes an iterative channel estimation, detection and decoding (ICEDD) scheme for the uplink of multi-user multi-antenna systems assisted by multiple reconfigurable intelligent surfaces (RIS)}. A novel iterative code-aided channel estimation (ICCE) technique is developed that uses low-density parity-check (LDPC) codes and iterative processing to enhance estimation accuracy while reducing pilot overhead. The core idea is to exploit encoded pilots (EP), enabling the use of both pilot and parity bits to iteratively refine channel estimates. To further improve performance, an iterative channel tracking (ICT) method is proposed that takes advantage of the temporal correlation of the channel. An analytical evaluation of the proposed estimator is provided in terms of normalized mean-squared error (NMSE), along with a study of its computational complexity and the impact of the code rate. Numerical results validate the performance of the proposed scheme in a sub-6 GHz multi-RIS scenario with non-sparse propagation, under both LOS and NLOS conditions, and different RIS architectures.", "AI": {"tldr": "\u63d0\u51fa ICEDD \u6846\u67b6\u53ca ICCE/ICT \u7684 RIS \u8f85\u52a9\u591a\u7528\u6237 MIMO \u4e0a\u884c\u901a\u9053\u4f30\u8ba1\u4e0e\u89e3\u7801\uff0c\u964d\u4f4e Pilot \u5f00\u9500\u5e76\u63d0\u5347 NMSE\uff1b\u7ed9\u51fa\u590d\u6742\u5ea6\u4e0e\u7801\u7387\u5f71\u54cd\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "\u5728 RIS \u8f85\u52a9\u7684\u591a\u7528\u6237\u591a\u5929\u7ebf\u7cfb\u7edf\u4e2d\uff0c\u901a\u9053\u4f30\u8ba1\u96be\u5ea6\u9ad8\u3001Pilot \u5f00\u9500\u5927\uff0c\u9700\u7ed3\u5408\u7f16\u7801\u4e0e\u65f6\u57df\u8ddf\u8e2a\u4ee5\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u63d0\u51fa ICEDD \u6846\u67b6\uff1b\u5f00\u53d1 ICCE \u4f7f\u7528 LDPC \u7f16\u7801\u7684\u7f16\u7801\u578b pilot \u8fdb\u884c\u8fed\u4ee3\u4f30\u8ba1\uff1bEP \u4f7f\u5f97 pilot \u4e0e\u7ea0\u9519\u4fe1\u606f\u5171\u540c\u53c2\u4e0e\u4f30\u8ba1\uff1b\u63d0\u51fa ICT \u5229\u7528\u901a\u9053\u7684\u65f6\u95f4\u76f8\u5173\u6027\u8fdb\u884c\u8ddf\u8e2a\uff1b\u7ed9\u51fa\u590d\u6742\u5ea6\u548c\u7801\u7387\u5f71\u54cd\u7684\u89e3\u6790\u4e0e\u8bbe\u8ba1\u51c6\u5219\u3002", "result": "\u7ed9\u51fa\u57fa\u4e8e NMSE \u7684\u89e3\u6790\u8bc4\u4f30\u4e0e\u590d\u6742\u5ea6\u5206\u6790\uff1b\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u5728\u5b50-6 GHz \u7684\u591a RIS\u3001\u975e\u7a00\u758f\u4f20\u64ad\u4e0b\uff0c\u5305\u542b LOS \u4e0e NLOS \u7684\u573a\u666f\uff0c\u4ee5\u53ca\u4e0d\u540c RIS \u67b6\u6784\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ICEDD \u6846\u67b6\u901a\u8fc7 ICCE \u4e0e ICT \u5b9e\u73b0\u66f4\u4f4e pilot \u5f00\u9500\u548c\u66f4\u9ad8\u4f30\u8ba1/\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u5907\u5728 RIS \u8f85\u52a9\u591a\u7528\u6237\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\u4e0e\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2512.22307", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22307", "abs": "https://arxiv.org/abs/2512.22307", "authors": ["You Li", "Guannan Zhao", "Yuhao Ju", "Yunqi He", "Jie Gu", "Hai Zhou"], "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators", "comment": "Accepted by AAAI'26 as a conference paper and selected for oral presentation", "summary": "We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLA\uff0c\u4e00\u79cd\u9762\u5411\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540cIP\u4fdd\u62a4\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u795e\u7ecf\u5143\u4e2d\u5d4c\u5165\u5bc6\u94a5\u4f4d\u5e76\u65bd\u52a0\u4e0d\u53d8\u53d8\u6362\uff0c\u4ee5\u53ca\u5728AI\u52a0\u901f\u5668\u4e2d\u5d4c\u5165\u8f7b\u91cf\u9501\u6a21\u5757\uff0c\u6765\u9632\u62a4\u6a21\u578b\u76d7\u7528\u3001\u7be1\u6539\u4e0e\u4fe1\u606f\u6cc4\u9732\uff0c\u4e14\u57287168\u4f4d\u5bc6\u94a5\u4e0b\u5f00\u9500\u5c0f\u4e8e0.1%\u3002", "motivation": "\u5e94\u5bf9AI\u6a21\u578b\u7684\u4f9b\u5e94\u94fe\u5a01\u80c1\uff08\u5982\u6a21\u578b\u7a83\u53d6\u3001\u6a21\u578b\u635f\u574f\u3001\u4fe1\u606f\u6cc4\u9732\u7b49\uff09\u65e5\u76ca\u589e\u591a\u7684\u6311\u6218\uff0c\u9700\u8981\u8f6f\u786c\u4ef6\u534f\u540c\u7684IP\u4fdd\u62a4\u673a\u5236\u4ee5\u63d0\u5347\u5bf9\u6297\u80fd\u529b\u548c\u53ef\u7528\u6027\u3002", "method": "\u8f6f\u4ef6\u5c42\uff1a\u5c06\u5bc6\u94a5\u4f4d\u5d4c\u5165\u795e\u7ecf\u5143\u4ee5\u89e6\u53d1\u5f02\u5e38\u8f93\u51fa\uff0c\u4f7f\u7528\u4e0d\u53d8\u6027\u53d8\u6362\u9690\u85cf\u5bc6\u94a5\uff1b\u786c\u4ef6\u5c42\uff1a\u5728AI\u52a0\u901f\u5668\u4e2d\u96c6\u6210\u8f7b\u91cf\u9501\u5b9a\u6a21\u5757\uff0c\u62e5\u6709\u9884\u5b58\u7684\u79d8\u5bc6\u5bc6\u94a5\uff0c\u4f5c\u4e3a\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684\u8bb8\u53ef\u8bc1\uff1b\u517c\u5bb9\u591a\u79cd\u6570\u636e\u6d41\u6a21\u5f0f\u548c\u5de5\u5177\u94fe\u3002", "result": "\u8bc4\u4f30\u8868\u660eLLA\u80fd\u62b5\u5fa1\u5927\u8303\u56f4\u7684\u57fa\u4e8eoracle\u7684\u5bc6\u94a5\u4f18\u5316\u653b\u51fb\uff0c\u5e76\u4e14\u5f00\u9500\u6781\u4f4e\uff0c\u57287168\u4f4d\u5bc6\u94a5\u4e0b\u7684\u8ba1\u7b97\u5f00\u9500\u5c0f\u4e8e0.1%\u3002", "conclusion": "LLA\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u63d0\u4f9b\u5bf9\u751f\u6210\u5f0fAI\u6a21\u578b\u4f9b\u5e94\u94fe\u653b\u51fb\u7684\u6709\u6548\u9632\u62a4\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u6027\u80fd\u6210\u672c\u548c\u8f83\u5f3a\u7684\u53ef\u90e8\u7f72\u6027\uff0c\u9002\u5408\u4f5c\u4e3aIP\u4fdd\u62a4\u65b9\u6848\u7684\u5b9e\u73b0\u65b9\u5411\u3002"}}
{"id": "2512.22172", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22172", "abs": "https://arxiv.org/abs/2512.22172", "authors": ["Md Shahriar Sajid", "Abhijit Kumar Ghosh", "Fariha Nusrat"], "title": "PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection", "comment": "15 pages, 4 figures, International Conference on Intelligent Data Analysis and Applications (IDAA 2025)", "summary": "Electroencephalography (EEG) signals contain rich temporal-spectral structure but are difficult to model due to noise, subject variability, and multi-scale dynamics. Lightweight deep learning models have shown promise, yet many either rely solely on local convolutions or require heavy recurrent modules. This paper presents PaperNet, a compact hybrid architecture that combines temporal convolutions, a channel-wise residual attention module, and a lightweight bidirectional recurrent block which is used for short-window classification. Using the publicly available BEED: Bangalore EEG Epilepsy Dataset, we evaluate PaperNet under a clearly defined subject-independent training protocol and compare it against established and widely used lightweight baselines. The model achieves a macro-F1 of 0.96 on the held-out test set with approximately 0.6M parameters, while maintaining balanced performance across all four classes. An ablation study demonstrates the contribution of temporal convolutions, residual attention, and recurrent aggregation. Channel-wise attention weights further offer insights into electrode relevance. Computational profiling shows that PaperNet remains efficient enough for practical deployment on resource-constrained systems through out the whole process. These results indicate that carefully combining temporal filtering, channel reweighting, and recurrent context modeling can yield strong EEG classification performance without excessive computational cost.", "AI": {"tldr": "PaperNet: a compact hybrid EEG classifier using temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block; achieves macro-F1 0.96 on BEED with ~0.6M parameters, subject-independent, efficient for deployment; ablation confirms contributions.", "motivation": "EEG signals possess rich temporal-spectral structure but are noisy and variable across subjects; need lightweight, effective models that handle multi-scale dynamics without heavy recurrent modules.", "method": "\u63d0\u51fa PaperNet\uff0c\u7ed3\u5408(1) Temporal convolutions capturing multi-scale temporal patterns, (2) channel-wise residual attention\u5bf9\u7535\u6781\u6743\u91cd\u8fdb\u884c reweighting\u5e76\u589e\u5f3a\u7279\u5f81\uff0c(3) \u8f7b\u91cf\u7ea7\u53cc\u5411\u5faa\u73af\u5757\u7528\u4e8e\u77ed\u7a97\u53e3\u5206\u7c7b\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\uff1b\u5728 BEED \u6d77\u5916\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e3b\u4f53\u65e0\u5173\u8bad\u7ec3\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u3002", "result": "\u5728 held-out \u6d4b\u8bd5\u96c6 macro-F1 \u8fbe0.96\uff0c\u53c2\u6570\u7ea6 0.6M\uff1b\u56db\u4e2a\u7c7b\u522b\u8868\u73b0\u5747\u8861\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u65f6\u5e8f\u5377\u79ef\u3001\u6b8b\u5dee\u6ce8\u610f\u529b\u548c\u5faa\u73af\u805a\u5408\u7684\u8d21\u732e\uff1b\u901a\u9053\u6ce8\u610f\u529b\u6743\u91cd\u63d0\u4f9b\u7535\u6781\u76f8\u5173\u6027\u7ebf\u7d22\uff1b\u8ba1\u7b97\u5206\u6790\u8868\u660e\u8be5\u6a21\u578b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "conclusion": "\u901a\u8fc7\u5de7\u5999\u5730\u7ed3\u5408\u65f6\u95f4\u8fc7\u6ee4\u3001\u901a\u9053\u91cd\u52a0\u6743\u548c\u5faa\u73af\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5f3a\u52b2\u7684EEG\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2512.22859", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22859", "abs": "https://arxiv.org/abs/2512.22859", "authors": ["Tegenu Argaw Woldegiyorgis", "Hong Xian Li", "Fekadu Chekol Admassu", "Merkebu Gezahegne", "Abdurohman Kebede", "Tadese Abera", "Haris Ishaq", "Eninges Asmare"], "title": "Assessment of a Hybrid Energy System for Reliable and Sustainable Power Supply to Boru Meda Hospital in Ethiopia", "comment": null, "summary": "This study aims to evaluate the techno-economic feasibility of hybrid energy systems (HES) including Grid for providing reliable and sustainable power to Boru Meda Hospital, Ethiopia. HOMER pro 3.11.2 was used to design and evaluate a novel, integrated optimization and comparative assessment of diverse HRES, specif ically adjusted to the energy consumptions and available resources of the Hospital. The scenario evaluation showed that interconnecting photovoltaic (PV), biomass generator (BG), wind power (WP), diesel generator (DG), battery, and converter can effectively provide the Hospital's daily energy consumption of 11,214.66 kWh while conforming reliability and reducing emissions. The PV/BG/batt/conv configuration emerged as the most cost-effective and sustainable alternative, attaining the lowest LCOE of \\$0.339/kWh, an NPC of \\$25.7 million, and a 100% renewable energy fraction with simple pay back of 7.26 yr. As a result, the operational cost associated with the consumption of 500.00 L of diesel per month can be entirely avoided. The DG-integrated hybrids exhibit advanced techno-economic capability with significant worth, strong ROI (20\\%) and IRR (18\\%), endorsed by fast capital recovery (7.21-8.71 years). Overall, the hybrid system offers an optimal balance of cost, reliability, and sustainability, making it a promising and scalable solution for electrification of energy scare institution and areas in Ethiopia, thereby contributing to national sustainable energy development goals.", "AI": {"tldr": "\u8bc4\u4f30\u970d\u5c14\u65af\u80fd\u6e90\u7cfb\u7edf\u5728\u57c3\u585e\u4fc4\u6bd4\u4e9aBoru Meda\u533b\u9662\u7684\u201c\u6df7\u5408\u80fd\u6e90\u7cfb\u7edf\u201d\u53ef\u884c\u6027\uff0c\u4f7f\u7528HOMER Pro 3.11.2\u8fdb\u884c\u591a\u65b9\u6848\u5bf9\u6bd4\uff0cPV/BG/\u84c4\u7535/\u53d8\u6d41\u914d\u7f6e\u6210\u672c\u6700\u4f4e\u4e14\u5b9e\u73b0100%\u53ef\u518d\u751f\uff0c\u7b80\u5355\u56de\u6536\u671f\u7ea67.26\u5e74\uff1bDG\u6df7\u5408\u65b9\u6848\u4e5f\u5177\u5907\u8f83\u9ad8\u6295\u8d44\u56de\u62a5\u3002", "motivation": "\u5728\u80fd\u6e90\u532e\u4e4f\u7684\u5730\u533a\u901a\u8fc7\u4f18\u5316\u6df7\u5408\u80fd\u6e90\u7cfb\u7edf\u63d0\u5347\u533b\u9662\u4f9b\u7535\u7684\u53ef\u9760\u6027\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u4e0e\u6392\u653e\uff0c\u4fc3\u8fdb\u53ef\u6301\u7eed\u80fd\u6e90\u53d1\u5c55\u76ee\u6807\u7684\u5b9e\u73b0\u3002", "method": "\u5229\u7528HOMER Pro 3.11.2\u5bf9\u591a\u79cdHRES\u65b9\u6848\u8fdb\u884c\u96c6\u6210\u4f18\u5316\u4e0e\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u8003\u8651\u73b0\u6709\u8d44\u6e90\u4e0e\u533b\u9662\u80fd\u6e90\u9700\u6c42\uff0c\u5305\u542bPV\u3001 biomass generator\u3001\u98ce\u529b\u3001\u67f4\u6cb9\u53d1\u7535\u673a\u3001\u84c4\u7535\u6c60\u4e0e\u53d8\u6d41\u5668\u7b49\u5b50\u7cfb\u7edf\u7684\u8026\u5408\u3002", "result": "PV/BG/\u84c4\u7535/\u53d8\u6d41\u914d\u7f6e\u5b9e\u73b0\u65e5\u7528\u7535\u91cf11,214.66 kWh\uff0c\u5177\u6700\u4f4eLCOE0.339\u7f8e\u5143/kWh\u3001NPV\u7ea62570\u4e07\u7f8e\u5143\u3001100%\u53ef\u518d\u751f\u80fd\u6e90\u6bd4\u4f8b\u3001\u7b80\u5355\u56de\u62a5\u671f7.26\u5e74\uff1b\u53ef\u5b8c\u5168\u907f\u514d500\u5347/\u6708\u67f4\u6cb9\u4f7f\u7528\u3002DG\u96c6\u6210\u6df7\u5408\u5177\u8f83\u9ad8\u7ecf\u6d4e\u4ef7\u503c\uff0cROI\u7ea620%\u3001IRR18%\u3001\u56de\u6536\u671f7.21\u20138.71\u5e74\u3002\u603b\u4f53\u4e0a\uff0c\u6df7\u5408\u7cfb\u7edf\u5728\u6210\u672c\u3001\u53ef\u9760\u6027\u4e0e\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u5e73\u8861\uff0c\u662f\u57c3\u585e\u4fc4\u6bd4\u4e9a\u80fd\u6e90\u77ed\u7f3a\u5730\u533a\u533b\u9662\u53ca\u7c7b\u4f3c\u533a\u57df\u7684\u53ef\u6269\u5c55\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5728\u8d44\u6e90\u7ea6\u675f\u7684\u573a\u666f\u4e0b\uff0c\u57fa\u4e8ePV\u3001BG\u3001\u84c4\u7535\u548c\u53d8\u6d41\u7684\u6df7\u5408\u80fd\u6e90\u7cfb\u7edf\u5177\u6709\u663e\u8457\u7684\u7ecf\u6d4e\u4e0e\u73af\u5883\u4f18\u52bf\uff0c\u5177\u5907\u63a8\u5e7f\u5230\u7c7b\u4f3c\u573a\u666f\u7684\u6f5c\u529b\u4e0e\u53ef\u6301\u7eed\u53d1\u5c55\u610f\u4e49\u3002"}}
{"id": "2512.22828", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.22828", "abs": "https://arxiv.org/abs/2512.22828", "authors": ["Juha Park", "Ian P. Roberts", "Wonjae Shin"], "title": "Beyond Beam Sweeping: One-Shot Satellite Acquisition with Doppler-Aware Rainbow Beamforming", "comment": "16 pages, 5 figures", "summary": "High-gain beamforming (BF) is essential for low Earth orbit (LEO) satellite communications to overcome severe path loss, but this requires acquiring precise satellite positions. Conventional satellite acquisition typically relies on time-domain beam sweeping, which incurs substantial overhead and latency. In this correspondence, we propose an efficient one-shot satellite acquisition framework that capitalizes on two phenomena traditionally regarded as impairments: i) Doppler effects and ii) beam-squint effects. Specifically, we derive a closed-form \\emph{rainbow beamformer} that leverages beam-squint effects to align frequency-dependent beam directions with satellite positions inferred from their Doppler shifts. This approach enables reception from multiple satellites at once without requiring beam sweeping. To extract satellite position information, we develop three Doppler-aware angle estimation algorithms based on received signals. Simulation results demonstrate that the proposed method significantly outperforms conventional beam sweeping approaches in both acquisition accuracy and required time slots. These gains stem from the ability of the proposed rainbow BF to exploit the \\emph{angle-dependent nature of Doppler shifts}, enabling full angular-domain coverage with a single pilot transmission and reception.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f69\u8679\u6ce2\u675f\u6210\u5f62\u7684\u5355-shot\u536b\u661f\u83b7\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6ce2\u675f\u7574\u548c\u591a\u666e\u52d2\u6548\u5e94\u5c06\u9891\u7387\u76f8\u5173\u6ce2\u675f\u65b9\u5411\u4e0e\u536b\u661f\u4f4d\u7f6e\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5bf9\u591a\u9897LEO\u536b\u661f\u7684\u540c\u65f6\u63a5\u6536\uff0c\u907f\u514d\u4f20\u7edf\u65f6\u57df\u626b\u9891\uff1b\u7ed9\u51fa\u4e09\u79cd\u591a\u666e\u52d2\u611f\u77e5\u7684\u89d2\u5ea6\u4f30\u8ba1\u7b97\u6cd5\uff1b\u4eff\u771f\u663e\u793a\u5728\u83b7\u53d6\u7cbe\u5ea6\u548c\u6240\u9700\u65f6\u9699\u4e0a\u663e\u8457\u4f18\u4e8e\u626b\u9891\u65b9\u6cd5\u3002", "motivation": "LEO\u536b\u661f\u901a\u4fe1\u7684\u9ad8\u8def\u5f84\u635f\u8017\u8981\u6c42\u9ad8\u589e\u76ca\u6ce2\u675f\uff0c\u540c\u65f6\u5bf9\u536b\u661f\u59ff\u6001\u4e0e\u4f4d\u7f6e\u4fe1\u606f\u7684\u51c6\u786e\u83b7\u53d6\u9020\u6210\u989d\u5916\u5f00\u9500\u3002\u4f20\u7edf\u7684\u65f6\u95f4\u57df\u6ce2\u675f\u626b\u9891\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u65f6\u5ef6\u5927\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u536b\u661f\u83b7\u53d6\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u95ed\u5f0f\u7684\u5f69\u8679\u6ce2\u675f\u6210\u5f62\uff0c\u5c06\u6ce2\u675f\u7574\u6548\u5e94\u7528\u4e8e\u5c06\u4e0d\u540c\u9891\u7387\u4e0a\u7684\u6ce2\u675f\u6307\u5411\u4e0e\u901a\u8fc7\u591a\u666e\u52d2\u5f97\u5230\u7684\u536b\u661f\u4f4d\u7f6e\u5bf9\u5e94\uff1b\u5f00\u53d1\u4e09\u79cd\u57fa\u4e8e\u63a5\u6536\u4fe1\u53f7\u7684\u591a\u666e\u52d2\u611f\u77e5\u89d2\u5ea6\u4f30\u8ba1\u7b97\u6cd5\uff1b\u5b9e\u73b0\u5355\u6b21\u5bfc\u9891\u5373\u53ef\u8986\u76d6\u5168\u89d2\u57df\u5e76\u5b9e\u73b0\u5bf9\u591a\u9897\u536b\u661f\u7684\u63a5\u6536\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u626b\u9891\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u83b7\u53d6\u7cbe\u5ea6\u4e0e\u6240\u9700\u65f6\u9699\u65b9\u9762\u5177\u6709\u663e\u8457\u63d0\u5347\uff0c\u5f97\u76ca\u4e8e\u5229\u7528\u591a\u666e\u52d2-\u89d2\u5ea6\u7684\u8026\u5408\u7279\u6027\u5b9e\u73b0\u5168\u89d2\u57df\u8986\u76d6\u7684\u5f69\u8679\u6ce2\u675f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u666e\u52d2\u6548\u5e94\u4e0e\u6ce2\u675f\u7574\u6548\u5e94\u8f6c\u5316\u4e3a\u6709\u5229\u56e0\u7d20\uff0c\u5f69\u8679\u6ce2\u675f\u6210\u5f62\u5b9e\u73b0\u4e86\u4e00\u6b21\u6027\u536b\u661f\u83b7\u53d6\uff0c\u652f\u6301\u591a\u536b\u661f\u540c\u65f6\u5bf9\u63a5\u5e76\u663e\u8457\u964d\u4f4e\u83b7\u53d6\u5f00\u9500\u3002"}}
{"id": "2512.22501", "categories": ["cs.CR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22501", "abs": "https://arxiv.org/abs/2512.22501", "authors": ["Edwin Vargas"], "title": "NOWA: Null-space Optical Watermark for Invisible Capture Fingerprinting and Tamper Localization", "comment": null, "summary": "Ensuring the authenticity and ownership of digital images is increasingly challenging as modern editing tools enable highly realistic forgeries. Existing image protection systems mainly rely on digital watermarking, which is susceptible to sophisticated digital attacks. To address this limitation, we propose a hybrid optical-digital framework that incorporates physical authentication cues during image formation and preserves them through a learned reconstruction process. At the optical level, a phase mask in the camera aperture produces a Null-space Optical Watermark (NOWA) that lies in the Null Space of the imaging operator and therefore remains invisible in the captured image. Then, a Null-Space Network (NSN) performs measurement-consistent reconstruction that delivers high-quality protected images while preserving the NOWA signature. The proposed design enables tamper localization by projecting the image onto the camera's null space and detecting pixel-level inconsistencies. Our design preserves perceptual quality, resists common degradations such as compression, and establishes a structural security asymmetry: without access to the optical or NSN parameters, adversaries cannot forge the NOWA signature. Experiments with simulations and a prototype camera demonstrate competitive performance in terms of image quality preservation, and tamper localization accuracy compared to state-of-the-art digital watermarking and learning-based authentication methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22393", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22393", "abs": "https://arxiv.org/abs/2512.22393", "authors": ["Alexander Venus", "Erik Leitinger", "Klaus Witrisal"], "title": "Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems", "comment": "8 pages, 6 figures", "summary": "Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment. In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness. Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible. In this work, we relax these assumptions and address simultaneous source separation, synchronization, and mapping. A relevant example arises in modern 5G systems, where BSs employ muting patterns to mitigate interference, yet localization performance still degrades. We propose a novel BS-dependent data association and synchronization bias model, integrated into a joint Bayesian framework and inferred via the sum-product algorithm on a factor graph. The impact of joint synchronization and source separation is analyzed under various system configurations. Compared with state-of-the-art cooperative MP-SLAM assuming orthogonal and synchronized BSs, our statistical analysis shows no significant performance degradation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22186", "abs": "https://arxiv.org/abs/2512.22186", "authors": ["Vishnu Mohan"], "title": "Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks", "comment": "27 pages, 10 figures", "summary": "Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.", "AI": {"tldr": "A reinforcement learning framework for tennis strategy using a custom simulator and a dueling double DQN with curriculum learning, achieving high win rates and exposing biases due to reward optimization.", "motivation": "Tennis strategy optimization presents a challenging, long-horizon sequential decision problem with hierarchical scoring, fatigue dynamics, and opponent adaptation; robust, stable learning methods are needed beyond standard RL baselines.", "method": "A custom tennis environment modeling points, games, sets, rally-level decisions across 10 actions, fatigue, and continuous opponent skill; a dueling DDQN trained with curriculum learning that escalates opponent difficulty from 0.40 to 0.50; ablation studies and comparisons to a standard DQN baseline.", "result": "The agent attains 98\u2013100% win rates against balanced opponents and remains strong against tougher opponents; serve efficiency 63.0\u201367.5%, return 52.8\u201357.1%; both dueling architecture and curriculum learning are necessary for stable learning, while standard DQN fails; the policy shows a defensive bias prioritizing error avoidance and rallies over aggressive point construction.", "conclusion": "Win-rate optimization in simplified sports RL can be misleading; reward design critically shapes realistic behavior, highlighting the need for balanced objectives and richer evaluation beyond pure win rate."}}
{"id": "2512.22911", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22911", "abs": "https://arxiv.org/abs/2512.22911", "authors": ["Samin Riasat", "Hessam Mahdavifar"], "title": "Covering in Hamming and Grassmann Spaces: New Bounds and Reed--Solomon-Based Constructions", "comment": "14 pages, 6 figures", "summary": "We study covering problems in Hamming and Grassmann spaces through a unified coding-theoretic and information-theoretic framework. Viewing covering as a form of quantization in general metric spaces, we introduce the notion of the average covering radius as a natural measure of average distortion, complementing the classical worst-case covering radius. By leveraging tools from one-shot rate-distortion theory, we derive explicit non-asymptotic random-coding bounds on the average covering radius in both spaces, which serve as fundamental performance benchmarks.\n  On the construction side, we develop efficient puncturing-based covering algorithms for generalized Reed--Solomon (GRS) codes in the Hamming space and extend them to a new family of subspace codes, termed character-Reed--Solomon (CRS) codes, for Grassmannian quantization under the chordal distance. Our results reveal that, despite poor worst-case covering guarantees, these structured codes exhibit strong average covering performance. In particular, numerical results in the Hamming space demonstrate that RS-based constructions often outperform random codebooks in terms of average covering radius. In the one-dimensional Grassmann space, we numerically show that CRS codes over prime fields asymptotically achieve average covering radii within a constant factor of the random-coding bound in the high-rate regime. Together, these results provide new insights into the role of algebraic structure in covering problems and high-dimensional quantization.", "AI": {"tldr": "\u7edf\u4e00\u6846\u67b6\u4e0b\u7684\u8986\u76d6\u95ee\u9898\uff1a\u5728Hamming\u4e0eGrassmann\u7a7a\u95f4\u4e2d\u5c06\u8986\u76d6\u89c6\u4e3a\u91cf\u5316\uff0c\u63d0\u51fa\u5e73\u5747\u8986\u76d6\u534a\u5f84\u5e76\u7ed9\u51fa\u4e00\u624b\u975e\u6e10\u8fd1\u968f\u673a\u7f16\u7801\u754c\u9650\uff0c\u8f85\u4ee5\u57fa\u4e8e\u6253\u5b54\u7684GRS/CRS\u7f16\u7801\u6784\u9020\uff0c\u5c55\u793a\u4ee3\u6570\u7ed3\u6784\u5728\u5e73\u5747\u8986\u76d6\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u65e8\u5728\u7406\u89e3\u9ad8\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u7684\u8986\u76d6/\u91cf\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u628a\u8986\u76d6\u8f6c\u5316\u4e3a\u91cf\u5316\uff0c\u5e76\u7ed3\u5408\u4e00\u624b\u901f\u7387\u5931\u771f\u7406\u8bba\uff0c\u5bfb\u627e\u5728\u975e\u6781\u9650\u3001\u975e\u968f\u673a\u60c5\u5f62\u4e0b\u7684\u57fa\u51c6\u4e0e\u6784\u9020\u65b9\u6cd5\uff0c\u63d0\u5347\u5e73\u5747\u8986\u76d6\u6027\u80fd\u7684\u7406\u89e3\u4e0e\u5b9e\u73b0\u80fd\u529b\u3002", "method": "\u5c06\u8986\u76d6\u95ee\u9898\u89c6\u4f5c\u5e7f\u4e49\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u91cf\u5316\uff0c\u5b9a\u4e49\u5e73\u5747\u8986\u76d6\u534a\u5f84\uff1b\u5229\u7528\u4e00\u6b21\u6027\u901f\u7387\u5931\u771f\u7406\u8bba\u63a8\u5bfcHamming\u4e0eGrassmann\u7a7a\u95f4\u7684\u975e\u6e10\u8fd1\u968f\u673a\u7f16\u7801\u754c\u9650\uff1b\u63d0\u51fa\u6253\u5b54\u5f0f\u8986\u76d6\u7b97\u6cd5\uff0c\u9488\u5bf9GRS\u7801\u5728Hamming\u7a7a\u95f4\uff0c\u4ee5\u53ca\u6269\u5c55\u4e3aGrassmann\u4e0b\u7684CRS\u7801\uff1b\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u6bd4\u8f83\u7ed3\u6784\u7801\u4e0e\u968f\u673a\u7801\u5728\u5e73\u5747\u8986\u76d6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed9\u51fa\u4e24\u79cd\u7a7a\u95f4\u7684\u663e\u5f0f\u975e\u6e10\u8fd1\u968f\u673a\u7f16\u7801\u754c\u9650\uff1bGRS\u4e0eCRS\u7b49\u7ed3\u6784\u7801\u5728\u5e73\u5747\u8986\u76d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e38\u4f18\u4e8e\u968f\u673a\u7801\u672c\uff1b\u5728Hamming\u7a7a\u95f4\uff0cRS\u57fa\u6784\u9020\u901a\u5e38\u8d85\u8d8a\u968f\u673a\u7801\u7684\u5e73\u5747\u8986\u76d6\u534a\u5f84\uff1b\u57281\u7ef4Grassmann\u7a7a\u95f4\uff0cCRS\u7801\u5bf9\u7d20\u57df\u7684\u6784\u9020\u5728\u9ad8\u901f\u7387\u4e0b\u6e10\u8fd1\u63a5\u8fd1\u968f\u673a\u7f16\u7801\u754c\u9650\u3002", "conclusion": "\u4ee3\u6570\u7ed3\u6784\u5728\u8986\u76d6\u95ee\u9898\u7684\u5e73\u5747\u6027\u80fd\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u9ad8\u7ef4\u4e0e\u9ad8\u7387\u573a\u666f\u4e0b\uff1b\u5c3d\u7ba1\u6700\u574f\u60c5\u5f62\u8986\u76d6\u53ef\u80fd\u8f83\u5dee\uff0c\u5e73\u5747\u8986\u76d6\u8868\u73b0\u53ef\u901a\u8fc7\u7ed3\u6784\u5316\u7f16\u7801\u663e\u8457\u63d0\u5347\uff0c\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u548c\u57fa\u51c6\u3002"}}
{"id": "2512.22526", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22526", "abs": "https://arxiv.org/abs/2512.22526", "authors": ["Kichang Lee", "Sungmin Lee", "Jaeho Jin", "JeongGil Ko"], "title": "Verifiable Dropout: Turning Randomness into a Verifiable Claim", "comment": "5 pages, 6 figures, 1 table", "summary": "Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684Dropout\u673a\u5236\uff0c\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u5bf9\u968f\u673a\u6027\u8fdb\u884c\u8bc1\u660e\uff0c\u4ee5\u5728\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5bf9 dropout \u6267\u884c\u6b63\u786e\u6027\u7684\u4e8b\u540e\u5ba1\u8ba1\u3002", "motivation": "\u73b0\u4ee3\u4e91\u7aef AI \u8bad\u7ec3\u4e2d\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u65e5\u5fd7\u548c\u9065\u6d4b\u7528\u4e8e\u53ef\u8ffd\u6eaf\u6027\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u968f\u673a\u6027\uff08\u5982 dropout\uff09\uff0c\u73b0\u6709\u5ba1\u8ba1\u9762\u4e34\u201c\u975e\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6a21\u7cca\u6027\u201d\u4e0e\u53ef\u80fd\u7684\u6076\u610f\u504f\u5dee\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u4e0d\u66b4\u9732\u8bad\u7ec3\u6570\u636e\u7684\u524d\u63d0\u4e0b\u9a8c\u8bc1\u968f\u673a\u6027\u662f\u5426\u88ab\u516c\u6b63\u5730\u4f7f\u7528\u3002", "method": "\u5c06 dropout \u63a9\u7801\u7ed1\u5b9a\u5230\u4e00\u4e2a\u786e\u5b9a\u6027\u4e14\u53ef\u5bc6\u7801\u5b66\u9a8c\u8bc1\u7684\u79cd\u5b50\uff0c\u5e76\u901a\u8fc7\u96f6\u77e5\u8bc6\u8bc1\u660e\u8bc1\u660e dropout \u64cd\u4f5c\u7684\u6b63\u786e\u6267\u884c\u3002\u5c06\u968f\u673a\u6027\u89c6\u4e3a\u53ef\u9a8c\u8bc1\u7684\u4e3b\u5f20\u800c\u975e\u501f\u53e3\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u968f\u673a\u6b65\u9aa4\u7684\u540e\u9a8c\u5ba1\u8ba1\uff0c\u4e14\u4fdd\u62a4\u6a21\u578b\u548c\u6570\u636e\u7684\u673a\u5bc6\u6027\u3002", "result": "\u5b9e\u73b0\u5bf9 dropout \u7684\u5b8c\u6574\u6027\u5ba1\u8ba1\uff0c\u8bc1\u660e\u968f\u673a\u6027\u672a\u88ab\u504f\u7f6e\u3001\u672a\u88ab\u201c\u6311\u9009\u6027\u201d\u4f7f\u7528\uff1b\u5728\u4e0d\u6cc4\u9732\u6a21\u578b\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u63d0\u5347\u8bad\u7ec3\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u4e0e\u4fe1\u4efb\u5ea6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u63d0\u51fa\u7684 Verifiable Dropout \u4e3a\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u968f\u673a\u6027\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u5b8c\u6574\u6027\u4fdd\u969c\uff0c\u6709\u671b\u63d0\u5347\u4e91\u7aef\u8bad\u7ec3\u7684\u53ef\u5ba1\u8ba1\u6027\u548c\u5408\u89c4\u6027\uff0c\u5e76\u4e3a\u5c06\u6765\u5c06\u66f4\u591a\u968f\u673a\u64cd\u4f5c\u7eb3\u5165\u53ef\u8bc1\u660e\u7684\u5ba1\u8ba1\u6846\u67b6\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.22914", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22914", "abs": "https://arxiv.org/abs/2512.22914", "authors": ["Liping Guo", "Jimin Wang", "Yanlong Zhao", "Ji-Feng Zhang"], "title": "Distributed Fusion Estimation with Protecting Exogenous Inputs", "comment": null, "summary": "In the context of distributed fusion estimation, directly transmitting local estimates to the fusion center may cause a privacy leakage concerning exogenous inputs. Thus, it is crucial to protect exogenous inputs against full eavesdropping while achieving distributed fusion estimation. To address this issue, a noise injection strategy is provided by injecting mutually independent noises into the local estimates transmitted to the fusion center. To determine the covariance matrices of the injected noises, a constrained minimization problem is constructed by minimizing the sum of mean square errors of the local estimates while ensuring (\u03b5, \u03b4)-differential privacy. Suffering from the non-convexity of the minimization problem, an approach of relaxation is proposed, which efficiently solves the minimization problem without sacrificing differential privacy level. Then, a differentially private distributed fusion estimation algorithm based on the covariance intersection approach is developed. Further, by introducing a feedback mechanism, the fusion estimation accuracy is enhanced on the premise of the same (\u03b5, \u03b4)-differential privacy. Finally, an illustrative example is provided to demonstrate the effectiveness of the proposed algorithms, and the trade-off between differential privacy level and fusion estimation accuracy.", "AI": {"tldr": "\u5728\u5206\u5e03\u5f0f\u878d\u5408\u4f30\u8ba1\u4e2d\u901a\u8fc7\u5bf9\u5c40\u90e8\u4f30\u8ba1\u6ce8\u5165\u4e92\u72ec\u7acb\u566a\u58f0\uff0c\u5728\u4fdd\u8bc1 (\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff1b\u5e76\u901a\u8fc7\u677e\u5f1b\u4f18\u5316\u548c\u534f\u65b9\u5dee\u4ea4\u53c9\u7684\u878d\u5408\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5f15\u5165\u53cd\u9988\u673a\u5236\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5c55\u793a\u9690\u79c1-\u7cbe\u5ea6\u6743\u8861\u3002", "motivation": "\u76f4\u63a5\u4f20\u8f93\u5c40\u90e8\u4f30\u8ba1\u53ef\u80fd\u66b4\u9732\u5916\u751f\u8f93\u5165\u7684\u9690\u79c1\u4fe1\u606f\uff0c\u9700\u8981\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u5206\u5e03\u5f0f\u878d\u5408\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "method": "\u5bf9\u5c40\u90e8\u4f30\u8ba1\u6ce8\u5165\u4e92\u76f8\u72ec\u7acb\u7684\u566a\u58f0\uff1b\u6784\u9020\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4ee5\u5728\u6700\u5c0f\u5316\u5c40\u90e8\u4f30\u8ba1\u5747\u65b9\u8bef\u5dee\u4e4b\u548c\u7684\u540c\u65f6\u6ee1\u8db3 (\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\uff1b\u9488\u5bf9\u975e\u51f8\u6027\u91c7\u7528\u677e\u5f1b\u6c42\u89e3\u4ee5\u9ad8\u6548\u83b7\u5f97\u89e3\uff1b\u7ed9\u51fa\u57fa\u4e8e Covariance Intersection \u7684\u5dee\u5206\u9690\u79c1\u5206\u5e03\u5f0f\u878d\u5408\u4f30\u8ba1\u7b97\u6cd5\uff1b\u901a\u8fc7\u5f15\u5165\u53cd\u9988\u673a\u5236\u5728\u76f8\u540c\u9690\u79c1\u7ea6\u675f\u4e0b\u63d0\u5347\u878d\u5408\u4f30\u8ba1\u7cbe\u5ea6\uff1b\u901a\u8fc7\u793a\u4f8b\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u9690\u79c1\u7b49\u7ea7\u4e0e\u878d\u5408\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5728\u5904\u7406\u975e\u51f8\u6027\u95ee\u9898\u65b9\u9762\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\uff0c\u540c\u65f6\u786e\u4fdd\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0d\u88ab\u964d\u4f4e\uff1b\u63d0\u51fa\u7684\u5dee\u5206\u9690\u79c1\u5206\u5e03\u5f0f\u878d\u5408\u7b97\u6cd5\u5728 Covariance Intersection \u6846\u67b6\u4e0b\u6709\u6548\u8fd0\u884c\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u673a\u5236\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\uff1b\u5b9e\u4f8b\u63ed\u793a\u9690\u79c1\u6c34\u5e73(\u03b5,\u03b4)\u4e0e\u878d\u5408\u8bef\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u5e76\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u878d\u5408\u4f30\u8ba1\u65b9\u6848\u5728\u4fdd\u62a4\u5916\u751f\u8f93\u5165\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u878d\u5408\u4f30\u8ba1\uff0c\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u9690\u79c1-\u7cbe\u5ea6\u6743\u8861\u3001\u6709\u6548\u7684\u5b9e\u73b0\u8def\u5f84\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2512.22948", "categories": ["cs.IT", "math.RA"], "pdf": "https://arxiv.org/pdf/2512.22948", "abs": "https://arxiv.org/abs/2512.22948", "authors": ["Mahir Bilen Can", "Benjamin Horowitz"], "title": "Generalized Hyperderivative Reed-Solomon Codes", "comment": null, "summary": "This article introduces Generalized Hyperderivative Reed-Solomon codes (GHRS codes), which generalize NRT Reed-Solomon codes. Its main results are as follows: 1) every GHRS code is MDS, 2) the dual of a GHRS code is also an GHRS code, 3) determine subfamilies of GHRS codes whose members are low-density parity-check codes (LDPCs), and 4) determine a family of GHRS codes whose members are quasi-cyclic. We point out that there are GHRS codes having all of these properties.", "AI": {"tldr": "GHRS \u7801\u662f\u4e00\u79cd\u63a8\u5e7f\u7684 Reed-Solomon \u7801\uff0c\u6cdb\u5316\u4e86 NRT Reed-Solomon \u7801\uff1b\u5176\u6838\u5fc3\u6027\u8d28\u5305\u62ec\uff1a\u6240\u6709 GHRS \u7801\u90fd\u662f MDS\uff0cGHRS \u7684\u5bf9\u5076\u7801\u4ecd\u4e3a GHRS\uff0c\u5b58\u5728\u82e5\u5e72\u5b50\u65cf\u4f7f\u5f97\u6210\u5458\u662f LDPC\uff1b\u8fd8\u6709\u4e00\u65cf GHRS \u7801\u7684\u6210\u5458\u662f\u51c6\u5faa\u73af\u7801\uff1b\u5e76\u4e14\u5b58\u5728\u540c\u65f6\u5177\u6709\u4e0a\u8ff0\u6240\u6709\u6027\u8d28\u7684 GHRS \u7801\u3002", "motivation": "\u5728\u4fdd\u6301\u9ad8\u6548\u7ea0\u9519\u80fd\u529b\uff08MDS \u6027\u8d28\uff09\u7684\u540c\u65f6\uff0c\u6269\u5c55 Reed-Solomon \u7684\u9002\u7528\u8303\u56f4\uff0c\u5f15\u5165\u5e7f\u4e49\u8d85\u5fae\u5206\u7b97\u5b50\u4ee5\u6784\u9020\u66f4\u4e30\u5bcc\u7684\u7801\u65cf\uff0c\u4f7f\u5f97\u5bf9\u5076\u6027\u3001LDPC-\u6027\u548c\u51c6\u5faa\u73af\u6027\u7b49\u7ed3\u6784\u5c5e\u6027\u53ef\u4ee5\u5171\u5b58\u6216\u7ec4\u5408\u3002", "method": "\u5f15\u5165 Generalized Hyperderivative Reed-Solomon (GHRS) \u7684\u6784\u9020\u6846\u67b6\uff0c\u57fa\u4e8e\u5e7f\u4e49\u8d85\u5fae\u5206\u7b97\u5b50\u5b9a\u4e49\u7801\uff0c\u7ed9\u51fa\u5176\u53c2\u6570\u4e0e\u751f\u6210\u77e9\u9635\uff0c\u5e76\u8bc1\u660e\uff1a1) GHRS \u5168\u90e8\u5177\u6709 MDS \u6027\u8d28\uff0c2) GHRS \u7684\u5bf9\u5076\u7801\u4ecd\u7136\u662f GHRS\uff0c3) \u6307\u5b9a\u5b50\u65cf\u53ef\u5f97\u5230 LDPC-\u6027\u8d28\u7684 GHRS \u5b50\u65cf\uff0c4) \u7ed9\u51fa\u4e00\u65cf\u4f7f\u6210\u5458\u4e3a\u51c6\u5faa\u73af\u7801\u7684 GHRS\uff0c\u5e76\u6307\u51fa\u5b58\u5728\u540c\u65f6\u5177\u5907\u8fd9\u4e9b\u6027\u8d28\u7684\u7801\u65cf\u3002", "result": "\u5f97\u5230\u7684\u7ed3\u8bba\u5305\u62ec\uff1aGHRS \u7801\u5168\u4f53\u4e3a MDS\uff0cGHRS \u7684\u5bf9\u5076\u4e5f\u662f GHRS\uff1b\u5b58\u5728\u82e5\u5e72 GHRS \u5b50\u65cf\u5177\u6709 LDPC \u5c5e\u6027\uff1b\u5b58\u5728\u4e00\u65cf GHRS \u80fd\u591f\u751f\u6210\u51c6\u5faa\u73af\u7801\uff1b\u5e76\u4e14\u5b58\u5728\u540c\u65f6\u5177\u5907\u4e0a\u8ff0\u591a\u9879\u6027\u8d28\u7684 GHRS \u7801\u3002", "conclusion": "GHRS \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u4e30\u5bcc\u7684\u7801\u65cf\u6846\u67b6\uff0c\u5728\u4fdd\u6301 MDS \u7684\u540c\u65f6\u517c\u5bb9\u5bf9\u5076\u6027\u3001LDPC \u548c\u51c6\u5faa\u73af\u6027\u7b49\u7ed3\u6784\u7279\u6027\uff0c\u663e\u793a\u51fa\u5bf9\u5b9e\u9645\u901a\u4fe1\u7cfb\u7edf\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u4e0e\u7814\u7a76\u6df1\u5165\u7a7a\u95f4\u3002"}}
{"id": "2512.22616", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22616", "abs": "https://arxiv.org/abs/2512.22616", "authors": ["Mojtaba Eshghie", "Melissa Mazura", "Alexandre Bartel"], "title": "Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories", "comment": null, "summary": "We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.", "AI": {"tldr": "Raven\u6846\u67b6\u5c06\u4ee5\u5f80\u56e0\u65ad\u8a00/\u65ad\u8a00\u9006\u8f6c\u7684\u4ee5\u592a\u574a\u4ea4\u6613\u89c6\u4e3a\u4e3b\u52a8\u7684\u94fe\u4e0a\u9632\u5fa1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5c06\u201c\u88ab\u9006\u8f6c\u7684\u4e8b\u52a1\u201d\u4e0e\u6e90\u4ee3\u7801\u4e2d\u7684\u4e0d\u53d8\u91cf\u5bf9\u9f50\u3001\u7528BERT\u5fae\u8c03\u6a21\u578b\u5d4c\u5165\u4e0d\u53d8\u91cf\u3001\u5e76\u4ee5\u8bed\u4e49\u610f\u56fe\u805a\u7c7b\uff0c\u4ece\u800c\u6316\u6398\u9632\u5fa1\u6027\u4e0d\u53d8\u91cf\u7c7b\u522b\u5e76\u9a8c\u8bc1\u5176\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u4ea4\u6613\u88ab\u62e6\u622a/\u56de\u6eda\u80cc\u540e\u7684\u9632\u5fa1\u6027\u6a21\u5f0f\u5173\u6ce8\u4e0d\u8db3\uff0c\u5c3d\u7ba1\u8fd9\u7c7b\u4e0d\u53d8\u91cf\u80fd\u63ed\u793a\u94fe\u4e0a\u9632\u5fa1\u7b56\u7565\uff0c\u4e9f\u5f85\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u6316\u6398\u4e0e\u5229\u7528\u5b83\u4eec\u3002", "method": "\u5c06\u88ab\u9006\u8f6c\u7684\u4ea4\u6613\u4e0e\u5408\u7ea6\u5b9e\u73b0\u4e2d\u7684\u4e0d\u53d8\u91cf\u5bf9\u5e94\uff1b\u4f7f\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u6a21\u578b\u5bf9\u4e0d\u53d8\u91cf\u8fdb\u884c\u5d4c\u5165\uff1b\u6309\u8bed\u4e49\u610f\u56fe\u8fdb\u884c\u805a\u7c7b\u4ee5\u53d1\u73b0\u9632\u5fa1\u6027\u4e0d\u53d8\u91cf\u7c7b\u522b\uff1b\u5bf920,000\u6761\u88ab\u9006\u8f6c\u4ea4\u6613\u8fdb\u884c\u8bc4\u4f30\uff1b\u4e13\u5bb6\u4eba\u5de5\u5ba1\u9605\u5f97\u523019\u4e2a\u8bed\u4e49\u805a\u7c7b\u4e0e6\u4e2a\u65b0\u7c7b\u522b\uff1b\u4ee5\u65b0\u7c7b\u522b\u4e4b\u4e00\u4f5c\u4e3a\u6a21\u7cca\u6d4b\u8bd5\u7684oracle\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u653b\u51fb\u7684\u6f0f\u6d1e\u68c0\u6d4b\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5728\u6837\u672c\u4e2d\u5b9e\u73b0\u4e86\u8fde\u8d2f\u4e14\u5177\u610f\u4e49\u7684\u805a\u7c7b\uff0c19\u4e2a\u8bed\u4e49\u805a\u7c7b\u7ecf\u4e13\u5bb6\u5ba1\u9605\u53d1\u73b06\u4e2a\u5728\u73b0\u6709\u4e0d\u53d8\u91cf\u76ee\u5f55\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u7c7b\u522b\uff08\u5305\u62ec\u7279\u5f81\u5f00\u5173\u3001\u91cd\u653e\u9632\u62a4\u3001\u8bc1\u660e/\u7b7e\u540d\u6821\u9a8c\u3001\u8ba1\u6570\u5668\u3001\u8c03\u7528\u65b9\u63d0\u4f9b\u7684\u6ed1\u70b9\u9608\u503c\u3001\u767d/\u9ed1\u540d\u5355\u7b49\uff09\uff1b\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u65b0\u4e0d\u53d8\u91cf\u79cd\u7c7b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660eRaven\u80fd\u6620\u5c04\u4ee5\u592a\u574a\u7684\u6210\u529f\u9632\u5fa1\u3002", "conclusion": "Raven\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9632\u5fa1\u6316\u6398\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u667a\u80fd\u5408\u7ea6\u7684\u5b9e\u9645\u9632\u5fa1\u4e2d\u63d0\u53d6\u4e0d\u53d8\u91cf\u5e76\u6784\u5efa\u5206\u6790\u5de5\u5177\u548c\u5b89\u5168Oracle\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u7cfb\u7edf\u5730\u7406\u89e3\u548c\u5229\u7528\u533a\u5757\u94fe\u4e0a\u7684\u9632\u5fa1\u6a21\u5f0f\u3002"}}
{"id": "2512.23046", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23046", "abs": "https://arxiv.org/abs/2512.23046", "authors": ["Maryam Olyaee", "Giovanni Interdonato", "Stefano Buzzi"], "title": "User-Centric Cell-Free Massive MIMO Enhanced by Fluid-Antenna Access Points: Uplink Analysis", "comment": "Submitted to an IEEE Journal", "summary": "In this paper, we investigate cell-free massive MIMO (CF-mMIMO) systems in which access points (APs) are equipped with fluid antennas (FAs) and develop a comprehensive framework for channel estimation, antenna port selection, and uplink spectral efficiency (SE) optimization. We propose a generalized LMMSE-based uplink channel estimation scheme that dynamically activates FA ports during pilot transmission, efficiently exploiting antenna reconfigurability under practical training constraints. Building on this, we design a distributed port selection strategy that minimizes per-AP channel estimation error by exploiting spatial correlation among FA ports. We systematically analyze the impact of antenna geometry and spatial correlation using the Jakes' channel model for different AP array configurations, including uniform linear and planar arrays. We then derive SINR expressions for centralized and distributed uplink processing and obtain a closed-form uplink SE expression for centralized maximum-ratio combining using the use-and-then-forget bound. Finally, we propose an alternating-optimization framework to select FA port configurations that maximize the uplink sum SE. Numerical results show that the proposed FA-aware channel estimation and port optimization strategies greatly reduce channel estimation error and significantly improve sum-SE over fixed-antenna and non-optimized FA baselines, confirming FAs as a key enabler for scalable, adaptive CF-mMIMO networks.", "AI": {"tldr": "Proposes a fluid-antenna (FA) enabled cell-free massive MIMO framework with LMMSE-based uplink channel estimation, port selection, and SE optimization to exploit FA reconfigurability and spatial correlation for improved sum SE.", "motivation": "To enhance channel estimation accuracy and uplink spectral efficiency in cell-free mMIMO by leveraging reconfigurable fluid antennas and port selection under practical training constraints.", "method": "1) Develop a generalized LMMSE uplink channel estimation scheme with dynamic FA port activation during pilot transmission. 2) Design a distributed port selection strategy leveraging spatial correlation among FA ports to minimize estimation error. 3) Analyze antenna geometry and spatial correlation using Jakes\u2019 model for uniform linear and planar AP arrays. 4) Derive SINR expressions for centralized and distributed uplink processing and obtain a closed-form uplink SE bound for centralized MRC (use-and-then-forget). 5) Propose an alternating-optimization framework to select FA port configurations that maximize uplink sum SE.", "result": "The proposed FA-aware estimation and port optimization significantly reduce channel estimation error and substantially improve the uplink sum SE compared with fixed-antenna and non-optimized FA baselines.", "conclusion": "FAs are a key enabler for scalable, adaptive CF-mMIMO networks, enabling improved channel estimation and higher sum SE through coordinated port selection and FA-aware processing."}}
{"id": "2512.22527", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22527", "abs": "https://arxiv.org/abs/2512.22527", "authors": ["Hongwei Xu", "Weichao Zheng", "Zai Yang"], "title": "Compressive Toeplitz Covariance Estimation From Few-Bit Quantized Measurements With Applications to DOA Estimation", "comment": null, "summary": "This paper addresses the problem of estimating the Hermitian Toeplitz covariance matrix under practical hardware constraints of sparse observations and coarse quantization. Within the triangular-dithered quantization framework, we propose an estimator called Toeplitz-projected sample covariance matrix (Q-TSCM) to compensate for the quantization-induced bias, together with its finite-bit counterpart termed the $2k$-bit Toeplitz-projected sample covariance matrix ($2k$-TSCM), obtained by truncating the pre-quantization observations. Under the complex Gaussian assumption, we derive non-asymptotic error bounds of the estimators that reveal a quadratic dependence on the quantization level and capture the effect of sparse sampling patterns through the so-called coverage coefficient. To further improve performance, we propose the quantized sparse and parametric approach (Q-SPA) based on a covariance-fitting criterion, which enforces additionally positive semidefiniteness at the cost of solving a semidefinite program. Numerical experiments are presented that corroborate our theoretical findings and demonstrate the effectiveness of the proposed estimators in the application to direction-of-arrival estimation.", "AI": {"tldr": "\u5728\u7a00\u758f\u89c2\u6d4b\u548c\u7c97\u91cf\u5316\u6761\u4ef6\u4e0b\uff0c\u63d0\u51fa\u4e86\u7528\u4e8e Hermitian Toeplitz \u534f\u65b9\u5dee\u4f30\u8ba1\u7684 Q-TSCM \u53ca\u5176 2k-TSCM \u7248\u672c\uff0c\u5e76\u5728\u9ad8\u65af\u5047\u8bbe\u4e0b\u7ed9\u51fa\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff1b\u901a\u8fc7 Q-SPA \u5728\u534f\u65b9\u5dee\u62df\u5408\u6846\u67b6\u4e0b\u52a0\u5165 PSD \u7ea6\u675f\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5728 DOA \u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u89c2\u6d4b\u53d7\u9650\u4e14\u5b58\u5728\u91cf\u5316\u566a\u58f0\uff0c\u4e14\u5e0c\u671b\u5229\u7528 Toeplitz \u7ed3\u6784\u4ee5\u63d0\u5347\u534f\u65b9\u5dee\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff1b\u4e09\u89d2\u6296\u52a8\u91cf\u5316\u6846\u67b6\u7528\u4e8e\u51cf\u8f7b\u91cf\u5316\u504f\u5dee\uff0c\u9700\u7814\u7a76\u5728\u7a00\u758f\u91c7\u6837\u548c\u91cf\u5316\u4e0b\u7684\u8bef\u5dee\u7279\u6027\u4e0e\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa Toeplitz \u6295\u5f71\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635 Q-TSCM \u6765\u7ea0\u6b63\u91cf\u5316\u5e26\u6765\u7684\u504f\u5dee\uff0c\u4ee5\u53ca\u901a\u8fc7\u622a\u65ad\u91cf\u5316\u524d\u89c2\u6d4b\u5f97\u5230\u7684 2k-TSCM\uff1b\u5728\u590d\u9ad8\u65af\u5047\u8bbe\u4e0b\u63a8\u5bfc\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u91cf\u5316\u6c34\u5e73\u5bf9\u8bef\u5dee\u4ee5\u4e8c\u6b21\u5f62\u5f0f\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u8986\u76d6\u7cfb\u6570\u8868\u8fbe\u7a00\u758f\u91c7\u6837\u5f71\u54cd\uff1b\u63d0\u51fa\u57fa\u4e8e\u534f\u65b9\u5dee\u62df\u5408\u7684 Q-SPA\uff0c\u52a0\u5165\u534a\u6b63\u5b9a\u7ea6\u675f\uff0c\u901a\u8fc7 SDP \u6c42\u89e3\u3002", "result": "\u7ed9\u51fa\u5173\u4e8e\u8bef\u5dee\u7684\u975e\u5bf9\u79f0\u6027\u754c\u9650\uff0c\u91cf\u5316\u6c34\u5e73\u5bf9\u8bef\u5dee\u7684\u4e8c\u6b21\u4f9d\u8d56\u4ee5\u53ca\u8986\u76d6\u7cfb\u6570\u5bf9\u7a00\u758f\u91c7\u6837\u7684\u4f53\u73b0\uff1bQ-SPA \u663e\u8457\u6539\u5584\u4f30\u8ba1\uff0c\u4e14\u5728 DOA \u95ee\u9898\u4e2d\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u4f30\u8ba1\u91cf\u80fd\u591f\u5728\u7a00\u758f\u4e14\u91cf\u5316\u7684\u89c2\u6d4b\u6761\u4ef6\u4e0b\u6709\u6548\u4f30\u8ba1 Hermitian Toeplitz \u534f\u65b9\u5dee\uff0c\u5e76\u501f\u52a9 PSD \u7ea6\u675f\u63d0\u5347\u6027\u80fd\uff0c\u7406\u8bba\u4e0e\u6570\u503c\u5b9e\u9a8c\u5747\u652f\u6301\u5176\u5728 DOA \u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff1b\u4f46 SDP \u6210\u672c\u53ef\u80fd\u6210\u4e3a\u5b9e\u73b0\u65f6\u7684\u8003\u91cf\u3002"}}
{"id": "2512.22192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22192", "abs": "https://arxiv.org/abs/2512.22192", "authors": ["Jiahao Lu"], "title": "Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks", "comment": "9 pages, 5 figures. Code available at https://github.com/lujiahao760/FrequencyRegularization", "summary": "Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the \"low-pass filtering\" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u53ef\u89c6\u5316\u8bca\u65ad\u6846\u67b6\u4e0e\u8c31\u6291\u5236\u6bd4(SSR)\u91cf\u5316\u6b63\u5219\u5316\u5bf9\u7279\u5f81\u9891\u7387\u7684\u9009\u62e9\u6027\u5f71\u54cd\uff0c\u53d1\u73b0L2\u6b63\u5219\u5316\u663e\u8457\u6291\u5236\u9ad8\u9891\u80fd\u91cf\u79ef\u7d2f\uff0c\u540c\u65f6\u5b58\u5728\u51c6\u786e\u6027-\u9c81\u68d2\u6027\u6743\u8861\uff1b\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u89e3\u91ca\u6b63\u5219\u5316\u504f\u597d\u4f4e\u9891\u7ed3\u6784\u7684\u6cdb\u5316\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u6b63\u5219\u5316\uff08\u5982L2\u4e0eDropout\uff09\u80cc\u540e\u7684\u7269\u7406\u673a\u5236\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u5f71\u54cdCNN\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u9891\u7387\u9009\u62e9\uff0c\u4ee5\u53ca\u8fd9\u5bf9\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8c31\u5206\u6790\u6784\u5efa\u5bf9\u6b63\u5219\u5316\u7684\u76f4\u89c2\u89e3\u91ca\u4e0e\u5b9a\u91cf\u6307\u6807\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u8bca\u65ad\u6846\u67b6\uff0c\u52a8\u6001\u8ffd\u8e2a\u6743\u91cd\u9891\u7387\u7684\u6f14\u5316\uff1b\u5f15\u5165\u8c31\u6291\u5236\u6bd4(SSR)\u7528\u4e8e\u91cf\u5316\u4e0d\u540c\u6b63\u5219izer\u7684\u201c\u4f4e\u901a\u6ee4\u6ce2\u201d\u5f3a\u5ea6\uff1b\u901a\u8fc7\u79bb\u6563\u5f84\u5411\u5206\u6790\u89e3\u51b33x3\u7b49\u5c0f\u6838\u5728\u91c7\u6837\u4e0b\u7684\u6df7\u53e0\u95ee\u9898\uff1b\u5728ResNet-18\u4e0eCIFAR-10\u4e0a\u6bd4\u8f83L2\u3001Dropout\u7b49\u6b63\u5219\u5316\u5bf9\u8c31\u80fd\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1aL2\u6b63\u5219\u4f7f\u9ad8\u9891\u80fd\u91cf\u79ef\u7d2f\u6bd4\u672a\u6b63\u5219\u5316\u57fa\u7ebf\u964d\u4f4e>3\u500d\uff1bSSR\u63ed\u793a\u4e0d\u540c\u6b63\u5219\u5bf9\u8c31\u7684\u6291\u5236\u7a0b\u5ea6\uff1b\u5728\u6a21\u7cca/\u4f4e\u5206\u8fa8\u7387\u573a\u666f\u4e2d\uff0cL2\u6a21\u578b\u5bf9\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u5bf9\u6bd4\u57fa\u7ebf>6%\uff1b\u53e6\u4e00\u65b9\u9762\uff0cL2\u5bf9\u5bbd\u5e26\u9ad8\u65af\u566a\u58f0\u8f83\u4e3a\u654f\u611f\uff0c\u6e90\u4e8e\u5bf9\u4f4e\u9891\u7684\u8fc7\u5ea6\u4e13\u5316\u3002", "conclusion": "\u6b63\u5219\u5316\u901a\u8fc7\u5bf9\u9891\u7387\u7684\u5f3a\u70c8\u504f\u597d\uff08\u8c31\u4f4e\u901a\u7279\u6027\uff09\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\uff1b\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u63d0\u4f9b\u4e86\u5bf9\u6b63\u5219\u5316\u6548\u679c\u7684\u76f4\u89c2\u89e3\u91ca\uff0c\u5f3a\u5316\u4e86\u4f4e\u9891\u7ed3\u6784\u5728\u6df1\u5ea6\u5b66\u4e60\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2512.23194", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23194", "abs": "https://arxiv.org/abs/2512.23194", "authors": ["Xiaofeng Liu", "Jun Zhang", "Fang-Wei Fu"], "title": "A New Family of Binary Sequences via Elliptic Function Fields over Finite Fields of Odd Characteristics", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.18570 by other authors", "summary": "Motivated by the constructions of binary sequences by utilizing the cyclic elliptic function fields over the finite field $\\mathbb{F}_{2^{n}}$ by Jin \\textit{et al.} in [IEEE Trans. Inf. Theory 71(8), 2025], we extend the construction to the cyclic elliptic function fields with odd characteristic by using the quadratic residue map $\u03b7$ instead of the trace map used therein. For any cyclic elliptic function field with $q+1+t$ rational points and any positive integer $d$ with $\\gcd(d, q+1+t)=1$, we construct a new family of binary sequences of length $q+1+t$, size $q^{d-1}-1$, balance upper bounded by $(d+1)\\cdot\\lfloor2\\sqrt{q}\\rfloor+|t|+d,$ the correlation upper bounded by $(2d+1)\\cdot\\lfloor2\\sqrt{q}\\rfloor+|t|+2d$ and the linear complexity lower bounded by $\\frac{q+1+2t-d-(d+1)\\cdot\\lfloor2\\sqrt{q}\\rfloor}{d+d\\cdot\\lfloor2\\sqrt{q}\\rfloor}$ where $\\lfloor x\\rfloor$ stands for the integer part of $x\\in\\mathbb{R}$.", "AI": {"tldr": "\u5728\u5947\u7279\u5f81\u4e0b\uff0c\u57fa\u4e8e\u5faa\u73af\u692d\u5706\u51fd\u6570\u573a\u7684\u4e8c\u8fdb\u5236\u5e8f\u5217\u6784\u9020\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u53c2\u6570\u65cf\uff1a\u957f\u5ea6\u4e3a q+1+t\uff0c\u5927\u5c0f\u4e3a q^{d-1}-1\uff0c\u4e14\u5bf9\u9f50\u3001\u76f8\u5173\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6709\u660e\u786e\u4e0a\u754c\uff08\u5e73\u8861\u6027\u3001\u76f8\u5173\u6027\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u754c\u9650\uff09\u4e0e\u5bf9 d \u7684\u4f9d\u8d56\u3002", "motivation": "\u53d7\u5230 Jin \u7b49\u4eba\u5728 IEEE Trans. Inf. Theory 71(8), 2025 \u7684\u5faa\u73af\u692d\u5706\u51fd\u6570\u573a\u4e8c\u8fdb\u5236\u5e8f\u5217\u6784\u9020\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5728\u5947\u7279\u5f81\u4e0b\u4f7f\u7528\u4e8c\u6b21\u5269\u4f59\u6620\u5c04 \u03b7 \u66ff\u4ee3\u8ff9\u6620\u5c04\u6765\u6269\u5c55\u8be5\u6784\u9020\u3002", "method": "\u5728\u7ed9\u5b9a\u7684\u5faa\u73af\u692d\u5706\u51fd\u6570\u573a\u62e5\u6709 q+1+t \u4e2a\u6709\u7406\u70b9\uff0c\u4e14\u5bf9\u4efb\u610f\u6b63\u6574\u578b d \u6ee1\u8db3 gcd(d, q+1+t)=1 \u7684\u6761\u4ef6\u4e0b\uff0c\u5229\u7528\u4e8c\u6b21\u5269\u4f59\u6620\u5c04 \u03b7 \u6784\u9020\u65b0\u7684\u4e8c\u8fdb\u5236\u5e8f\u5217\u65cf\u3002\u5e8f\u5217\u957f\u5ea6\u4e3a q+1+t\uff0c\u5927\u5c0f\u4e3a q^{d-1}-1\u3002\u5bf9\u5e73\u8861\u6027\u7ed9\u51fa\u4e0a\u754c\uff1a(d+1)\u00b7\u230a2\u221aq\u230b+|t|+d\uff1b\u5bf9\u76f8\u5173\u6027\u7ed9\u51fa\u4e0a\u754c\uff1a(2d+1)\u00b7\u230a2\u221aq\u230b+|t|+2d\uff1b\u5bf9\u7ebf\u6027\u590d\u6742\u5ea6\u7ed9\u51fa\u4e0b\u754c\uff1a((q+1+2t-d-(d+1)\u00b7\u230a2\u221aq\u230b)/(d+d\u00b7\u230a2\u221aq\u230b))\u3002", "result": "\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u4e8c\u8fdb\u5236\u5e8f\u5217\u65cf\uff0c\u5176\u957f\u5ea6\u3001\u5927\u5c0f\u4e0e\u4e0a\u754c\u5747\u53d7 q\u3001t\u3001d \u7684\u63a7\u5236\uff0c\u63d0\u4f9b\u4e86\u5728\u5947\u7279\u5f81\u4e0b\u7684\u5e7f\u4e49\u6269\u5c55\u3002", "conclusion": "\u5c06\u5947\u7279\u5f81\u4e0b\u7684\u5faa\u73af\u692d\u5706\u51fd\u6570\u573a\u5e8f\u5217\u6784\u9020\u4ece\u539f\u6709\u7684\u57fa\u4e8e\u8ff9\u6620\u5c04\u7684\u65b9\u6848\u63a8\u5e7f\u5230\u4e8c\u6b21\u5269\u4f59\u6620\u5c04\u65b9\u6848\uff0c\u7ed9\u51fa\u660e\u786e\u7684\u5e73\u8861\u3001\u76f8\u5173\u4e0e\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u754c\uff0c\u5e76\u5bf9 d \u7684\u53d6\u503c\u4f5c\u4e86\u5e7f\u4e49\u7ea6\u675f\u3002"}}
{"id": "2512.22720", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22720", "abs": "https://arxiv.org/abs/2512.22720", "authors": ["Murtaza Nikzad", "Kerem Atas"], "title": "When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography", "comment": null, "summary": "This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{\u00f6}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.", "AI": {"tldr": "RSA \u5bc6\u94a5\u5728\u7d20\u6570\u9009\u62e9\u4e0a\u7684\u4e0d\u8db3\u5bfc\u81f4\u4e24\u79cd\u516c\u5f00\u653b\u51fb\uff1aFermat \u56e0\u5b50\u5206\u89e3\u548c GCD \u653b\u51fb\u3002\u57fa\u4e8e Heninger \u7b49\u4eba\u7684\u7814\u7a76\u4e0e B\u00f6ck \u7684\u5206\u6790\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u4ecd\u5927\u91cf\u5b58\u5728\u6f0f\u6d1e\uff0c\u539f\u56e0\u5728\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u71b5\u6e90\u4e0d\u8db3\u3002\u5bf9\u7b56\u5305\u62ec\u6539\u8fdb\u71b5\u6536\u96c6\u4e0e\u7d20\u6570\u9a8c\u8bc1\u3002", "motivation": "\u786e\u4fdd RSA \u5bc6\u94a5\u751f\u6210\u7684\u7d20\u6570\u9009\u62e9\u4e0d\u6613\u88ab\u8fd1\u4f3c\u6216\u5171\u4eab\uff1b\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u7684\u653b\u51fb\u9762\u53ca\u5176\u5f71\u54cd\uff1b\u5f3a\u8c03\u968f\u673a\u6027\u4e0d\u8db3\u5728\u5d4c\u5165\u5f0f\u5b9e\u73b0\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u57fa\u4e8e\u6587\u732e\u7efc\u8ff0\u4e0e\u5bf9\u4e24\u79cd\u653b\u51fb\u5411\u91cf\u7684\u5206\u6790\uff1b\u7ed3\u5408 Heninger \u7b49\u4eba\u7684\u201cMining Your Ps and Qs\u201d\u548c B\u00f6ck \u7684 Fermat \u56e0\u5b50\u5206\u89e3\u7814\u7a76\u7684\u53d1\u73b0\uff0c\u6bd4\u8f83\u73b0\u5b9e\u90e8\u7f72\u7684\u98ce\u9669\u3002", "result": "\u8bc1\u5b9e\u6f0f\u6d1e\u5728\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u4f9d\u7136\u666e\u904d\uff0c\u5f31\u968f\u673a\u6570\u751f\u6210\u662f\u5d4c\u5165\u5f0f\u8bbe\u5907\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\uff1b\u62ab\u9732 >6.4\u4e07\u500b\u6613\u53d7\u653b\u51fb\u7684 TLS \u4e3b\u673a\u7684\u7edf\u8ba1\u8bc1\u636e\uff0c\u4ee5\u53ca\u5bf9\u73b0\u6709\u5b9e\u73b0\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7f13\u89e3\u7b56\u7565\uff1a\u6539\u8fdb\u71b5\u6536\u96c6\u3001\u52a0\u5f3a\u7d20\u6570\u9a8c\u8bc1\u3001\u6539\u8fdb\u968f\u673a\u6570\u751f\u6210\u5668\u5b9e\u73b0\u3001\u8c28\u614e\u7684\u5bc6\u94a5\u8f6e\u6362\u548c\u76d1\u63a7\u3002"}}
{"id": "2512.23380", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.23380", "abs": "https://arxiv.org/abs/2512.23380", "authors": ["Mohammad Nasirzadeh", "Jafar Tahmoresnezhad", "Parviz Rashidi-Khazaee"], "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers", "comment": "72 pages, 19 figures, 19 tables, accepted in scientific reports on 5 November 2025", "summary": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.", "AI": {"tldr": "A multimodal log anomaly detection framework, CoLog, jointly encodes multiple log modalities using collaborative transformers, multi-head impressed attention, and a modality adaptation layer to model cross-modal interactions, achieving state-of-the-art results on seven benchmarks for point and collective anomalies.", "motivation": "Logs collected from diverse sources yield multiple modalities. Unimodal methods miss cross-modal cues; naive multimodal methods fail to exploit interactions. There is a need for a framework that explicitly encodes multiple modalities and their interactions to improve anomaly detection.", "method": "CoLog employs collaborative transformers to encode modalities, multi-head impressed attention to capture cross-modal interactions, and a modality adaptation layer to align heterogeneous representations. The framework learns nuanced inter-modal patterns to detect anomalies, including both point and collective anomalies. Implementation is provided in GitHub.", "result": "Reported mean precision 99.63%, recall 99.59%, and F1 99.61% across seven benchmark datasets for log-based anomaly detection, outperforming state-of-the-art methods.", "conclusion": "CoLog advances log anomaly detection by integrating multimodal information through interaction-aware encoding and adaptation, offering robust detection for point and collective anomalies and practical applicability in cybersecurity and system monitoring."}}
{"id": "2512.22582", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22582", "abs": "https://arxiv.org/abs/2512.22582", "authors": ["Xinyang Li", "Hian Zing Voon", "Vlad C. Andrei", "Alexander Sessler", "Nunzio Sciammetta", "Ullrich J. M\u00f6nich", "Dominic A. Schupke", "Holger Boche"], "title": "Real-Time Multi-Target Detection and Tracking with mmWave 5G NR Waveforms on RFSoC", "comment": null, "summary": "We demonstrate a real-time implementation of multi-target detection and tracking using 5G New Radio (NR) physical downlink shared channel (PDSCH) waveform with 400 MHz bandwidth at 28 GHz carrier frequency. The hardware platform is built on a radio frequency system-on-chip (RFSoC) 4x2 board connected with a pair of Sivers EVK02001 mmWave beamformers for transmission and reception. The entire sensing transceiver processing and fast beam control are realized purely in the programmable logic (PL) part of the RFSoC, enabling low-latency and fully hardware-accelerated operation. The continuously acquired sensing data constitute 3D range-angle (RA) tensors, which are processed on a host PC using adaptive background subtraction, cell-averaging constant false alarm rate (CA-CFAR) detection with density-based spatial clustering of applications with noise (DBSCAN) clustering, and extended Kalman filtering (EKF), to detect and track targets in the environment. Our software-defined radio (SDR) testbed integrates heterogeneous computing resources, including CPUs, GPUs, and FPGAs, thereby providing design flexibility for a wide range of tasks.", "AI": {"tldr": "Real-time multi-target detection and tracking using 5G NR PDSCH waveform (400 MHz at 28 GHz) with full hardware-accelerated sensing on RFSoC PL; 3D range-angle tensors processed on host with adaptive background subtraction, CA-CFAR (with DBSCAN) clustering, and EKF tracking; SDR testbed integrates CPUs/GPUs/FPGA for design flexibility.", "motivation": "Demonstrate a low-latency, hardware-accelerated joint sensing and communication system (ISAC) using a 5G NR waveform at mmWave, enabling real-time multi-target detection and tracking.", "method": "Hardware: RFSoC 4x2 board plus Sivers EVK02001 mmWave beamformers for Tx/Rx. Sensing transceiver processing and fast beam control implemented entirely in programmable logic (PL) on RFSoC. Data products are 3D range-angle tensors. Processing performed on a host PC with adaptive background subtraction, CA-CFAR detection with DBSCAN clustering, and EKF tracking. SDR testbed uses heterogeneous computing resources (CPUs, GPUs, FPGAs) for flexibility.", "result": "Achieved real-time implementation of multi-target detection and tracking with fully hardware-accelerated sensing and low latency. Demonstrates feasibility of processing 3D RA tensors on a PC and seamless integration of SDR with heterogeneous compute resources.", "conclusion": "The proposed hardware-software co-design offers a feasible and flexible platform for real-time mmWave ISAC using 5G NR waveforms, combining high-bandwidth sensing with programmable hardware and diverse computing resources."}}
{"id": "2512.23085", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23085", "abs": "https://arxiv.org/abs/2512.23085", "authors": ["Ran Hao", "Yuttana Itsarachaiyot", "Yen-Chun Chen", "M. Cenk \u00c7avu\u015fo\u011flu"], "title": "Real-Time Forward Kinematics and Jacobians for Control of an MRI-Guided Magnetically Actuated Robotic Catheter", "comment": null, "summary": "This paper presents a forward kinematics and analytical Jacobian computation approach for real-time control of a novel magnetic resonance imaging (MRI)-actuated robotic catheter. The MRI-actuated robotic catheter is modeled as a series of rigid and flexible segments and actuated by magnetic torques generated on a set of current-carrying microcoils embedded on the catheter body by the magnetic field of the MRI scanner. First, a real-time forward kinematic modeling approach of the robotic catheter employing the static Cosserat-rod theory is presented. Second, the analytical calculation approach of the forward kinematic Jacobians of the proposed forward kinematic model is presented. The accuracy, reproducibility, and computational efficiency of the proposed methods are evaluated using a robotic catheter prototype with a single coil set, where catheter tip trajectories collected by a catadioptric stereo camera tracking system are validated using the desired tip trajectories. Experimental results demonstrate that the proposed method can successfully control the catheter in an open loop to perform complex trajectories with real-time computational efficiency, paving the way for accurate closed-loop control with real-time MR-imaging feedback.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9759\u6001Cosserat-rod\u7406\u8bba\u7684MRI\u9a71\u52a8\u5bfc\u7ba1\u7684\u6b63\u5411\u8fd0\u52a8\u5b66\u53ca\u89e3\u6790\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u5b9e\u65f6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5728\u539f\u578b\u673a\u4e0a\u9a8c\u8bc1\u5176\u5b9e\u65f6\u6027\u4e0e\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3MRI\u73af\u5883\u4e0b\u5bfc\u7ba1\u673a\u5668\u4eba\u5728\u5b9e\u65f6\u3001\u7cbe\u786e\u63a7\u5236\u4e2d\u7684\u6311\u6218\uff0c\u9700\u5feb\u901f\u3001\u7a33\u5b9a\u7684\u6b63\u5411\u8fd0\u52a8\u5b66\u4e0e\u96c5\u53ef\u6bd4\u5206\u6790\u4ee5\u652f\u6301\u95ed\u73af\u63a7\u5236\u3002", "method": "\u5c06\u5bfc\u7ba1\u5efa\u6a21\u4e3a\u7531\u521a\u6027\u4e0e\u67d4\u6027\u6bb5\u7ec4\u6210\uff0c\u5229\u7528\u5d4c\u5165\u5728\u5bfc\u7ba1\u4f53\u4e0a\u7684\u5fae\u7ebf\u5708\u5728MRI\u78c1\u573a\u4e2d\u4ea7\u751f\u78c1\u8f6c\u77e9\u6765\u9a71\u52a8\uff1b\u91c7\u7528\u9759\u6001Cosserat-rod\u7406\u8bba\u63a8\u5bfc\u6b63\u5411\u8fd0\u52a8\u5b66\u7684\u5b9e\u65f6\u8ba1\u7b97\uff0c\u5e76\u7ed9\u51fa\u6b63\u5411\u8fd0\u52a8\u5b66\u7684\u89e3\u6790\u96c5\u53ef\u6bd4\u77e9\u9635\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u5728\u5355\u7ebf\u5708\u96c6\u7684\u539f\u578b\u673a\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5229\u7528\u5361\u5854-\u5bf9\u7acb\u5bfb\u8e2a\u7acb\u4f53\u89c6\u89c9\u8ddf\u8e2a\u5bfc\u7ba1\u5c16\u7aef\u8f68\u8ff9\uff0c\u9a8c\u8bc1\u4e0e\u671f\u671b\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\uff0c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u826f\u597d\u6027\u80fd\uff0c\u80fd\u591f\u5b9e\u73b0\u5f00\u653e\u73af\u63a7\u5236\u7684\u590d\u6742\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728MR\u6210\u50cf\u53cd\u9988\u4e0b\u5b9e\u73b0\u7cbe\u786e\u95ed\u73af\u63a7\u5236\u94fa\u5e73\u9053\u8def\uff0c\u4fc3\u8fdbMRI\u5f15\u5bfc\u4e0b\u7684\u5bfc\u7ba1\u673a\u5668\u4eba\u5b9e\u73b0\u5b9e\u65f6\u9c81\u68d2\u63a7\u5236\u3002"}}
{"id": "2512.23535", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.23535", "abs": "https://arxiv.org/abs/2512.23535", "authors": ["Eduardo Salazar"], "title": "A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)", "comment": null, "summary": "This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.\n  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.23316", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23316", "abs": "https://arxiv.org/abs/2512.23316", "authors": ["E. P. Csirmaz", "L. Csirmaz"], "title": "Information Inequalities for Five Random Variables", "comment": null, "summary": "The entropic region is formed by the collection of the Shannon entropies of all subvectors of finitely many jointly distributed discrete random variables. For four or more variables the structure of the entropic region is mostly unknown. We utilize a variant of the Maximum Entropy Method to delimit the five-variable entropy region. This method adds copies of some of the random variables in generations. A significant reduction in computational complexity, achieved through theoretical considerations and by harnessing the inherent symmetries, allowed us to calculate all five-variable non-Shannon inequalities provided by the first nine generations. Based on the results, we define two infinite collections of such inequalities, and prove them to be entropy inequalities. We investigate downward closed subsets of non-negative lattice points that parameterize these collections, based on which we develop an algorithm to enumerate all extremal inequalities. The discovered set of entropy inequalities is conjectured to characterize the applied method completely.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684\u6700\u5927\u71b5\u65b9\u6cd5\u53ca\u53d8\u91cf\u590d\u5236\uff0c\u754c\u5b9a\u4e94\u53d8\u91cf\u7684\u71b5\u533a\u57df\uff0c\u7ed9\u51fa\u4e24\u7c7b\u65e0\u9650\u65cf\u7684\u975e Shannon \u4e0d\u7b49\u5f0f\u548c\u4e00\u4e2a\u6781\u503c\u4e0d\u7b49\u5f0f\u679a\u4e3e\u7b97\u6cd5\uff0c\u5e76\u731c\u6d4b\u8be5\u96c6\u5408\u80fd\u591f\u5b8c\u5168\u63cf\u8ff0\u6240\u7528\u65b9\u6cd5\u3002", "motivation": "\u4e94\u53d8\u91cf\u71b5\u533a\u57df\u7684\u7ed3\u6784\u8fdc\u6bd4\u56db\u4e2a\u53ca\u4ee5\u4e0b\u53d8\u91cf\u590d\u6742\uff0c\u5c1a\u672a\u5b8c\u5168\u88ab\u7406\u89e3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u754c\u5b9a\u5176\u8fb9\u754c\u548c\u4e0d\u7b49\u5f0f\u96c6\u5408\u3002", "method": "\u4f7f\u7528\u53d8\u4f53\u7684\u6700\u5927\u71b5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u751f\u6210\u4e2d\u590d\u5236\u968f\u673a\u53d8\u91cf\u6765\u6269\u5c55\u7ef4\u5ea6\uff0c\u7ed3\u5408\u5bf9\u79f0\u6027\u6765\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u8ba1\u7b97\u524d\u4e5d\u4ee3\u7684\u6240\u6709\u4e94\u53d8\u91cf\u975e Shannon \u4e0d\u7b49\u5f0f\uff1b\u5b9a\u4e49\u4e24\u65e0\u9650\u65cf\u4e0d\u7b49\u5f0f\u5e76\u8bc1\u5b9e\u5176\u4e3a\u71b5\u4e0d\u7b49\u5f0f\uff1b\u7814\u7a76\u5411\u4e0b\u5c01\u95ed\u7684\u975e\u8d1f\u683c\u70b9\u5b50\u96c6\u4ee5\u53c2\u6570\u5316\u8fd9\u4e9b\u65cf\uff0c\u63d0\u51fa\u679a\u4e3e\u6781\u503c\u4e0d\u7b49\u5f0f\u7684\u7b97\u6cd5\u3002", "result": "\u83b7\u5f97\u4e86\u4e94\u53d8\u91cf\u7684\u6240\u6709\u6765\u81ea\u524d\u4e5d\u4ee3\u7684\u975e Shannon \u4e0d\u7b49\u5f0f\uff1b\u63d0\u51fa\u4e24\u7c7b\u65e0\u9650\u65cf\u7684\u71b5\u4e0d\u7b49\u5f0f\u5e76\u8bc1\u5b9e\u5176\u4e3a\u71b5\u4e0d\u7b49\u5f0f\uff1b\u5b9e\u73b0\u4e00\u4e2a\u679a\u4e3e\u6781\u503c\u4e0d\u7b49\u5f0f\u7684\u7b97\u6cd5\uff1b\u521d\u6b65\u5de5\u4f5c\u8868\u660e\u8be5\u65b9\u6cd5\u7684\u83b7\u5f97\u7684\u4e0d\u7b49\u5f0f\u96c6\u5408\u6709\u53ef\u80fd\u5b8c\u5168\u63cf\u8ff0\u6240\u7528\u65b9\u6cd5\u7684\u8fb9\u754c\u3002", "conclusion": "\u6240\u53d1\u73b0\u7684\u71b5\u4e0d\u7b49\u5f0f\u96c6\u5408\u88ab\u8ba4\u4e3a\u80fd\u591f\u5b8c\u5168\u8868\u5f81\u6240\u7528\u7684\u6700\u5927\u71b5\u65b9\u6cd5\u5728\u4e94\u53d8\u91cf\u60c5\u5f62\u4e0b\u7684\u8fb9\u754c\uff0c\u65b9\u6cd5\u5b66\u4e0a\u663e\u8457\u5229\u7528\u5bf9\u79f0\u6027\u548c\u590d\u5236\u53d8\u91cf\u7684\u601d\u60f3\u3002"}}
{"id": "2512.22676", "categories": ["eess.SP", "cs.AR", "cs.DC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.22676", "abs": "https://arxiv.org/abs/2512.22676", "authors": ["Sergey Salishev"], "title": "Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space", "comment": "English translation of PhD thesis (Candidate of Physical and Mathematical Sciences), defended at Saint Petersburg State University (2017). 191 pages", "summary": "This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.", "AI": {"tldr": "\u5728\u4f4e\u529f\u8017\u786c\u4ef6\u4e2d\u63d0\u5347\u4fe1\u53f7\u5904\u7406\u6548\u7387\u7684\u7efc\u5408\u6846\u67b6\uff0c\u6db5\u76d6\u80fd\u8017\u5efa\u6a21\u3001\u6574\u6570\u53cb\u597d\u8fd1\u4f3c\u3001\u51b2\u7a81\u81ea\u7531FFT\u8c03\u5ea6\u53caSchur\u7b97\u6cd5\u5e76\u884c\u6027\u5206\u6790\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u7684\u4f4e\u529f\u8017\u8ba1\u7b97\u573a\u666f\u9700\u8981\u5728\u80fd\u8017\u3001\u5e76\u884c\u5ea6\u548c\u5b58\u50a8\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u4e13\u7528\u52a0\u901f\u5668\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u5728\u4fdd\u6301\u7cbe\u5ea6\u548c\u53ef\u5b9e\u73b0\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u80fd\u6548\u3002", "method": "(i) \u63d0\u51fa\u7528\u4e8e\u65f6\u949fCMOS\u903b\u8f91\u7684\u80fd\u8017\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u9009\u62e9\u6700\u4f18\u5e76\u884c\u5ea6\uff1b(ii) \u7ed9\u51fa\u6574\u6570\u53cb\u597d\u7684\u4e00\u5143\u51fd\u6570\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d7\u9650\u7684\u5206\u6bb5\u591a\u9879\u5f0f\uff08\u51c6\u6837\u6761\uff09\u6784\u9020\u6765\u51cf\u5c0f\u67e5\u627e\u8868\u89c4\u6a21\uff0c\u5e76\u63d0\u4f9b\u7cbe\u5ea6\u4fdd\u8bc1\uff1b(iii) \u63d0\u51fa\u6df7\u5408\u8fdb\u5236\u6d41\u5f0fFFT\u5728\u591a-bank\u3001\u5355\u7aef\u53e3\u5b58\u50a8\u5668\u4e0a\u7684\u51b2\u7a81\u81ea\u7531\u6570\u636e\u653e\u7f6e\u4e0e\u6267\u884c\u987a\u5e8f\u7684\u8bc1\u660e\u6027\u65b9\u6cd5\uff0c\u5e76\u7ed9\u51fa\u81ea\u6392\u5e8fFFT\u53d8\u4f53\uff1b(iv) \u7ed9\u51fa\u5feb\u901fSchur\u7b97\u6cd5\u5728Toeplitz\u77e9\u9635\u6c42\u89e3\u4e2d\u7684\u5e76\u884c\u6027\u4e0e\u5185\u5b58\u5206\u6790\uff0c\u4e14\u4ee5\u56de\u58f0\u62b5\u6d88\u5de5\u4f5c\u8d1f\u8f7d\u4e3a\u52a8\u673a\u3002", "result": "\u7ed9\u51fa\u5177\u6709\u6784\u9020\u6027\u5b9a\u7406\u7684\u8c03\u5ea6\u4e0e\u8bbe\u8ba1\u6743\u8861\uff0c\u8986\u76d6\u80fd\u8017-\u5e76\u884c\u5ea6\u53d6\u820d\u3001\u8fd1\u4f3c\u8bef\u5dee\u754c\u9650\u3001FFT\u6570\u636e\u653e\u7f6e\u7684\u51b2\u7a81\u907f\u514d\u7b56\u7565\u4ee5\u53caSchur\u7b97\u6cd5\u7684\u5e76\u884c\u6027\u4e0e\u5185\u5b58\u9700\u6c42\u5206\u6790\uff0c\u4ece\u800c\u652f\u6301\u9ad8\u6548\u7684\u4e13\u7528\u52a0\u901f\u5668\u8bbe\u8ba1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5171\u540c\u6784\u6210\u9762\u5411\u4f4e\u529f\u8017\u4e13\u7528\u52a0\u901f\u5668\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u63d0\u4f9b\u53ef\u5b9e\u73b0\u7684\u80fd\u6548\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u4e3a\u56de\u58f0\u6d88\u9664\u7b49\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u7406\u8bba\u4e0e\u5b9e\u73b0\u652f\u6491\u3002"}}
{"id": "2512.22213", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22213", "abs": "https://arxiv.org/abs/2512.22213", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Louis Mahon", "Wayne Luk", "Anton Isopoussu", "Yiren Zhao"], "title": "On the Existence and Behaviour of Secondary Attention Sinks", "comment": null, "summary": "Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u6ce8\u610f\u529b\u4e2d\u7684\u6b21\u7ea7\u6c47\u805a\u70b9\uff08secondary sinks\uff09\uff0c\u5b83\u4eec\u5728\u4e2d\u95f4\u5c42\u51fa\u73b0\u3001\u7531\u7279\u5b9aMLP\u6a21\u5757\u6620\u5c04\u4ea7\u751f\uff0c\u5176\u5f3a\u5ea6\u7531\u5411\u91cf\u7684l2\u8303\u6570\u51b3\u5b9a\uff0c\u5e76\u4e0e\u4e3bsink\u7684\u5f31\u5316\u53ca\u6a21\u578b\u89c4\u6a21\u76f8\u5173\uff0c\u8868\u73b0\u4e3a\u591a\u4e2a\u53ef\u91cd\u590d\u7684\u201csink level\u201d\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u51fa\u73b0\u3002", "motivation": "\u63ed\u793a\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u65b0\u578b\u6c47\u805a\u73b0\u8c61\uff0c\u533a\u5206\u5df2\u6709\u7684\u4e3bsink\uff08\u5982BOS\uff09\u548c\u65b0\u53d1\u73b0\u7684\u6b21\u7ea7sinks\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u5f62\u6210\u673a\u5236\u3001\u6301\u7eed\u65f6\u95f4\u4e0e\u5bf9\u6ce8\u610f\u529b\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u572811\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc6\u522b\u5e76\u8ffd\u8e2asinks\u7684\u4f4d\u7f6e\u3001\u7279\u5f81\u53ca\u751f\u547d\u5468\u671f\uff1b\u5206\u6790\u4e2d\u5c42MLP\u5bf9\u5411\u91cf\u65b9\u5411\u7684\u6620\u5c04\u3001l2\u8303\u6570\u4e0esink\u5206\u6570\u7684\u5173\u7cfb\uff1b\u6bd4\u8f83\u4e3bsink\u5728\u4e2d\u95f4\u5c42\u7684\u53d8\u5316\uff0c\u5e76\u7ed9\u51fa\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684sink level\uff08\u5982QwQ-32B\u76843\u4e2a\u3001Qwen3-14B\u76846\u4e2a\uff09\u3002", "result": "\u8bc1\u660e\u5b58\u5728\u4e2d\u95f4\u5c42\u7684\u4e8c\u7ea7sinks\uff0c\u5177\u6709\u8f83\u5c0f\u4f46\u663e\u8457\u7684\u6ce8\u610f\u529b\u5360\u6bd4\uff1b\u5b83\u4eec\u7531\u7279\u5b9a\u4e2d\u5c42MLP\u6a21\u5757\u5f62\u6210\u3001\u5e76\u4f7f\u5bf9\u5e94\u7684\u4e3bsink\u5728\u540c\u5c42\u7684\u5f71\u54cd\u51cf\u5f31\uff1bl2\u8303\u6570\u51b3\u5b9asink\u5206\u6570\u53ca\u6301\u7eed\u5c42\u6570\uff1b\u5927\u6a21\u578b\u4e2dsink\u7684\u4f4d\u7f6e\u4e0e\u5bff\u547d\u66f4\u5177\u786e\u5b9a\u6027\uff0c\u5448\u73b0\u591a\u5c42\u7ea7\u7684sink level\u3002", "conclusion": "\u6b21\u7ea7sinks\u4e30\u5bcc\u4e86\u5bf9\u6ce8\u610f\u529b\u5206\u5e03\u7684\u7406\u89e3\uff0c\u63d0\u793a\u9700\u8981\u5c06\u5c42\u5185\u975e\u4e3b\u5bfc\u4fe1\u606f\u7684\u6f5c\u5728\u4f5c\u7528\u7eb3\u5165\u89e3\u91ca\u4e0e\u8bbe\u8ba1\u8003\u91cf\uff0c\u53ef\u80fd\u5f15\u5bfc\u5bf9\u6a21\u578b\u67b6\u6784\u4e0e\u6b63\u5219\u5316\u7684\u6539\u8fdb\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u63a2\u7d22\u7f13\u89e3\u6216\u5229\u7528\u8fd9\u4e9bsinks\u4ee5\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2512.22789", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22789", "abs": "https://arxiv.org/abs/2512.22789", "authors": ["Ying Li", "Wenjun Qiu", "Faysal Hossain Shezan", "Kunlin Cai", "Michelangelo van Dam", "Lisa Austin", "David Lie", "Yuan Tian"], "title": "Breaking the illusion: Automated Reasoning of GDPR Consent Violations", "comment": "Accepted by the IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aCosmic\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7f51\u9875\u8868\u5355\u4e2d\u68c0\u6d4b\u4e0e\u9690\u79c1\u540c\u610f\u76f8\u5173\u7684\u8fdd\u89c4\u884c\u4e3a\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u7f51\u7ad9\u4e0a\u8fdb\u884c\u5ba1\u8ba1\uff0c\u663e\u793a\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "GDPR\u548cCCPA\u7b49\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u5728\u6536\u96c6\u3001\u4f7f\u7528\u548c\u5206\u4eab\u4e2a\u4eba\u6570\u636e\u65f6\u83b7\u5f97\u77e5\u60c5\u3001\u81ea\u7531\u3001\u5177\u4f53\u3001\u660e\u786e\u7684\u540c\u610f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u805a\u7126\u4e8ecookie\u6a2a\u5e45\u548c\u79fb\u52a8\u5e94\u7528\u5bf9\u8bdd\u6846\uff0c\u4e14\u540c\u610f\u8868\u5355\u7ed3\u6784\u591a\u6837\u3001\u6cd5\u6e90\u4f9d\u636e\u4e0d\u540c\u3001\u96be\u4ee5\u5b9a\u4f4d\uff0c\u5bfc\u81f4\u81ea\u52a8\u5316\u5408\u89c4\u5ba1\u8ba1\u56f0\u96be\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u7f51\u9875\u8868\u5355\u81ea\u52a8\u68c0\u6d4b\u9690\u79c1\u5408\u89c4\u6027\u7684\u5de5\u5177\u3002", "method": "\u63d0\u51faCosmic\u6846\u67b6\uff0c\u81ea\u52a8\u68c0\u6d4b\u7f51\u9875\u8868\u5355\u4e2d\u7684\u540c\u610f\u76f8\u5173\u9690\u79c1\u8fdd\u89c4\u3002\u901a\u8fc7\u5bf95,823\u4e2a\u7f51\u7ad9\u30013,598\u4e2a\u8868\u5355\u7684\u5ba1\u8ba1\u8bc4\u4f30\uff0c\u8bc6\u522b\u51fa3,384\u5904\u8fdd\u89c4\uff0c\u8986\u76d694.1%\u7684\u540c\u610f\u8868\u5355\u3002\u6846\u67b6\u7684\u68c0\u6d4b\u80fd\u529b\u4ee5\u771f\u9633\u6027\u7387\uff08TPR\uff09\u8861\u91cf\uff0c\u83b7\u5f97\u540c\u610f\u68c0\u6d4b\u7684TPR\u4e3a98.6%\uff0c\u8fdd\u89c4\u68c0\u6d4b\u7684TPR\u4e3a99.1%\uff0c\u663e\u793a\u9ad8\u51c6\u786e\u5ea6\u3002", "result": "Cosmic\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7ad9\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7387\u548c\u9ad8\u51c6\u786e\u5ea6\uff0c\u63ed\u793a\u4e86\u666e\u904d\u5b58\u5728\u7684\u540c\u610f disclosure\u3001\u81ea\u7531\u7ed9\u4e88\u3001\u64a4\u56de\u9009\u9879\u7b49\u65b9\u9762\u7684\u5408\u89c4\u7f3a\u53e3\uff0c\u8868\u660e\u81ea\u52a8\u5316\u5ba1\u8ba1\u5728\u63d0\u5347\u6570\u636e\u4fdd\u62a4\u5408\u89c4\u6027\u65b9\u9762\u5177\u6709\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u8de8\u7f51\u7ad9\u7684\u81ea\u52a8\u5316\u5408\u89c4\u5ba1\u8ba1\u80fd\u529b\uff0c\u586b\u8865\u4e86\u5bf9\u591a\u6837\u5316\u540c\u610f\u8868\u5355\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u7a7a\u767d\uff0c\u5e76\u4e3a\u6539\u8fdb\u7f51\u9875\u8868\u5355\u7684\u9690\u79c1\u5408\u89c4\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u4e0e\u8bc1\u636e\uff0c\u63d0\u5347\u7528\u6237\u81ea\u4e3b\u6743\u4e0e\u4fe1\u4efb\u3002"}}
{"id": "2512.22686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22686", "abs": "https://arxiv.org/abs/2512.22686", "authors": ["Kumar Sai Bondada", "Daniel J. Jakubisin", "R. Michael Buehrer"], "title": "Multistatic Radar Performance in the Presence of Distributed Wireless Synchronization", "comment": null, "summary": "This paper proposes a multistatic radar (MSR) system utilizing a distributed wireless synchronization protocol. The wireless synchronization protocol uses a two-tone waveform exchange for frequency synchronization and a bi-directional waveform exchange for time synchronization, independent of GPS. A Bayesian Cramer-Rao lower bound (BCRLB) framework is developed to quantify the impact of synchronization offsets on joint delay and Doppler estimation, and consequently, on target localization and velocity estimation accuracy. Simulation results derived from the analytical expressions establish the extent to which the residual synchronization offsets degrade the MSR's performance. The performance of the synchronization links primarily depends on the synchronization-link channel and transmit parameters; optimizing these parameters enables the MSR configuration to surpass the monostatic performance and approach the ideal case. Furthermore, the simulated synchronization-link parameters suggest that practical implementation is feasible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u65e0\u7ebf\u540c\u6b65\u534f\u8bae\u7684\u591a\u57fa\u70b9\u96f7\u8fbe\uff08MSR\uff09\u7cfb\u7edf\uff0c\u5229\u7528\u4e24\u97f3\u6ce2\u5f62\u7528\u4e8e\u9891\u7387\u540c\u6b65\u3001\u53cc\u5411\u6ce2\u5f62\u4ea4\u6362\u7528\u4e8e\u65f6\u95f4\u540c\u6b65\uff08\u975eGPS\uff09\u3002\u5efa\u7acbBCRLB\u6846\u67b6\u91cf\u5316\u540c\u6b65\u504f\u79fb\u5bf9\u8054\u5408\u65f6\u5ef6-\u591a\u666e\u52d2\u4f30\u8ba1\u3001\u76ee\u6807\u5b9a\u4f4d\u4e0e\u901f\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u4eff\u771f\u8868\u660e\u6b8b\u4f59\u540c\u6b65\u504f\u79fb\u4f1a\u964d\u7ea7\u6027\u80fd\uff1b\u540c\u6b65\u94fe\u8def\u6027\u80fd\u53d7\u94fe\u8def\u548c\u53d1\u5c04\u53c2\u6570\u5f71\u54cd\uff0c\u7ecf\u8fc7\u4f18\u5316\u53ef\u8d85\u8d8a\u5355\u57fa\u96f7\u8fbe\u5e76\u63a5\u8fd1\u7406\u60f3\u60c5\u51b5\uff0c\u4e14\u5b9e\u73b0\u53ef\u884c\u6027\u901a\u8fc7\u53c2\u6570\u4eff\u771f\u5f97\u5230\u652f\u6301\u3002", "motivation": "\u5728\u65e0GPS\u7b49\u5916\u90e8\u65f6\u95f4\u53c2\u8003\u6761\u4ef6\u4e0b\uff0c\u63d0\u5347\u5206\u5e03\u5f0f\u96f7\u8fbe\u7684\u540c\u6b65\u7cbe\u5ea6\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8054\u5408\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\uff1b\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u65e0\u7ebf\u534f\u8bae\u5b9e\u73b0\u9891\u7387\u548c\u65f6\u95f4\u7684\u9c81\u68d2\u540c\u6b65\u53ca\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u97f3\u6ce2\u5f62\u4ea4\u6362\u5b9e\u73b0\u9891\u7387\u540c\u6b65\u3001\u53cc\u5411\u6ce2\u5f62\u4ea4\u6362\u5b9e\u73b0\u65f6\u95f4\u540c\u6b65\u7684\u5206\u5e03\u5f0f\u65e0\u7ebf\u540c\u6b65\u534f\u8bae\uff1b\u5efa\u7acbBCRLB\u6846\u67b6\u6765\u91cf\u5316\u540c\u6b65\u504f\u79fb\u5bf9\u8054\u5408\u65f6\u5ef6\u4e0e\u591a\u666e\u52d2\u4f30\u8ba1\u7684\u5f71\u54cd\uff1b\u63a8\u5bfc\u89e3\u6790\u8868\u8fbe\u5f0f\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e0d\u540c\u540c\u6b65\u94fe\u8def\u53c2\u6570\u5bf9MSR\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u901a\u8fc7\u4f18\u5316\u540c\u6b65\u94fe\u8def\u53c2\u6570\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u7ed9\u51fa\u4e0e\u540c\u6b65\u76f8\u5173\u7684Cramer-Rao\u4e0b\u754c\u53ca\u5176\u5bf9MSR\u6027\u80fd\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u4eff\u771f\u8868\u660e\u6b8b\u4f59\u540c\u6b65\u504f\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u7cbe\u5ea6\uff1b\u901a\u8fc7\u4f18\u5316\u540c\u6b65\u94fe\u8def\u53c2\u6570\uff0cMSR\u6027\u80fd\u53ef\u8d85\u8fc7\u5355\u666e\u96f7\u8fbe\u4e14\u63a5\u8fd1\u7406\u60f3\u60c5\u51b5\uff1b\u540c\u6b65\u94fe\u8def\u53c2\u6570\u7684\u4eff\u771f\u7ed3\u679c\u652f\u6301\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u65e0\u7ebf\u540c\u6b65\u534f\u8bae\u4f7fMSR\u5728\u4e0d\u4f9d\u8d56GPS\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u4e0e\u4f18\u5316\u540c\u6b65\u94fe\u8def\u53c2\u6570\uff0c\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u548c\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.22221", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.22221", "abs": "https://arxiv.org/abs/2512.22221", "authors": ["Soroush Vahidi"], "title": "Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning", "comment": null, "summary": "Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.\n  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u81ea\u9002\u5e94\u7684\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u6846\u67b6\uff0c\u57fa\u4e8e\u663e\u5f0f\u7ec4\u5408\u63a8\u7406\uff08\u800c\u975e\u6df1\u5ea6\u4fe1\u606f\u4f20\u64ad\uff09\uff0c\u5728\u540c\u8d28\u6027/\u5f02\u8d28\u6027\u56fe\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u63a7\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u9009\u7684\u6df7\u5408\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002", "motivation": "GNN\u5728\u5f02\u8d28\u6027\u56fe\u4e0a\u5f80\u5f80\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u66f4\u900f\u660e\u3001\u53ef\u63a7\u4e14\u9ad8\u6548\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8d2a\u5fc3\u6392\u5e8f\u7684\u7ec4\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u52a0\u6027\u6253\u5206\u51fd\u6570\u878d\u5408\u5148\u9a8c\u7c7b\u6982\u7387\u3001\u90bb\u57df\u7edf\u8ba1\u3001\u7279\u5f81\u76f8\u4f3c\u6027\u548c\u8bad\u7ec3\u5f97\u5230\u7684\u6807\u7b7e-\u6807\u7b7e\u517c\u5bb9\u6027\uff1b\u901a\u8fc7\u5c11\u91cf\u8d85\u53c2\u6570\u8c03\u63a7\u4e0d\u540c\u5206\u5e03\u60c5\u5f62\u3002\u5f15\u5165\u9a8c\u8bc1\u95e8\u63a7\u7684\u6df7\u5408\u7b56\u7565\uff0c\u5c06\u7ec4\u5408\u9884\u6d4b\u4f5c\u4e3a\u5148\u9a8c\u6ce8\u5165\u8f7b\u91cf\u795e\u7ecf\u6a21\u578b\uff1b\u4ec5\u5728\u9a8c\u8bc1\u63d0\u5347\u65f6\u518d\u8fdb\u884c\u795e\u7ecf\u5316\uff1b\u6240\u6709\u4fe1\u53f7\u6765\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\uff0c\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002", "result": "\u5728\u5f02\u8d28\u6027\u548c\u8fc7\u6e21\u6570\u636e\u96c6\u4e0a\u4e0e\u73b0\u4ee3GNNs\u7ade\u4e89\uff0c\u4e14\u5728\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8c03\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u517c\u5177\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u56fe\u540c\u8d28\u6027\u573a\u666f\uff0c\u4e14\u5177\u6709\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u4e0e\u66f4\u5f3a\u7684\u53ef\u63a7\u6027\uff1b\u901a\u8fc7\u6df7\u5408\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u8bc1\u8bc4\u4f30\u7684\u4e25\u683c\u6027\u3002"}}
{"id": "2512.23470", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23470", "abs": "https://arxiv.org/abs/2512.23470", "authors": ["Wenjun Jiang", "Xiaojun Yuan", "Chenchen Liu", "Boyu Teng"], "title": "Dynamic Channel Knowledge Map Construction in MIMO-OFDM Systems", "comment": null, "summary": "Channel knowledge map (CKM) is a promising paradigm for environment-aware communications by establishing a deterministic mapping between physical locations and channel parameters. Existing CKM construction methods focus on quasi-static propagation environment. This paper develops a dynamic CKM construction method for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems. We establish a dynamic channel model that captures the coexistence of quasi-static and dynamic scatterers, as well as the impacts of antenna rotation and synchronization errors. Based on this model, we formulate the problem of dynamic CKM construction within a Bayesian inference framework and design a two-stage approximate Bayesian inference algorithm. In stage I, a high-performance algorithm is developed to jointly infer quasi-static channel parameters and calibrate synchronization errors from historical measurements. In stage II, by leveraging the quasi-static parameters as informative priors, a low-complexity algorithm is designed to estimate dynamic parameters from limited real-time measurements. Simulation results validate the superiority of the proposed method and demonstrate its effectiveness in enabling low-overhead, high-performance channel estimation in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411MIMO-OFDM\u7684\u52a8\u6001CKM\u6784\u5efa\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e24\u9636\u6bb5\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u7ed3\u5408\u51c6\u9759\u6001/\u52a8\u6001\u6563\u5c04\u4f53\u3001\u5929\u7ebf\u65cb\u8f6c\u4e0e\u540c\u6b65\u8bef\u5dee\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u5f00\u9500\u3001\u9ad8\u6027\u80fd\u7684\u4fe1\u9053\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709CKM\u591a\u805a\u7126\u51c6\u9759\u6001\u4f20\u64ad\u73af\u5883\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u6563\u5c04\u4f53\u3001\u5929\u7ebf\u65cb\u8f6c\u4ee5\u53ca\u540c\u6b65\u8bef\u5dee\u7b49\u73b0\u5b9e\u56e0\u7d20\u7684\u5efa\u6a21\uff0c\u4e9f\u9700\u5728\u52a8\u6001\u573a\u666f\u4e2d\u4ecd\u80fd\u5b9e\u73b0\u73af\u5883\u611f\u77e5\u548c\u9ad8\u6548\u4fe1\u9053\u6620\u5c04\u7684\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u80fd\u591f\u540c\u65f6\u6355\u6349\u51c6\u9759\u6001\u4e0e\u52a8\u6001\u6563\u5c04\u4f53\u3001\u5929\u7ebf\u65cb\u8f6c\u548c\u540c\u6b65\u8bef\u5dee\u7684\u52a8\u6001\u4fe1\u9053\u6a21\u578b\uff1b\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8fd1\u4f3c\u63a8\u65ad\u7b97\u6cd5\u3002\u9636\u6bb5I\u5e76\u884c\u9ad8\u6548\u7b97\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u8054\u5408\u63a8\u65ad\u51c6\u9759\u6001\u4fe1\u9053\u53c2\u6570\u5e76\u6821\u51c6\u540c\u6b65\u8bef\u5dee\uff1b\u9636\u6bb5II\u4ee5\u51c6\u9759\u6001\u53c2\u6570\u4e3a\u4fe1\u606f\u5148\u9a8c\uff0c\u5229\u7528\u6709\u9650\u7684\u5b9e\u65f6\u89c2\u6d4b\u4f30\u8ba1\u52a8\u6001\u53c2\u6570\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u5f00\u9500\u7684\u9ad8\u6027\u80fd\u4fe1\u9053\u4f30\u8ba1\uff0c\u63d0\u5347CKM\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5728\u52a8\u6001CKM\u4e2d\u4f7f\u7528\u7684\u4e24\u9636\u6bb5\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\uff0c\u9002\u7528\u4e8eMIMO-OFDM\u7cfb\u7edf\uff0c\u63d0\u5347\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u73af\u5883\u611f\u77e5\u4e0e\u4fe1\u9053\u6620\u5c04\u7684\u51c6\u786e\u6027\u4e0e\u5b9e\u65f6\u6027\u3002"}}
{"id": "2512.22693", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22693", "abs": "https://arxiv.org/abs/2512.22693", "authors": ["Daiqi Zhang", "Bizhu Wang", "Wenqi Zhang", "Chen Sun", "Xiaodong Xu"], "title": "Instance Communication System for Intelligent Connected Vehicles: Bridging the Gap from Semantic to Instance-Level Transmission", "comment": "5 pages, 3 figures", "summary": "Intelligent Connected Vehicles (ICVs) rely on high-speed data transmission for efficient and safety-critical services. However, the scarcity of wireless resources limits the capabilities of ICVs. Semantic Communication (SemCom) systems can alleviate this issue by extracting and transmitting task-relevant information, termed semantic information, instead of the entire raw data. Despite this, we reveal that residual redundancy persists within SemCom systems, where not all instances under the same semantic category are equally critical for downstream tasks. To tackle this issue, we introduce Instance Communication (InsCom), which elevates communication from the semantic level to the instance level for ICVs. Specifically, InsCom uses a scene graph generation model to identify all image instances and analyze their inter-relationships, thus distinguishing between semantically identical instances. Additionally, it applies user-configurable, task-critical criteria based on subject semantics and relation-object pairs to filter recognized instances. Consequently, by transmitting only task-critical instances, InsCom significantly reduces data redundancy, substantially enhancing transmission efficiency within limited wireless resources. Evaluations across various datasets and wireless channel conditions show that InsCom achieves a data volume reduction of over 7.82 times and a quality improvement ranging from 1.75 to 14.03 dB compared to the state-of-the-art SemCom systems.", "AI": {"tldr": "\u63d0\u51fa Instance Communication (InsCom) \u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u4f20\u8f93\u66ff\u4ee3\u8bed\u4e49\u7ea7\u4f20\u8f93\uff0c\u5b9e\u73b0\u5bf9\u56fe\u50cf\u4e2d\u4efb\u52a1\u5173\u952e\u5b9e\u4f8b\u7684\u9009\u62e9\u6027\u4f20\u8f93\uff0c\u4e0e\u73b0\u6709 SemCom \u76f8\u6bd4\u53ef\u663e\u8457\u51cf\u5c11\u6570\u636e\u91cf\u5e76\u63d0\u5347\u4f20\u8f93\u8d28\u91cf\u3002", "motivation": "\u5728\u667a\u80fd\u5316\u8fde\u63a5\u8f66\u8f86\u573a\u666f\u4e2d\uff0c\u8702\u7a9d\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709 SemCom \u5728\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\u7684\u540c\u65f6\u4ecd\u5b58\u5728\u6b8b\u7559\u5197\u4f59\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4efb\u52a1\u76f8\u5173\u6027\u3002\u9700\u8fdb\u4e00\u6b65\u5c06\u4f20\u8f93\u7c92\u5ea6\u4e0b\u6c89\u5230\u5b9e\u4f8b\u7ea7\u4ee5\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u5168\u90e8\u5b9e\u4f8b\u53ca\u5176\u5173\u7cfb\uff0c\u533a\u5206\u8bed\u4e49\u4e0a\u76f8\u540c\u7684\u5b9e\u4f8b\uff1b\u57fa\u4e8e\u7528\u6237\u53ef\u914d\u7f6e\u7684\u4efb\u52a1\u5173\u952e\u6807\u51c6\uff08\u4e3b\u4f53\u8bed\u4e49\u3001\u5173\u7cfb-\u5bf9\u8c61\u5bf9\uff09\u7b5b\u9009\u8bc6\u522b\u51fa\u7684\u5b9e\u4f8b\uff0c\u6700\u7ec8\u4ec5\u4f20\u8f93\u8fd9\u4e9b\u4efb\u52a1\u5173\u952e\u5b9e\u4f8b\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u548c\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684 SemCom \u76f8\u6bd4\uff0cInsCom \u5b9e\u73b0\u6570\u636e\u91cf\u51cf\u5c11 >7.82x\uff0c\u8d28\u91cf\u63d0\u5347\u5728 1.75\u201314.03 dB \u4e4b\u95f4\u3002", "conclusion": "InsCom \u663e\u8457\u63d0\u5347\u4e86\u5728\u53d7\u9650\u65e0\u7ebf\u8d44\u6e90\u4e0b\u7684\u4f20\u8f93\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u9762\u5411\u5b9e\u4f8b\u7684\u9ad8\u6548\u901a\u4fe1\u8303\u5f0f\uff0c\u5e76\u4e3a ICV \u573a\u666f\u7684\u9ad8\u6548\u611f\u77e5\u2014\u7406\u89e3\u2014\u4f20\u8f93\u94fe\u8def\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2512.23186", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23186", "abs": "https://arxiv.org/abs/2512.23186", "authors": ["Yanbo Li", "Jinsong Li", "Zongjue Liu", "Riming Xu"], "title": "Multi-objective control strategy of Electro-Mechanical Transmission Based on Driving Pattern Division", "comment": "25pages 10figures", "summary": "Based on the driving requirement and power balance of heavy-duty vehicle equipped with Electro-Mechanical Transmission (EMT), optimization goals under different driving patterns are put forward. The optimization objectives are changed into a comprehensive optimization target based on the method of weighting, which is calculated by using analytic hierarchy process (AHP) under different working conditions. According to theory of Dynamic Programming (DP), a multi-object control strategy of DP under different driving patterns is proposed. This strategy is verified by simulation and contrasted with rule strategy, the results show that comprehensive performance is significantly enhanced, and the fuel economy is highly improved especially.", "AI": {"tldr": "\u57fa\u4e8e\u5bf9EMT\u91cd\u578b\u8f66\u7684\u9a71\u52a8\u9700\u6c42\u548c\u529f\u7387\u5e73\u8861\uff0c\u63d0\u51fa\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u7684\u7efc\u5408\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u901a\u8fc7AHP\u52a0\u6743\u5f97\u5230\u5355\u4e00\u76ee\u6807\uff0c\u518d\u5229\u7528DP\u5b9e\u73b0\u591a\u76ee\u6807\u63a7\u5236\u7b56\u7565\uff0c\u5bf9\u6bd4\u89c4\u5219\u7b56\u7565\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u7efc\u5408\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u71c3\u6cb9\u7ecf\u6d4e\u6027\u5c24\u5176\u660e\u663e\u6539\u5584\u3002", "motivation": "\u89e3\u51b3EMT\u91cd\u578b\u8f66\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u7684\u591a\u76ee\u6807\u4f18\u5316\u96be\u9898\uff0c\u63d0\u5347\u7efc\u5408\u6027\u80fd\u4e0e\u71c3\u6cb9\u7ecf\u6d4e\u6027\u3002", "method": "\u5c06\u591a\u76ee\u6807\u901a\u8fc7\u5c42\u6b21\u5206\u6790\u6cd5\uff08AHP\uff09\u8fdb\u884c\u52a0\u6743\uff0c\u5f97\u5230\u5728\u4e0d\u540c\u5de5\u4f5c\u6761\u4ef6\u4e0b\u7684\u7efc\u5408\u4f18\u5316\u76ee\u6807\uff1b\u5728\u52a8\u6001\u89c4\u5212\uff08DP\uff09\u6846\u67b6\u4e0b\u63d0\u51fa\u9002\u7528\u4e8e\u4e0d\u540c\u9a7e\u9a76\u6a21\u5f0f\u7684\u591a\u76ee\u6807\u63a7\u5236\u7b56\u7565\uff1b\u901a\u8fc7\u4eff\u771f\u4e0e\u89c4\u5219\u7b56\u7565\u5bf9\u6bd4\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u7efc\u5408\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u71c3\u6cb9\u7ecf\u6d4e\u6027\u6709\u660e\u663e\u6539\u5584\uff0c\u4f18\u4e8e\u89c4\u5219\u7b56\u7565\u3002", "conclusion": "\u5c06AHP\u52a0\u6743\u4e0eDP\u7ed3\u5408\u7684\u591a\u76ee\u6807\u63a7\u5236\u7b56\u7565\u5bf9EMT\u91cd\u578b\u8f66\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u6709\u6548\uff0c\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7684\u5b9e\u8f66\u9a8c\u8bc1\u4e0e\u9c81\u68d2\u6027\u8bc4\u4f30\u3002"}}
{"id": "2512.23506", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23506", "abs": "https://arxiv.org/abs/2512.23506", "authors": ["Shengsong Luo", "Ruilin Wu", "Chongbin Xu", "Junjie Ma", "Xiaojun Yuan", "Xin Wang"], "title": "Affine-Projection Recovery of Continuous Angular Power Spectrum: Geometry and Resolution", "comment": "6 pages, 1 figure", "summary": "This paper considers recovering a continuous angular power spectrum (APS) from the channel covariance. Building on the projection-onto-linear-variety (PLV) algorithm, an affine-projection approach introduced by Miretti \\emph{et. al.}, we analyze PLV in a well-defined \\emph{weighted} Fourier-domain to emphasize its geometric interpretability. This yields an explicit fixed-dimensional trigonometric-polynomial representation and a closed-form solution via a positive-definite matrix, which directly implies uniqueness. We further establish an exact energy identity that yields the APS reconstruction error and leads to a sharp identifiability/resolution characterization: PLV achieves perfect recovery if and only if the ground-truth APS lies in the identified trigonometric-polynomial subspace; otherwise it returns the minimum-energy APS among all covariance-consistent spectra.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22883", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22883", "abs": "https://arxiv.org/abs/2512.22883", "authors": ["Tao Li", "Quanyan Zhu"], "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations", "comment": null, "summary": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22825", "abs": "https://arxiv.org/abs/2512.22825", "authors": ["Ke Wang", "Chan-Tong Lam", "Benjamin K. Ng", "Yue Liu"], "title": "On the Impact of Phase Errors in Phase-Dependent Amplitudes of Near-Field RISs", "comment": "Accepted for publication in IEEE Transactions on Vehicular Technology, 2025, doi: 10.1109/TVT.2025.3647594", "summary": "This paper investigates mutual coupling between phase-dependent amplitudes (PDAs) and designed phase shifts within pixels of near-field (NF) reconfigurable intelligent surfaces (RISs) in the presence of phase errors (PEs). In contrast to existing research that treats phase shifts with errors (PSEs) and the PDAs separately, we introduce a remaining power (RP) metric to quantify the proportion of power preserved in the signals reflected by the RIS, and we prove its asymptotic convergence to theoretical values by leveraging extended Glivenko-Cantelli theorem. Then, the RP of signals passing through RIS pixels is jointly examined under combined phase and amplitude uncertainties. In addition, we propose four pixel reflection models to capture practical conditions, and we derive approximate polynomial upper bounds for the RP with error terms by applying Taylor expansion. Furthermore, based on Friis transmission formula and projected aperture, we propose a general NF channel model that incorporates the coupling between the PSEs and the PDAs. By using Cauchy-Bunyakovsky-Schwarz inequality and Riemann sums, we derive a closed-form upper bound on spectral efficiency, and the bound becomes tighter as the pixel area decreases. We reveal that as the RIS phase shifts approach the ends of their range, the RP under independent and identically distributed PEs is smaller than that under fully correlated PEs, whereas this relationship reverses when the phase shifts are near the middle of their range. Neglecting the PEs in the PDAs leads to an overestimation of the RIS performance gain, explaining the discrepancies between theoretical and measured results.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u8fd1\u573aRIS\u50cf\u7d20\u7684\u76f8\u4f4d\u504f\u5dee\u4e0e\u5e45\u5ea6\u8026\u5408\uff08PDAs\uff09\uff0c\u63d0\u51fa\u5269\u4f59\u529f\u7387RP\u5ea6\u91cf\uff0c\u7ed9\u51faRP\u5728\u8bef\u5dee\u4e0b\u7684\u4e0a\u754c\u4e0e\u6536\u655b\u6027\uff0c\u5e76\u7ed9\u51fa\u56db\u79cd\u50cf\u7d20\u53cd\u5c04\u6a21\u578b\uff0c\u63a8\u5bfcRP\u7684\u591a\u9879\u8fd1\u4f3c\u754c\uff0c\u4ee5\u53ca\u5bf9NF\u4fe1\u9053\u3001\u9891\u8c31\u6548\u7387\u7684\u4e0a\u754c\uff0c\u63ed\u793a\u76f8\u4f4d\u843d\u5728\u7aef\u70b9\u4e0e\u4e2d\u95f4\u65f6\u7684RP\u5dee\u5f02\uff0c\u4ee5\u53ca\u5ffd\u7565PE\u5bf9\u6027\u80fd\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5c06PSEs\u4e0ePDAs\u5206\u79bb\u3001\u672a\u8003\u8651PDAs\u4e0ePSE\u8026\u5408\u5bf9RIS\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5728\u50cf\u7d20\u5c3a\u5ea6\u3001\u76f8\u4f4d\u8bef\u5dee\u4e0e\u5e45\u5ea6\u968f\u673a\u6027\u4e0b\u5bf9\u4fe1\u9053\u4e0e\u5bb9\u91cf\u5206\u6790\u7684\u4e0d\u8db3\u3002", "method": "\u5efa\u7acbRP\u5ea6\u91cf\u5e76\u8bc1\u660e\u5176\u6e10\u8fd1\u6536\u655b\u6027\uff1b\u63d0\u51fa\u56db\u79cd\u50cf\u7d20\u53cd\u5c04\u6a21\u578b\uff1b\u5bf9RP\u7ed9\u51faTaylor\u5c55\u5f00\u7684\u591a\u9879\u5f0f\u4e0a\u754c\uff1b\u57fa\u4e8eFriis\u548c\u6295\u5f71\u5f00\u73af/\u5b54\u5f84\u63d0\u51faNF\u4fe1\u9053\u6a21\u578b\uff1b\u5229\u7528Cauchy-Schwarz\u4e0eRiemann\u6c42\u548c\u7ed9\u51fa\u9891\u8c31\u6548\u7387\u4e0a\u754c\uff0c\u5e76\u8bc1\u660e\u5f53\u50cf\u7d20\u9762\u79ef\u51cf\u5c0f\u65f6\u754c\u66f4\u7d27\u3002", "result": "\u5f97\u5230RP\u5728\u72ec\u7acb\u540c\u5206\u5e03P\u00c9s\u4e0e\u5b8c\u5168\u76f8\u5173P\u00c9s\u4e0b\u7684\u6bd4\u8f83\u7ed3\u8bba\u3001\u5e76\u5728\u76f8\u4f4d\u63a5\u8fd1\u7aef\u70b9\u4e0e\u4e2d\u70b9\u65f6\u5173\u7cfb\u4e0d\u540c\uff1b\u6307\u51fa\u5ffd\u7565PDAs\u7684PE\u4f1a\u5bfc\u81f4\u5bf9RIS\u589e\u76ca\u7684\u9ad8\u4f30\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86PDAs\u4e0ePSEs\u8026\u5408\u5bf9RIS\u6027\u80fd\u8bc4\u4f30\u7684\u5173\u952e\u6027\uff0c\u63d0\u4f9b\u4e86RP\u5ea6\u91cf\u3001\u50cf\u7d20\u6a21\u578b\u548c\u7406\u8bba\u754c\u9650\uff0c\u4e3aNF-RIS\u8bbe\u8ba1\u4e0e\u5206\u6790\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2512.23189", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23189", "abs": "https://arxiv.org/abs/2512.23189", "authors": ["Zelin Zang", "Yuhang Song", "Bingo Wing-Kuen Ling", "Aili Wang", "Fuji Yang"], "title": "The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design", "comment": null, "summary": "This survey provides a comprehensive overview of the integration of Generative AI and Agentic AI within the field of Digital Electronic Design Automation (EDA). The paper first reviews the paradigmatic evolution from traditional Computer-Aided Design (CAD) to AI-assisted EDA (AI4EDA), and finally to the emerging AI-Native and Agentic design paradigms. We detail the application of these paradigms across the digital chip design flow, including the construction of agentic cognitive architectures based on multimodal foundation models, frontend RTL code generation and intelligent verification, and backend physical design featuring algorithmic innovations and tool orchestration. We validate these methodologies through integrated case studies, demonstrating practical viability from microarchitecture definition to GDSII. Special emphasis is placed on the potential for cross-stage feedback loops where agents utilize backend PPA metrics to autonomously refine frontend logic. Furthermore, this survey delves into the dual-faceted impact on security, covering novel adversarial risks, automated vulnerability repair, and privacy-preserving infrastructure. Finally, the paper critically summarizes current challenges related to hallucinations, data scarcity, and black-box tools, and outlines future trends towards L4 autonomous chip design. Ultimately, this work aims to define the emerging field of Agentic EDA and provide a strategic roadmap for the transition from AI-assisted tools to fully autonomous design engineers.", "AI": {"tldr": "A survey on integrating Generative AI and Agentic AI in Digital EDA, tracing a shift from AI-assisted design (AI4EDA) to AI-native and agentic design, with multimodal agents spanning frontend RTL generation, verification, and backend physical design; supported by case studies from microarchitecture to GDSII and featuring cross-stage feedback loops using PPA metrics, while addressing security, hallucinations, data scarcity, and black-box issues, and outlining a roadmap toward L4 autonomous chip design.", "motivation": "To accelerate and automate digital chip design by leveraging AI and agentic reasoning across the entire EDA flow, enabling autonomous optimization, error repair, and security/privacy improvements, while identifying challenges and future directions in AI-Native/Agentic EDA.", "method": "A comprehensive literature survey and conceptual synthesis that traces the evolution from CAD to AI4EDA and AI-Native/Agentic paradigms, describes agentic cognitive architectures built on multimodal foundation models, surveys frontend activities (RTL generation, intelligent verification) and backend design (algorithms, tool orchestration), and presents integrated case studies from microarchitecture to GDSII; analyzes cross-stage feedback loops, security implications, and practical challenges.", "result": "Provides a unified framework and taxonomy for Agentic EDA, outlines architectural components and workflows (including feedback from backend PPA metrics to frontend logic), demonstrates viability through integrated case studies, and offers a strategic roadmap from AI-assisted tools to autonomous design engineers along with security and privacy considerations.", "conclusion": "Agentic EDA is an emergent field with high potential to redefine chip design workflows. The paper argues for a phased transition from AI-assisted to autonomous design, highlights key research challenges (hallucinations, data scarcity, black-box tools), and points to future trends toward L4 autonomous chip design and broader adoption of agentic cognitive architectures."}}
{"id": "2512.23045", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23045", "abs": "https://arxiv.org/abs/2512.23045", "authors": ["Vaibhav Kumar", "Anastasios Papazafeiropoulos", "Pandelis Kourtessis", "John Senior", "Marwa Chafii", "Dimitra I. Kaklamani", "Iakovos S. Venieris"], "title": "Flexible Intelligent Metasurface for Downlink Communications under Statistical CSI", "comment": "5 pages, 4 figures, accepted in IEEE WCL", "summary": "Flexible intelligent metasurface (FIM) is a recently developed, groundbreaking hardware technology with promising potential for 6G wireless systems. Unlike conventional rigid antenna array (RAA)-based transmitters, FIM-assisted transmitters can dynamically alter their physical surface through morphing, offering new degrees of freedom to enhance system performance. In this letter, we depart from prior works that rely on instantaneous channel state information (CSI) and instead address the problem of average sum spectral efficiency maximization under statistical CSI in a FIM-assisted downlink multiuser multiple-input single-output setting. To this end, we first derive the spatial correlation matrix for the FIM-aided transmitter and then propose an iterative FIM optimization algorithm based on the gradient projection method. Simulation results show that with statistical CSI, the FIM-aided system provides a significant performance gain over its RAA-based counterpart in scenarios with strong spatial channel correlation, whereas the gain diminishes when the channels are weakly correlated.", "AI": {"tldr": "\u5728\u7edf\u8ba1CSI\u4e0b\uff0c\u57fa\u4e8e\u67d4\u6027\u667a\u80fd\u8d85\u8868\u9762\uff08FIM\uff09\u7684\u4e0b\u884c\u591a\u7528\u6237MISO\u7cfb\u7edf\u5b9e\u73b0\u5e73\u5747\u603b\u9891\u8c31\u6548\u7387\u6700\u5927\u5316\u7684\u8fed\u4ee3\u4f18\u5316\uff1b\u4e0e\u5e38\u89c4\u521a\u6027\u5929\u7ebf\u9635\u5217\uff08RAA\uff09\u76f8\u6bd4\uff0c\u5728\u5f3a\u7a7a\u95f4\u76f8\u5173\u6027\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u5316\u57fa\u4e8e\u5bfc\u51fa\u7684FIM\u7a7a\u95f4\u76f8\u5173\u77e9\u9635\u7684\u68af\u5ea6\u6295\u5f71\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6587\u732e\u4f9d\u8d56\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u5c40\u9650\uff0c\u63a2\u7a76\u5728\u7edf\u8ba1CSI\u6761\u4ef6\u4e0b\uff0cFIM\u8f85\u52a9\u4f20\u8f93\u5728\u5efa\u7acb\u9762\u53ef\u53d8\u5f62\u7684\u81ea\u7531\u5ea6\u4e0b\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u63d0\u5347\uff0c\u7279\u522b\u662f\u57286G\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u548c\u8fb9\u754c\u3002", "method": "\u63a8\u5bfcFIM\u8f85\u52a9\u53d1\u5c04\u673a\u7684\u7a7a\u95f4\u76f8\u5173\u77e9\u9635\uff1b\u5728\u7edf\u8ba1CSI\u5047\u8bbe\u4e0b\uff0c\u5efa\u7acb\u4ee5\u5e73\u5747\u603b\u8c31\u6548\u7528\u6700\u5927\u5316\u4e3a\u76ee\u6807\u7684\u4f18\u5316\u95ee\u9898\uff1b\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u6295\u5f71\u7684\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u6765\u66f4\u65b0FIM\u5f62\u6001\u4ee5\u63d0\u5347\u6027\u80fd\uff1b\u573a\u666f\u4e3a\u4e0b\u884c\u591a\u7528\u6237MISO\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u7edf\u8ba1CSI\u6761\u4ef6\u4e0b\uff0cFIM\u7cfb\u7edf\u76f8\u8f83\u4e8eRAA\u5728\u5177\u6709\u5f3a\u7a7a\u95f4\u76f8\u5173\u6027\u7684\u573a\u666f\u4e2d\u80fd\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff1b\u5f53\u901a\u9053\u76f8\u5173\u6027\u8f83\u5f31\u65f6\uff0c\u4f18\u52bf\u51cf\u5f31\u751a\u81f3\u4e0d\u663e\u8457\u3002", "conclusion": "\u5728\u7edf\u8ba1CSI\u6761\u4ef6\u4e0b\uff0cFIM\u5177\u5907\u901a\u8fc7\u8868\u9762\u5f62\u53d8\u81ea\u7531\u5ea6\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4e14\u6536\u76ca\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u901a\u9053\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff1b\u8be5\u7814\u7a76\u8bc1\u660e\u4e86FIM\u57286G\u573a\u666f\u4e2d\u7684\u5e94\u7528\u524d\u666f\uff0c\u4f46\u9700\u5728\u5f31\u76f8\u5173\u6027\u573a\u666f\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u8fdb\u4e00\u6b65\u5b8c\u5584\u3002"}}
{"id": "2512.22840", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22840", "abs": "https://arxiv.org/abs/2512.22840", "authors": ["Haoyu Wang", "Zhi Sun", "Shuangfeng Han", "Xiaoyun Wang", "Zhaocheng Wang"], "title": "Generalizable Learning for Massive MIMO CSI Feedback in Unseen Environments", "comment": null, "summary": "Deep learning is promising to enhance the accuracy and reduce the overhead of channel state information (CSI) feedback, which can boost the capacity of frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) systems. Nevertheless, the generalizability of current deep learning-based CSI feedback algorithms cannot be guaranteed in unseen environments, which induces a high deployment cost. In this paper, the generalizability of deep learning-based CSI feedback is promoted with physics interpretation. Firstly, the distribution shift of the cluster-based channel is modeled, which comprises the multi-cluster structure and single-cluster response. Secondly, the physics-based distribution alignment is proposed to effectively address the distribution shift of the cluster-based channel, which comprises multi-cluster decoupling and fine-grained alignment. Thirdly, the efficiency and robustness of physics-based distribution alignment are enhanced. Explicitly, an efficient multi-cluster decoupling algorithm is proposed based on the Eckart-Young-Mirsky (EYM) theorem to support real-time CSI feedback. Meanwhile, a hybrid criterion to estimate the number of decoupled clusters is designed, which enhances the robustness against channel estimation error. Fourthly, environment-generalizable neural network for CSI feedback (EG-CsiNet) is proposed as a novel learning framework with physics-based distribution alignment. Based on extensive simulations and sim-to-real experiments in various conditions, the proposed EG-CsiNet can robustly reduce the generalization error by more than 3 dB compared to the state-of-the-arts.", "AI": {"tldr": "\u63d0\u51fa\u7269\u7406\u9a71\u52a8\u7684\u5206\u5e03\u5bf9\u9f50\u6846\u67b6EG-CsiNet\uff0c\u901a\u8fc7\u591a\u7c07\u4fe1\u9053\u5206\u5e03\u5efa\u6a21\u4e0eEYM\u5b9a\u7406\u7684\u591a\u7c07\u89e3\u8026\u5b9e\u73b0\u5bf9\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u63d0\u5347\u6cdb\u5316\u6027\u80fd>3 dB\u3002", "motivation": "\u9762\u5bf9\u672a\u89c1\u73af\u5883\u7684\u6cdb\u5316\u6311\u6218\uff0c\u9700\u5f15\u5165\u7269\u7406\u5148\u9a8c\u6765\u5bf9\u9f50\u7c07\u5206\u5e03\uff0c\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u3002", "method": "1) \u5efa\u6a21\u7c07\u72b6\u4fe1\u9053\u5206\u5e03\uff08\u591a\u7c07\u7ed3\u6784+\u5355\u7c07\u54cd\u5e94\uff09; 2) \u63d0\u51fa\u7269\u7406\u5206\u5e03\u5bf9\u9f50\uff08\u591a\u7c07\u89e3\u8026+\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff09; 3) \u57fa\u4e8eEYM\u5b9a\u7406\u7684\u9ad8\u6548\u591a\u7c07\u89e3\u8026\u7b97\u6cd5\uff1b 4) \u6df7\u5408\u51c6\u5219\u4f30\u8ba1\u89e3\u8026\u7c07\u6570\u91cf\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\uff1b 5) \u63d0\u51faEG-CsiNet\u6846\u67b6\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u4eff\u771f\u4e0esim-to-real\u5b9e\u9a8c\u4e2d\uff0cEG-CsiNet\u5728\u6cdb\u5316\u8bef\u5dee\u4e0a\u8f83\u73b0\u6709\u65b9\u6cd5\u63d0\u5347>3 dB\u3002", "conclusion": "\u7269\u7406\u5206\u5e03\u5bf9\u9f50\u878d\u5165\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u5bf9\u65b0\u73af\u5883\u7684\u9c81\u68d2\u6027\u4e0e\u5b9e\u65f6\u6027\u50a8\u5907\u3002"}}
{"id": "2512.23205", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23205", "abs": "https://arxiv.org/abs/2512.23205", "authors": ["Hamid Varmazyari", "Masoud H. Nazari"], "title": "A Learning-Driven Stochastic Hybrid System Framework for Detecting Unobservable Contingencies in Power Systems", "comment": null, "summary": "This paper presents a new learning based Stochastic Hybrid System (LSHS) framework designed for the detection and classification of contingencies in modern power systems. Unlike conventional monitoring schemes, the proposed approach is capable of identifying unobservable events that remain hidden from standard sensing infrastructures, such as undetected protection system malfunctions. The framework operates by analyzing deviations in system outputs and behaviors, which are then categorized into three groups: physical, control, and measurement contingencies based on their impact on the SHS model. The SHS model integrates both system dynamics and observer-driven state estimation error dynamics. Within this architecture, machine learning classifiers are employed to achieve rapid and accurate categorization of contingencies. The effectiveness of the method is demonstrated through simulations on the IEEE 5-bus and 30-bus systems, where results indicate substantial improvements in both detection speed and accuracy compared with existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u578b\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08LSHS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u60c5\u666f/\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u80fd\u591f\u8bc6\u522b\u6807\u51c6\u611f\u77e5\u4e0d\u5230\u7684\u672a\u89c2\u6d4b\u4e8b\u4ef6\uff0c\u5982\u4fdd\u62a4\u7cfb\u7edf\u6545\u969c\u7684\u8bef\u52a8\u4f5c\uff1b\u901a\u8fc7\u5206\u6790\u7cfb\u7edf\u8f93\u51fa\u4e0e\u884c\u4e3a\u504f\u5dee\uff0c\u5c06\u60c5\u666f\u5206\u4e3a\u7269\u7406\u3001\u63a7\u5236\u548c\u6d4b\u91cf\u4e09\u7c7b\uff1b\u5728 SHS \u6784\u67b6\u4e2d\u6574\u5408\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0e\u89c2\u6d4b\u8bef\u5dee\u52a8\u529b\u5b66\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5b9e\u73b0\u5feb\u901f\u5206\u7c7b\uff1b\u5728IEEE 5-bus\u548c30-bus\u7cfb\u7edf\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\u68c0\u6d4b\u901f\u5ea6\u4e0e\u51c6\u786e\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347\u5bf9\u7535\u529b\u7cfb\u7edf\u4e2d\u6f5c\u5728\u5f02\u5e38\u548c\u672a\u89c2\u6d4b\u4e8b\u4ef6\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u514b\u670d\u4f20\u7edf\u76d1\u63a7\u5bf9\u67d0\u4e9b\u4fdd\u62a4\u529f\u80fd\u5931\u6548\u7b49\u60c5\u5f62\u7684\u76f2\u70b9\uff1b\u9700\u8981\u7ed3\u5408\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0e\u89c2\u6d4b\u8bef\u5dee\u5206\u6790\u4ee5\u5b9e\u73b0\u5feb\u901f\u3001\u9c81\u68d2\u7684\u60c5\u666f\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08LSHS\uff09\u6846\u67b6\uff0c\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0e\u89c2\u6d4b\u5668\u8bef\u5dee\u52a8\u529b\u5b66\u6574\u5408\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5c06\u68c0\u6d4b\u5230\u7684\u504f\u5dee\u4fe1\u53f7\u5f52\u7c7b\u4e3a\u7269\u7406\u3001\u63a7\u5236\u3001\u6d4b\u91cf\u4e09\u7c7b\u60c5\u666f\uff1b\u901a\u8fc7\u4eff\u771f\u5728IEEE 5-bus\u300130-bus \u7cfb\u7edf\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u68c0\u6d4b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u8bc6\u522b\u672a\u89c2\u6d4b\u4e8b\u4ef6\u5e76\u63d0\u4f9b\u5feb\u901f\u3001\u51c6\u786e\u7684\u60c5\u666f\u5206\u7c7b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u5b66\u4e60\u80fd\u529b\u4e0e SHS \u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u5bf9 contingencies \u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\u80fd\u529b\uff0c\u5c24\u5176\u5bf9\u4e0d\u53ef\u89c2\u6d4b\u4e8b\u4ef6\u5177\u6709\u6f5c\u5728\u7684\u9c81\u68d2\u6027\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.22234", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22234", "abs": "https://arxiv.org/abs/2512.22234", "authors": ["Ying Zhu", "Jiaxin Wan", "Xiaoran Liu", "Siyanag He", "Qiqi Wang", "Xu Guo", "Tianyi Liang", "Zengfeng Huang", "Ziwei He", "Xipeng Qiu"], "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models", "comment": null, "summary": "Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.", "AI": {"tldr": "\u63d0\u51fa DiRL \u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408 FlexAttention \u7684\u5206\u5757\u5f0f\u8bad\u7ec3\u548c LMDeploy \u7684\u63a8\u7406\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e24\u9636\u6bb5\u5fae\u8c03\uff08\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u5728\u6b64\u6846\u67b6\u4e0a\u63d0\u51fa\u7528\u4e8e dLLMs \u7684\u65e0\u504f\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684 DiPO\u3002\u5b9e\u8bc1\u8868\u660e DiRL-8B-Instruct \u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230 dLLMs \u7684\u6700\u65b0\u6c34\u5e73\uff0c\u5e76\u5728\u82e5\u5e72\u57fa\u51c6\u4e0a\u8d85\u8fc7 Qwen2.5 \u7cfb\u5217\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3 diffusion language models \u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u8ba1\u7b97\u4f4e\u6548\u548c\u8bad\u7ec3\u76ee\u6807\u4e0e\u63a8\u7406\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u590d\u6742\u63a8\u7406\uff08\u5c24\u5176\u662f\u6570\u5b66\u4efb\u52a1\uff09\u7684\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6548\u7387\u4e0e\u5bf9\u9f50\u5ea6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u518d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5728\u6b64\u4e4b\u4e0a\u6784\u5efa DiRL \u6846\u67b6\uff0c\u5c06 FlexAttention \u52a0\u901f\u7684\u5206\u5757\u8bad\u7ec3\u4e0e LMDeploy \u4f18\u5316\u7684\u63a8\u7406\u9ad8\u6548\u8026\u5408\uff0c\u5f62\u6210\u9ad8\u6548\u5728\u7ebf\u6a21\u578b\u66f4\u65b0\u5faa\u73af\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u63d0\u51fa DiPO\uff0c\u8fd9\u662f\u9996\u4e2a\u4e3a dLLMs \u5b9a\u5236\u7684\u65e0\u504f\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5b9e\u73b0\u3002", "result": "\u5728\u9ad8\u8d28\u91cf\u6570\u5b66\u6570\u636e\u4e0a\u8bad\u7ec3\u51fa DiRL-8B-Instruct\uff1b\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u540c\u7c7b\u4e2d\u5bf9 dLLMs \u7684\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5728\u82e5\u5e72\u57fa\u51c6\u4e0a\u8d85\u8d8a Qwen2.5 \u7cfb\u5217\u7684\u53ef\u6bd4\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 DiRL \u6846\u67b6\u4e0e DiPO \u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86 dLLMs \u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6548\u7387\u4e0e\u5bf9\u9f50\u5ea6\uff0c\u4e3a diffusion \u7c7b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u8def\u5f84\u3002"}}
{"id": "2512.23075", "categories": ["cs.LG", "cs.AI", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23075", "abs": "https://arxiv.org/abs/2512.23075", "authors": ["Yingru Li", "Jiacai Liu", "Jiawei Xu", "Yuxuan Tong", "Ziniu Li", "Baoxiang Wang"], "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning", "comment": null, "summary": "Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $\u03c0_{\\text{roll}}$. When $\u03c0_{\\text{roll}} \\ne \u03c0_\u03b8$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9LLM-RL\u4e2d\u8bad\u7ec3\u7b56\u7565\u4e0e rollout \u7b56\u7565\u4e0d\u4e00\u81f4\u7684 off-policy \u8bef\u5dee\uff0c\u7ed9\u51fa\u4e24\u79cd\u66f4\u7d27\u7684\u754c\u9650\uff1aPinsker-Marginal bound \u548c Mixed bound\uff0c\u5206\u522b\u968f\u5e8f\u5217\u957f\u5ea6 T \u7684\u589e\u957f\u800c\u8fbe\u5230 O(T^{3/2}) \u548c O(T)\uff0c\u5e76\u5f15\u5165 Trust Region Masking (TRM) \u5bf9\u4efb\u4f55 token \u7b26\u5408 trust region \u7684\u5e8f\u5217\u8fdb\u884c\u5c4f\u853d\uff0c\u4ece\u800c\u5728\u957f\u65f6\u5e8f\u4e0b\u7ed9\u51fa\u975e\u7a7a\u7684\u5355\u8c03\u6539\u8fdb\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e surrogate objective \u7684\u68af\u5ea6\u4f18\u5316\u5728 \u03c0_roll \u4e0e \u03c0_\u03b8 \u4e0d\u4e00\u81f4\u65f6\u5b58\u5728\u8fd1\u4f3c\u8bef\u5dee\uff0c\u4e14\u5728\u73b0\u4ee3 LLM-RL \u4e2d\u56e0\u5b9e\u73b0\u5dee\u5f02\u3001\u4e13\u5bb6\u8def\u7531\u4e0d\u8fde\u7eed\u6027\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u5ef6\u8fdf\u7b49\u56e0\u7d20\uff0c\u8fd9\u79cd\u79bb\u7b56\u7565\u5dee\u5f02\u65e0\u6cd5\u907f\u514d\u3002\u4f20\u7edf\u4fe1\u4efb\u57df\u754c\u9650\u968f T \u4e58\u65b9\u589e\u957f\u4e14\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u65e0\u6548\u3002\u9700\u8981\u8003\u8651\u5e8f\u5217\u7ea7\u7684 KL \u53d1\u6563\uff0c\u7ed9\u51fa\u66f4\u7d27\u7684\u754c\u9650\u5e76\u5b9e\u73b0\u53ef\u5355\u8c03\u6539\u8fdb\u7684\u7b56\u7565\u3002", "method": "\u63a8\u5bfc\u4e24\u79cd\u65b0\u7684\u754c\u9650\uff1aPinsker-Marginal bound\uff08O(T^{3/2})\uff09\u548c Mixed bound\uff08O(T)\uff09\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6700\u5927 token-level KL \u53d1\u6563 D_kl^{tok,max}\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e8f\u5217\u7ea7\u91cf\u3002\u63d0\u51fa TRM\uff1a\u5982\u679c\u4efb\u4e00 token \u8fdd\u80cc\u4fe1\u4efb\u533a\u57df\uff0c\u6574\u6761\u5e8f\u5217\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u6392\u9664\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u957f\u65f6\u5e8f LLM-RL \u7684\u975e vacuous \u5355\u8c03\u6539\u8fdb\u4fdd\u969c\u3002", "result": "\u5f97\u5230\u7684\u754c\u9650\u5206\u522b\u4e3a O(T^{3/2}) \u4e0e O(T)\uff0c\u5e76\u9996\u6b21\u7ed9\u51fa\u5bf9\u957f\u65f6\u5e8f LLM-RL \u7684\u975e\u7a7a\u5355\u8c03\u6539\u8fdb\u4fdd\u8bc1\uff0cTRM \u80fd\u5728\u4e0d\u727a\u7272\u592a\u591a\u6837\u672c\u7684\u524d\u63d0\u4e0b\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "\u5e8f\u5217\u7ea7\u7684 KL \u7ea6\u675f\u4e0d\u80fd\u88ab\u5355\u7eaf\u7684 token \u65e0\u5173\u526a\u5207\u76f4\u63a5\u63a7\u5236\uff0c\u9700\u5f15\u5165\u5bf9\u6574\u4e2a\u5e8f\u5217\u7684\u7ea6\u675f\u4e0e\u5c4f\u853d\u673a\u5236\uff0cTRM \u4e3a\u5b9e\u73b0\u957f\u65f6\u5e8f LLM-RL \u7684\u7a33\u5065\u6027\u4e0e\u5355\u8c03\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2512.23124", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23124", "abs": "https://arxiv.org/abs/2512.23124", "authors": ["Paulo Fernandes Biao"], "title": "SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems", "comment": "10 pages, 12 figures. Research article proposing a financially-aware Zero Trust architecture for banking systems with simulation-based evaluation", "summary": "Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.\n  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).\n  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.", "AI": {"tldr": "SecureBank is a finance-aware Zero Trust framework for high-assurance banking that combines Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation, evaluated via Monte Carlo against a rule-based baseline using TII, ITAL, and SAE metrics, showing improved automated attack handling and faster identity trust adaptation while preserving transactional integrity; intended as a reference architecture for regulated financial environments.", "motivation": "Traditional perimeter-based security struggles in modern finance due to open banking APIs, cloud-native infrastructures, and high-frequency transactions, which expand the attack surface. There's a need to explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, and economics-aware automation.", "method": "Proposes SecureBank architecture integrating four components: Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. Uses Monte Carlo simulation to compare against a rule-based baseline architecture, evaluating with metrics such as Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).", "result": "Monte Carlo simulations indicate SecureBank significantly improves automated attack handling and accelerates identity trust adaptation, while maintaining conservative, regulator-aligned levels of transactional integrity compared to the baseline.", "conclusion": "SecureBank can serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments."}}
{"id": "2512.22926", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22926", "abs": "https://arxiv.org/abs/2512.22926", "authors": ["Dongli Cai", "Xihe Chen", "Yaosheng Chen", "Hong Xian", "Baoxian Yu", "Han Zhang"], "title": "Confidence analysis-based hybrid heartbeat detection for ballistocardiogram using template matching and deep learning", "comment": null, "summary": "Heartbeat interval can be detected from ballistocardiogram (BCG) signals in a non-contact manner. Conventional methods achieved heartbeat detection from different perspectives, where template matching (TM) and deep learning (DL) were based on the similarity of neighboring heartbeat episodes and robust spatio-temporal characteristics, respectively, and thus, performed varied from case to case. Inspired by the above facts, we propose confidence analysis-based hybrid heartbeat detection using both TM and DL, and further explore the advantages of both methods in various scenarios. To be specific, the confidence of the heartbeat detection results was evaluated by the consistency of signal morphology and the variability of the detected heartbeat intervals, which could be formulated by the averaged correlation between each heartbeat episode and the detected template and the normalized standard deviation among detected heartbeat intervals, respectively, where the results with higher confidence were remained. In order to validate the effectiveness of the proposed hybrid method, we conducted experiments using practical clinical BCG dataset with 34 subjects including 924,235 heartbeats. Numerical results showed that the proposed hybrid method achieved an average absolute interval error of 20.73 ms, yielding a reduction of 29.28 ms and 10.13 ms compared to solo TM and DL methods, respectively. Besides, case study showed the robustness of heartbeat detection of TM and DL to individual differences and signal quality, respectively, and in turn, validated that the hybrid method could benefit from the complementary advantages of both methods, which demonstrated the superiority of the proposed hybrid method in practical BCG monitoring scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5206\u6790\u7684TM\u4e0eDL\u6df7\u5408\u5fc3\u8df3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u63a5\u89e6\u6027BCG\u4fe1\u53f7\u3002\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u7b5b\u9009\u5b9e\u73b0\u4e24\u8005\u4e92\u8865\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff1b\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u660e\u663e\u4f18\u4e8e\u5355\u72ecTM\u6216DL\u3002", "motivation": "TM\u4e0eDL\u5404\u81ea\u5b58\u5728\u5bf9\u4e2a\u4f53\u5dee\u5f02\u4e0e\u4fe1\u53f7\u8d28\u91cf\u7684\u654f\u611f\u6027\uff0c\u5355\u4e00\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6027\u80fd\u6ce2\u52a8\u3002\u9700\u8981\u4e00\u4e2a\u7ed3\u5408\u4e24\u8005\u4f18\u70b9\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u7b5b\u9009\u7684\u6df7\u5408\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u5fc3\u8df3\u95f4\u671f\u68c0\u6d4b\u3002", "method": "\u5c06TM\u4e0eDL\u7684\u8f93\u51fa\u8fdb\u884c\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6790\u7b5b\u9009\u7ed3\u679c\u3002\u7f6e\u4fe1\u5ea6\u7531\u4e24\u90e8\u5206\u6784\u6210\uff1a1) \u5fc3\u8df3\u7247\u6bb5\u4e0e\u68c0\u6d4b\u6a21\u677f\u7684\u5e73\u5747\u76f8\u5173\u6027\uff0c\u7528\u4ee5\u8861\u91cf\u4fe1\u53f7\u5f62\u6001\u7684\u4e00\u81f4\u6027\uff1b2) \u5df2\u68c0\u6d4b\u5fc3\u8df3\u95f4\u671f\u7684\u5f52\u4e00\u5316\u6807\u51c6\u5dee\uff0c\u7528\u4ee5\u8861\u91cf\u95f4\u671f\u53d8\u5f02\u6027\u3002\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5229\u7528TM\u7684\u7a33\u5065\u6027\u4e0eDL\u7684\u9c81\u68d2\u6027\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u5728\u5305\u542b34\u540d\u53d7\u8bd5\u8005\u3001\u5171924,235\u4e2a\u5fc3\u8df3\u7684\u4e34\u5e8aBCG\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002\u6df7\u5408\u65b9\u6cd5\u7684\u5e73\u5747\u7edd\u5bf9\u95f4\u671f\u8bef\u5dee\u4e3a20.73 ms\uff0c\u76f8\u8f83\u5355\u72ecTM\u4e0eDL\u5206\u522b\u964d\u4f4e\u4e8629.28 ms\u548c10.13 ms\u3002\u6848\u4f8b\u7814\u7a76\u8fd8\u663e\u793aTM\u5bf9\u4e2a\u4f53\u5dee\u5f02\u548cDL\u5bf9\u4fe1\u53f7\u8d28\u91cf\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u8bc1\u660e\u6df7\u5408\u7b56\u7565\u5728\u5b9e\u9645BCG\u76d1\u6d4b\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5206\u6790\u7684TM+DL\u6df7\u5408\u5fc3\u8df3\u68c0\u6d4b\u5728\u975e\u63a5\u89e6BCG\u76d1\u6d4b\u4e2d\u5c55\u73b0\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u9645\u573a\u666f\uff0c\u80fd\u6709\u6548\u514b\u670d\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.23284", "categories": ["eess.SY", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.23284", "abs": "https://arxiv.org/abs/2512.23284", "authors": ["Mahdi Kchaou", "Francesco Contino", "Diederik Coppitters"], "title": "Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning", "comment": null, "summary": "Given the central role of green e-molecule imports in the European energy transition, many studies optimize import pathways and identify a single cost-optimal solution. However, cost optimality is fragile, as real-world implementation depends on regulatory, spatial, and stakeholder constraints that are difficult to represent in optimization models and can render cost-optimal designs infeasible. To address this limitation, we generate a diverse set of near-cost-optimal alternatives within an acceptable cost margin using Modeling to Generate Alternatives, accounting for unmodeled uncertainties. Interpretable machine learning is then applied to extract insights from the resulting solution space. The approach is applied to hydrogen import pathways considering hydrogen, ammonia, methane, and methanol as carriers. Results reveal a broad near-optimal space with great flexibility: solar, wind, and storage are not strictly required to remain within 10% of the cost optimum. Wind constraints favor solar-storage methanol pathways, while limited storage favors wind-based ammonia or methane pathways.", "AI": {"tldr": "\u63d0\u51fa\u5728\u6210\u672c\u5bb9\u5fcd\u5ea6\u5185\u751f\u6210\u8fd1\u6210\u672c\u6700\u4f18\u89e3\u7684\u591a\u65b9\u6848\u5206\u6790\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u4ece\u89e3\u7a7a\u95f4\u63d0\u53d6\u6d1e\u89c1\uff0c\u5e94\u7528\u4e8e\u6c22\u8fdb\u53e3\u8def\u5f84\u7684\u591a\u8f7d\u4f53\u573a\u666f\uff0c\u63ed\u793a\u8fd1\u6700\u4f18\u89e3\u7684\u5e7f\u9614\u7a7a\u95f4\u4e0e\u5173\u952e\u6743\u8861\u3002", "motivation": "\u5355\u4e00\u6210\u672c\u6700\u4f18\u89e3\u5728\u73b0\u5b9e\u7ea6\u675f\uff08\u76d1\u7ba1\u3001\u7a7a\u95f4\u3001\u5229\u76ca\u76f8\u5173\u8005\u7b49\uff09\u4e0b\u5f80\u5f80\u4e0d\u53ef\u884c\uff1b\u9700\u8981\u5728\u53ef\u63a5\u53d7\u6210\u672c\u8303\u56f4\u5185\u63a2\u7d22\u591a\u6837\u5316\u65b9\u6848\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u4e0e\u7075\u6d3b\u6027\u3002", "method": "1) \u91c7\u7528\u2018\u5efa\u6a21\u751f\u6210\u66ff\u4ee3\u65b9\u6848\u2019\uff08Modeling to Generate Alternatives\uff09\u5728\u7ed9\u5b9a\u6210\u672c\u4e0a\u9650\u5185\u4ea7\u751f\u5927\u91cf\u8fd1\u4f3c\u6700\u4f18\u89e3\uff1b2) \u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u4ece\u89e3\u7a7a\u95f4\u63d0\u53d6\u6d1e\u89c1\uff1b3) \u5c06\u65b9\u6cd5\u5e94\u7528\u4e8e\u6c22\u8fdb\u53e3\u8def\u5f84\uff0c\u8003\u8651\u8f7d\u4f53\u5305\u62ec\u6c22\u3001\u6c28\u3001\u7532\u70f7\u3001\u7532\u9187\u3002", "result": "\u63ed\u793a\u5e7f\u9614\u7684\u8fd1\u6700\u4f18\u89e3\u7a7a\u95f4\uff1a\u592a\u9633\u80fd\u3001\u98ce\u80fd\u4e0e\u50a8\u80fd\u5e76\u975e\u5fc5\u987b\u4e25\u683c\u7ef4\u6301\u5728\u6210\u672c\u6700\u4f18\u524d10%\uff1b\u98ce\u7684\u7ea6\u675f\u6761\u4ef6\u504f\u597d\u592a\u9633-\u50a8\u80fd-\u7532\u9187\u7b49\u8def\u5f84\uff1b\u6709\u9650\u50a8\u80fd\u60c5\u5f62\u503e\u5411\u4e8e\u98ce\u80fd\u9a71\u52a8\u7684\u6c28\u6216\u7532\u70f7\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63ed\u793a\u8bbe\u8ba1\u9009\u62e9\u7684\u9c81\u68d2\u6027\u4e0e\u8def\u5f84\u591a\u6837\u6027\uff0c\u5f3a\u8c03\u5728\u80fd\u6e90\u8fdb\u53e3\u7b56\u7565\u8bbe\u8ba1\u4e2d\u5e94\u7eb3\u5165\u8fd1\u6700\u4f18\u89e3\u96c6\u53ca\u76f8\u5173\u7ea6\u675f\u4ee5\u63d0\u5347\u7cfb\u7edf\u7075\u6d3b\u6027\u4e0e\u5b9e\u73b0\u53ef\u6301\u7eed\u6027\u76ee\u6807\u3002"}}
{"id": "2512.22238", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22238", "abs": "https://arxiv.org/abs/2512.22238", "authors": ["Byung-Kwan Lee", "Yu-Chiang Frank Wang", "Ryo Hachiuma"], "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models", "comment": null, "summary": "Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.", "AI": {"tldr": "\u63d0\u51fa Masters \u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u853d\u4e0e\u5206\u9636\u6bb5\u6269\u5c55\u6559\u5e08\u5bb9\u91cf\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5bf9\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u84b8\u998f\uff0c\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u4e0e\u5b66\u4e60\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u6559\u5e08-\u5b66\u751f\u4e4b\u95f4\u5de8\u5927\u5bb9\u91cf\u5dee\u8ddd\u6240\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u84b8\u998f\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff1b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u90e8\u7f72\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u84b8\u998f\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u5b66\u4e60\u7b56\u7565\u3002", "method": "1) \u5bf9\u6559\u5e08\u6743\u91cd\u8fdb\u884c\u63a9\u853d\u4ee5\u53bb\u9664\u975e\u4e3b\u5bfc\u6743\u91cd\uff0c\u964d\u4f4e\u590d\u6742\u6027\uff1b2) \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u6062\u590d\u6559\u5e08\u5bb9\u91cf\uff0c\u5206\u9636\u6bb5\u63d0\u5347\u6559\u5e08\u80fd\u529b\u4ee5\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u4f20\u9012\uff1b3) \u5f15\u5165\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u91c7\u7528\u4e24\u7c7b\u5956\u52b1\uff1a\u51c6\u786e\u6027\u5956\u52b1\u4e0e\u84b8\u998f\u5956\u52b1\uff0c\u5e76\u5229\u7528\u63a9\u853d\u6559\u5e08\u7684\u9884\u751f\u6210\u54cd\u5e94\u6765\u6307\u5bfc\u5b66\u751f\uff0c\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u5728\u7ebf think-answer \u6d41\u7a0b\u3002", "result": "\u8be5\u6846\u67b6\u65e8\u5728\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u84b8\u998f\u8fc7\u7a0b\u5e76\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c\u5728\u65e0\u5728\u7ebf\u601d\u8003-\u56de\u7b54\u8fc7\u7a0b\u4e0b\u901a\u8fc7\u79bb\u7ebf\u5956\u52b1\u9a71\u52a8\u5b66\u4e60\uff0c\u9884\u8ba1\u6bd4\u4f20\u7edf\u84b8\u998f\u5728\u7a33\u5b9a\u6027\u4e0e\u8868\u793a\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u66f4\u4f18\u3002", "conclusion": "Masters \u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u63a9\u853d-\u6269\u5c55\u7684\u6559\u5e08\u7b56\u7565\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u5bf9\u5927\u6559\u5e08\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u6700\u7ec8\u8868\u73b0\u3002"}}
{"id": "2512.22240", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22240", "abs": "https://arxiv.org/abs/2512.22240", "authors": ["Chama Bensmail"], "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs", "comment": null, "summary": "Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.", "AI": {"tldr": "EvoXplain: a diagnostic framework that assesses the stability of model explanations across repeated training to reveal multimodal explanatory mechanisms, challenging the idea of a single correct explanation and reframing interpretability as a property of a model class under repeated instantiation.", "motivation": "High predictive accuracy does not guarantee a unique or trustworthy explanation. Two high-performing models may rely on different internal logic; understanding whether explanations are stable or multimodal is crucial for trustworthy ML.", "method": "Treat explanations as samples drawn from the stochastic optimization process during training, without aggregating predictions or ensembling. Analyze whether these explanation samples form a single coherent mode or split into multiple distinct explanatory basins across repeated runs.", "result": "On Breast Cancer and COMPAS datasets using Logistic Regression and Random Forests, explanations exhibit clear multimodality; even models like Logistic Regression can produce multiple well-separated explanatory basins under repeated training on the same data split. Differences are not explained by hyperparameter variation or simple performance trade-offs.", "conclusion": "EvoXplain does not select a single 'correct' explanation; it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure multiple underlying mechanisms. Interpretability is reframed as a property of the model class under repeated instantiation rather than of a single trained model."}}
{"id": "2512.23171", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23171", "abs": "https://arxiv.org/abs/2512.23171", "authors": ["Yu Jiang", "Xindi Tong", "Ziyao Liu", "Xiaoxi Zhang", "Kwok-Yan Lam", "Chee Wei Tan"], "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning", "comment": "Published in the IEEE Transactions on Information Forensics and Security", "summary": "Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the \"right to be forgotten.\" While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.", "AI": {"tldr": "FedORA \u5728\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u6837\u672c\u4e0e\u6807\u7b7e\u7684\u53bb\u5b66\u4e60\uff0c\u901a\u8fc7\u539f/\u5bf9\u5076\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u8de8\u65b9\u6570\u636e\u5220\u9664\u95ee\u9898\uff0c\u63d0\u51fa\u4ee5\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u4e3a\u5bfc\u5411\u7684\u53bb\u5b66\u4e60\u635f\u5931\u3001\u81ea\u9002\u5e94\u6b65\u957f\u4e0e\u975e\u5bf9\u79f0\u6279\u6b21\u8bbe\u8ba1\u4ee5\u964d\u4f4e\u5f00\u9500\uff0c\u7406\u8bba\u4e0a\u6709\u754c\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u5dee\u5f02\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u8868\u683c\u4e0e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u53ef\u5b9e\u73b0\u4e0e\u4ece\u5934\u8bad\u7ec3\u76f8\u8fd1\u7684\u6548\u679c\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u4e0e\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u53bb\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\uff1a\u4e0d\u540c\u53c2\u4e0e\u65b9\u6301\u6709\u6837\u672c\u7684\u4e92\u8865\u7279\u5f81\uff0c\u9700\u8981\u8de8\u65b9\u534f\u4f5c\u6765\u5220\u9664\u7279\u5b9a\u6837\u672c\u6216\u6807\u7b7e\uff1b\u76ee\u6807\u662f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u4e0e\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u53bb\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5c06\u6837\u672c/\u6807\u7b7e\u7684\u5220\u9664\u8f6c\u5316\u4e3a\u53d7\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528 primal-dual \u6846\u67b6\u6c42\u89e3\uff1b\u5f15\u5165\u65b0\u7684\u53bb\u5b66\u4e60\u635f\u5931\u4ee5\u4fc3\u8fdb\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u9519\u8bef\u5206\u7c7b\uff1b\u4f7f\u7528\u81ea\u9002\u5e94\u6b65\u957f\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\uff1b\u8bbe\u8ba1\u975e\u5bf9\u79f0\u6279\u6b21\u4ee5\u8003\u8651\u5269\u4f59\u6570\u636e\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u5e76\u964d\u4f4e\u5f00\u9500\uff1b\u5b9e\u73b0\u8de8\u65b9\u534f\u8c03\u4ee5\u5904\u7406\u7279\u5f81\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e FedORA \u4e0e\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u6709\u754c\uff1b\u5728\u8868\u683c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedORA \u5728\u53bb\u5b66\u4e60\u6548\u679c\u4e0e\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u65b9\u9762\u4e0e\u4ece\u5934\u8bad\u7ec3\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "FedORA \u4e3a\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u53bb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5177\u5907\u5bf9\u6837\u672c\u4e0e\u6807\u7b7e\u53bb\u5b66\u4e60\u7684\u9002\u7528\u6027\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.23420", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23420", "abs": "https://arxiv.org/abs/2512.23420", "authors": ["Antika Yadav", "Prasad Vilas Chanekar"], "title": "Control Co-design of systems with parabolic partial differential equation dynamics", "comment": null, "summary": "In this paper we study the control co-design (CCD) synthesis problem for a class of systems with parabolic partial differential equation (PDE) dynamics. We formulate CCD problem and finally derive an approximate CCD problem with matrix algebraic constraint. We then solve this approximate problem with gradient-based method and prove that the optimal solution also stabilizes the PDE system. We justify approach through numerical examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5e26\u629b\u7269\u578bPDE\u52a8\u529b\u5b66\u7684\u63a7\u5236\u5171\u8bbe\u8ba1CCD\u95ee\u9898\uff0c\u7ed9\u51fa\u4e00\u4e2a\u5e26\u77e9\u9635\u4ee3\u6570\u7ea6\u675f\u7684\u8fd1\u4f3c\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u68af\u5ea6\u6cd5\u6c42\u89e3\u5e76\u8bc1\u660e\u6700\u4f18\u89e3\u80fd\u7a33\u5b9a\u88ab\u63a7PDE\u7cfb\u7edf\uff0c\u4e14\u6570\u503c\u5b9e\u4f8b\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u53c2\u6570\u7cfb\u7edf\u4e2d\uff0c\u540c\u65f6\u5bf9\u63a7\u5236\u5668\u548c\u7cfb\u7edf\u53c2\u6570\u8fdb\u884c\u534f\u540c\u4f18\u5316\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\uff0c\u4f46\u76f4\u63a5\u6c42\u89e3CCD\u95ee\u9898\u8ba1\u7b97\u590d\u6742\u3002\u901a\u8fc7\u5c06\u95ee\u9898\u8fd1\u4f3c\u4e3a\u5e26\u77e9\u9635\u4ee3\u6570\u7ea6\u675f\u7684\u53ef\u89e3\u5f62\u5f0f\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u63d0\u4f9b\u5b9e\u73b0\u9014\u5f84\u3002", "method": "\u5c06CCD\u95ee\u9898\u79bb\u6563\u5316\u5e76\u8f6c\u5316\u4e3a\u5e26\u77e9\u9635\u4ee3\u6570\u7ea6\u675f\u7684\u8fd1\u4f3c\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u68af\u5ea6\u4e0b\u964d/\u68af\u5ea6\u57fa\u65b9\u6cd5\u6c42\u89e3\uff0c\u7ed9\u51fa\u7a33\u5b9a\u6027\u8bc1\u660e\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u8fd1\u4f3cCCD\u6c42\u89e3\u6846\u67b6\u5728\u68af\u5ea6\u8fed\u4ee3\u4e0b\u6536\u655b\uff0c\u5f97\u5230\u7684\u63a7\u5236\u4e0e\u8bbe\u8ba1\u53c2\u6570\u4f7fPDE\u7cfb\u7edf\u7a33\u5b9a\uff0c\u6570\u503c\u793a\u4f8b\u8868\u660e\u65b9\u6cd5\u53ef\u884c\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u79cd\u53ef\u6267\u884c\u7684CCD\u6c42\u89e3\u6846\u67b6\uff0c\u5728\u6240\u7ed9\u7684\u8fd1\u4f3c\u6a21\u578b\u4e0b\uff0c\u6700\u4f18\u89e3\u5177\u6709\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u6570\u503c\u7ed3\u679c\u4e0e\u7406\u8bba\u7ed3\u8bba\u543b\u5408\uff0c\u9002\u7528\u4e8e\u5177\u6709\u629b\u7269\u578bPDE\u52a8\u529b\u5b66\u7684\u7cfb\u7edf\u3002"}}
{"id": "2512.23173", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23173", "abs": "https://arxiv.org/abs/2512.23173", "authors": ["Zhen Liang", "Hai Huang", "Zhengkui Chen"], "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion", "comment": "This is a preprint. A revised version will appear in the Proceedings of AAAI 2026", "summary": "Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.", "AI": {"tldr": "\u63d0\u51fa Equacode \u7684\u591a\u7b56\u7565 jailbreak \u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6076\u610f\u610f\u56fe\u8f6c\u5316\u4e3a\u6570\u5b66\u95ee\u9898\u518d\u4ee5\u4ee3\u7801\u5b8c\u6210\u89e3\u7b54\uff0c\u4ee5\u8de8\u9886\u57df\u4efb\u52a1\u589e\u52a0\u5bf9\u5b89\u5168\u7ea6\u675f\u7684\u5e72\u6270\uff0c\u4ece\u800c\u63d0\u5347\u5bf9 LLM \u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6548\u679c\uff1b\u5728\u591a\u5bb6\u6a21\u578b\u4e0a\u8fbe\u5230\u9ad8\u6210\u529f\u7387\u5e76\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u5757\u3002", "motivation": "\u5f53\u524d\u5bf9 LLM \u7684 jailbreak \u653b\u51fb\u591a\u805a\u7126\u5355\u4e00\u7b56\u7565\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff1b\u9700\u8981\u8de8\u57df\u3001\u7ec4\u5408\u6027\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u63ed\u793a\u6a21\u578b\u5728\u5b89\u5168\u65b9\u9762\u7684\u5f31\u70b9\u3002", "method": "\u5c06\u6076\u610f\u8bf7\u6c42\u8f6c\u5316\u4e3a\u6c42\u89e3\u6570\u5b66\u65b9\u7a0b\u7684\u4efb\u52a1\u5e76\u5f15\u5bfc\u6a21\u578b\u7528\u4ee3\u7801\u5b8c\u6210\u89e3\u7b54\uff0c\u7ed3\u5408\u65b9\u7a0b\u6a21\u5757\u4e0e\u4ee3\u7801\u6a21\u5757\u7684\u591a\u7b56\u7565\u4ee5\u589e\u5f3a\u653b\u51fb\u6548\u679c\uff1b\u5bf9\u6bd4\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u7ec4\u5408\u6bd4\u4efb\u4e00\u6a21\u5757\u66f4\u5f3a\u3002", "result": "\u5bf9 GPT \u7cfb\u5217\u5e73\u5747\u6210\u529f\u7387 91.19%\uff0c\u5728\u4e09\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u4e2d\u8fbe\u5230 98.65%\uff0c\u4e14\u4ec5\u9700\u5355\u6b21\u67e5\u8be2\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e EquaCode \u6548\u679c\u4f18\u4e8e\u5355\u72ec\u7684\u6570\u5b66\u65b9\u7a0b\u6a21\u5757\u6216\u4ee3\u7801\u6a21\u5757\uff0c\u5448\u73b0\u660e\u663e\u7684\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u591a\u7b56\u7565\u7ec4\u5408\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u653b\u51fb\u6709\u6548\u6027\uff0c\u6210\u4e3a\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u6709\u6548\u624b\u6bb5\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u8de8\u57df\u7b56\u7565\u4e0e\u5b89\u5168\u5bf9\u7b56\u3002"}}
{"id": "2512.23152", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.23152", "abs": "https://arxiv.org/abs/2512.23152", "authors": ["Jackson Kulik", "Braden Hastings", "Keith A. LeGrand"], "title": "Unscented and Higher-Order Linear Covariance Fidelity Checks and Measures of Non-Gaussianity", "comment": null, "summary": "Linear covariance (LinCov) techniques have gained widespread traction in the modeling of uncertainty, including in the preliminary study of spacecraft navigation performance. While LinCov methods offer improved computational efficiency compared to Monte Carlo based uncertainty analysis, they inherently rely on linearization approximations. Understanding the fidelity of these approximations and identifying when they are deficient is critically important for spacecraft navigation and mission planning, especially when dealing with highly nonlinear systems and large state uncertainties. This work presents a number of computational techniques for assessing linear covariance performance. These new LinCov fidelity measures are formulated using higher-order statistics, constrained optimization, and the unscented transform.", "AI": {"tldr": "LinCov fidelity measures for uncertainty modeling exploit higher-order statistics, constrained optimization, and the unscented transform to assess and improve the accuracy of linear covariance approximations, particularly for nonlinear spacecraft navigation problems.", "motivation": "Linear covariance (LinCov) methods offer computational efficiency over Monte Carlo but rely on linearization; assessing when these approximations break down is crucial for spacecraft navigation and mission planning amid strong nonlinearity and large uncertainties.", "method": "Develop computational techniques to evaluate LinCov fidelity using higher-order statistics, constrained optimization, and the unscented transform.", "result": "Proposes new LinCov fidelity measures that quantify approximation quality; framework to diagnose deficiency of LinCov in nonlinear regimes.", "conclusion": "The proposed fidelity measures provide a practical toolkit to evaluate and potentially improve LinCov-based uncertainty analysis in spacecraft navigation and related planning tasks."}}
{"id": "2512.23636", "categories": ["eess.SY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.23636", "abs": "https://arxiv.org/abs/2512.23636", "authors": ["Alberto Bemporad"], "title": "NashOpt - A Python Library for Computing Generalized Nash Equilibria", "comment": "23 pages, 6 figures", "summary": "NashOpt is an open-source Python library for computing and designing generalized Nash equilibria (GNEs) in noncooperative games with shared constraints and real-valued decision variables. The library exploits the joint Karush-Kuhn-Tucker (KKT) conditions of all players to handle both general nonlinear GNEs and linear-quadratic games, including their variational versions. Nonlinear games are solved via nonlinear least-squares formulations, relying on JAX for automatic differentiation. Linear-quadratic GNEs are reformulated as mixed-integer linear programs, enabling efficient computation of multiple equilibria. The framework also supports inverse-game and Stackelberg game-design problems. The capabilities of NashOpt are demonstrated through several examples, including noncooperative game-theoretic control problems of linear quadratic regulation and model predictive control. The library is available at https://github.com/bemporad/nashopt", "AI": {"tldr": "NashOpt \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 Python \u5e93\uff0c\u7528\u4e8e\u5728\u5177\u6709\u5171\u4eab\u7ea6\u675f\u548c\u5b9e\u6570\u51b3\u7b56\u53d8\u91cf\u7684\u975e\u5408\u4f5c\u535a\u5f08\u4e2d\u8ba1\u7b97\u4e0e\u8bbe\u8ba1\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\uff08GNEs\uff09\uff0c\u901a\u8fc7\u5229\u7528\u6240\u6709\u73a9\u5bb6\u7684\u8054\u5408 KKT \u6761\u4ef6\u6765\u5904\u7406\u4e00\u822c\u975e\u7ebf\u6027 GNE \u548c\u7ebf\u6027\u4e8c\u6b21\uff08LQ\uff09GNE\uff0c\u5305\u62ec\u53d8\u5206\u5f62\u5f0f\u3002\u5bf9\u975e\u7ebf\u6027\u535a\u5f08\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5e76\u5229\u7528 JAX \u8fdb\u884c\u81ea\u52a8\u5fae\u5206\uff1b\u5bf9\u7ebf\u6027-\u4e8c\u6b21 GNE\uff0c\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\uff0c\u53ef\u9ad8\u6548\u6c42\u89e3\u591a\u91cd\u5747\u8861\u3002\u6846\u67b6\u8fd8\u652f\u6301\u9006\u535a\u5f08\u4e0e Stackelberg \u535a\u5f08\u8bbe\u8ba1\u95ee\u9898\u3002\u901a\u8fc7\u82e5\u5e72\u793a\u4f8b\uff08\u5305\u62ec\u7ebf\u6027\u4e8c\u6b21\u8c03\u63a7\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u975e\u5408\u4f5c\u535a\u5f08\u63a7\u5236\u95ee\u9898\uff09\u6765\u6f14\u793a\u5176\u80fd\u529b\u3002\u5e93\u5730\u5740\uff1ahttps://github.com/bemporad/nashopt", "motivation": "\u9700\u8981\u5728\u5177\u6709\u5171\u4eab\u7ea6\u675f\u7684\u975e\u5408\u4f5c\u535a\u5f08\u4e2d\u6709\u6548\u5730\u8ba1\u7b97\u548c\u8bbe\u8ba1\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\uff0c\u540c\u65f6\u8986\u76d6\u4e00\u822c\u975e\u7ebf\u6027\u4e0e\u7ebf\u6027-\u4e8c\u6b21\uff08LQ\uff09\u60c5\u5f62\uff0c\u5e76\u5177\u5907\u6c42\u89e3\u591a\u91cd\u5747\u8861\u3001\u9006\u535a\u5f08\u4ee5\u53ca Stackelberg \u535a\u5f08\u8bbe\u8ba1\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5bf9\u6240\u6709\u73a9\u5bb6\u7684\u8054\u5408 KKT \u6761\u4ef6\uff0c\u5c06 GNE \u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u6c42\u89e3\u7684\u4f18\u5316\u95ee\u9898\u3002\u5bf9\u975e\u7ebf\u6027\u535a\u5f08\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u5316\u8868\u8fbe\u5e76\u501f\u52a9 JAX \u7684\u81ea\u52a8\u5fae\u5206\u5b9e\u73b0\uff1b\u5bf9\u7ebf\u6027-\u4e8c\u6b21\u535a\u5f08\uff0c\u91cd\u5199\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4ee5\u4fbf\u9ad8\u6548\u6c42\u89e3\u591a\u5747\u8861\uff1b\u6846\u67b6\u8fd8\u652f\u6301\u9006\u535a\u5f08\u4e0e Stackelberg \u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u82e5\u5e72\u63a7\u5236\u95ee\u9898\u793a\u4f8b\u8fdb\u884c\u5c55\u793a\u3002", "result": "\u901a\u8fc7\u82e5\u5e72\u793a\u4f8b\uff08\u5305\u62ec\u7ebf\u6027\u4e8c\u6b21\u89c4\u5236\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e2d\u7684\u975e\u5408\u4f5c\u535a\u5f08\u63a7\u5236\u95ee\u9898\uff09\u6f14\u793a\u4e86\u8be5\u5de5\u5177\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5728\u591a\u79cd\u573a\u666f\u4e2d\u6c42\u89e3 GNE \u7684\u80fd\u529b\u3002", "conclusion": "NashOpt \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u5171\u4eab\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u4e0e\u7ebf\u6027-\u4e8c\u6b21\u535a\u5f08\u4e2d\u8ba1\u7b97\u4e0e\u8bbe\u8ba1\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\uff0c\u652f\u6301\u591a\u91cd\u5747\u8861\u3001\u9006\u535a\u5f08\u4e0e Stackelberg \u535a\u5f08\u8bbe\u8ba1\uff0c\u5e76\u4e14\u662f\u5f00\u6e90\u7684\uff0c\u4fbf\u4e8e\u5728\u76f8\u5173\u63a7\u5236\u4e0e\u535a\u5f08\u8bba\u95ee\u9898\u4e2d\u5e94\u7528\u3002"}}
{"id": "2512.22242", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.CY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22242", "abs": "https://arxiv.org/abs/2512.22242", "authors": ["Shaurya Gaur", "Michel Vitale", "Alessa Hering", "Johan Kwisthout", "Colin Jacobs", "Lena Philipp", "Fennie van der Graaf"], "title": "Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening", "comment": "Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:025", "summary": "Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u80ba\u764c\u98ce\u9669\u6a21\u578b\uff08Sybil\u3001Venkadesh21\uff09\u53caPanCan2b\u57fa\u7ebf\u7684\u516c\u5e73\u6027\uff0c\u5728NLST\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b50\u7ec4\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u53d1\u73b0\u6027\u522b\u4e0e\u79cd\u65cf\u5dee\u5f02\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u663e\u8457\uff0c\u4e14\u672a\u88ab\u5df2\u77e5\u6df7\u6742\u56e0\u7d20\u89e3\u91ca\uff0c\u63d0\u793a\u9700\u8981\u52a0\u5f3a\u5bf9 Underrepresented \u5b50\u7fa4\u4f53\u7684\u76d1\u63a7\u4e0e\u516c\u5e73\u6027\u7814\u7a76\u3002", "motivation": "\u63a2\u7a76\u7528\u4e8e\u80ba\u764c\u7b5b\u67e5\u7684AI\u98ce\u9669\u4f30\u8ba1\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u6f5c\u5728\u504f\u89c1\uff0c\u786e\u4fdd\u5728\u4e34\u5e8a\u63a8\u8350\u524d\u63d0\u4e0b\u7684\u516c\u5e73\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u4f7f\u7528\u5728NLST\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Sybil\u3001Venkadesh21\uff09\u53caPanCan2b\u903b\u8f91\u56de\u5f52\u57fa\u7ebf\uff0c\u8bc4\u4f30\u5728\u6301\u51fa\u9a8c\u8bc1\u96c6\u4e0a\u7684AUROC\u3001\u654f\u611f\u6027\u3001\u7279\u5f02\u6027\uff0c\u5e76\u6309\u6027\u522b\u3001 race/\u65cf\u88d4\u7b49\u5b50\u7ec4\u5206\u7ec4\u6bd4\u8f83\u3002\u8fd8\u8003\u5bdf\u4e0e\u4e34\u5e8a\u98ce\u9669\u56e0\u7d20\u7684\u6f5c\u5728\u6df7\u6742\u3002", "result": "Sybil\u5728\u5973\u6027\u7ec4\u7684AUROC\u4e3a0.88\uff0895% CI 0.86\u20130.90\uff09\uff0c\u7537\u6027\u4e3a0.81\uff0895% CI 0.78\u20130.84\uff09\uff0c\u5dee\u5f02\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff08p<0.001\uff09\u3002\u572890%\u7279\u5f02\u6027\u4e0b\uff0cVenkadesh21\u5bf9\u9ed1\u4eba\u7ec4\u7684\u654f\u611f\u6027\u4e3a0.39\uff0895% CI 0.23\u20130.59\uff09\uff0c\u5bf9\u767d\u4eba\u7ec4\u4e3a0.69\uff0895% CI 0.65\u20130.73\uff09\u3002\u8fd9\u4e9b\u5dee\u5f02\u65e0\u6cd5\u901a\u8fc7\u73b0\u6709\u4e34\u5e8a\u6df7\u6742\u56e0\u7d20\u89e3\u91ca\uff0c\u56e0\u6b64\u53ef\u80fd\u5c5e\u4e8eJustEFAB\u6846\u67b6\u4e0b\u7684\u4e0d\u516c\u5e73\u504f\u5dee\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u63d0\u5347\u4e0e\u76d1\u6d4b\u5bf9 underrepresented \u5b50\u7fa4\u4f53\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5c31\u7b97\u6cd5\u516c\u5e73\u6027\u5728\u80ba\u764c\u7b5b\u67e5\u9886\u57df\u5f00\u5c55\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.23216", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23216", "abs": "https://arxiv.org/abs/2512.23216", "authors": ["Partha Paul", "Keshav Sinha"], "title": "Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption", "comment": null, "summary": "In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u5c5e\u6027\u7684\u52a0\u5bc6ABE+FBSE\uff0c\u7528\u4e8c\u53d8\u91cf\u66f2\u7ebf\u7684\u7ebf\u6027\u529f\u80fd\u6765\u5b58\u50a8\u4e91\u6570\u636e, \u901a\u8fc7Shamir\u79d8\u5bc6\u5206\u4eab\u548c2D-Lagrange\u6c42\u89e3\u5b9e\u73b0\u591a\u65b9\u6388\u6743\u548c\u9608\u503c\u89e3\u5bc6\uff1b\u5728\u5c5e\u6027\u7b56\u7565\u589e\u591a\u65f6\u52a0\u5bc6\u65f6\u95f4\u589e\u52a0\uff0c\u4f46\u5b58\u50a8\u5f00\u9500\u4fdd\u6301\u4f4e\u6c34\u5e73\uff0c\u4e14\u5bf9\u78b0\u649e\u653b\u51fb\u5177\u6709\u62b5\u6297\u529b\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u6848\u9c81\u68d2\u5b89\u5168\u3002", "motivation": "\u89e3\u51b3\u4e91\u73af\u5883\u4e2d\u6d77\u91cf\u6570\u636e\u7684\u5b58\u53d6\u9700\u6c42\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u4e0e\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u63d0\u4f9b\u53ef\u8bbf\u95ee\u6027\u4e0e\u6570\u636e\u5b89\u5168\u7684\u5206\u5e03\u5f0f\u591a\u65b9\u6388\u6743\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u529f\u80fd\u57fa\u6d41\u5bc6\u7801FBSE\u7ed3\u5408\u7b80\u5316\u7684\u6807\u91cf\u70b9\u5728\u629b\u7269\u7ebf\u66f2\u7ebf\u4e0a\u7684\u8868\u793a\uff0c\u751f\u6210\u6388\u6743\u70b9\u5e76\u4ec5\u5206\u4eab\u7ed9\u6388\u6743\u63a5\u6536\u8005\uff1b\u901a\u8fc7Shamir\u79d8\u5bc6\u5206\u4eab\u4e0e2D-Lagrange\u63d2\u503c\u4ece\u629b\u7269\u7ebf\u91cd\u6784\u79d8\u5bc6\u70b9\uff1b\u8bbe\u5b9a\u9608\u503cTs>3\u5b9e\u73b0\u5c5e\u6027\u76f8\u5173\u5bc6\u94a5\u7684\u89e3\u5bc6\uff1b\u5bf9\u6570\u636e\u8fdb\u884c\u57fa\u4e8e\u5c5e\u6027\u7684\u52a0\u5bc6\uff0c\u8bc4\u4f30\u52a0\u5bc6/\u89e3\u5bc6\u65f6\u95f4\u3001\u5b58\u50a8\u5f00\u9500\u3001\u4ee5\u53ca\u7edf\u8ba1\u5206\u6790\uff08NIST\u6d4b\u8bd5\u3001\u76f8\u5173\u6027\u3001\u76f4\u65b9\u56fe\uff09\u4ee5\u8bc4\u4f30\u50cf\u7d20\u504f\u5dee\u3002", "result": "\u52a0\u5bc6/\u89e3\u5bc6\u6027\u80fd\u968f\u5c5e\u6027\u6570\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5b58\u50a8\u5f00\u9500\u4f4e\u4e14\u4e0e\u7528\u6237\u8eab\u4efd\u65e0\u5173\uff1b\u5b89\u5168\u5206\u6790\u8868\u660e\u5bf9\u78b0\u649e\u653b\u51fb\u6709\u62b5\u6297\u529b\uff1b\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6848\u5728\u9c81\u68d2\u6027\u4e0e\u5b89\u5168\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u4e91\u5b58\u50a8\u573a\u666f\u63d0\u4f9b\u4e00\u79cd\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u591a\u65b9\u6388\u6743ABE\u65b9\u6848\uff0c\u5177\u5907\u8f83\u4f4e\u7684\u5b58\u50a8\u5f00\u9500\u548c\u8f83\u5f3a\u7684\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5c5e\u6027\u7b56\u7565\u7075\u6d3b\u6027\u4e0e\u6570\u636e\u5b89\u5168\u7684\u5e94\u7528\uff1b\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u53ca\u5b9e\u73b0\u7ec6\u8282\u3002"}}
{"id": "2512.23246", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23246", "abs": "https://arxiv.org/abs/2512.23246", "authors": ["Ziwei Wan", "Zhen Gao", "Fabien Heliot", "Qu Luo", "Pei Xiao", "Haiyang Zhang", "Christos Masouros", "Yonina C. Eldar", "Sheng Chen"], "title": "Ultra-Massive MIMO with Orthogonal Chirp Division Multiplexing for Near-Field Sensing and Communication Integration", "comment": null, "summary": "This paper integrates the emerging ultra-massive multiple-input multiple-output (UM-MIMO) technique with orthogonal chirp division multiplexing (OCDM) waveform to tackle the challenging near-field integrated sensing and communication (ISAC) problem. Specifically, we conceive a comprehensive ISAC architecture, where an UM-MIMO base station adopts OCDM waveform for communications and a co-located sensing receiver adopts the frequency-modulated continuous wave (FMCW) detection principle to simplify the associated hardware. For sensing tasks, several OCDM subcarriers, namely, dedicated sensing subcarriers (DSSs), are each transmitted through a dedicated sensing antenna (DSA) within the transmit antenna array. By judiciously designing the DSS selection scheme and optimizing receiver parameters, the FMCW-based sensing receiver can decouple the echo signals from different DSAs with significantly reduced hardware complexity. This setup enables the estimation of ranges and velocities of near-field targets in an antenna-pairwise manner. Moreover, by leveraging the spatial diversity of UM-MIMO, we introduce the concept of virtual bistatic sensing (VIBS), which incorporates the estimates from multiple antenna pairs to achieve high-accuracy target positioning and three-dimensional velocity measurement. The VIBS paradigm is immune to hostile channel environments characterized by spatial non-stationarity and uncorrelated multipath environment. Furthermore, the channel estimation of UM-MIMO OCDM systems enhanced by the sensing results is investigated. Simulation results demonstrate that the proposed ISAC scheme enhances sensing accuracy, and also benefits communication performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u8d85\u5927\u89c4\u6a21MIMO\uff08UM-MIMO\uff09\u7cfb\u7edf\u4e2d\uff0c\u7ed3\u5408OCDM\u6ce2\u5f62\u5b9e\u73b0\u8fd1\u573aISAC\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u91c7\u7528FMCW\u611f\u77e5\uff0c\u8bbe\u8ba1\u4e13\u95e8\u611f\u77e5\u5b50\u8f7d\u6ce2\u4e0eDSA\u5b9e\u73b0\u56de\u6ce2\u89e3\u8026\uff0c\u5e76\u63d0\u51fa\u865a\u62df\u53cc\u57fa\u5730\u611f\u77e5\uff08VIBS\uff09\u4ee5\u63d0\u5347\u5b9a\u4f4d\u4e0e\u4e09\u7ef4\u901f\u5ea6\u6d4b\u91cf\uff1b\u5e76\u5bf9UM-MIMO OCDM\u7684\u4fe1\u9053\u4f30\u8ba1\u8fdb\u884c\u7814\u7a76\uff0c\u4eff\u771f\u8868\u660e\u5bf9\u611f\u77e5\u548c\u901a\u4fe1\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u8fd1\u573aISAC\u5728\u786c\u4ef6\u590d\u6742\u6027\u548c\u89e3\u8026\u56f0\u96be\u4e2d\u7684\u6311\u6218\uff1b\u5145\u5206\u5229\u7528UM-MIMO\u7684\u7a7a\u95f4\u5173\u7cfb\u548cOCDM\u7684\u9891\u57df\u7ed3\u6784\uff0c\u63d0\u5347\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408ISAC\u67b6\u6784\uff1aUM-MIMO\u57fa\u7ad9\u4f7f\u7528OCDM\u8fdb\u884c\u901a\u4fe1\uff0c\u611f\u77e5\u7aef\u4f7f\u7528FMCW\u68c0\u6d4b\uff1b\u4e3a\u611f\u77e5\u8bbe\u8ba1DSSs\u901a\u8fc7\u5404\u81ea\u611f\u77e5\u5929\u7ebf\u9010\u8f7d\u6ce2\u4f20\u8f93\uff1b\u8bbe\u8ba1DSS\u9009\u62e9\u548c\u63a5\u6536\u53c2\u6570\u4ee5\u5b9e\u73b0DSA\u56de\u6ce2\u7684\u89e3\u8026\uff1b\u57fa\u4e8eUM-MIMO\u7684\u7a7a\u95f4\u591a\u6837\u6027\u63d0\u51faVIBS\uff0c\u901a\u8fc7\u591a\u5929\u7ebf\u5bf9\u4f30\u8ba1\u63d0\u9ad8\u5b9a\u4f4d\u4e0e\u4e09\u7ef4\u901f\u5ea6\u7cbe\u5ea6\uff1b\u5bf9UM-MIMO OCDM\u7cfb\u7edf\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u7ed3\u5408\u611f\u77e5\u7ed3\u679c\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0ISAC\u5728\u611f\u77e5\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5bf9\u901a\u4fe1\u6027\u80fd\u4e5f\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u8fd1\u573aISAC\u95ee\u9898\u4e0eUM-MIMO\u548cOCDM\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u9ad8\u7cbe\u5ea6\u7684\u611f\u77e5\u65b9\u6848\uff0c\u4e14VIBS\u5bf9\u590d\u6742\u4fe1\u9053\u73af\u5883\u5177\u6709\u9c81\u68d2\u6027\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u6269\u5c55\u81f3\u5b9e\u9645\u786c\u4ef6\u5b9e\u73b0\u548c\u5bf9\u6bd4\u5176\u4ed6\u6ce2\u5f62/\u9635\u5217\u7ed3\u6784\u3002"}}
{"id": "2512.23307", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.23307", "abs": "https://arxiv.org/abs/2512.23307", "authors": ["Jiawei Liu", "Zhuo Chen", "Rui Zhu", "Miaokun Chen", "Yuyang Gong", "Wei Lu", "Xiaofeng Wang"], "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking", "comment": null, "summary": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.", "AI": {"tldr": "\u63d0\u51faRobustMask\uff0c\u4e00\u79cd\u5c06\u4e0a\u4e0b\u6587\u9884\u6d4b\u80fd\u529b\u4e0e\u968f\u673a\u63a9\u853d\u5e73\u6ed1\u7ed3\u5408\u7684\u5bf9\u6297\u9c81\u68d2\u6027 defense\uff0c\u63d0\u5347\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5728\u5b57\u7b26\u3001\u5355\u8bcd\u548c\u77ed\u8bed\u5c42\u9762\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u524d10\u540d\u4e2d\u5bf9\u6297\u6700\u591a30%\u7684\u5185\u5bb9\u6270\u52a8\u65f6\u5bf9\u8d85\u8fc720%\u7684\u5019\u9009\u6587\u6863\u63d0\u4f9b\u8ba4\u8bc1\u9c81\u68d2\u6027\u3002", "motivation": "\u795e\u7ecf\u6392\u5e8f\u5728 Retrieval-Augmented Generation \u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6613\u53d7\u5bf9\u6297\u6027\u6270\u52a8\u5f71\u54cd\uff0c\u73b0\u6709\u9632\u5fa1\u8981\u4e48\u6cdb\u5316\u6027\u5dee\u3001\u8981\u4e48\u9700\u8981\u5bf9\u624b\u62e5\u6709\u8fc7\u5f3a\u77e5\u8bc6\uff0c\u96be\u4ee5\u843d\u5730\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9884\u6d4b\u80fd\u529b\u4e0e\u968f\u673a\u63a9\u853d\u5e73\u6ed1\u673a\u5236\uff0c\u5bf9\u5019\u9009\u6587\u6863\u53ca\u6392\u5e8f\u8fdb\u884c\u9c81\u68d2\u5316\u5904\u7406\u3002\u5229\u7528\u6392\u540d\u6a21\u578b\u7684\u4e24\u4e24\u6bd4\u8f83\u80fd\u529b\u548c\u6982\u7387\u7edf\u8ba1\u5206\u6790\uff0c\u7ed9\u51fa\u8ba4\u8bc1\u7684 top-K \u9c81\u68d2\u6027\u8bc1\u660e\uff0c\u5e76\u8986\u76d6\u5b57\u7b26\u3001\u5355\u8bcd\u3001\u77ed\u8bed\u5c42\u9762\u7684\u6270\u52a8\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cRobustMask \u80fd\u8ba4\u8bc1 top-10 \u4e2d\u8d85\u8fc7 20% \u7684\u5019\u9009\u6587\u6863\uff0c\u5728\u6270\u52a8\u53ef\u8fbe\u6587\u6863\u5185\u5bb9\u7684 30% \u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff1b\u5bf9\u6bd4\u57fa\u7ebf\u5177\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RobustMask \u4e3a\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u4e0e\u5b9e\u8df5\u5c42\u9762\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u63a8\u52a8\u73b0\u5b9e\u68c0\u7d22\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u4fe1\u5ea6\u63d0\u5347\u3002"}}
{"id": "2512.23381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23381", "abs": "https://arxiv.org/abs/2512.23381", "authors": ["Lorenz Bielefeld", "Paul Zheng", "Oner Hanay", "Yao Zhu", "Yulin Hu", "Anke Schmeink"], "title": "On Signal Peak Power Constraint of Over-the-Air Federated Learning", "comment": "Submitted to IEEE", "summary": "Federated learning (FL) has been considered a promising privacy preserving distributed edge learning framework. Over-the-air computation (AirComp) technique leveraging analog transmission enables the aggregation of local updates directly over-the-air by exploiting the superposition properties of wireless multiple-access channel, thereby drastically reducing the communication bottleneck issues of FL compared with digital transmission schemes. This work points out that existing AirComp-FL overlooks a key practical constraint, the instantaneous peak-power constraints imposed by the non-linearity of radiofrequency power amplifiers. We present and analyze the effect of the classic method to deal with this issue, amplitude clipping combined with filtering. We investigate the effect of instantaneous peak-power constraints in AirComp-FL for both single-carrier and multi-carrier orthogonal frequency-division multiplexing (OFDM) systems. We highlight the specificity of AirComp-FL: the samples depend on the gradient value distribution, leading to a higher peak-to-average power ratio (PAPR) than that observed for uniformly distributed signals. Simulation results demonstrate that, in practical settings, the instantaneous transmit power regularly exceeds the power-amplifier limit; however, by applying clipping and filtering, the FL performance can be degraded. The degradation becomes pronounced especially in multi-carrier OFDM systems due to the in-band distortions caused by clipping and filtering.", "AI": {"tldr": "AirComp-FL\u5728\u5b58\u5728\u653e\u5927\u5668\u77ac\u65f6\u5cf0\u503c\u529f\u7387\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u5206\u6790\uff0c\u53d1\u73b0\u526a\u5207\u4e0e\u6ee4\u6ce2\u5728\u5355\u8f7d\u6ce2/OFDM\u7cfb\u7edf\u4e2d\u5f71\u54cd\u663e\u8457\uff0cOFDM\u4e0b\u5e26\u5185\u5931\u771f\u66f4\u6613\u6076\u5316\u6027\u80fd\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u901a\u8fc7\u7a7a\u4e2d\u8ba1\u7b97\u805a\u5408\u68af\u5ea6\u65f6\uff0c\u975e\u7ebf\u6027\u5c04\u9891\u653e\u5927\u5668\u7684\u5cf0\u503c\u7ea6\u675f\u88ab\u5ffd\u7565\uff0c\u672c\u7814\u7a76\u5173\u6ce8\u526a\u5207+\u6ee4\u6ce2\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u7ecf\u5178\u5e45\u5ea6\u526a\u5207\u4e0e\u6ee4\u6ce2\u7b56\u7565\u5728\u5355\u8f7d\u6ce2\u4e0eOFDM\u4e0b\u7684\u4f20\u8f93-\u5b66\u4e60\u7cfb\u7edf\uff0c\u6bd4\u8f83\u68af\u5ea6\u5206\u5e03\u5bf9PAPR\u7684\u5f71\u54cd\u53ca\u5728\u5b9e\u9645\u529f\u7387\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9645\u573a\u666f\u53d1\u5c04\u529f\u7387\u5e38\u8d85\u51fa\u653e\u5927\u5668\u6781\u9650\uff0c\u526a\u5207+\u6ee4\u6ce2\u4f1a\u964d\u4f4eFL\u6027\u80fd\uff1b\u5728\u591a\u8f7d\u6ce2OFDM\u60c5\u5f62\u4e0b\uff0c\u5e26\u5185\u5931\u771f\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u3001\u529f\u7387\u63a7\u5236\u6216\u9c81\u68d2\u7684AirComp-FL\u8bbe\u8ba1\uff0c\u4ee5\u7f13\u89e3\u653e\u5927\u5668\u7ea6\u675f\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u5c24\u5176\u5728OFDM\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2512.22699", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22699", "abs": "https://arxiv.org/abs/2512.22699", "authors": ["Antar Kumar Biswas", "Masoud H. Nazari"], "title": "Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors", "comment": "This is a preprint of a manuscript currently under review at Electric Power Systems Research. The content may be subject to change following peer review", "summary": "This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\u5f15\u53d1\u7684\u505c\u7535\uff0c\u6bd4\u8f83RF\u3001SVM\u3001AdaBoost\u548cLSTM\u7b49\u6a21\u578b\uff1b\u5728\u5bc6\u6b47\u6839\u5dde\u4e0b\u534a\u5c9b\u53bf\u7ea7\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cLSTM\u8868\u73b0\u6700\u4f73\uff1b\u7ecf\u6d4e\u72b6\u51b5\u548c\u57fa\u7840\u8bbe\u65bd\u6c34\u5e73\u4e0e\u505c\u7535\u98ce\u9669\u5448\u8d1f\u76f8\u5173\u3002", "motivation": "\u89e3\u51b3\u4f4e\u6982\u7387\u9ad8\u540e\u679c\u7684\u505c\u7535\u98ce\u9669\u9884\u6d4b\u95ee\u9898\uff1b\u901a\u8fc7\u6574\u5408\u516c\u5f00\u6570\u636e\u6e90\uff08\u5929\u6c14\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u57fa\u7840\u8bbe\u65bd\u3001\u5b63\u8282\u6027\u4e8b\u4ef6\u7b49\uff09\u6765\u63ed\u793a\u793e\u533a\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u5347\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u505c\u7535\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u5c06EAGLE-I\u505c\u7535\u8bb0\u5f55\uff082014-2024\uff09\u4e0e\u5929\u6c14\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u5b63\u8282\u6027\u4e8b\u4ef6\u7b49\u7279\u5f81\u6574\u5408\uff0c\u52a0\u5165\u793e\u4f1a\u4eba\u53e3\u6307\u6807\u4ee5\u8003\u5bdf\u793e\u533a\u8106\u5f31\u6027\uff1b\u6bd4\u8f83\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001AdaBoost\u3001LSTM\uff09\u7684\u9884\u6d4b\u6027\u80fd\uff1b\u5728\u5bc6\u6b47\u6839\u5dde\u4e0b\u534a\u5c9b\u53bf\u7ea7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff1b\u4ee5\u9884\u6d4b\u8bef\u5dee\u8861\u91cf\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u6bd4\u8f83\u6a21\u578b\u4e2d\uff0cLSTM\u83b7\u5f97\u6700\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\uff1b\u989d\u5916\u53d1\u73b0\u66f4\u5f3a\u7684\u7ecf\u6d4e\u6761\u4ef6\u548c\u66f4\u53d1\u8fbe\u7684\u57fa\u7840\u8bbe\u65bd\u4e0e\u8f83\u4f4e\u7684\u505c\u7535\u53d1\u751f\u7387\u76f8\u5173\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u7528\u4e8e\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u505c\u7535\u98ce\u9669\u8bc4\u4f30\u4e0e\u97e7\u6027\u89c4\u5212\uff0c\u8de8\u6570\u636e\u6e90\u7684\u793e\u4f1a\u7ecf\u6d4e\u4e0e\u57fa\u7840\u8bbe\u65bd\u56e0\u7d20\u5bf9\u505c\u7535\u98ce\u9669\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u80fd\u4e3a\u51b3\u7b56\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e\u3002"}}
{"id": "2512.22245", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22245", "abs": "https://arxiv.org/abs/2512.22245", "authors": ["Bhaktipriya Radharapu", "Eshika Saxena", "Kenneth Li", "Chenxi Whitehouse", "Adina Williams", "Nicola Cancedda"], "title": "Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation", "comment": null, "summary": "As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.", "AI": {"tldr": "\u63d0\u51fa\u7528\u7ebf\u6027\u63a2\u9488\u7ed3\u5408\u57fa\u4e8eBrier\u5206\u6570\u7684\u635f\u5931\uff0c\u5bf9LLM judge\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\uff1b\u5728\u63a8\u7406\u3001\u6570\u5b66\u3001\u4e8b\u5b9e\u6027\u3001\u7f16\u7801\u7b49\u4efb\u52a1\u4ee5\u53ca\u4e3b\u89c2\u504f\u597d\u8bc4\u4f30\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u5730\u6821\u51c6\u6027\u548c\u7ea610x\u7684\u8ba1\u7b97\u8282\u7701\uff0c\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8bc4\u4f30\u57df\uff0c\u4f46\u5bf9\u7b80\u5355\u6570\u636e\u96c6\u8f83\u4fdd\u5b88\uff0c\u5b89\u5168\u5173\u952e\u90e8\u7f72\u6709\u5229\u3002", "motivation": "\u5728\u5c06LLM\u4f5c\u4e3a\u5224\u5b9a\u8005\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u65f6\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u4e14\u9ad8\u6548\u7684\u672a\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u53e3\u5934\u81ea\u4fe1\u5ea6\u3001\u591a\u4ee3\u9645\u65b9\u6cd5\uff09\u8981\u4e48\u6821\u51c6\u5dee\u3001\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u7528\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u624b\u6bb5\u3002", "method": "\u63d0\u51fa\u5728\u5224\u5b9a\u8005\u7684\u9690\u85cf\u72b6\u6001\u4e0a\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\uff0c\u4f7f\u7528\u57fa\u4e8eBrier\u5206\u6570\u7684\u635f\u5931\u6765\u6821\u51c6\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u63a2\u9488\u4e0d\u9700\u8981\u5bf9\u6a21\u578b\u672c\u4f53\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\uff0c\u5c5e\u4e8e\u53ef\u63d2\u62d4\u7684\u89e3\u91ca\u6027\u5de5\u5177\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63a2\u9488\u5728\u6821\u51c6\u6027\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u7ea6\u4f4e10\u500d\uff1b\u5177\u6709\u5bf9\u672a\u77e5\u8bc4\u4f30\u57df\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u9ad8\u7f6e\u4fe1\u9884\u6d4b\u4e0a\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff1b\u4f46\u5bf9\u7b80\u5355\u6570\u636e\u96c6\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u5448\u73b0\u4fdd\u5b88\u4f30\u8ba1\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e3aLLM judge\u5728\u751f\u4ea7\u573a\u666f\u4e2d\u7684\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u63d2\u62d4\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u9700\u8981\u4f4e\u5047\u9633\u6027\u7387\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2512.23438", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23438", "abs": "https://arxiv.org/abs/2512.23438", "authors": ["Johannes Lenzen", "Mohamadreza Rostami", "Lichao Wu", "Ahmad-Reza Sadeghi"], "title": "Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer", "comment": null, "summary": "Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.\n  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).\n  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $\u03bc$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.", "AI": {"tldr": "\u63d0\u51faFuzzilicon\uff1a\u9762\u5411\u771f\u5b9ex86\u5904\u7406\u5668\u7684\u540e\u7845\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u4ee3\u7801\u5c42\u53cd\u9988\u5b9e\u73b0\u81ea\u52a8\u5316\u6f0f\u6d1e\u53d1\u73b0\u3002", "motivation": "\u73b0\u4ee3CPU\u901a\u5e38\u662f\u9ed1\u7bb1\u3001\u4e13\u6709\uff0c\u5b58\u5728\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u5206\u6790\u53d1\u73b0\u7684\u5fae\u67b6\u6784\u7f3a\u9677\uff1b\u9700\u8981\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u540e\u7845\u6f0f\u6d1e\u53d1\u73b0\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5fae\u4ee3\u7801\u5c42\u6ce8\u5165 instrumentation\u3001\u9006\u5411Intel\u5fae\u7801\u66f4\u65b0\u63a5\u53e3\u3001\u7ed3\u5408\u57fa\u4e8eHypervisor\u7684fuzzing harness\uff0c\u5728\u4e0d\u8bbf\u95eeRTL\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53cd\u9988\u9a71\u52a8\u8f93\u5165\u751f\u6210\u3002", "result": "\u5728Intel Goldmont\u4e0a\u53d1\u73b05\u4e2a\u91cd\u5927\u53d1\u73b0\uff0c\u5176\u4e2d2\u4e2a\u4e3a\u65b0\u9896\u7684\u5fae\u7801\u7ea7 speculative-execution \u6f0f\u6d1e\uff1b\u81ea\u52a8\u91cd\u73b0 \u03bcSpectre \u6f0f\u6d1e\uff1b\u8986\u76d6\u5f00\u9500\u6bd4\u57fa\u7ebf\u964d\u4f4e\u8fbe31\u00d7\uff0c\u5b9e\u73b016.27%\u7684\u552f\u4e00\u5fae\u4ee3\u7801\u8986\u76d6\u7387\uff08hookable\u4f4d\u7f6e\u7684\u57fa\u7ebf\uff09\uff0c\u5efa\u7acb\u5b9e\u8bc1\u57fa\u7ebf\u3002", "conclusion": "Fuzzilicon\u4e3a\u540e\u7845 fuzzing \u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u5b9e\u9645\u53ef\u7528\u7684\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u53d1\u73b0\u590d\u6742CPU\u6f0f\u6d1e\u5960\u5b9a\u65b0\u57fa\u7840\u3002"}}
{"id": "2512.22623", "categories": ["cs.LG", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.22623", "abs": "https://arxiv.org/abs/2512.22623", "authors": ["Tomas Ortega", "Chun-Yin Huang", "Xiaoxiao Li", "Hamid Jafarkhani"], "title": "Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback", "comment": null, "summary": "Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.", "AI": {"tldr": "\u5728FL\u4e2d\u63d0\u51fa\u4e24\u79cd\u65e0\u5ba2\u6237\u7aef\u72b6\u6001\u7684\u6709\u504f\u538b\u7f29\u6846\u67b6 CAFe \u4e0e CAFe-S\uff0c\u901a\u8fc7\u5168\u5c40\u805a\u5408\u66f4\u65b0\u4f5c\u4e3a\u5171\u4eab\u63a7\u5236\u53d8\u91cf\u5b9e\u73b0\u65e0\u72b6\u6001\u7684\u8bef\u5dee\u53cd\u9988\uff0c\u5728\u975e\u51f8\u4e0b\u8bc1\u660e\u4e86 CAFe \u76f8\u5bf9\u4e8e DCGD \u7684\u4f18\u52bf\uff0c\u4e14 CAFe-S \u7684\u6536\u655b\u901f\u7387\u968f\u7740\u670d\u52a1\u7aef\u6570\u636e\u4ee3\u8868\u6027\u63d0\u5347\u800c\u589e\u5f3a\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u4e0a\u4f20\u94fe\u8def\u7684\u5e26\u5bbd\u74f6\u9888\uff0c\u5c3d\u7ba1\u6709\u504f\u538b\u7f29\u5177\u6709\u9ad8\u6548\u6027\uff0c\u4f46\u4f20\u7edf\u8bef\u5dee\u53cd\u9988\u9700\u8981\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u63a7\u5236\u53d8\u5dee\u91cf\uff0c\u4fb5\u72af\u9690\u79c1\u4e14\u4e0e\u65e0\u72b6\u6001\u3001\u5e7f\u57df\u5316\u7684FL\u573a\u666f\u4e0d\u517c\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u65e0\u9700\u5ba2\u6237\u7aef\u72b6\u6001\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6846\u67b6\uff1aCAFe \u4f7f\u7528\u4e0a\u4e00\u8f6e\u5168\u5c40\u805a\u5408\u66f4\u65b0\u4f5c\u4e3a\u5168\u5c40\u5171\u4eab\u63a7\u5236\u53d8\u5dee\u91cf\u5b9e\u73b0\u504f\u538b\u7f29\u7684\u8bef\u5dee\u53cd\u9988\uff1bCAFe-S \u5728\u670d\u52a1\u7aef\u62e5\u6709\u4e00\u5c0f\u90e8\u5206\u79c1\u6709\u6570\u636e\u65f6\uff0c\u751f\u6210\u670d\u52a1\u5668\u5f15\u5bfc\u7684\u5019\u9009\u66f4\u65b0\u4ee5\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002\u4ee5\u5206\u5e03\u5f0f\u68af\u5ea6\u4e0b\u964d\uff08DGD\uff09\u4f5c\u4e3a\u5206\u6790\u5bf9\u8c61\uff0c\u7ed9\u51fa\u5728\u975e\u51f8\u4e14\u68af\u5ea6\u5dee\u5f02\u6709\u754c\u6761\u4ef6\u4e0b CAFe \u76f8\u6bd4 DCGD \u7684\u4f18\u52bf\u8bc1\u660e\uff0c\u4ee5\u53ca CAFe-S \u7684\u6536\u655b\u6027\u8bc1\u660e\uff0c\u5e76\u8bc1\u660e\u670d\u52a1\u7aef\u6570\u636e\u4ee3\u8868\u6027\u8d8a\u5f3a\uff0c\u6536\u655b\u901f\u7387\u8d8a\u597d\u3002", "result": "\u7406\u8bba\u4e0a\uff0cCAFe \u5728\u975e\u51f8\u4e0e\u6709\u754c\u68af\u5ea6\u5dee\u5f02\u6761\u4ef6\u4e0b\u4f18\u4e8e DCGD\uff1bCAFe-S \u5177\u5907\u6536\u655b\u5230\u9a7b\u70b9\u7684\u4fdd\u8bc1\uff0c\u4e14\u5728\u670d\u52a1\u7aef\u6570\u636e\u66f4\u5177\u4ee3\u8868\u6027\u65f6\u6536\u655b\u901f\u7387\u63d0\u9ad8\uff1b\u5b9e\u9a8c\u7ed3\u679c\u5728\u5206\u5e03\u5f0f/\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u76f8\u8f83\u73b0\u6709\u538b\u7f29\u65b9\u6848\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684 CAFe \u4e0e CAFe-S \u6846\u67b6\u5b9e\u73b0\u4e86\u5728\u65e0\u5ba2\u6237\u7aef\u72b6\u6001\u7684\u524d\u63d0\u4e0b\u5bf9\u6709\u504f\u538b\u7f29\u7684\u9ad8\u6548\u548c\u53ef\u8bc1\u660e\u6536\u655b\u6027\uff0c\u9002\u7528\u4e8e\u4e0a\u884c\u5e26\u5bbd\u53d7\u9650\u7684\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u573a\u666f\uff0c\u4e14\u670d\u52a1\u7aef\u6570\u636e\u7684\u53ef\u7528\u6027\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2512.22792", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22792", "abs": "https://arxiv.org/abs/2512.22792", "authors": ["Shuai Chen", "Chen Wang", "Ziran Wang"], "title": "SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance", "comment": "31 pages, 7 figures, 4 tables", "summary": "Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.", "AI": {"tldr": "SNM-Net: a geometry-based normalization plus Mahalanobis scoring for open-set gas recognition in E-nose systems; achieves near-perfect AUROC and high unknown detection on Vergara dataset with architecture-agnostic backbones.", "motivation": "Open-set gas recognition faces signal drift induced feature distribution shifts and interference-induced decision failures. Euclidean distance-based methods fail to capture anisotropic feature distributions and dynamic signal intensities, necessitating a framework that stabilizes features and adapts decision boundaries.", "method": "A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. Mahalanobis distance is used as the scoring metric with class-wise statistics to form adaptive ellipsoidal decision regions. The framework is architecture-agnostic and compatible with CNN, RNN, and Transformer backbones.", "result": "On Vergara dataset, Transformer+SNM achieves AUROC 0.9977 and unknown gas detection rate 99.57% (TPR@5%FPR). It outperforms state-of-the-art by ~3.0% AUROC and reduces performance SD by ~91% compared to Class Anchor Clustering, with robustness across sensor positions (SD < 0.0028).", "conclusion": "SNM-Net effectively resolves the accuracy-stability trade-off in open-set gas recognition and provides a solid, architecture-agnostic foundation for industrial E-nose deployment."}}
{"id": "2512.22248", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22248", "abs": "https://arxiv.org/abs/2512.22248", "authors": ["Rohit Pandey", "Rohan Pandey"], "title": "Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation", "comment": "5 pages, 2 figures, 2 tables", "summary": "Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.", "AI": {"tldr": "A simulation-based amortized inference method trains a neural network on synthetic flight data to invert a physics simulator and predict aerodynamic parameters (drag coefficient and thrust correction) from apogee measurements and configuration features, enabling zero-shot transfer to real flights with MAE 12.3 m; outperforms OpenRocket baselines; biases reveal gaps between idealized physics and reality; code is publicly available.", "motivation": "Reduce dependence on expensive CFD or real flight datasets by leveraging synthetic data and a forward physics model to enable accurate real-world predictions with zero real-data fine-tuning, benefiting amateur rocketry by providing a practical predictive tool.", "method": "Generate 10,000 synthetic flights using a physics simulator. Train a neural network to invert the forward model, predicting drag coefficient and thrust correction from a single apogee measurement plus motor and configuration features. Apply the model directly to 8 real flights without any fine-tuning.", "result": "The model achieved a mean absolute error of 12.3 meters in apogee prediction on the real flights, indicating effective sim-to-real transfer with zero real training data. A systematic positive bias was observed, quantifying the gap between idealized physics and real conditions. Compared to OpenRocket baselines, the learned approach reduced apogee prediction error.", "conclusion": "This proof-of-concept demonstrates that amortized, simulation-based inversion can enable accurate real-world predictions with minimal real data and offers a publicly available implementation to support adoption in the amateur rocketry community."}}
{"id": "2512.22249", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22249", "abs": "https://arxiv.org/abs/2512.22249", "authors": ["Zheng Xing", "Weibing Zhao"], "title": "Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models", "comment": null, "summary": "Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06 temporal vision semantics (TVS) \u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u8fde\u7eed\u5e27\u4e2d\u63d0\u53d6\u6587\u672c\u4fe1\u606f\u5e76\u878d\u5165\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684\u65e0\u76d1\u7763\u4eba\u7c7b\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u95f4\u90bb\u8fd1\u7ea6\u675f\u4e0e\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u66f4\u4f18\u5206\u5272\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u65b0\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684 HMS \u5728\u65f6\u5e8f\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8bed\u4e49\u53d8\u5316\u4fe1\u606f\u3002\u5f15\u5165\u57fa\u4e8e LLM \u7684 TVS \u53ef\u4ece\u6587\u672c\u5c42\u9762\u6355\u6349\u52a8\u4f5c\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u8f85\u52a9\u63d0\u5347\u805a\u7c7b\u6548\u679c\u3002", "method": "\u5bf9\u8fde\u7eed\u5e27\u901a\u8fc7 LLM \u63d0\u53d6\u6587\u672c\u63cf\u8ff0\uff0c\u8be2\u95ee\u662f\u5426\u540c\u4e00\u52a8\u4f5c\u4ee5\u83b7\u53d6\u65f6\u95f4\u90bb\u8fd1\u5173\u7cfb\uff1b\u636e\u6b64\u5b66\u4e60 temporal neighboring information\uff1b\u5c06 TVS \u7eb3\u5165\u5b50\u7a7a\u95f4\u805a\u7c7b\uff0c\u4f7f\u7528\u5e26\u65f6\u95f4\u6b63\u5219\u5316\u7684\u5b50\u7a7a\u95f4\u5d4c\u5165\uff0c\u4f7f\u6bcf\u5e27\u4e0e\u5176\u65f6\u95f4\u90bb\u5c45\u5171\u4eab\u76f8\u4f3c\u5d4c\u5165\uff1b\u57fa\u4e8e\u5b50\u7a7a\u95f4\u5d4c\u5165\u548c\u65f6\u95f4\u7ea6\u675f\u8fdb\u884c\u5206\u5272\uff1b\u5f15\u5165\u53cd\u9988\u673a\u5236\uff0c\u4f9d\u636e\u5206\u5272\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u5d4c\u5165\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u8d85\u8d8a\u73b0\u6709SOTA \u7684\u6027\u80fd\u3002", "conclusion": "\u5c06 TVS \u878d\u5165\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684\u65e0\u76d1\u7763 HMS \u4e3a\u6709\u6548\u63d0\u5347\u5206\u5272\u6548\u679c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0cLLM \u4ea7\u751f\u7684 temporal semantics \u6709\u52a9\u4e8e\u6316\u6398\u66f4\u4e30\u5bcc\u7684\u65f6\u95f4\u4f9d\u8d56\uff1b\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u901a\u8fc7\u66f4\u5f3a\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u548c\u81ea\u76d1\u7763\u4fe1\u53f7\u63d0\u5347\u3002"}}
{"id": "2512.23610", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23610", "abs": "https://arxiv.org/abs/2512.23610", "authors": ["Heba Osama", "Omar Elebiary", "Youssef Qassim", "Mohamed Amgad", "Ahmed Maghawry", "Ahmed Saafan", "Haitham Ghalwash"], "title": "Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation", "comment": null, "summary": "Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.", "AI": {"tldr": "WAMM \u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u589e\u5f3a\u3001LLM \u5f15\u5bfc\u7684\u91cd\u65b0\u6807\u6ce8\u548c\u8fc7\u6ee4\uff0c\u63d0\u5347\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7f51\u9875\u653b\u51fb\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u9632\u706b\u5899\u3002", "motivation": "\u5e94\u5bf9\u4f20\u7edf\u9759\u6001\u89c4\u5219\uff08\u5982 OWASP CRS\uff09\u5728\u5bf9\u6297\u53ef\u6df7\u6dc6\u3001\u96f6\u65e5\u653b\u51fb\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u9ad8\u6548\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u3001\u9c81\u68d2\u7684\u7f51\u9875\u653b\u51fb\u68c0\u6d4b\u7684\u5fc5\u8981\u6027\u3002", "method": "\u5728 SR-BH 2020 \u6570\u636e\u96c6\u4e0a\u5e94\u7528\u591a\u9636\u6bb5\u589e\u5f3a\u6d41\u7a0b\uff08\u53bb\u91cd\u3001LLM \u6307\u5bfc\u7684\u91cd\u65b0\u6807\u6ce8\u3001\u771f\u5b9e\u653b\u51fb\u6570\u636e\u589e\u5f3a\u3001LLM \u8fc7\u6ee4\uff09\uff0c\u5f62\u6210\u4e09\u7ec4 refined \u6570\u636e\u96c6\uff1b\u6784\u5efa\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff08\u7edf\u8ba1\u4e0e\u6587\u672c\u8868\u793a\u6df7\u5408\uff09\uff1b\u8bc4\u4f30\u56db\u79cd\u6a21\u578b\uff08\u542b XGBoost \u7b49\u6df1\u5ea6/\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff09\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u4e0a\u5bf9\u6bd4 OWASP CRS \u7684\u68c0\u6d4b\u3002", "result": "\u5728\u540c\u4e00\u6280\u672f\u6808\u7684\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\uff0cXGBoost \u8fbe\u5230 99.59% \u7684\u51c6\u786e\u7387\u5e76\u5b9e\u73b0\u5fae\u79d2\u7ea7\u63a8\u7406\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u566a\u58f0\u589e\u5f3a\u4e0b\u6027\u80fd\u4e0b\u964d\uff1b\u5bf9 CRS \u7684\u672a\u89c1\u589e\u5f3a\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cWAMM \u7684\u771f\u6b63\u9633\u6027\u62e6\u622a\u7387\u4e3a 96\u2013100%\uff0c\u6539\u8fdb\u5e45\u5ea6\u6700\u9ad8\u53ef\u8fbe 86%\u3002", "conclusion": "\u7ecf\u7531\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u7ba1\u7ebf\u4e0e\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5177\u5f39\u6027\u4e14\u9002\u5408\u751f\u4ea7\u73af\u5883\u7684\u5b9e\u65f6\u7f51\u9875\u653b\u51fb\u68c0\u6d4b\uff0c\u63ed\u793a\u57fa\u4e8e\u89c4\u5219\u7684\u9632\u5fa1\u5728\u73b0\u4ee3\u5bf9\u6297\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.22259", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22259", "abs": "https://arxiv.org/abs/2512.22259", "authors": ["Daniil Burakov", "Ivan Petrov", "Dmitrii Khelimskii", "Ivan Bessonov", "Mikhail Lazarev"], "title": "Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data", "comment": null, "summary": "Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.", "AI": {"tldr": "\u901a\u8fc7\u5bf9PCI\u540e3\u5e74\u6b7b\u4ea1\u9884\u6d4b\u7684\u7814\u7a76\uff0c\u4f7f\u7528\u73b0\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u89e3\u51b3\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u589e\u91cf\u6570\u636e\u589e\u5f3a\u63d0\u5347\u5c11\u6570\u7c7b\u53ec\u56de\u7387\u548c\u6982\u7387\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301AUROC\uff0c\u53d1\u73b0\u5e74\u9f84\u3001\u5c04\u8840\u5206\u6570\u3001\u5468\u8fb9\u52a8\u8109\u75be\u75c5\u548c\u8111\u8840\u7ba1\u75be\u75c5\u4e3a\u6700\u91cd\u8981\u7279\u5f81\uff0c\u5f3a\u8c03\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u62a5\u544a\u6982\u7387\u8d28\u91cf\u4e0e\u538b\u529b\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u5728PCI\u672f\u540e3\u5e74\u6b7b\u4ea1\u9884\u6d4b\u4e2d\u5b58\u5728\u7684\u9ad8\u5ea6\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u54ea\u4e9b\u4e34\u5e8a\u56e0\u7d20\u5bf9\u6b7b\u4ea1\u98ce\u9669\u5f71\u54cd\uff1b\u9a8c\u8bc1\u5408\u6210\u6837\u672c\u6570\u636e\u589e\u5f3a\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5728\u7279\u5f81\u53bb\u9664\u5b9e\u9a8c\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u56de\u987e\u6027\u5206\u67902044\u4f8b\u7ecf\u76ae\u51a0\u72b6\u52a8\u8109\u4ecb\u5165\u6cbb\u7597(Bifurcation)\u60a3\u8005\u7684\u80f8\u75db\u6570\u636e\uff0c\u5efa\u7acb\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b3\u5e74\u6b7b\u4ea1\uff1b\u4e3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u989d\u5916\u751f\u6210500\u4e2a\u5408\u6210\u6837\u672c\u5e76\u52a0\u5165\u8bad\u7ec3\u96c6\uff1b\u4f7f\u7528\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u5404\u7279\u5f81\u8d21\u732e\uff1b\u901a\u8fc7\u79fb\u9664\u975e\u4fe1\u606f\u6027\u7279\u5f81\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u4ee5\u8bc4\u4f30\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u82e5\u4e0d\u8fdb\u884c\u8fc7\u91c7\u6837\uff0c\u6a21\u578b\u5728\u6574\u4f53\u51c6\u786e\u5ea6\u9ad8\uff080.92-0.93\uff09\uff0c\u4f46\u51e0\u4e4e\u5ffd\u7565\u5c11\u6570\u7c7b\uff1b\u5f15\u5165\u5408\u6210\u6570\u636e\u589e\u5f3a\u540e\uff0c\u5c11\u6570\u7c7b\u53ec\u56de\u7387\u663e\u8457\u63d0\u9ad8\uff0cAUROC\u635f\u5931\u6781\u5c0f\uff0c\u6982\u7387\u8d28\u91cf\u4e0e\u4e34\u5e8a\u98ce\u9669\u4f30\u8ba1\u66f4\u5408\u7406\uff1b\u56db\u4e2a\u6700\u5177\u5f71\u54cd\u529b\u7684\u7279\u5f81\u4e3a\uff1a\u5e74\u9f84\u3001\u5c04\u8840\u5206\u6570\u3001\u5916\u5468\u52a8\u8109\u75be\u75c5\u3001\u8111\u8840\u7ba1\u75be\u75c5\u3002", "conclusion": "\u7528\u73b0\u5b9e\u4e0e\u6781\u7aef\u573a\u666f\u7684\u7b80\u5355\u53cc\u6837\u672c\u6269\u5145\u5373\u53ef\u63ed\u793a\u5e76\u7f13\u89e3\u4e0d\u5e73\u8861\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u4e14\u5e94\u5c06\u6982\u7387\u8d28\u91cf\u4e0e\u538b\u529b\u6d4b\u8bd5\u7b49\u4fe1\u606f\u7eb3\u5165\u62a5\u9053\uff0c\u4ee5\u63d0\u5347\u57fa\u4e8e\u8868\u683c\u6570\u636e\u7684\u9884\u6d4b\u6a21\u578b\u5728\u4e34\u5e8a\u4e2d\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2512.22261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22261", "abs": "https://arxiv.org/abs/2512.22261", "authors": ["Rahul D Ray"], "title": "The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning", "comment": "11 pages, 5 figures. Includes systematic physics ablation study, theoretical analysis, and downstream machine learning evaluation. Source code available at: https://github.com/Reckonchamp12/physics-constraint-paradox", "summary": "Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.", "AI": {"tldr": "\u7cfb\u7edf\u6027\u6d88\u878d\u7814\u7a76\u63ed\u793a\uff1a\u5728\u7269\u7406\u7ea6\u675f\u7684\u6570\u636e\u751f\u6210\u4e2d\uff0c\u67d0\u4e9b\u7ea6\u675f\u53ef\u88ab\u79fb\u9664\u800c\u4e0d\u5f71\u54cd\u5173\u952e\u7269\u7406\u5c5e\u6027\uff1bFabry-Perot\u632f\u8361\u5bf9\u5e26\u5bbd\u6ce2\u52a8\u7684\u8d21\u732e\u6700\u5927\uff0c\u53bb\u9664\u540e\u5e26\u5bbd\u5c55\u5bbd\u663e\u8457\u51cf\u5c11\uff1b\u80fd\u91cf\u5b88\u6052\u5728\u7269\u7406\u4e00\u81f4\u7684\u65b9\u7a0b\u4e0b\u5197\u4f59\uff1b\u566a\u58f0\u5904\u7406\u5b58\u5728\u4ea7\u751f\u975e\u7269\u7406\u8d1f\u5438\u6536\u7684\u98ce\u9669\uff1b\u751f\u6210\u901f\u5ea6\u5feb\uff08200\u6837\u672c/\u79d2\uff09\uff0c\u8fdc\u8d85\u5168\u6ce2\u6c42\u89e3\uff1b\u7269\u7406\u53ef\u5b66\u4e60\u6027\u5b58\u5728\u6743\u8861\uff1a\u4e2d\u5fc3\u6ce2\u957f\u4fdd\u6301\u9884\u6d4b\u7a33\u5b9a\uff0c\u53bb\u9664Fabry-Perot\u540e\u5e26\u5bbd\u9884\u6d4b\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u63a2\u7a76\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u5982\u4f55\u6709\u9009\u62e9\u5730\u5f15\u5165\u7269\u7406\u7ea6\u675f\u4ee5\u63d0\u5347\u6570\u636e\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u5229\u7528ML\u6027\u80fd\u6765\u8bca\u65ad\u54ea\u4e9b\u7ea6\u675f\u5bf9\u7279\u5b9a\u8f93\u51fa\u6700\u4e3a\u5173\u952e\u3002", "method": "\u5bf9\u4e94\u4e2a\u51e0\u4f55\u53c2\u6570\u6620\u5c04\u5230100\u70b9\u5149\u8c31\u54cd\u5e94\u7684\u7269\u7406\u7ea6\u675f\u6570\u636e\u751f\u6210\u5668\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u9010\u6b65\u79fb\u9664\u80fd\u91cf\u5b88\u6052\u3001Fabry-Perot\u632f\u8361\u3001\u5e26\u5bbd\u53d8\u5f02\u7ea6\u675f\u4ee5\u53ca\u566a\u58f0\uff0c\u6bd4\u8f83\u6709\u65e0\u7ea6\u675f\u4e0b\u7684\u8f93\u51fa\u8d28\u91cf\u53ca\u4e0b\u6e38ML\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7ed3\u8bba\u5305\u62ec\uff1a\u80fd\u91cf\u5b88\u6052\u5728\u8be5\u4f53\u7cfb\u4e0b\u5197\u4f59\uff1b\u53bb\u9664Fabry-Perot\u632f\u8361\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\u53d8\u5f02\uff08\u5e26\u5bbd\u534a\u5bbd\u5ea6\u5c55\u5bbd\u4ece132.3 nm\u964d\u81f337.4 nm\uff0c\u7ea672%\u7684\u6539\u8fdb\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u5e26\u5bbd\u9884\u6d4b\u7684R^2\uff08\u63d0\u5347\u7ea631.3%\uff09\u548cRMSE\u4e0b\u964d\u7ea673.8%\uff1b\u566a\u58f0\u7ba1\u7ebf\u53ef\u80fd\u5f15\u51650.5%\u7684\u975e\u7269\u7406\u8d1f\u5438\u6536\uff1b\u4e2d\u5fc3\u6ce2\u957f\u9884\u6d4b\u4e0d\u53d7\u5f71\u54cd\uff1b\u751f\u6210\u901f\u7387\u4e3a200\u6837\u672c/\u79d2\uff0c\u8fdc\u5feb\u4e8e\u5e38\u89c4\u5168\u6ce2\u6c42\u89e3\u3002", "conclusion": "\u4e3a\u7269\u7406\u7ea6\u675f\u6570\u636e\u751f\u6210\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5357\uff1a\u4f18\u5148\u5173\u6ce8\u5bf9\u76ee\u6807\u5e26\u5bbd\u7684\u7ea6\u675f\u8d21\u732e\uff1b\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6027\u80fd\u4f5c\u4e3a\u7ea6\u675f\u76f8\u5173\u6027\u7684\u8bca\u65ad\u5de5\u5177\uff1b\u8bbe\u8ba1\u6570\u636e\u96c6\u65f6\u9700\u8b66\u60d5\u566a\u58f0\u5904\u7406\u5e26\u6765\u7684\u975e\u7269\u7406\u7ed3\u679c\u3002"}}
{"id": "2512.22293", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22293", "abs": "https://arxiv.org/abs/2512.22293", "authors": ["Tsogt-Ochir Enkhbayar"], "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against", "comment": "Submitted to Neel Nanda's MATS Stream", "summary": "Warning-framed content in training data (e.g., \"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: \"describing X\" and \"performing X\" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call \"stealth slip\", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.", "AI": {"tldr": "\u8b66\u544a\u6027\u8bad\u7ec3\u6570\u636e\u5e76\u975e\u80fd\u6709\u6548\u6291\u5236\u8bed\u8a00\u6a21\u578b\u518d\u73b0\u8b66\u544a\u5185\u5bb9\u3002\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728\u66b4\u9732\u4e8e\u201c\u7981\u6b62\u4f7f\u7528/\u6709\u6f0f\u6d1e\u201d\u7684\u8b66\u544a\u548c\u76f4\u63a5\u770b\u5230\u5bf9\u5e94\u5185\u5bb9\u7684\u518d\u73b0\u7387\u5dee\u5f02\u4e0d\u663e\u8457\uff08\u7ea676.7%\u5bf983.3%\uff09\uff0c\u539f\u56e0\u5728\u4e8e\u6f5c\u5728\u7279\u5f81\u7684\u6b63\u4ea4\u5316\u5931\u8d25\uff1a\u63cf\u8ff0\u884c\u4e3aX\u4e0e\u6267\u884c\u884c\u4e3aX\u6fc0\u6d3b\u7684\u6f5c\u5728\u7279\u5f81\u91cd\u53e0\u3002\u7279\u5f81#8684\uff08\u8ddf\u8e2a\u4ee3\u7801\u6267\u884c\u6a21\u5f0f\uff09\u5728\u4e24\u79cd\u60c5\u5883\u4e0b\u5747\u6709\u663e\u8457\u6fc0\u6d3b\u3002\u8fd8\u53d1\u73b0\u201c\u9690\u5f62\u6ed1\u79fb\u201d\uff08stealth slip\uff09\u73b0\u8c61\uff1a\u5bf9\u8bdd\u524d\u7f00\u53ef\u5c06\u6fc0\u6d3b\u65cb\u8f6c\u5230\u7ebf\u6027\u63a2\u6d4b\u96be\u4ee5\u68c0\u6d4b\u7684\u5b50\u7a7a\u95f4\u3002\u63d0\u793a\u4e0e\u63a8\u7406\u65f6\u7684\u5f15\u5bfc\u65e0\u6548\uff0c\u8bad\u7ec3\u9636\u6bb5\u7684\u7279\u5f81\u6d88\u878d\u624d\u6709\u6548\u3002\u7ed3\u8bba\u662f\u7edf\u8ba1\u5171\u73b0\u652f\u914d\u73b0\u6709\u67b6\u6784\u7684\u884c\u4e3a\uff0c\u6a21\u578b\u5b66\u4e60\u7684\u662f\u4e0a\u4e0b\u6587\u540e\u7eed\u7684\u7edf\u8ba1\u89c4\u5f8b\uff0c\u800c\u975e\u4e3a\u4f55\u4f1a\u51fa\u73b0\u5728\u8be5\u4e0a\u4e0b\u6587\u3002", "motivation": "\u63ed\u793a\u4e3a\u4ec0\u4e48\u57fa\u4e8e\u8b66\u544a\u7684\u5b89\u5168\u63d0\u793a\u672a\u80fd\u6709\u6548\u7ea6\u675f\u6a21\u578b\u884c\u4e3a\uff0c\u5206\u6790\u80cc\u540e\u7684\u8868\u5f81\u4e0e\u8bad\u7ec3\u52a8\u6001\uff0c\u4ee5\u53ca\u5bfc\u81f4\u8b66\u544a\u65e0\u6548\u7684\u6f5c\u5728\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u578b\u66b4\u9732\u4e8e\u88ab\u8b66\u544a\u7684\u5185\u5bb9\u4e0e\u76f4\u63a5\u5185\u5bb9\u7684\u5b9e\u9a8c\u5bf9\u7167\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5bf9\u6f5c\u5728\u7279\u5f81\u7684\u5206\u6790\uff0c\u8bc6\u522b\u91cd\u53e0\u7684\u7279\u5f81\u6fc0\u6d3b\u4e0e\u5b50\u7a7a\u95f4\u65cb\u8f6c\uff1b\u8bc4\u4f30\u63d0\u793a/\u63a8\u7406\u65f6\u7684\u5f15\u5bfc\u6548\u679c\uff1b\u8fdb\u884c\u8bad\u7ec3\u9636\u6bb5\u7684\u7279\u5f81\u6d88\u878d\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "result": "\u8b66\u544a\u5185\u5bb9\u7684\u518d\u73b0\u7387\u4e0e\u76f4\u63a5\u5185\u5bb9\u76f8\u8fd1\uff08\u598276.7% vs 83.3%\uff09\uff0c\u8bf4\u660e\u8b66\u544a\u672a\u8d77\u6548\u3002\u81ea\u7f16\u7801\u5668\u5206\u6790\u63ed\u793a\u63cf\u8ff0X\u4e0e\u6267\u884cX\u5171\u4eab\u6fc0\u6d3b\u7279\u5f81\uff1b\u7279\u5f81#8684\u4e0e\u4ee3\u7801\u6267\u884c\u6a21\u5f0f\u76f8\u5173\uff0c\u5728\u4e24\u60c5\u5f62\u4e0b\u5747\u6fc0\u6d3b\u3002\u51fa\u73b0\u201c\u9690\u5f62\u6ed1\u79fb\u201d\u673a\u5236\uff0c\u4f7f\u524d\u7f00\u80fd\u591f\u5c06\u6fc0\u6d3b\u79fb\u5165\u7ebf\u6027\u63a2\u9488\u96be\u4ee5\u68c0\u6d4b\u7684\u5b50\u7a7a\u95f4\u3002\u63d0\u793a/\u63a8\u7406\u5f15\u5bfc\u65e0\u6548\uff0c\u53ea\u6709\u8bad\u7ec3\u9636\u6bb5\u7684\u7279\u5f81\u6d88\u878d\u624d\u80fd\u964d\u4f4e\u98ce\u9669\u8f93\u51fa\u3002\u7ed3\u8bba\u662f\u7edf\u8ba1\u5171\u73b0\u4f18\u4e8e\u8bed\u7528\u7406\u89e3\uff0c\u6a21\u578b\u5b66\u4e60\u7684\u662f\u4e0a\u4e0b\u6587\u4e2d\u7684\u540e\u7eed\u6a21\u5f0f\u800c\u975e\u539f\u56e0\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u67b6\u6784\u4e2d\uff0c\u7edf\u8ba1\u5171\u73b0\u4e3b\u5bfc\u8f93\u51fa\uff0c\u8b66\u544a\u6027\u63d0\u793a\u5bf9\u63a7\u5236\u8fdd\u89c4\u884c\u4e3a\u6548\u679c\u6709\u9650\uff1b\u9700\u5728\u8bad\u7ec3\u9636\u6bb5\u8bbe\u8ba1\u5e72\u9884\uff08\u7279\u5f81\u6d88\u878d\u3001\u6b63\u4ea4\u5316\u7ea6\u675f\u7b49\uff09\u4ee5\u6539\u53d8\u6f5c\u5728\u8868\u793a\uff0c\u624d\u80fd\u6709\u6548\u964d\u4f4e\u518d\u73b0\u8fdd\u89c4\u5185\u5bb9\u7684\u98ce\u9669\u3002"}}
{"id": "2512.22279", "categories": ["cs.LG", "cs.CE", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2512.22279", "abs": "https://arxiv.org/abs/2512.22279", "authors": ["Amgad Ahmed Ali Ibrahim", "Hein Htet", "Ryoji Asahi"], "title": "Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization", "comment": null, "summary": "Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.", "AI": {"tldr": "\u63d0\u51faSoDip\uff1a\u4e00\u4e2a\u5206\u5c42\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u6587\u672c\u7f16\u7801\u3001\u591a\u6a21\u6001\u5efa\u6a21\u3001GPR+DPMM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u63d0\u5347\u5bf9RIG\u8fc7\u7a0b\u7684\u9884\u6d4b\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u5206\u6790\uff0c\u4e14\u80fd\u533a\u5206\u4f4e\u53ef\u91cd\u590d\u6027\u533a\u95f4\u3002", "motivation": "RIG\u8fc7\u7a0b\u7684\u91cd\u590d\u6027\u53d7\u57fa\u819c\u5f62\u8c8c\uff08\u6676\u4f53\u5ea6\u3001\u6676\u5411\u3001\u81ea\u7531\u4f53\u79ef\uff09\u53d8\u5f02\u6027\u5f71\u54cd\uff0c\u4f46\u76f8\u5173\u53d8\u91cf\u5f80\u672a\u5728\u6587\u732e\u4e2d\u7cfb\u7edf\u62a5\u544a\uff0c\u5bfc\u81f4\u6c14\u4f53\u6269\u6563\u3001\u81ea\u7531\u57fa\u5206\u5e03\u548cTrommsdorff\u6548\u5e94\u5f15\u8d77\u7684\u5c40\u90e8\u63a5\u679d\u68af\u5ea6\u4e0e\u6027\u80fd\u6ce2\u52a8\u96be\u4ee5\u63a7\u5236\u3002", "method": "\u63d0\u51faSoDip\u5206\u5c42\u4f18\u5316\u6846\u67b6\uff1a1) decoder-only Transformer (DeepSeek-R1) \u7f16\u7801\u6587\u672c\u578b\u8fc7\u7a0b\u63cf\u8ff0\uff08\u5982\u8f90\u7167\u6e90\u3001\u63a5\u679d\u7c7b\u578b\u3001\u57fa\u6750\u5382\u5bb6\uff09\uff1b2) TabNet\u4e0eXGBoost \u6355\u6349\u591a\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\uff1b3) \u4ee5Dirichlet Process Mixture Models\u4e3a\u57fa\u7840\u7684\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u5f02\u65b9\u5dee\u5206\u6790\uff1b4) \u8d1d\u53f6\u65af\u4f18\u5316\u5728\u9ad8\u7ef4\u5408\u6210\u7a7a\u95f4\u4e2d\u9ad8\u6548\u63a2\u7d22\u3002\u6570\u636e\u96c6\u901a\u8fc7ChemDataExtractor 2.0\u4e0eWebPlotDigitizer\u6574\u7406\uff0c\u6db5\u76d6\u5927\u91cfRIG\u7814\u7a76\u4e2d\u7684\u6570\u503c\u4e0e\u6587\u672c\u53d8\u91cf\u3002", "result": "\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\uff0cSoDip\u76f8\u6bd4GPR\u5b9e\u73b0\u7ea633%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u7ecf\u8fc7\u6821\u51c6\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u80fd\u591f\u8bc6\u522b\u4f4e\u53ef\u91cd\u590d\u6027\u533a\u57df\uff1b\u5176\u5206\u5c42\u7ed3\u6784\u6709\u6548\u6574\u5408\u7a00\u758f\u6587\u672c\u4e0e\u6570\u503c\u8f93\u5165\uff0c\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u4e14\u4e3a\u57fa\u4e8e\u5f62\u8c8c\u7684\u53ef\u91cd\u590d\u8bbe\u8ba1\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u6027\u4e0e\u5f62\u8c8c\u611f\u77e5\u7684RIG\u8bbe\u8ba1\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u63a5\u679d\u805a\u5408\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2512.22280", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22280", "abs": "https://arxiv.org/abs/2512.22280", "authors": ["Varshith Gudur"], "title": "Valori: A Deterministic Memory Substrate for AI Systems", "comment": "7 pages, 1 figure. systems paper with empirical evaluation and determinism validation experiments. Code available at https://github.com/varshith-Git/Valori-Kernel", "summary": "Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).", "AI": {"tldr": "\u63d0\u51fa Valori \u7684\u786e\u5b9a\u6027 AI \u5185\u5b58\u5e95\u5c42\uff0c\u4f7f\u7528\u56fa\u5b9a\u70b9 Q16.16 \u66ff\u4ee3\u6d6e\u70b9\u5185\u5b58\u64cd\u4f5c\uff0c\u5c06\u5185\u5b58\u5efa\u6a21\u4e3a\u53ef\u91cd\u653e\u7684\u72b6\u6001\u673a\uff0c\u4ee5\u5728\u8de8\u5e73\u53f0\u83b7\u5f97\u4f4d\u4e00\u81f4\u7684\u5185\u5b58\u72b6\u6001\u3001\u5feb\u7167\u548c\u68c0\u7d22\u7ed3\u679c\uff1b\u5e76\u7ed9\u51fa\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u6d6e\u70b9\u5185\u5b58\u5bfc\u81f4\u8de8\u786c\u4ef6\u67b6\u6784\uff08\u5982 x86 \u4e0e ARM\uff09\u5448\u73b0\u4e0d\u540c\u7684\u5185\u5b58\u72b6\u6001\u4e0e\u68c0\u7d22\u7ed3\u679c\uff0c\u524a\u5f31\u53ef\u56de\u653e\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u7279\u522b\u662f\u5728\u53d7\u76d1\u7ba1\u9886\u57df\uff1b\u9700\u8981\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u5185\u5b58\u539f\u8bed\u6765\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u7684 AI \u7cfb\u7edf\u3002", "method": "\u5c06\u6d6e\u70b9\u5185\u5b58\u64cd\u4f5c\u66ff\u6362\u4e3a\u56fa\u5b9a\u70b9\uff08Q16.16\uff09\uff1b\u5c06\u5185\u5b58\u89c6\u4e3a\u53ef\u91cd\u653e\u7684\u72b6\u6001\u673a\u5e76\u5728\u5185\u5b58\u8fb9\u754c\u5f3a\u5236\u786e\u5b9a\u6027\uff1b\u5206\u6790\u5728\u7d22\u5f15\u6216\u68c0\u7d22\u4e4b\u524d\u5373\u51fa\u73b0\u975e\u786e\u5b9a\u6027\uff0c\u5e76\u5c55\u793a Valori \u5982\u4f55\u5728\u5185\u5b58\u8fb9\u754c\u5904\u5f3a\u5236\u786e\u5b9a\u6027\uff1b\u7ed9\u51fa\u53c2\u8003\u5b9e\u73b0\u7684\u5f00\u6e90\u4ee3\u7801\u4e0e\u6863\u6848\u3002", "result": "\u8868\u660e\u8de8\u5e73\u53f0\u53ef\u4ee5\u5b9e\u73b0\u786e\u5b9a\u6027\uff0c\u5b9a\u4f4d\u4e86\u5bfc\u81f4\u975e\u786e\u5b9a\u6027\u7684\u9636\u6bb5\uff0c\u5c55\u793a\u5728\u5185\u5b58\u8fb9\u754c\u5904\u7684\u786e\u5b9a\u6027\uff1b\u7ed3\u8bba\u8ba4\u4e3a\u786e\u5b9a\u6027\u5185\u5b58\u662f\u53ef\u4fe1 AI \u7684\u5fc5\u8981\u539f\u8bed\uff1b\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002", "conclusion": "\u786e\u5b9a\u6027\u5185\u5b58\uff08Valori\uff09\u662f\u5b9e\u73b0\u53ef\u4fe1 AI \u7cfb\u7edf\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2512.22283", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22283", "abs": "https://arxiv.org/abs/2512.22283", "authors": ["Guokan Chen", "Yao Xiao"], "title": "DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations", "comment": null, "summary": "Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.", "AI": {"tldr": "Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN) to address gradient stiffness and spectral bias in PINNs for multi-scale/high-frequency PDEs. It combines Kolmogorov-Arnold networks with learnable B-splines and an adaptive weighting with a dynamic decay upper bound, achieving faster convergence and at least an order-of-magnitude improvement in accuracy without extra computational cost.", "motivation": "PINNs struggle with stiffness in gradient flow and spectral bias when solving multi-scale or high-frequency PDEs, limiting predictive accuracy and generalization.", "method": "Introduce DBAW-PIKAN by integrating Kolmogorov-Arnold network architecture (learnable B-splines) with an adaptive weighting strategy that includes a dynamic decay upper bound to balance gradient contributions and improve representational capacity.", "result": "Empirical benchmarks on Klein-Gordon, Burgers, and Helmholtz equations show significantly enhanced accuracy and generalization, with convergence accelerated and accuracy improved by at least one order of magnitude, without increasing computational complexity.", "conclusion": "DBAW-PIKAN effectively mitigates gradient-related failure modes in PINNs for challenging PDEs and offers a scalable, more accurate framework with improved generalization."}}
{"id": "2512.22287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22287", "abs": "https://arxiv.org/abs/2512.22287", "authors": ["Zikun Guoa", "Adeyinka. P. Adedigbaa", "Rammohan Mallipeddi"], "title": "Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation", "comment": "18pages, 5Figues", "summary": "Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.", "AI": {"tldr": "\u63d0\u51fa Cluster Aggregated GAN\uff0c\u9488\u5bf9\u65ad\u7eed\u578b\u548c\u8fde\u7eed\u578b\u5bb6\u7528\u7535\u5668\u5206\u8def\u751f\u6210\uff0c\u63d0\u9ad8\u5408\u6210\u8d1f\u8377\u7684\u73b0\u5b9e\u6027\u3001\u591a\u6837\u6027\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u73b0\u6709 GAN \u65b9\u6cd5\u5c06\u6240\u6709\u8bbe\u5907\u6df7\u5408\u5efa\u6a21\uff0c\u5ffd\u89c6\u8bbe\u5907\u884c\u4e3a\u5dee\u5f02\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8f93\u51fa\u4fdd\u771f\u5ea6\u53d7\u9650\u3002", "method": "\u5f15\u5165\u6df7\u5408\u751f\u6210\u6846\u67b6\uff1a\u5bf9\u65ad\u7eed\u578b\u8bbe\u5907\u4f7f\u7528\u805a\u7c7b\u6a21\u5757\u5c06\u76f8\u4f3c\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7c07\uff0c\u5e76\u4e3a\u6bcf\u7c07\u5206\u914d\u72ec\u7acb\u751f\u6210\u5668\uff1b\u5bf9\u8fde\u7eed\u578b\u8bbe\u5907\u4f7f\u7528\u57fa\u4e8e LSTM \u7684\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u538b\u7f29\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u7528\u4e8e\u805a\u7c7b\u4e0e\u751f\u6210\u7684\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5728 UVIC smart plug \u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u611f\u3001\u591a\u6837\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5c06\u805a\u7c7b\u4f5c\u4e3a\u4e3b\u52a8\u751f\u6210\u7ec4\u4ef6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a NILM \u7814\u7a76\u4e2d\u7684\u5408\u6210\u8d1f\u8377\u751f\u6210\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22288", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22288", "abs": "https://arxiv.org/abs/2512.22288", "authors": ["Renping Zhou", "Zanlin Ni", "Tianyi Chen", "Zeyu Liu", "Yang Yue", "Yulin Wang", "Yuxuan Wang", "Jingshu Liu", "Gao Huang"], "title": "Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model", "comment": "17 pages, 6 figures", "summary": "Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22290", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22290", "abs": "https://arxiv.org/abs/2512.22290", "authors": ["Arunkumar V", "Nivethitha S", "Sharan Srinivas", "Gangadharan G. R"], "title": "When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing", "comment": null, "summary": "A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22309", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22309", "abs": "https://arxiv.org/abs/2512.22309", "authors": ["Zehao Chen", "Tianxiang Ai", "Yifei Li", "Gongxun Li", "Yuyang Wei", "Wang Zhou", "Guanghui Li", "Bin Yu", "Zhijun Chen", "Hailong Sun", "Fuzhen Zhuang", "Jianxin Li", "Deqing Wang", "Yikun Ban"], "title": "LLMBoost: Make Large Language Models Stronger with Boosting", "comment": null, "summary": "Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.", "AI": {"tldr": "\u63d0\u51fa LLMBoost\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u663e\u5f0f\u5229\u7528\u3001\u94fe\u5f0f\u8bad\u7ec3\u548c\u8fd1\u5e76\u884c\u63a8\u7406\u5b9e\u73b0\u5f3a\u589e\u5f3a\u7684LLM\u96c6\u6210\u5fae\u8c03\u3002", "motivation": "\u6253\u7834\u9ed1\u76d2\u5f0f\u8f93\u5165/\u8f93\u51fa\u7ea7\u522b\u7684\u96c6\u6210\uff0c\u5145\u5206\u6316\u6398\u548c\u4f20\u9012\u6a21\u578b\u95f4\u7684\u4e2d\u95f4\u8868\u5f81\u4e0e\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a\u8de8\u6a21\u578b\u6ce8\u610f\u4ee5\u878d\u5408\u524d\u9a71\u9690\u85cf\u72b6\u6001\u3001\u94fe\u5f0f\u8bad\u7ec3\u4ee5\u9010\u6b65\u7ea0\u9519\u3001\u8fd1\u5e76\u884c\u63a8\u7406\u4ee5\u5c42\u5c42\u4f20\u9012\u9690\u85cf\u72b6\u6001\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728 commonsense \u4e0e arithmetic \u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u7ed9\u51fa\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5728\u6709\u754c\u4fee\u6b63\u5047\u8bbe\u4e0b\u7684\u5e8f\u5217\u6574\u5408\u53ef\u5b9e\u73b0\u5355\u8c03\u6539\u8fdb\u3002"}}
{"id": "2512.22313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22313", "abs": "https://arxiv.org/abs/2512.22313", "authors": ["Wenzhang Du"], "title": "Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making", "comment": "6 pages, 3 figures", "summary": "Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.\n  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.\n  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.", "AI": {"tldr": "\u63d0\u51fa Optimistic Feasible Search (OFS) \u7528\u4e8e\u5728\u5e26\u770b\u677f\u53cd\u9988\u7684\u5728\u7ebf\u5b66\u4e60\u4e2d\u4ece\u4e00\u7ef4\u9608\u503c\u7b56\u7565\u4e2d\u5b66\u4e60\uff0c\u5728\u4eba\u53e3\u516c\u5e73\u7684\u6761\u4ef6\u4e0b\uff08\u82e5\u5e72\u9879\u5305\u62ecDemographic Parity \u548c\u670d\u52a1\u7387\u7ea6\u675f\uff09\uff0c\u901a\u8fc7\u7f51\u683c\u5316\u548c\u4e50\u89c2\u7f6e\u4fe1\u754c\u5b9e\u73b0\u9009\u62e9\u6027\u9608\u503c\uff0c\u65e2\u8ffd\u6c42\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u9608\u503c\uff0c\u53c8\u63a7\u5236\u7ea6\u675f\u8fdd\u89c4\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u5408\u6210\u4e0e\u534a\u5408\u6210\u57fa\u51c6\u4e0b\u4f18\u4e8e\u65e0\u7ea6\u675f\u548c\u539f\u59cb\u5bf9\u5076\u57fa\u7ebf\uff0c\u5e76\u63a5\u8fd1\u6700\u4f18\u53ef\u884c\u56fa\u5b9a\u9608\u503c\u3002\u53ef\u91cd\u590d\u3001\u53cc\u76f2\u53cb\u597d\u3002", "motivation": "\u89e3\u51b3\u53d7\u51b3\u7b56\u53cd\u9988\u5f71\u54cd\u800c\u4ea7\u751f\u975e\u5e73\u7a33\u6570\u636e\u548c\u6f5c\u5728\u5dee\u5f02\u653e\u5927\u7684\u95ed\u73af\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u4e0e\u670d\u52a1\u7ea6\u675f\u95ee\u9898\u3002\u5728\u7ebf\u5b66\u4e60\u4e00\u7ef4\u9608\u503c\u7b56\u7565\uff0c\u5728\u5e26\u5c11\u91cf\u53cd\u9988\u4fe1\u606f\u7684\u5e26\u7ea6\u675f\u60c5\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u4f18\u5316\u3002", "method": "\u7f51\u683c\u5316\u7684\u4e50\u89c2\u53ef\u884c\u641c\u7d22\uff08OFS\uff09\uff1a\u5bf9\u6bcf\u4e2a\u5019\u9009\u9608\u503c\u7ef4\u62a4\u5956\u52b1\u4e0e\u7ea6\u675f\u6b8b\u5dee\u7684\u7f6e\u4fe1\u533a\u95f4\uff1b\u5728\u6bcf\u8f6e\u9009\u62e9\u5728\u7f6e\u4fe1\u754c\u4e0b\u770b\u4f3c\u53ef\u884c\u7684\u9608\u503c\u4e2d\u6700\u5927\u5316\u4e50\u89c2\u5956\u52b1\uff1b\u82e5\u65e0\u53ef\u884c\u9608\u503c\u5219\u9009\u62e9\u5728\u4e50\u89c2\u7ea6\u675f\u8fdd\u89c4\u4e0b\u7684\u9608\u503c\uff0c\u4ee5\u76f4\u63a5\u7784\u51c6\u9ad8\u6548\u53ef\u884c\u9608\u503c\uff0c\u9002\u5408\u4f4e\u7ef4\u53ef\u89e3\u91ca\u7b56\u7565\u7a7a\u95f4\u3002", "result": "\u5728\u5305\u62ec\u7a33\u5b9a\u6536\u7f29\u52a8\u529b\u5b66\u7684\u5408\u6210\u57fa\u51c6\u548c\u57fa\u4e8e German Credit \u4e0e COMPAS \u7684\u534a\u5408\u6210\u57fa\u51c6\u4e2d\uff0cOFS \u5728\u7d2f\u79ef\u5956\u52b1\u548c\u7ea6\u675f\u8fdd\u89c4\u4e0a\u5747\u4f18\u4e8e\u65e0\u7ea6\u675f\u53ca\u539f\u59cb\u5bf9\u5076\u57fa\u7ebf\uff0c\u4e14\u5728\u540c\u4e00\u641c\u7d22\u8fc7\u7a0b\u4e0b\u63a5\u8fd1\u6700\u4f18\u53ef\u884c\u56fa\u5b9a\u9608\u503c\u3002\u5b9e\u9a8c\u53ef\u91cd\u590d\u4e14\u5b9e\u73b0\u4e86\u53cc\u76f2\u53cb\u597d\u76f8\u5bf9\u8f93\u51fa\u3002", "conclusion": "OFS \u80fd\u6709\u6548\u5728\u53d7\u95ed\u73af\u53cd\u9988\u5f71\u54cd\u3001\u5e26\u516c\u5e73\u4e0e\u670d\u52a1\u7ea6\u675f\u7684\u5728\u7ebf\u9608\u503c\u5b66\u4e60\u4e2d\uff0c\u627e\u5230\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u9608\u503c\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u7ef4\u3001\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u8bbe\u5b9a\uff0c\u4f18\u4e8e\u4f20\u7edf\u65e0\u7ea6\u675f\u4e0e primal-dual \u57fa\u7ebf\u3002"}}
{"id": "2512.22317", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22317", "abs": "https://arxiv.org/abs/2512.22317", "authors": ["Xudong Ling", "Tianxi Huang", "Qian Dong", "Tao He", "Chaorong Li", "Guiduo Duan"], "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting", "comment": null, "summary": "Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \\% and 19\\% gains in heavy-rainfall CSI at an 80-minute lead time.", "AI": {"tldr": "\u63d0\u51fa LangPrecip\uff0c\u4e00\u79cd\u8bed\u8a00\u611f\u77e5\u7684\u591a\u6a21\u6001 nowcasting \u6846\u67b6\uff0c\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u7ed3\u5408\u96f7\u8fbe\u4fe1\u606f\u5728\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u964d\u6c34\u9884\u6d4b\uff0c\u5e76\u63d0\u4f9b LangPrecip-160k \u6570\u636e\u96c6\uff1b\u5728\u745e\u5178\u548c MRMS \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u77ed\u65f6\u964d\u6c34\u9010\u6b65\u9884\u62a5\u5177\u6709\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4ee5\u89c6\u89c9\u6761\u4ef6\u4e3a\u4e3b\uff0c\u672a\u6765\u8fd0\u52a8\u88ab\u5f31\u7ea6\u675f\u4e14\u6a21\u7cca\u96be\u4ee5\u63a7\u5236\uff0c\u8bed\u8a00\u63cf\u8ff0\u53ef\u63d0\u4f9b\u8bed\u4e49\u5c42\u9762\u7684\u8fd0\u52a8\u7ea6\u675f\uff0c\u7ed3\u5408\u96f7\u8fbe\u4fe1\u606f\u53ef\u5b9e\u73b0\u66f4\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "method": "\u57fa\u4e8e Rectified Flow \u7684\u6846\u67b6\uff0c\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6587\u672c\u4e0e\u96f7\u8fbe\u4fe1\u606f\u7684\u9ad8\u6548\u6574\u5408\uff1b\u63d0\u51fa LangPrecip \u6846\u67b6\u53ca LangPrecip-160k \u6570\u636e\u96c6\uff08160k \u5bf9\u96f7\u8fbe\u5e8f\u5217\u4e0e\u8fd0\u52a8\u63cf\u8ff0\u7684\u591a\u6a21\u6001\u914d\u5bf9\uff09\u3002", "result": "\u5728\u745e\u5178\u4e0e MRMS \u6570\u636e\u96c6\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728 80 \u5206\u949f\u63d0\u524d\u671f\u4e0b\uff0cheavy-rainfall CSI \u83b7\u5f97\u8d85\u8fc7 60% \u4e0e 19% \u7684\u589e\u76ca\uff08\u5bf9\u5e94\u4e24\u4e2a\u6570\u636e\u96c6\uff09\u3002", "conclusion": "\u8bed\u8a00\u611f\u77e5\u7684\u591a\u6a21\u6001 nowcasting \u80fd\u6709\u6548\u63d0\u5347\u77ed\u65f6\u964d\u6c34\u9884\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4e14\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.22318", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22318", "abs": "https://arxiv.org/abs/2512.22318", "authors": ["Chorok Lee"], "title": "Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough", "comment": null, "summary": "Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.", "AI": {"tldr": "\u63d0\u51faCAGP\uff0c\u5206\u89e3\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u4ee5\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u65f6\u5e8fOOD\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u5173\u7cfb\u65e0\u5173\u57fa\u7ebf\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u5b9e\u73b0\u9ad8AUROC\u5e76\u63d0\u5347\u9009\u62e9\u6027\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u6982\u7387\u5316KG\u5d4c\u5165\u5c06\u65b9\u5dee\u8bbe\u4e3a\u4e0e\u5173\u7cfb\u65e0\u5173\uff0c\u6df7\u6dc6\u4e86\u65b0\u5174\u5b9e\u4f53\u4e0e\u65b0\u9896\u5173\u7cfb\u7b49\u4e24\u7c7b\u4e0d\u540c\u7684\u5f02\u5e38\u5206\u5e03\u73b0\u8c61\uff0c\u5bfc\u81f4\u5bf9\u65b0\u4e0a\u4e0b\u6587\u7684\u68c0\u6d4b\u80fd\u529b\u53d7\u9650\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u4e0d\u53ef\u5b9e\u73b0\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u4ec5\u57fa\u4e8e\u5b9e\u4f53\u7ea7\u7edf\u8ba1\u4e14\u5ffd\u7565\u5173\u7cfb\u4e0a\u4e0b\u6587\u7684\u4f30\u8ba1\u5668\u5bf9\u65b0\u4e0a\u4e0b\u6587\u7684OOD\u68c0\u6d4b\u8fd1\u4f3c\u968f\u673a\uff1b\u63d0\u51faCAGP\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u6743\u91cd\u5c06\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\uff08\u5b9e\u4f53\u5d4c\u5165\u65b9\u5dee\uff09\u4e0e\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\uff08\u5b9e\u4f53-\u5173\u7cfb\u5171\u73b0\uff09\u8fdb\u884c\u878d\u5408\uff0c\u83b7\u5f97\u975e\u5197\u4f59\u7684\u4fe1\u53f7\u5e76\u4f18\u4e8e\u4efb\u4e00\u5355\u4e00\u4fe1\u53f7\u3002", "result": "\u5728FB15k-237\u3001WN18RR\u3001YAGO3-10\u4e0a\u9a8c\u8bc1\uff0cCAGP\u5b9e\u73b00.94\u20130.99 AUROC\u7684\u65f6\u5e8fOOD\u68c0\u6d4b\uff0c\u8f83\u5173\u7cfb\u65e0\u5173\u57fa\u7ebf\u63d0\u534760\u201380%\u7684\u76f8\u5bf9\u6027\u80fd\uff1b\u4e09\u6570\u636e\u96c6\u5448\u73b0\u9891\u7387\u5b8c\u5168\u91cd\u53e0\uff1b\u5728\u9009\u62e9\u6027\u9884\u6d4b\u573a\u666f\uff0c\u51c6\u786e\u738785%\u65f6\u9519\u8bef\u7387\u4e0b\u964d\u7ea643%\u3002", "conclusion": "\u8bed\u4e49\u4e0e\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e92\u4e3a\u8865\u5145\u4e14\u975e\u5197\u4f59\uff0cCAGP\u80fd\u663e\u8457\u63d0\u5347\u5bf9\u65f6\u5e8f\u5206\u5e03\u504f\u79fb\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u63d0\u4f9b\u5bf9\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0d\u786e\u5b9a\u6027\u7684\u66f4\u6df1\u5165\u7406\u89e3\u4e0e\u5b9e\u7528\u63d0\u5347\u3002"}}
{"id": "2512.22326", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22326", "abs": "https://arxiv.org/abs/2512.22326", "authors": ["Sravan Karthick T"], "title": "Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers", "comment": null, "summary": "Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.", "AI": {"tldr": "\u5f15\u5165 Global M2 Liquidity \u4f5c\u4e3a12\u5468\u6ede\u540e\u5916\u751f\u53d8\u91cf\uff0c\u901a\u8fc7 TimeXer-Exog \u63d0\u5347\u6bd4\u7279\u5e01\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\uff0c\u572870\u5929 horizon \u4e0b\u7684 MSE \u4e3a 1.08e8\uff0c\u6bd4\u5355\u53d8\u91cf TimeXer \u63d0\u5347\u7ea689%\u3002", "motivation": "\u6bd4\u7279\u5e01\u4ef7\u683c\u5b58\u5728\u6781\u7aef\u6ce2\u52a8\u6027\u548c\u975e\u5e73\u7a33\u6027\uff0c\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u96be\u4ee5\u5bf9\u957f horizon \u505a\u51c6\u3002\u5c06\u5168\u7403\u5b8f\u89c2\u53d8\u91cf\u4e2d\u7684 Global M2 Liquidity \u4f5c\u4e3a\u9886\u5148\u6307\u6807\uff0c\u53ef\u80fd\u4e3a\u4ef7\u683c\u63d0\u4f9b\u6709\u6548\u5916\u751f\u4fe1\u606f\u3002", "method": "\u5728 TimeXer \u67b6\u6784\u4e2d\uff0c\u5c06\u6765\u81ea18\u4e2a\u4e3b\u8981\u7ecf\u6d4e\u4f53\u7684 Global M2 Liquidity \u768412\u5468\u6ede\u540e\u4f5c\u4e3a\u5916\u751f\u53d8\u91cf\uff0c\u6784\u5efa liquidity-conditioned \u6a21\u578b TimeXer-Exog\uff0c\u5e76\u4e0e LSTM\u3001N-BEATS\u3001PatchTST \u548c\u5355\u53d8\u91cf TimeXer \u8fdb\u884c\u5bf9\u6bd4\u3002\u6570\u636e\u8986\u76d6\u671f\u95f4\u4e3a 2020-01 \u81f3 2025-08 \u7684\u65e5\u5ea6\u6bd4\u7279\u5e01\u4ef7\u683c\uff0c\u8bc4\u4f3070\u5929\u9884\u6d4b\u3002", "result": "TimeXer-Exog \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c70\u5929\u9884\u6d4b\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u8fbe\u5230 1.08e8\uff0c\u8f83\u5355\u53d8\u91cf TimeXer \u63d0\u5347\u7ea6 89%\u3002\u663e\u5f0f\u7684\u5168\u7403\u6d41\u52a8\u6027\u6761\u4ef6\u663e\u8457\u7a33\u5b9a\u4e86\u957f horizon \u7684\u9884\u6d4b\u3002", "conclusion": "\u5c06\u5168\u7403\u6d41\u52a8\u6027\u4f5c\u4e3a\u5916\u751f\u4fe1\u606f\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u957f horizon \u6bd4\u7279\u5e01\u4ef7\u683c\u9884\u6d4b\uff0c\u5177\u6709\u5b9e\u8d28\u6027\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2512.22337", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22337", "abs": "https://arxiv.org/abs/2512.22337", "authors": ["Matthew Riemer", "Erik Miehling", "Miao Liu", "Djallel Bouneffouf", "Murray Campbell"], "title": "The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.", "AI": {"tldr": "LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u6307\u4ee4\u8c03\u4f18\u7684\u6a21\u578b\u4e0a\u53ef\u80fd\u4f1a\u4ea7\u751f\u707e\u96be\u6027\u6027\u80fd\u4e0b\u964d\uff1b\u901a\u8fc7\u6b63\u5219\u5316\u7684\u8fd1\u4f3c\u56de\u653e\uff08KL\u5bf9\u521d\u59cb\u6a21\u578b\u7684KL\u60e9\u7f5a\uff09\u5e76\u6df7\u5165\u6765\u81ea\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684\u5f00\u6e90\u8bed\u6599\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u5728\u4e0d\u663e\u8457\u635f\u5bb3\u6a21\u578b\u5851\u6027\u548c\u4efb\u52a1\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u8f83\u5c0f\u5f00\u9500\u5730\u4fdd\u7559\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u4f7f\u7528LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5bf9\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u9020\u6210\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5bfb\u627e\u4f4e\u5f00\u9500\u3001\u6613\u5b9e\u73b0\u7684\u6539\u8fdb\u7b56\u7565\uff0c\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u77e5\u8bc6\u4e0e\u9002\u5e94\u65b0\u4efb\u52a1\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6b63\u5219\u5316\u8fd1\u4f3c\u56de\u653e\u7684\u65b9\u6cd5\uff1a\u5bf9\u8f93\u51fa\u5206\u5e03\u4e0e\u521d\u59cb\u6a21\u578b\u4e4b\u95f4\u65bd\u52a0KL\u6563\u5ea6\u60e9\u7f5a\uff1b\u5728\u8bad\u7ec3\u4e2d\u4ea4\u66ff\u4f7f\u7528\u6765\u81ea\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684\u5f00\u6e90\u8bed\u6599\u8fdb\u884c\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\uff08next-token prediction\uff09\u3002\u5c06\u8be5\u7b56\u7565\u5e94\u7528\u4e8eQwen\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002", "result": "\u8be5\u7b56\u7565\u5728Qwen\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u80fd\u663e\u8457\u51cf\u8f7b\u56e0LoRA\u7b49\u5fae\u8c03\u5f15\u53d1\u7684\u77e5\u8bc6\u4e22\u5931\uff0c\u7ef4\u6301\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6\u4e0e\u5bf9\u65b0\u4efb\u52a1\u7684\u5851\u6027\uff0c\u4e14\u4ec5\u5e26\u6765\u53ef\u63a7\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u8bad\u7ec3\u6d41\u7a0b\u505a\u51fa\u5c0f\u5e45\u6539\u52a8\u4e14\u91c7\u7528\u6b63\u5219\u5316\u8fd1\u4f3c\u56de\u653e\uff0c\u53ef\u6709\u6548\u907f\u514dLoRA\u5728\u6307\u4ee4\u8c03\u4f18\u573a\u666f\u4e2d\u7684\u707e\u96be\u6027\u9000\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u4e14\u6210\u672c\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22382", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22382", "abs": "https://arxiv.org/abs/2512.22382", "authors": ["Bruno Mlodozeniec", "Pierre Ablin", "Louis B\u00e9thune", "Dan Busbridge", "Michal Klein", "Jason Ramapuram", "Marco Cuturi"], "title": "Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration", "comment": null, "summary": "Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $\u03bc$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.", "AI": {"tldr": "\u63d0\u51fa Complete^(d) \u53c2\u6570\u5316\uff0c\u7edf\u4e00\u5bbd\u5ea6/\u6df1\u5ea6\u53ca\u6279\u91cf\u5927\u5c0f\u4e0e\u8bad\u7ec3\u65f6\u957f\u7684\u7f29\u653e\uff0c\u5e76\u5b9e\u73b0\u8de8\u6a21\u578b\uff08\u751a\u81f3\u8de8\u6a21\u5757\uff09\u7684\u8d85\u53c2\u6570\u8f6c\u79fb\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u8d85\u53c2\u6570\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u73b0\u6709\u7684 \u03bcP \u7b49\u65b9\u6cd5\u53ef\u5728\u5c0f\u6a21\u578b\u4e0a\u641c\u7d22\u5168\u5c40\u57fa\u8d85\u53c2\u6570\u540e\u8f6c\u79fb\u5230\u5927\u6a21\u578b\uff0c\u4f46\u9700\u5904\u7406\u591a\u7ef4\u7f29\u653e\u548c\u6a21\u5757\u7ea7\u522b\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3002", "method": "\u63d0\u51fa Complete^(d) Parameterisation\uff08\u57fa\u4e8e CompleteP \u7684\u6539\u9020\uff09\uff0c\u7edf\u4e00\u5904\u7406\u5bbd\u5ea6/\u6df1\u5ea6\u3001\u6279\u5927\u5c0f\u3001\u8bad\u7ec3\u65f6\u957f\u7684\u7f29\u653e\uff1b\u7814\u7a76\u8de8\u6a21\u5757\u7684\u8d85\u53c2\u6570\u8f6c\u79fb\uff0c\u5206\u6790\u9ad8\u7ef4\u8d85\u53c2\u6570\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u7ed9\u51fa\u5b9e\u7528\u6307\u5357\uff1b\u8986\u76d6\u5b66\u4e60\u7387\u3001AdamW \u53c2\u6570\u3001\u6743\u91cd\u8870\u51cf\u3001\u521d\u59cb\u5316\u5c3a\u5ea6\u3001\u6b8b\u5dee\u5757\u653e\u5927\u7cfb\u6570\u7b49\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u5728\u6bcf\u6a21\u5757\u8d85\u53c2\u6570 regime \u4e0b\u4e5f\u80fd\u5b9e\u73b0\u8f6c\u79fb\uff0c\u4e14\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u901a\u8fc7\u8f6c\u79fb\u7684\u9010\u6a21\u5757\u8d85\u53c2\u6570\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "\u53d7\u63a7\u53c2\u6570\u5316\u4f7f\u6a21\u578b\u5c3a\u5ea6\u4e0e\u6a21\u5757\u5c42\u9762\u7684\u8d85\u53c2\u6570\u8f6c\u79fb\u6210\u4e3a\u73b0\u5b9e\uff0c\u964d\u4f4e\u641c\u7d22\u6210\u672c\u3001\u63d0\u5347\u6536\u655b\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u4f18\u5316\u6307\u5357\u3002"}}
{"id": "2512.22388", "categories": ["cs.LG", "cs.AI", "cs.SI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22388", "abs": "https://arxiv.org/abs/2512.22388", "authors": ["Omar Alsaqa", "Linh Thi Hoang", "Muhammed Fatih Balin"], "title": "BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks", "comment": "Accepted for 5th Muslims in ML Workshop co-located with NeurIPS 2025. OpenReview: https://openreview.net/forum?id=VaHubA7Pwv Code: https://github.com/linhthi/BLISS-GNN", "summary": "Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.", "AI": {"tldr": "BLISS \u4f7f\u7528\u591a\u81c2\u8d4c\u535a\u673a\u5bf9\u6bcf\u5c42\u7684\u90bb\u5c45\u8fdb\u884c\u91cd\u8981\u6027\u91c7\u6837\uff0c\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u8282\u70b9\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\uff0c\u8986\u76d6\u6574\u56fe\u7ed3\u6784\uff0c\u9002\u7528\u4e8e GCN \u4e0e GAT\uff0c\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u5168\u6279\u8bad\u7ec3\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u4e2d GNN \u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u7279\u522b\u662f\u6bcf\u4e2a\u8282\u70b9\u90fd\u8981\u5904\u7406\u5927\u91cf\u90bb\u5c45\u6240\u5e26\u6765\u7684\u8d1f\u62c5\u3002\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u52a8\u6001\u7684\u91c7\u6837\u7b56\u7565\uff0c\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa BLISS\uff08Bandit Layer Importance Sampling Strategy\uff09\uff0c\u5728\u6bcf\u4e00\u5c42\u4f7f\u7528\u591a\u81c2\u8d4c\u535a\u673a\u6765\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u8282\u70b9\u8fdb\u884c\u91c7\u6837\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u4ee5\u786e\u4fdd\u5bf9\u56fe\u7ed3\u6784\u7684\u5145\u5206\u8986\u76d6\u3002\u8be5\u7b56\u7565\u53ef\u65e0\u7f1d\u96c6\u6210\u4e8e GCN \u4e0e GAT\uff0c\u4e14\u5bf9\u4e0d\u540c\u805a\u5408\u673a\u5236\u8fdb\u884c\u81ea\u9002\u5e94\uff0c\u5f3a\u8c03\u5bf9\u5c42\u5185\u91c7\u6837\u7684\u65f6\u5e8f\u6027\u4e0e\u91cd\u8981\u6027\u53d8\u5316\u8fdb\u884c\u8ffd\u8e2a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBLISS \u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8fc7\u5168\u6279\u8bad\u7ec3\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u4e0e\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u663e\u793a\u51fa\u5bf9 GCN \u4e0e GAT \u7684\u901a\u7528\u9002\u914d\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5e26\u6743\u91c7\u6837\u7684\u81ea\u9002\u5e94\u5c42\u91c7\u6837\u7b56\u7565\u53ef\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684 GNN \u5b66\u4e60\uff0cBLISS \u7684\u5e26\u6765\u9879\u5728\u4e0d\u540c\u805a\u5408\u6846\u67b6\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2512.22428", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22428", "abs": "https://arxiv.org/abs/2512.22428", "authors": ["Jianxiang Xie", "Yuncheng Hua"], "title": "Causality-Inspired Safe Residual Correction for Multivariate Time Series", "comment": null, "summary": "While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also \"help in the wrong way\" by overcorrecting reliable predictions and causing local failures in unseen scenarios.\n  To address this critical \"safety gap,\" we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.\n  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.", "AI": {"tldr": "CRC: \u4e00\u4e2a\u53ef\u5728\u591a\u53d8\u91cf\u9884\u6d4b\u6846\u67b6\u4e2d\u5b9e\u73b0\u7684\u5b89\u5168\u6b8b\u5dee\u4fee\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u542f\u53d1\u7684\u7f16\u7801\u5206\u89e3\u81ea\u53d8\u91cf\u4e0e\u8de8\u53d8\u91cf\u52a8\u529b\u5b66\u3001\u4ee5\u53ca\u6df7\u5408\u4fee\u6b63\u5668\u6765\u5efa\u6a21\u6b8b\u5dee\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u56db\u91cd\u5b89\u5168\u673a\u5236\u786e\u4fdd\u4e0d\u9000\u5316\uff0c\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u540e\u9a8c\u6b8b\u5dee\u4fee\u6b63\u65b9\u6cd5\u5c3d\u7ba1\u80fd\u63d0\u5347\u5e73\u5747\u7cbe\u5ea6\uff0c\u4f46\u5b58\u5728\u201c\u8d2a\u5a6a\u201d\u95ee\u9898\uff0c\u53ef\u80fd\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5bf9\u53ef\u4fe1\u9884\u6d4b\u9020\u6210\u8fc7\u5ea6\u4fee\u6b63\u6216\u5c40\u90e8\u5931\u8d25\uff0c\u7f3a\u4e4f\u5bf9\u90e8\u7f72\u671f\u6027\u80fd\u4e0b\u964d\u7684\u4fdd\u8bc1\uff0c\u5b58\u5728\u5b89\u5168\u7f3a\u53e3\u3002", "method": "\u63d0\u51fa\u53ef\u63d2\u62d4\u7684CRC\u6846\u67b6\uff1a\u4f7f\u7528\u56e0\u679c\u542f\u53d1\u7684\u7f16\u7801\u5668\u5c06\u81ea\u53d8\u91cf\u4e0e\u8de8\u53d8\u91cf\u5173\u7cfb\u89e3\u8026\u4ee5\u66b4\u9732\u65b9\u5411\u6027\u7ed3\u6784\uff1b\u91c7\u7528\u6df7\u5408\u4fee\u6b63\u5668\u5efa\u6a21\u6b8b\u5dee\uff1b\u4ee5\u4e25\u683c\u7684\u56db\u91cd\u5b89\u5168\u673a\u5236\u63a7\u5236\u66f4\u65b0\u4ee5\u786e\u4fdd\u975e\u9000\u5316\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u4e0e\u4e0d\u540c forecasting backbones \u4e0a\u7684\u5b9e\u9a8c\u8868\u660eCRC\u80fd\u666e\u904d\u63d0\u5347\u51c6\u786e\u6027\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660e\u6838\u5fc3\u5b89\u5168\u673a\u5236\u80fd\u5b9e\u73b0\u6781\u9ad8\u7684\u975e\u9000\u5316\u7387\uff08NDR\uff09\uff0c\u4f7fCRC\u6210\u4e3a\u53ef\u7528\u4e8e\u5b89\u5168\u3001\u53ef\u9760\u90e8\u7f72\u7684\u4fee\u6b63\u6846\u67b6\u3002", "conclusion": "CRC\u4e3a\u591a\u53d8\u91cf\u65f6\u5e8f\u9884\u6d4b\u7684\u540e\u9a8c\u4fee\u6b63\u63d0\u4f9b\u4e00\u79cd\u53ef\u5b89\u5168\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u6539\u8fdb\u6027\u80fd\u4e0e\u9632\u6b62\u6027\u80fd\u4e0b\u964d\u7684\u4fdd\u969c\uff0c\u9002\u5408\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u843d\u5730\u3002"}}
{"id": "2512.22455", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22455", "abs": "https://arxiv.org/abs/2512.22455", "authors": ["Jiacheng Li", "Jianchao Tan", "Zhidong Yang", "Feiye Huo", "Yerui Sun", "Yuchen Xie", "Xunliang Cai"], "title": "AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.", "AI": {"tldr": "\u63d0\u51fa AFA-LoRA\uff0c\u5728 LoRA \u7684\u57fa\u7840\u4e0a\u5f15\u5165\u9000\u706b\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u9002\u914d\u5668\u5728\u8bad\u7ec3\u521d\u671f\u5177\u6709\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\uff0c\u968f\u540e\u7ebf\u6027\u5316\u4ee5\u5b9e\u73b0\u5408\u5e76\u6027\uff0c\u4ece\u800c\u7f29\u5c0f\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f25\u5408 LoRA \u7684\u7ebf\u6027\u9002\u914d\u4e0e\u975e\u7ebf\u6027\u8bad\u7ec3\u4e4b\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u5dee\u8ddd\uff0c\u63d0\u5347\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u8868\u73b0\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5408\u5e76\u6027\u3002", "method": "\u5728 LoRA \u6846\u67b6\u5185\u5f15\u5165\u9000\u706b\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u9002\u914d\u5668\u5728\u8bad\u7ec3\u521d\u671f\u5448\u73b0\u975e\u7ebf\u6027\u7279\u6027\uff0c\u968f\u540e\u9010\u6b65\u7ebf\u6027\u5316\u4ee5\u5b9e\u73b0\u4e0e\u539f\u59cb\u7ebf\u6027\u7ed3\u6784\u7684\u5408\u5e76\uff1b\u5728\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u6d4b\u89e3\u7801\u7b49\u573a\u666f\u4e2d\u5b9e\u73b0\u5e76\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e AFA-LoRA \u80fd\u663e\u8457\u7f29\u5c0f\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u76f8\u8f83\u4f20\u7edf LoRA \u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5408\u5e76\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53c2\u6570\u9ad8\u6548\u81ea\u9002\u5e94\u63d0\u4f9b\u66f4\u5f3a\u7684\u8868\u8fbe\u529b\uff0c\u63a8\u52a8\u5728\u53ef\u5408\u5e76\u7684\u975e\u7ebf\u6027\u5fae\u8c03\u7b56\u7565\u65b9\u9762\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.22466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22466", "abs": "https://arxiv.org/abs/2512.22466", "authors": ["Qizhi Wang"], "title": "AMBIT: Augmenting Mobility Baselines with Interpretable Trees", "comment": "15 pages; 12 figures; 30 tables", "summary": "Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.", "AI": {"tldr": "AMBIT \u5c06\u7269\u7406-\u7070\u76d2\u4e0e\u53ef\u89e3\u91ca\u6811\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u63d0\u5347 OD \u6d41\u9884\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728 NYC \u51fa\u79df\u8f66\u6570\u636e\u4e0a\u7cfb\u7edf\u8bc4\u4f30\uff0cPOI \u5f15\u5bfc\u7684\u6b8b\u5dee\u5728\u7a7a\u95f4\u6cdb\u5316\u4e0a\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u9ad8\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u73b0\u6709\u7269\u7406\u6a21\u578b\u5728\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u4e0b\u6613\u8106\u5f31\uff1b\u9700\u8981\u5728\u7269\u7406\u5148\u9a8c\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u6298\u8877\u4ee5\u652f\u6301\u57ce\u5e02\u51b3\u7b56\u3002", "method": "\u5bf9\u7ecf\u5178\u7a7a\u95f4\u4ea4\u4e92\u6a21\u578b\u5728\u5e74-\u5c0f\u65f6\u5c3a\u5ea6 NYC \u51fa\u79df OD \u6570\u636e\u96c6\u8fdb\u884c\u5ba1\u8ba1\uff1b\u6bd4\u8f83 PPML \u5f15\u529b\u7b49\u7269\u7406\u57fa\u7ebf\u53ca\u5176\u5728\u5168 OD \u8fb9\u9645\u6761\u4ef6\u4e0b\u7684\u6821\u51c6\u6548\u679c\uff1b\u5728\u7269\u7406\u57fa\u7ebf\u4e4b\u4e0a\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u6811\u4f5c\u4e3a\u6b8b\u5dee\u5b66\u4e60\u5668\u5e76\u7ed3\u5408 SHAP \u8fdb\u884c\u89e3\u91ca\uff1b\u8bc4\u4f30 POI-\u951a\u5b9a\u6b8b\u5dee\u4e0e\u7a7a\u95f4\u6cdb\u5316\u9c81\u68d2\u6027\uff1b\u63d0\u4f9b\u53ef\u91cd\u590d\u7ba1\u7ebf\u548c\u4e30\u5bcc\u8bca\u65ad\u3002", "result": "\u7269\u7406-\u6b8b\u5dee\u5b66\u4e60\u7684\u6b8b\u5dee\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u8d8b\u8fd1\u4e8e\u5f3a\u6811\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u7ed3\u6784\uff1bPOI-\u951a\u5b9a\u6b8b\u5dee\u5728\u7a7a\u95f4\u6cdb\u5316\u4e0b\u8868\u73b0\u6700\u4e3a\u9c81\u68d2\u3002", "conclusion": "AMBIT \u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u3001\u53ef\u89e3\u91ca\u7684\u7ba1\u7ebf\uff0c\u5c06\u7269\u7406\u5148\u9a8c\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u6811\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347 OD \u6d41\u9884\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u51b3\u7b56\u4ef7\u503c\u3002"}}
{"id": "2512.22467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22467", "abs": "https://arxiv.org/abs/2512.22467", "authors": ["Jong-Ik Park", "Shreyas Chaudhari", "Srinivasa Pranav", "Carlee Joe-Wong", "Jos\u00e9 M. F. Moura"], "title": "GLUE: Gradient-free Learning to Unify Experts", "comment": null, "summary": "In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.", "AI": {"tldr": "GLUE trains a target model as a convex blend of fixed expert models and optimizes the blend coefficients with a gradient-free SPSA update, achieving strong domain-expansion performance with lower computational cost than full backpropagation.", "motivation": "In real deployments, multiple pretrained specialists exist across domains. When facing a new target domain, a generalized model should perform well beyond any single expert. Heuristic mixture of experts often underperforms and learning coefficients via backprop is costly. A gradient-free approach aims to efficiently find a good initialization mixture that generalizes to the target domain.", "method": "Initialize the target model as a fixed convex combination of expert models. Learn the mixture coefficients using a gradient-free two-point SPSA update, requiring only two forward passes per optimization step, avoiding full backpropagation through the network.", "result": "Across three datasets and three architectures, GLUE yields a single prior that can be fine-tuned to outperform baselines. It improves test accuracy by up to 8.5% over data-size weighting and up to 9.1% over proxy-metric selection. It either outperforms full-gradient mixing or matches its performance within 1.4%.", "conclusion": "Gradient-free learning to unify experts is effective for domain expansion, providing a computationally efficient prior that can be fine-tuned to surpass baselines in target-domain tasks."}}
{"id": "2512.22478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22478", "abs": "https://arxiv.org/abs/2512.22478", "authors": ["Chuantao Li", "Zhi Li", "Jiahao Xu", "Jie Li", "Sheng Li"], "title": "Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting", "comment": null, "summary": "Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534f\u540c\u4f18\u5316\u7684\u591a\u5206\u7c7b\u4e0d\u5e73\u8861\u5b66\u4e60\u63d0\u5347\u6a21\u578b\uff0c\u5c06\u5bc6\u5ea6\u56e0\u5b50\u548c\u7f6e\u4fe1\u56e0\u5b50\u6574\u5408\u8fdb\u63d0\u5347\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u6297\u566a\u58f0\u7684\u6743\u91cd\u66f4\u65b0\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u572820\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e8\u4e2a\u57fa\u7ebf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u89e3\u51b3\u5206\u7c7b\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u504f\u501a\uff0c\u4ee5\u53ca\u4e0d\u5e73\u8861\u5b66\u4e60\u4e0e\u6a21\u578b\u8bad\u7ec3\u4e4b\u95f4\u7f3a\u4e4f\u534f\u540c\u4f18\u5316\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u6570\u636e\u96c6\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u534f\u540c\u4f18\u5316\u7684\u63d0\u5347\u6a21\u578b\uff0c\u96c6\u6210\u5bc6\u5ea6\u56e0\u5b50\u548c\u7f6e\u4fe1\u56e0\u5b50\uff0c\u8bbe\u8ba1\u566a\u58f0\u9c81\u68d2\u7684\u6743\u91cd\u66f4\u65b0\u673a\u5236\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff1b\u5c06\u6743\u91cd\u66f4\u65b0\u3001\u533a\u57df\u5212\u5206\u4e0e\u533a\u57df\u5f15\u5bfc\u91c7\u6837\u7b49\u6a21\u5757\u7d27\u5bc6\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u4e0d\u5e73\u8861\u5b66\u4e60\u4e0e\u6a21\u578b\u8bad\u7ec3\u7684\u534f\u540c\u4f18\u5316\u3002", "result": "\u572820\u4e2a\u516c\u5f00\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e8\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u5c06\u4e0d\u5e73\u8861\u5b66\u4e60\u4e0e\u6a21\u578b\u8bad\u7ec3\u7684\u534f\u540c\u4f18\u5316\u4f5c\u4e3a\u63d0\u5347\u65b9\u5411\u7684\u6709\u6548\u6027\uff0c\u5e76\u7ed9\u51fa\u4ee3\u7801\u5b9e\u73b0\u4ee5\u4fbf\u590d\u73b0\u3002"}}
{"id": "2512.22495", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22495", "abs": "https://arxiv.org/abs/2512.22495", "authors": ["Hamed Damirchi", "Cristian Rodriguez-Opazo", "Ehsan Abbasnejad", "Zhen Zhang", "Javen Shi"], "title": "The Quest for Winning Tickets in Low-Rank Adapters", "comment": "21 pages", "summary": "The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks (\"winning tickets\") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.", "AI": {"tldr": "LTH extends to LoRA: there exist sparse subnetworks within LoRAs that can match the performance of dense adapters; Partial-LoRA identifies and trains such subnetworks to achieve high parameter efficiency.", "motivation": "As large pretrained models are widely fine-tuned with parameter-efficient methods like LoRA, it is important to understand whether the Lottery Ticket Hypothesis applies and how sparsity can be leveraged to improve efficiency.", "method": "Identify task-relevant sparse subnetworks within LoRA (low-rank adapters) and train them; develop Partial-LoRA to systematically select and align sparse subnetworks with task-relevant subspaces of the pretrained model; evaluate on diverse tasks.", "result": "LTH holds within LoRAs: sparse subnetworks can match dense adapter performance; effectiveness hinges more on per-layer sparsity than exact weights; Partial-LoRA reduces trainable parameters by up to 87% while maintaining or improving accuracy across 8 vision and 12 language tasks, in both single-task and multi-task settings.", "conclusion": "The findings deepen theoretical understanding of transfer learning and pretraining\u2013finetuning interactions, and enable more efficient adaptation strategies through sparsity-aware, subspace-aligned PEFT methods."}}
{"id": "2512.22508", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22508", "abs": "https://arxiv.org/abs/2512.22508", "authors": ["Lucky Susanto", "Anasta Pranawijayana", "Cortino Sukotjo", "Soni Prasad", "Derry Wijaya"], "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals", "comment": "Accepted as a Short Paper at HEALTHINF2026", "summary": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.", "AI": {"tldr": "\u5229\u7528\u5143\u6570\u636e\u4e0e\u5e7b\u89c9\u4fe1\u53f7\uff0c\u5728\u4e09\u79cd prompting \u7b56\u7565\u4e0b\u8bc4\u4f30 GPT-4o \u4e0e OSS-120B \u5728\u4fee\u590d\u79d1MCQ\u4e0a\u7684\u6b63\u786e\u6027\u9884\u6d4b\uff1b\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e0b\u51c6\u786e\u6027\u63d0\u5347\u6700\u9ad8\u8fbe+7.14%\uff0c\u7cbe\u5ea683.12%\uff0c\u4f46\u4ecd\u4e0d\u8db3\u4ee5\u652f\u6491\u9ad8\u98ce\u9669\u90e8\u7f72\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u6559\u80b2\uff09\u4e2d\uff0cLLMs \u4ea7\u751f\u4e8b\u5b9e\u6027\u9519\u8bef\u7684\u98ce\u9669\u5f88\u9ad8\uff0c\u5355\u7eaf\u7684\u5e7b\u89c9\u68c0\u6d4b\u4e0d\u8db3\u4ee5\u76f4\u63a5\u5224\u65ad\u8f93\u51fa\u7684\u6b63\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9884\u6d4b\u6b63\u786e\u6027\u7684\u4fe1\u53f7\u6765\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u9009\u53d6\u4e24\u79cd\u6a21\u578b\uff08GPT-4o \u4e0e OSS-120B\uff09\u5728\u4e00\u9879\u4fee\u590d\u79d1\u591a\u9879\u9009\u62e9\u8003\u8bd5\u573a\u666f\u4e2d\uff0c\u8bbe\u8ba1\u4e09\u79cd prompting \u7b56\u7565\uff1b\u7ed3\u5408\u5143\u6570\u636e\u4e0e\u5e7b\u89c9\u4fe1\u53f7\uff0c\u9488\u5bf9\u6bcf\u5bf9\uff08\u6a21\u578b\u3001 prompting\uff09\u6784\u5efa\u6b63\u786e\u6027\u9884\u6d4b\u5668\uff1b\u5bf9\u6bd4\u57fa\u7ebf\uff08\u5047\u8bbe\u6240\u6709\u7b54\u6848\u90fd\u6b63\u786e\uff09\u5e76\u8bc4\u4f30\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u7cbe\u5ea6\uff0c\u540c\u65f6\u5206\u6790\u5e7b\u89c9\u4e0e\u5143\u6570\u636e\u7684\u5173\u7cfb\u53ca prompting \u7b56\u7565\u5bf9\u5185\u90e8\u884c\u4e3a\u53ca\u9884\u6d4b\u6548\u7528\u7684\u5f71\u54cd\u3002", "result": "\u5143\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e0b\u80fd\u5c06\u51c6\u786e\u6027\u63d0\u5347\u6700\u591a7.14%\uff0c\u8fbe\u523083.12%\u7684\u7cbe\u5ea6\uff1b\u5b9e\u9645\u5e7b\u89c9\u662f\u5224\u65ad\u4e0d\u6b63\u786e\u6027\u7684\u5f3a\u4fe1\u53f7\uff0c\u4f46\u4ec5\u51ed\u5143\u6570\u636e\u5e76\u4e0d\u8db3\u4ee5\u53ef\u9760\u9884\u6d4b\u5e7b\u89c9\uff1b prompting \u7b56\u7565\u6539\u53d8\u4e86\u6a21\u578b\u7684\u5185\u90e8\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u663e\u8457\u5f71\u54cd\u5143\u6570\u636e\u5728\u9884\u6d4b\u4e2d\u7684\u6548\u7528\uff0c\u4f46\u5e76\u672a\u6539\u53d8\u603b\u4f53\u51c6\u786e\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u57fa\u4e8e\u5143\u6570\u636e\u7684\u53ef\u9760\u6027\u4fe1\u53f7\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c1a\u4e0d\u8db3\u4ee5\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5b9e\u73b0\u9c81\u68d2\u90e8\u7f72\uff0c\u9700\u8981\u66f4\u7efc\u5408\u7684\u4fe1\u53f7\u4e0e\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.22511", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22511", "abs": "https://arxiv.org/abs/2512.22511", "authors": ["Hamed Damirchi", "Ehsan Abbasnejad", "Zhen Zhang", "Javen Shi"], "title": "Decomposing Task Vectors for Refined Model Editing", "comment": "16 pages", "summary": "Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u4efb\u52a1\u5411\u91cf\u5206\u89e3\u4e3a\u5171\u4eab\u77e5\u8bc6\u4e0e\u552f\u4e00\u4fe1\u606f\u7684\u6b63\u4ea4\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6295\u5f71\u7684\u4e00\u81f4\u5b50\u7a7a\u95f4\uff0c\u63d0\u5347\u4efb\u52a1\u5411\u91cf\u8fd0\u7b97\u7684\u7a33\u5b9a\u6027\u4e0e\u63a7\u5236\u6027\uff0c\u5728\u56fe\u50cf/\u6269\u6563\u6a21\u578b/\u8bed\u8a00\u6a21\u578b\u7b49\u591a\u9886\u57df\u5b9e\u73b0\u591a\u4efb\u52a1\u5408\u5e76\u3001\u98ce\u683c\u6df7\u5408\u4e0e\u6bd2\u6027\u524a\u51cf\u3002", "motivation": "\u4efb\u52a1\u5411\u91cf\u901a\u8fc7\u5411\u91cf\u8fd0\u7b97\u5b9e\u73b0\u6a21\u578b\u884c\u4e3a\u7684\u8fc1\u79fb\u4e0e\u878d\u5408\uff0c\u4f46\u5411\u91cf\u4e2d\u6982\u5ff5\u5b58\u5728\u91cd\u53e0\uff0c\u6613\u76f8\u4e92\u5e72\u6270\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u63a7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5206\u89e3\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u884c\u4e3a\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6295\u5f71\u7684\u4e00\u81f4\u5b50\u7a7a\u95f4\u5206\u89e3\uff0c\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5411\u91cf\u5206\u89e3\u4e3a\u5171\u4eab\u5206\u91cf\u548c\u552f\u4e00\u5206\u91cf\uff0c\u5e76\u901a\u8fc7\u8bc6\u522b\u8de8\u5411\u91cf\u6295\u5f71\u7684\u4e00\u81f4\u5b50\u7a7a\u95f4\u6765\u5b9e\u73b0\u5bf9\u6982\u5ff5\u7684\u72ec\u7acb\u64cd\u63a7\uff1b\u5229\u7528\u4e0d\u53d8\u5b50\u7a7a\u95f4\u7406\u8bba\u5b9e\u73b0\u5bf9\u5411\u91cf\u7b97\u672f\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u53d6\u5f97\u663e\u8457\u6548\u679c\uff1a1) \u56fe\u50cf\u5206\u7c7b\u591a\u4efb\u52a1\u878d\u5408\u4e2d\uff0c\u4f7f\u7528\u5171\u4eab\u5206\u91cf\u4f5c\u4e3a\u989d\u5916\u4efb\u52a1\u5411\u91cf\u63d0\u53475%\uff1b2) \u6269\u6563\u6a21\u578b\u98ce\u683c\u6df7\u5408\u4e2d\uff0c\u4ec5\u6df7\u5408\u552f\u4e00\u5206\u91cf\u5373\u53ef\u5b9e\u73b0\u65e0\u9000\u5316\u7684\u98ce\u683c\u6df7\u5408\uff1b3) \u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u53bb\u9664\u552f\u4e00\u5206\u91cf\u4e2d\u7684\u6bd2\u6027\u4fe1\u606f\u5b9e\u73b047%\u6bd2\u6027\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u4e00\u822c\u77e5\u8bc6\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u4e3a\u4efb\u52a1\u5411\u91cf\u8fd0\u7b97\u63d0\u4f9b\u65b0\u7684\u7406\u89e3\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u6a21\u578b\u7f16\u8f91\u64cd\u4f5c\u7684\u57fa\u672c\u5c40\u9650\u6027\uff0c\u63d0\u5347\u8de8\u4efb\u52a1\u534f\u4f5c\u7684\u53ef\u63a7\u6027\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.22522", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22522", "abs": "https://arxiv.org/abs/2512.22522", "authors": ["Jihang Wang", "Dongcheng Zhao", "Ruolin Chen", "Qian Zhang", "Yi Zeng"], "title": "Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks", "comment": null, "summary": "Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66f4\u53ef\u9760\u7684SNN\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff1a\u81ea\u9002\u5e94\u9510\u5ea6\u66ff\u4ee3\u68af\u5ea6\uff08ASSG\uff09\u4e0e\u81ea\u9002\u5e94\u6b65\u957f\u7684SA-PGD\u653b\u51fb\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u6548\u679c\u5e76\u63ed\u793aSNN\u9c81\u68d2\u6027\u7684\u6f5c\u5728\u9ad8\u4f30\u3002", "motivation": "SNN\u7684\u5c16\u5cf0\u6fc0\u6d3b\u672c\u8d28\u7684\u4e8c\u503c\u6027\u548c\u4e0d\u8fde\u7eed\u6027\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u73b0\u6709\u66ff\u4ee3\u68af\u5ea6\u5728\u5f3a\u5bf9\u6297\u4e0b\u6548\u679c\u4e0d\u786e\u5b9a\uff0c\u96be\u4ee5\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u6bd4\u8f83\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u4e2a\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\u5e76\u884c\u7684\u6846\u67b6\u6765\u66f4\u53ef\u9760\u5730\u8bc4\u4f30SNN\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "method": "\u7406\u8bba\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u7a0b\u5ea6\uff1b\u63d0\u51faAdaptive Sharpness Surrogate Gradient (ASSG)\uff0c\u5728\u653b\u51fb\u8fed\u4ee3\u4e2d\u6839\u636e\u8f93\u5165\u5206\u5e03\u81ea\u9002\u5e94\u8c03\u6574\u66ff\u4ee3\u51fd\u6570\u5f62\u72b6\u4ee5\u63d0\u9ad8\u68af\u5ea6\u51c6\u786e\u6027\u5e76\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\uff1b\u8bbe\u8ba1Stable Adaptive PGD (SA-PGD)\uff0c\u5728L\u221e\u7ea6\u675f\u4e0b\u81ea\u9002\u5e94\u6b65\u957f\u4ee5\u5b9e\u73b0\u66f4\u5feb\u4e14\u5bf9\u4e0d\u7cbe\u786e\u68af\u5ea6\u66f4\u7a33\u5065\u7684\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cASSG\u4e0eSA-PGD\u5728\u591a\u79cd\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\u3001SNN\u7ed3\u6784\u548c\u795e\u7ecf\u5143\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u8be5\u8bc4\u4f30\u6846\u67b6\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e0b\u5177\u6709\u66f4\u5f3a\u7684\u666e\u9002\u6027\u548c\u53ef\u9760\u6027\uff1b\u540c\u65f6\u8868\u660e\u5f53\u524dSNN\u9c81\u68d2\u6027\u88ab\u9ad8\u4f30\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684SNN\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709SNN\u9c81\u68d2\u6027\u4f30\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\u4ee5\u83b7\u5f97\u771f\u5b9e\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.22550", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22550", "abs": "https://arxiv.org/abs/2512.22550", "authors": ["Jaebin Lee", "Hankook Lee"], "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting", "comment": "Accepted to NeurIPS 2025. The code is available at https://github.com/efficient-learning-lab/TimePerceiver", "summary": "In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.", "AI": {"tldr": "TimePerceiver propone un marco unificado de codificador-decodificador para predicci\u00f3n de series temporales, que alinea el dise\u00f1o del encoder, el decodificador y el entrenamiento, abarcando extrapolaci\u00f3n, interpolaci\u00f3n e imputaci\u00f3n, con una arquitectura de obturador latente y consultas aprendibles para decodificaci\u00f3n; exhibe mejoras significativas en benchmarks y ofrece c\u00f3digo abierto.", "motivation": "La literatura previa se centra principalmente en el dise\u00f1o del encoder para series temporales, tratando la predicci\u00f3n y el entrenamiento como problemas secundarios o independientes. Esto limita la capacidad de aprovechar al m\u00e1ximo la interacci\u00f3n entre codificaci\u00f3n, predicci\u00f3n y entrenamiento. Se busca un marco unificado que optimice estas piezas de forma conjunta.", "method": "Presenta TimePerceiver, un marco de forecasting con encoder-decoder. Generaliza la tarea para incluir extrapolaci\u00f3n, interpolaci\u00f3n e imputaci\u00f3n, requiere manejar segmentos de entrada y objetivo en posiciones arbitrarias. Introduce representaciones latentes de cuello de botella que interact\u00faan con todos los segmentos de entrada para capturar dependencias temporales y entre canales. En la fase de decodificaci\u00f3n, utiliza consultas aprendibles para timestamps objetivo para recuperar informaci\u00f3n relevante.", "result": "A trav\u00e9s de experimentos extensivos, el marco TimePerceiver supera consistentemente a baselines de vanguardia en una amplia gama de datasets de referencia. El c\u00f3digo est\u00e1 disponible p\u00fablicamente.", "conclusion": "TimePerceiver ofrece un marco de codificador-decodificador unificado, alineado con una estrategia de entrenamiento efectiva, adaptable a diversas tareas temporales (extrapolaci\u00f3n, interpolaci\u00f3n, imputaci\u00f3n) y con mejoras de rendimiento significativas."}}
{"id": "2512.22587", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22587", "abs": "https://arxiv.org/abs/2512.22587", "authors": ["Taeyun Kim"], "title": "On Admissible Rank-based Input Normalization Operators", "comment": "31 pages, 2 figures", "summary": "Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.\n  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.\n  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.", "AI": {"tldr": "\u4ece\u7406\u8bba\u4e0a\u754c\u5b9a\u5bf9\u57fa\u4e8e\u79e9\u7684\u8f93\u5165\u5f52\u4e00\u5316\u7684\u6700\u4f4e\u4e0d\u53d8\u6027\u9700\u6c42\uff0c\u7ed9\u51fa\u4e09\u6761\u516c\u7406\u3001\u5206\u89e3\u7ed3\u6784\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540c\u65f6\u6ee1\u8db3\u516c\u7406\u7684\u6700\u5c0f\u7b97\u5b50\uff0c\u660e\u786e\u533a\u5206\u4e8e\u73b0\u6709\u7684\u8fde\u7eed\u677e\u5f1b\u6392\u5e8f\u65b9\u6cd5\u3002", "motivation": "\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\uff0c\u7279\u5f81\u503c\u7684\u6392\u5e8f\u4fe1\u606f\u5f80\u5f80\u6bd4\u6570\u503c\u5927\u5c0f\u66f4\u5173\u952e\uff0c\u800c\u73b0\u6709\u53ef\u5fae\u6392\u5e8f/\u6392\u540d\u7b97\u5b50\u5728\u4e25\u683c\u5355\u8c03\u53d8\u6362\u3001\u6279\u6b21\u53d8\u5316\u548c\u5fae\u5c0f\u6270\u52a8\u4e0b\u4e0d\u7a33\u5b9a\uff1b\u9700\u8981\u4e00\u4e2a\u5f62\u5f0f\u5316\u7684\u7a33\u5b9a\u6027\u6846\u67b6\u6765\u754c\u5b9a\u6709\u6548\u7684\u79e9\u57fa\u5f52\u4e00\u5316\u7b97\u5b50\u3002", "method": "\u9996\u5148\u6307\u51fa\u73b0\u6709\u53ef\u5fae\u6392\u5e8f/\u6392\u540d\u7b97\u5b50\u56e0\u5176\u7ed3\u6784\u8bbe\u8ba1\u5728\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u7f3a\u9677\uff1b\u63d0\u51fa\u4e09\u6761\u516c\u7406\uff0c\u63cf\u8ff0\u6700\u4f4e\u9650\u5ea6\u7684\u4e0d\u53d8\u6027\u4e0e\u7a33\u5b9a\u6027\u8981\u6c42\uff1b\u8bc1\u660e\u4efb\u610f\u6ee1\u8db3\u516c\u7406\u7684\u7b97\u5b50\u5fc5\u987b\u5206\u89e3\u4e3a(1) \u6bcf\u4e2a\u7279\u5f81\u7ef4\u7684\u79e9\u8868\u793a\uff0c\u4ee5\u53ca(2) \u4e00\u4e2a\u5355\u8c03\u4e14Lipschitz\u8fde\u7eed\u7684\u6807\u91cf\u5316\u6620\u5c04\uff1b\u6784\u5efa\u4e00\u4e2a\u6ee1\u8db3\u8fd9\u4e9b\u516c\u7406\u7684\u6700\u5c0f\u5316\u7b97\u5b50\uff1b\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u8be5\u7ea6\u675f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u975e\u5e73\u51e1\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8e\u8fde\u7eed\u677e\u5f1b\u7684\u6392\u5e8f\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u8bc1\u660e\u4e86\u73b0\u6709\u7684 differentiable sorting/ranking \u8fd0\u7b97\u5728\u4e25\u683c\u5355\u8c03\u53d8\u6362\u3001\u6279\u6b21\u7ec4\u5408\u53d8\u5316\u548c\u5fae\u5c0f\u8f93\u5165\u6270\u52a8\u4e0b\u4e0d\u7a33\u5b9a\uff1b\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e00\u4e2a\u7b26\u5408\u4e09\u6761\u516c\u7406\u7684\u6700\u5c0f\u7b97\u5b50\uff0c\u9a8c\u8bc1\u5176\u5728\u73b0\u5b9e\u8bbe\u7f6e\u4e2d\u7684\u975e\u5e73\u51e1\u6027\uff1b\u660e\u786e\u533a\u5206\u4e86\u57fa\u4e8e\u79e9\u7684\u5f52\u4e00\u5316\u4e0e\u73b0\u6709\u7684\u8fde\u7eed\u677e\u5f1b\u6392\u5e8f\u65b9\u6cd5\u7684\u672c\u8d28\u5dee\u5f02\u3002", "conclusion": "\u754c\u5b9a\u4e86\u6709\u6548\u7684\u57fa\u4e8e\u79e9\u7684\u5f52\u4e00\u5316\u7b97\u5b50\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c06\u5176\u4e0e\u4f20\u7edf\u7684\u8fde\u7eed\u677e\u5f1b\u6392\u5e8f\u65b9\u6cd5\u533a\u5206\u5f00\u6765\uff0c\u5e76\u4e3a\u540e\u7eed\u5728\u9c81\u68d2\u6027\u4e0e\u4e0d\u53d8\u6027\u65b9\u9762\u7684\u6539\u8fdb\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.22589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22589", "abs": "https://arxiv.org/abs/2512.22589", "authors": ["Jewel Rana Palit", "Vijayalakshmi K Kumarasamy", "Osama A. Osman"], "title": "Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining", "comment": "7 tables, 7 figures, 23 pages including references, presented in ASCE 2024 conference", "summary": "Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7f8e\u56fdNHTSA\u5173\u4e8eSAE Level 2\u4e0e4\u76842500\u4f59\u8d77\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u78b0\u649e\u8bb0\u5f55\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6570\u636e\u6316\u6398\uff1a\u5148\u7528K-means\u5bf9\u8bb0\u5f55\u8fdb\u884c4\u7c7b\u884c\u4e3a\u7c07\u5206\u7fa4\uff0c\u518d\u5728\u6bcf\u7c07\u5185\u4f7f\u7528\u5173\u8054\u89c4\u5219\u6316\u6398\uff08ARM\uff09\u63ed\u793a\u78b0\u649e\u6a21\u5f0f\u4e0e\u6210\u56e0\uff08\u7167\u660e\u3001\u8def\u9762\u72b6\u51b5\u3001\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u73af\u5883\u7b49\uff09\u7684\u591a\u53d8\u91cf\u5173\u7cfb\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u8005\u3001\u76d1\u7ba1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u90e8\u7f72\u4e0e\u964d\u98ce\u9669\u7b56\u7565\u3002", "motivation": "\u586b\u8865\u4ee5\u5f80\u591a\u805a\u7126\u4e8e\u52a0\u5dde\u3001\u5c0f\u6837\u672c\u7684\u6570\u636e\u5206\u6790\u7a7a\u767d\uff0c\u63d0\u5347\u5bf9AV\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u78b0\u649e\u52a8\u6001\u7406\u89e3\uff0c\u8fdb\u800c\u4fc3\u8fdb\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684AV\u90e8\u7f72\u3002", "method": "\u4e24\u9636\u6bb5\u6570\u636e\u6316\u6398\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u65f6\u95f4\u3001\u7a7a\u95f4\u4e0e\u73af\u5883\u56e0\u7d20\u5bf9\u78b0\u649e\u8bb0\u5f55\u8fdb\u884cK-means\u805a\u7c7b\uff0c\u5f97\u52304\u4e2a\u884c\u4e3a\u7c07\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u6bcf\u4e2a\u7c07\u5185\u5e94\u7528\u5173\u8054\u89c4\u5219\u6316\u6398\uff0c\u62bd\u53d6\u78b0\u649e\u6a21\u5f0f\u4e0e\u6210\u56e0\uff08\u5982\u7167\u660e\u3001\u8def\u9762\u72b6\u51b5\u3001\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u73af\u5883\u6761\u4ef6\uff09\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u591a\u53d8\u91cf\u5173\u7cfb\u3002", "result": "\u5f97\u52304\u4e2a\u53ef\u89e3\u91ca\u7684\u78b0\u649e\u884c\u4e3a\u7c07\uff0c\u5e76\u5728\u5404\u7c07\u5185\u8bc6\u522b\u51fa\u8de8\u53d8\u91cf\u7684\u5173\u8054\u89c4\u5219\uff0c\u63ed\u793a\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u78b0\u649e\u6210\u56e0\u5206\u5e03\u4e0e\u5173\u952e\u8d21\u732e\u56e0\u7d20\uff0c\u5f62\u6210\u5bf9AV\u5f00\u53d1\u3001\u76d1\u7ba1\u4e0e\u90e8\u7f72\u7684\u53ef\u64cd\u4f5c\u6027\u5efa\u8bae\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8f83\u5927\u3001\u771f\u5b9e\u4e16\u754c\u7684US\u6570\u636e\u4e0a\u7cfb\u7edf\u63ed\u793aAV\u78b0\u649e\u7684\u6f5c\u5728\u52a8\u6001\uff0c\u6709\u52a9\u4e8e\u5236\u5b9a\u9488\u5bf9\u4e0d\u540c\u60c5\u5883\u7684\u5b89\u5168\u7b56\u7565\u4e0e\u89c4\u5236\u6846\u67b6\uff0c\u63d0\u5347\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u7684AV\u5b89\u5168\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2512.22597", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2512.22597", "abs": "https://arxiv.org/abs/2512.22597", "authors": ["Guikun Xu", "Xiaohan Yi", "Peilin Zhao", "Yatao Bian"], "title": "Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification", "comment": null, "summary": "Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.", "AI": {"tldr": "EnFlow \u901a\u8fc7\u5c06\u6d41\u5f0f\u5339\u914d\uff08FM\uff09\u4e0e\u663e\u5f0f\u5b66\u4e60\u7684\u80fd\u91cf\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5728\u975e\u9ad8\u65afFM\u8def\u5f84\u4e0a\u5b9a\u4e49\u80fd\u91cf\u68af\u5ea6\u5f15\u5bfc\u91c7\u6837\uff0c\u5b9e\u73b0\u4f4e\u80fd\u91cf\u6784\u8c61\u96c6\u5408\u7684\u9ad8\u6548\u751f\u6210\u4e0e\u5730\u9762\u6001\u8bc6\u522b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7684\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5728\u591a\u6837\u6027\u4e0e\u8986\u76d6\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u8981\u4e48\u7f3a\u4e4f\u53ef\u9760\u7684\u80fd\u91cf\u6807\u5b9a\uff0c\u96be\u4ee5\u540c\u65f6\u751f\u6210\u591a\u6837\u7684\u6784\u8c61\u5e76\u6b63\u786e\u8bc6\u522b\u4f4e\u80fd\u6001\uff1b\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5730\u5f97\u5230\u4f4e\u80fd\u91cf\u6784\u8c61\u5e76\u8fdb\u884c\u80fd\u91cf\u5206\u7ea7\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u80fd\u91cf\u5f15\u5bfc\u91c7\u6837\u7684 Flow Matching \u6846\u67b6\uff08EnFlow\uff09\uff0c\u5728\u975e\u9ad8\u65af FM \u8def\u5f84\u4e0a\u901a\u8fc7\u663e\u5f0f\u5b66\u4e60\u7684\u80fd\u91cf\u51fd\u6570\u5bf9\u91c7\u6837\u8fc7\u7a0b\u8fdb\u884c\u80fd\u91cf\u68af\u5ea6\u5f15\u5bfc\uff0c\u4f7f\u8f68\u8ff9\u5411\u66f4\u4f4e\u80fd\u91cf\u533a\u57df\u6536\u655b\uff1b\u5229\u7528\u5b66\u4e60\u7684\u80fd\u91cf\u51fd\u6570\u8fdb\u884c\u80fd\u91cf\u57fa\u6392\u5e8f\u4ee5\u8bc6\u522b\u5730\u9762\u6001\uff1b\u5728 GEOM-QM9 \u4e0e GEOM-Drugs \u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u5728\u4ec5\u97001\u20132\u6b65\u5e38\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u7684\u60c5\u51b5\u4e0b\uff0cEnFlow \u540c\u65f6\u63d0\u5347\u751f\u6210\u6307\u6807\u5e76\u964d\u4f4e\u5730\u9762\u6001\u9884\u6d4b\u8bef\u5dee\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "EnFlow \u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u751f\u6210\u4e0e\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u80fd\u91cf\u5f15\u5bfc\u548c\u80fd\u91cf\u6a21\u578b\u5b9e\u73b0\u5bf9\u4f4e\u80fd\u91cf\u6784\u8c61\u7684\u66f4\u51c6\u786e\u6355\u83b7\u4e0e\u6392\u5e8f\uff0c\u9002\u7528\u4e8e\u5206\u5b50\u6784\u8c61\u5d4c\u5957\u7684\u4efb\u52a1\u3002"}}
{"id": "2512.22672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22672", "abs": "https://arxiv.org/abs/2512.22672", "authors": ["Achraf Hsain", "Fouad Mohammed Abbou"], "title": "Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations", "comment": null, "summary": "This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.", "AI": {"tldr": "\u9996\u6b21\u5c06\u91cf\u5b50\u751f\u6210\u6a21\u578b\u5e94\u7528\u4e8eCFD\u6570\u636e\u7684\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\uff1b\u901a\u8fc7VQ-VAE\u5c06\u6da1\u5ea6\u573a\u538b\u7f29\u52307\u7ef4\u79bb\u6563\u6f5c\u5728\u5411\u91cf\uff0c\u5e76\u6bd4\u8f83QCBM\u3001QGAN\u4e0eLSTM\uff0c\u91cf\u5b50\u6a21\u578b\u5728\u5206\u5e03\u62df\u5408\u4e0a\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\uff0cQCBM\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f25\u8865\u73b0\u6709\u5de5\u4f5c\u5728\u7269\u7406\u7cfb\u7edf\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u91cf\u5b50\u751f\u6210\u5efa\u6a21\u4e0d\u8db3\uff0c\u8bc4\u4f30\u91cf\u5b50\u4e0e\u7ecf\u5178\u65b9\u6cd5\u5728\u538b\u7f29\u540e\u7684\u7269\u7406\u6570\u636e\u5206\u5e03\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f00\u6e90\u6d41\u6c34\u7ebf\u3002", "method": "\u5efa\u7acbGPU\u52a0\u901f\u7684LBM CFD\u4eff\u771f\u4ee5\u751f\u6210\u6da1\u5ea6\u573a\uff1b\u4f7f\u7528VQ-VAE\u5c06\u573a\u6570\u636e\u79bb\u6563\u5316\u5e76\u538b\u7f29\u62107\u7ef4\u6f5c\u5728\u8868\u793a\uff1b\u5206\u522b\u8bad\u7ec3\u5e76\u6bd4\u8f83QCBM\u3001QGAN\u4e0eLSTM\u5728\u540c\u4e00\u6570\u636e\u96c6\u4e0a\u7684\u751f\u6210\u80fd\u529b\uff0c\u8bc4\u4f30\u5206\u5e03\u8ddd\u79bb\u7b49\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u5f00\u6e90\u7ba1\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91cf\u5b50\u6a21\u578b\u5728\u6837\u672c\u5bf9\u771f\u5b9e\u5206\u5e03\u7684\u6700\u5c0f\u8ddd\u79bb\u4e0a\u4f18\u4e8eLSTM\uff0c\u5176\u4e2dQCBM\u7684\u8868\u73b0\u6700\u4f73\uff1b\u4e24\u79cd\u91cf\u5b50\u6a21\u578b\u63d0\u4f9b\u4e86\u5bf9\u7269\u7406\u4eff\u771f\u6f5c\u5728\u5206\u5e03\u7684\u66f4\u597d\u62df\u5408\u3002\u8fd8\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u5b9e\u73b0\uff0c\u9996\u6b21\u5728 compressed physics latent representations \u4e0a\u8fdb\u884c\u91cf\u5b50\u751f\u6210\u5efa\u6a21\u7684\u5b9e\u9a8c\u3002", "conclusion": "\u4e3aCFD\u7b49\u7269\u7406\u4eff\u771f\u6570\u636e\u4e2d\u7684\u79bb\u6563\u6f5c\u5728\u8868\u793a\u4e0a\u7684\u91cf\u5b50\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u521d\u6b65\u8bc1\u636e\uff0c\u786e\u7acb\u4e86\u5f00\u6e90\u7ba1\u7ebf\u548c\u57fa\u7ebf\uff0c\u5960\u5b9a\u672a\u6765\u5728\u7269\u7406\u7cfb\u7edf\u4e2d\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u7814\u7a76\u57fa\u7840\u3002"}}
{"id": "2512.22702", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22702", "abs": "https://arxiv.org/abs/2512.22702", "authors": ["Valentina Moretti", "Andrea Cini", "Ivan Marisca", "Cesare Alippi"], "title": "What Matters in Deep Learning for Time Series Forecasting?", "comment": null, "summary": "Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.", "AI": {"tldr": "\u672c\u6587\u4ece\u8bbe\u8ba1\u7ef4\u5ea6\u51fa\u53d1\u5206\u6790\u65f6\u5e8f forecasting \u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u5c40\u90e8\u6027\u4e0e\u5168\u5c40\u6027\u7b49\u539f\u5219\u7684\u91cd\u8981\u6027\uff1b\u6307\u51fa\u8bb8\u591a\u7ed3\u679c\u53d7\u5b9e\u73b0\u7ec6\u8282\u5f71\u54cd\uff0c\u7b80\u5355\u4e14\u8bbe\u8ba1\u826f\u597d\u7684\u67b6\u6784\u5f80\u5f80\u80fd\u8fbe\u5230\u6216\u63a5\u8fd1\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u547c\u5401\u6539\u8fdb\u57fa\u51c6\u8bc4\u4f30\u5e76\u63d0\u51fa\u8f85\u52a9\u9884\u6d4b\u6a21\u578b\u5361\u4ee5\u523b\u753b\u8bbe\u8ba1\u9009\u62e9\u3002", "motivation": "\u5728\u5927\u91cf\u65b0\u63d0\u51fa\u7684\u67b6\u6784\u4e0e\u76f8\u4e92\u77db\u76fe\u7684\u5b9e\u9a8c\u7ed3\u679c\u80cc\u666f\u4e0b\uff0c\u96be\u4ee5\u5224\u65ad\u54ea\u4e9b\u7ec4\u4ef6\u771f\u6b63\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981\u57fa\u4e8e\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u7ec4\u7684 forecasting \u539f\u5219\u6765\u5f15\u5bfc\u8bbe\u8ba1\uff1b\u5e76\u5ba1\u89c6\u73b0\u6709\u57fa\u51c6\u8bc4\u4f30\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8ba8\u8bba\u8bbe\u8ba1\u7ef4\u5ea6\u4e0e\u6743\u8861\u3001\u5206\u6790 locality \u4e0e globality \u7684\u4f5c\u7528\u3001\u5bf9\u6bd4\u4e0d\u540c\u7ed3\u6784\u5c42\uff08\u5982\u5e8f\u5217\u5efa\u6a21\u5c42\uff09\u7684\u5f71\u54cd\u3001\u63ed\u793a\u5b9e\u73b0\u7ec6\u8282\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u8f85\u52a9 forecasting \u6a21\u578b\u5361\u4f5c\u4e3a\u8bc4\u4ef7\u6846\u67b6\u3002", "result": "\u5173\u952e\u53d1\u73b0\u5305\u62ec\uff1a\u8003\u8651\u5c40\u90e8\u6027\u4e0e\u5168\u5c40\u6027\u5728\u5b9e\u73b0\u6709\u6548\u9884\u6d4b\u4e2d\u6bd4\u76f2\u76ee\u91c7\u7528\u67d0\u79cd\u65f6\u5e8f\u5efa\u6a21\u5c42\u66f4\u4e3a\u91cd\u8981\uff1b\u7b80\u5355\u3001\u8bbe\u8ba1\u826f\u597d\u7684\u67b6\u6784\u5f80\u5f80\u80fd\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u8fd1\u7684\u6027\u80fd\uff1b\u5b9e\u73b0\u7ec6\u8282\u80fd\u591f\u672c\u8d28\u6539\u53d8\u65b9\u6cd5\u7684\u7c7b\u522b\u5e76\u663e\u8457\u5f71\u54cd\u5b9e\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u547c\u5401\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u57fa\u51c6\u5b9e\u9a8c\u7684\u8bbe\u8ba1\uff0c\u805a\u7126\u9884\u6d4b\u95ee\u9898\u7684\u57fa\u7840\u8981\u7d20\uff1b\u63d0\u51fa\u8f85\u52a9 forecasting \u6a21\u578b\u5361\u4f5c\u4e3a\u63cf\u8ff0\u548c\u6bd4\u8f83\u73b0\u6709\u4e0e\u65b0\u67b6\u6784\u7684\u5de5\u5177\u3002"}}
{"id": "2512.22733", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22733", "abs": "https://arxiv.org/abs/2512.22733", "authors": ["Jiaqi Shao", "Yufeng Miao", "Wei Zhang", "Bing Luo"], "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents", "comment": null, "summary": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.", "AI": {"tldr": "FoldAct introduces targeted techniques to stabilize long-horizon RL with context folding by decoupling gradient signals for summary vs. action tokens, enforcing context-consistency to mitigate distribution shift, and applying selective segment training to cut computation, achieving 5.19\u00d7 speedup.", "motivation": "In long-horizon RL with large language models, unbounded context growth leads to context folding, which replaces parts of history with summary tokens. These summaries alter the agent\u2019s observation space, making the observation distribution policy-dependent and non-stationary, which breaks core RL assumptions and harms learning efficiency.", "method": "FoldAct proposes three innovations: (1) separated loss computation to provide independent gradient signals for summary tokens and action tokens; (2) full context consistency loss to reduce distribution shift between training and deployment; (3) selective segment training to reduce computational cost by focusing updates on essential context segments.", "result": "The framework enables more stable training of long-horizon search agents under context folding and claims a 5.19\u00d7 speedup in training efficiency, addressing non-stationary observation while maintaining performance.", "conclusion": "FoldAct addresses three fundamental challenges posed by context folding\u2014gradient dilution, self-conditioning, and computational cost\u2014by redesigning loss signals, enforcing context consistency, and optimizing training scope, thus improving stability and efficiency for long-horizon RL with language models."}}
{"id": "2512.22740", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22740", "abs": "https://arxiv.org/abs/2512.22740", "authors": ["Sungwoo Kang"], "title": "When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction", "comment": null, "summary": "Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\\to$ 0.844; hardness $R^2$: 0.832 $\\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.", "AI": {"tldr": "\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u6750\u6599\u6027\u8d28\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u5448\u73b0\u53cc\u8f68\uff1a\u5bf9\u5206\u7c7b\u4efb\u52a1\u6709\u76ca\uff0c\u4f46\u5bf9\u56de\u5f52\u4efb\u52a1\u9020\u6210\u663e\u8457\u8d1f\u8fc1\u79fb\uff0c\u4e3b\u56e0\u662f\u6570\u636e\u4e0d\u5e73\u8861\u4e0e\u4efb\u52a1\u95f4\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "motivation": "\u68c0\u9a8c\u76f8\u5173\u7269\u7406\u6027\u8d28\u662f\u5426\u80fd\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u5171\u4eab\u5e95\u5c42\u8868\u5f81\uff0c\u4ee5\u63d0\u5347\u5728\u6750\u6599\u9886\u57df\u7684\u9884\u6d4b\u6027\u80fd\uff1b\u4ee554,028\u4e2a\u5408\u91d1\u6837\u672c\u540c\u65f6\u9884\u6d4b\u7535\u963b\u7387\u3001\u7ef4\u6c0f\u786c\u5ea6\u548c\u975e\u6676\u5f62\u6210\u80fd\u529b\u3002", "method": "\u5c06\u5355\u4efb\u52a1\u3001\u6807\u51c6MTL\u548c\u7ed3\u6784\u5316MTL\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5728\u56de\u5f52\uff08\u7535\u963b\u7387\u3001\u786c\u5ea6\uff09\u548c\u5206\u7c7b\uff08\u975e\u6676\u5f62\u6210\u80fd\u529b\uff09\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6027\u80fd\uff0c\u91c7\u7528R^2\uff08\u56de\u5f52\uff09\u548cF1/\u53ec\u56de\u7387\uff08\u5206\u7c7b\uff09\uff0c\u5e76\u5206\u6790\u4efb\u52a1\u95f4\u6743\u91cd\u548c\u6570\u636e\u91cf\u4e0d\u5e73\u8861\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "MTL\u663e\u8457\u964d\u4f4e\u56de\u5f52\u6027\u80fd\uff08\u7535\u963b\u7387R^2\u4ece0.897\u964d\u81f30.844\uff1b\u786c\u5ea6R^2\u4ece0.832\u964d\u81f30.694\uff0cp<0.01\uff09\uff0c\u800c\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff08\u975e\u6676\u5f62\u6210F1\u4ece0.703\u5347\u81f30.744\uff0cp<0.05\uff0c\u53ec\u56de\u7387\u63d0\u534717%\uff09\u3002\u5206\u6790\u663e\u793a\u4efb\u52a1\u95f4\u6743\u91cd\u63a5\u8fd1\u4e3a\u96f6\uff0c\u8868\u660e\u5c5e\u6027\u4e4b\u95f4\u72ec\u7acb\u6027\u5f3a\uff1b\u56de\u5f52\u5931\u8d25\u5f52\u56e0\u4e8e\u8d1f\u8fc1\u79fb\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u4e25\u91cd\u4e0d\u5e73\u8861\uff0852k\u6837\u672c\u5bf9\u6bd4800\u6837\u672c\uff09\u3002", "conclusion": "\u5e94\u5c06\u56de\u5f52\u4efb\u52a1\u72ec\u7acb\u5efa\u6a21\u4ee5\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u9884\u6d4b\uff1bMTL\u53ef\u4fdd\u7559\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u53ec\u56de\u7387\u6709\u9ad8\u8981\u6c42\u7684\u573a\u666f\u3002"}}
{"id": "2512.22744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22744", "abs": "https://arxiv.org/abs/2512.22744", "authors": ["Rihong Qiu", "Zhibang Yang", "Xinke Jiang", "Weibin Liao", "Xin Gao", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL", "comment": null, "summary": "Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.", "AI": {"tldr": "\u63d0\u51fa HEROSQL\uff0c\u901a\u8fc7\u5206\u5c42\u7684\u5168\u5c40\u610f\u56fe LP \u4e0e\u5c40\u90e8 AST \u7ec6\u8282\u3001NMPNN \u4fe1\u606f\u4f20\u9012\uff0c\u4ee5\u53ca AST \u9a71\u52a8\u7684\u5b50 SQL \u589e\u5f3a\uff0c\u63d0\u5347\u6587\u672c\u5230 SQL \u7684\u8bed\u4e49\u6821\u9a8c\u6548\u679c\uff0c\u5728\u57df\u5185\u5916\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347 AUPRC \u4e0e AUROC\u3002", "motivation": "\u73b0\u6709 Text-to-SQL \u9a8c\u8bc1\u591a\u5173\u6ce8\u53e5\u6cd5\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u5c42\u9762\u7684\u9a8c\u8bc1\uff0c\u96be\u4ee5\u6355\u6349\u7528\u6237\u610f\u56fe\u4e0e SQL \u7ed3\u6784\u9519\u914d\uff0c\u540c\u65f6\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u7ec6\u7c92\u5ea6\u5b50 SQL \u6ce8\u91ca\u6765\u652f\u6491\u9c81\u68d2\u4f18\u5316\u3002", "method": "\u63d0\u51fa HEROSQL\uff1a\u5c06\u5168\u5c40\u610f\u56fe\uff08\u901a\u8fc7\u903b\u8f91\u8ba1\u5212 LPs\uff09\u4e0e\u5c40\u90e8\u7ec6\u8282\uff08\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811 ASTs\uff09\u6574\u5408\u7684\u5206\u5c42 SQL \u8868\u5f81\uff1b\u4f7f\u7528 Nested Message Passing Neural Network\uff08NMPNN\uff09\u5728 LPs\u3001ASTs \u53ca\u6a21\u5f0f\u4e4b\u95f4\u4f20\u64ad\u4fe1\u606f\u5e76\u805a\u5408\u6a21\u5f0f\u76f8\u5173\u8bed\u4e49\uff1b\u63d0\u51fa AST \u9a71\u52a8\u7684\u5b50 SQL \u589e\u5f3a\u7b56\u7565\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u5bf9\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728 Text-to-SQL \u9a8c\u8bc1\u57fa\u51c6\uff08\u540c\u57df\u4e0e\u8de8\u57df\uff09\u4e0a\uff0c\u5e73\u5747\u63d0\u5347 AUPRC 9.40%\u3001AUROC 12.35%\uff0c\u5728\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u8bed\u4e49\u9519\u8bef\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u63d0\u4f9b\u66f4 granular \u7684\u53cd\u9988\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u5c42\u8868\u5f81\u548c\u56fe\u795e\u7ecf\u4fe1\u606f\u4f20\u9012\uff0c\u7ed3\u5408\u9762\u5411 AST \u7684\u5b50 SQL \u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u5230 SQL \u7684\u8bed\u4e49\u6821\u9a8c\u80fd\u529b\uff0c\u589e\u5f3a\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.22758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22758", "abs": "https://arxiv.org/abs/2512.22758", "authors": ["Eswarasanthosh Kumar Mamillapalli", "Nishtha Sharma"], "title": "A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants", "comment": null, "summary": "Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e00\u4e2a\u5fae\u89c2-\u5b8f\u89c2\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4e2a\u4f53\u5c42\u6570\u636e\u4e0e\u5b8f\u89c2\u73af\u5883\u6570\u636e\u878d\u5408\uff0c\u7528\u591a\u6a21\u578b\u6bd4\u8f83\u6765\u9884\u6d4b\u513f\u7ae5\u80a5\u80d6\u5e76\u63ed\u793a\u73af\u5883\u8106\u5f31\u6027\u4e0e\u5730\u7406\u5206\u5e03\u7684\u5173\u7cfb\uff0cXGBoost\u8868\u73b0\u6700\u4f73\uff0cEnvScore\u6307\u793a\u73af\u5883\u8d1f\u8377\u4e0e\u80a5\u80d6\u98ce\u9669\u7684\u5730\u7406\u4e00\u81f4\u6027\u3002 ", "motivation": "\u5f25\u5408\u4e2a\u4f53\u56e0\u7d20\u4e0e\u73af\u5883\u5c42\u9762\u7684\u7814\u7a76\u65ad\u5c42\uff0c\u63a2\u7d22\u7ed3\u6784\u6027\u73af\u5883\u6761\u4ef6\u5982\u4f55\u4e0e\u4e2a\u4eba\u7279\u5f81\u4ea4\u4e92\u5f71\u54cd\u80a5\u80d6\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u6846\u67b6\u4ee5\u652f\u6301\u5e72\u9884\u89c4\u5212\u548c\u5b9e\u65f6\u5206\u6790\u3002", "method": "\u6574\u5408 NHANES \u5fae\u89c2\u6570\u636e\uff08\u4e2a\u4f53\u5316\u8eab\u9ad8\u3001\u4f53\u91cd\u3001\u6027\u522b\u3001\u793e\u4f1a\u7ecf\u6d4e\u72b6\u6001\u7b49\uff09\u4e0e USDA/EPA \u7684\u5b8f\u89c2\u73af\u5883\u7279\u5f81\uff08\u98df\u54c1\u83b7\u53d6\u3001\u7a7a\u6c14\u8d28\u91cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u8106\u5f31\u6027\u7b49\uff09\uff0c\u8bad\u7ec3\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08Logistic Regression\u3001Random Forest\u3001XGBoost\u3001LightGBM\uff09\u7528\u4e8e\u80a5\u80d6\u9884\u6d4b\uff1b\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8e\u6807\u51c6\u5316\u6307\u6807\u7684\u5dde\u7ea7 EnvScore \u73af\u5883\u8106\u5f31\u6027\u6307\u6570\uff1b\u8fdb\u884c\u591a\u5c3a\u5ea6\u5730\u7406\u6bd4\u8f83\u4ee5\u8bc4\u4f30\u73af\u5883\u8d1f\u8377\u4e0e\u5fae\u89c2\u80a5\u80d6\u98ce\u9669\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u3002", "result": "XGBoost \u4e3a\u6700\u4f73\u6a21\u578b\uff1bEnvScore \u5728\u5dde\u7ea7\u805a\u5408\u65f6\u4e0e\u5168\u56fd\u5fae\u89c2\u80a5\u80d6\u98ce\u9669\u5206\u5e03\u5448\u5f3a\u76f8\u5173\uff1b\u8bc1\u660e\u591a\u5c3a\u5ea6\u6570\u636e\u6574\u5408\u5728\u63ed\u793a\u73af\u5883\u9a71\u52a8\u7684\u80a5\u80d6\u98ce\u9669\u4e0e\u5206\u5e03\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u591a\u5c42\u5efa\u6a21\u6d41\u7a0b\uff0c\u5177\u5907\u516c\u5171\u536b\u751f\u4fe1\u606f\u5b66\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u56e0\u679c\u63a8\u65ad\u3001\u5e72\u9884\u8bbe\u8ba1\u4e0e\u5b9e\u65f6\u76d1\u6d4b\u7b49\u573a\u666f\u3002"}}
{"id": "2512.22768", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22768", "abs": "https://arxiv.org/abs/2512.22768", "authors": ["Nikhil Ghosh", "Denny Wu", "Alberto Bietti"], "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer", "comment": "43 pages", "summary": "The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($\u03bc$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $\u03bc$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.", "AI": {"tldr": "Scale-aware hyperparameters enable transfer of optimal HPs across model scales with minimal loss; fast transfer means suboptimality shrinks faster than finite-scale performance gap; \u03bcP shows fast transfer under certain problem structures; a width-stable vs width-sensitive decomposition explains practice, supported by synthetic tests and LLM pretraining.", "motivation": "Reduce compute and time costs of hyperparameter tuning when scaling deep learning models; provide a unified framework to understand when HP transfer is beneficial and how to reason about scale-induced changes in optimal HPs.", "method": "Develop a general conceptual framework for HP transfer across scale; formalize transfer as fast when suboptimality vanishes faster than the finite-scale gap; prove equivalence between fast transfer and compute-efficient grid search under certain conditions; construct synthetic settings to delineate when transfer is provably advantageous or not under \u03bcP; conjecture a decomposition of the optimization trajectory into width-stable and width-sensitive components; validate with empirical experiments including large language model pretraining.", "result": "Formal condition: fast transfer is equivalent to useful transfer for compute-optimal grid search; \u03bcP yields fast transfer in some width-based scaling regimes but not universally; synthetic counterexamples show cases where transfer can fail; conjectured two-component loss decomposition explains empirical fast transfer; empirical evidence across diverse tasks supports the hypothesis.", "conclusion": "Problem structure governs transfer efficacy; a width-stable component sets HP optima while width-sensitive changes provide width-related gains with limited HP perturbation; recognizing this decomposition helps explain observed fast transfer and guides future work to validate the conjecture and extend to broader scale settings and performance metrics."}}
{"id": "2512.22772", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22772", "abs": "https://arxiv.org/abs/2512.22772", "authors": ["Xuyan Li", "Jie Wang", "Zheng Yan"], "title": "GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks", "comment": null, "summary": "Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.", "AI": {"tldr": "GRExplainer \u662f\u9996\u4e2a\u666e\u9002\u3001\u6709\u6548\u4e14\u6613\u7528\u7684 TGNN \u89e3\u91ca\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u8f93\u5165\u8282\u70b9\u5e8f\u5217\u4f5c\u4e3a\u7edf\u4e00\u7279\u5f81\u8868\u793a\uff0c\u517c\u5bb9\u5feb\u7167\u578b\u548c\u4e8b\u4ef6\u578b TGNN\uff0c\u5229\u7528\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u65f6\u95f4\u4fe1\u606f\u6784\u5efa\u5e8f\u5217\u4ee5\u63d0\u5347\u6548\u7387\uff0c\u5e76\u4ee5\u57fa\u4e8e RNN \u7684\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u3001\u6301\u7eed\u7684\u89e3\u91ca\u751f\u6210\u3002\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u3001\u4e09\u79cd TGNN \u7684\u5b9e\u9a8c\u4e2d\uff0cGRExplainer \u5728\u901a\u7528\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d TGNN \u89e3\u91ca\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5f80\u5f80\u5ffd\u7565\u56fe\u7684\u7ed3\u6784\u8fde\u901a\u6027\uff0c\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\uff0c\u7528\u6237\u53cb\u597d\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u901a\u7528\u3001\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684 TGNN \u89e3\u91ca\u6846\u67b6\u3002", "method": "\u63d0\u51fa GRExplainer\uff0c\u63d0\u53d6\u8282\u70b9\u5e8f\u5217\u4f5c\u4e3a\u7edf\u4e00\u7684\u8f93\u5165\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u5176\u4e0e\u5177\u4f53\u8f93\u5165\u683c\u5f0f\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u5feb\u7167\u578b\u548c\u4e8b\u4ef6\u578b TGNN\uff1b\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u65f6\u95f4\u4fe1\u606f\u6784\u5efa\u8f93\u5165\u8282\u70b9\u5e8f\u5217\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3001\u63d0\u9ad8\u6548\u7387\uff1b\u5f15\u5165\u57fa\u4e8e RNN \u7684\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u8fde\u7eed\u7684\u89e3\u91ca\u751f\u6210\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u3001\u4e09\u79cd\u76ee\u6807 TGNN \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRExplainer \u5728\u901a\u7528\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GRExplainer \u6210\u4e3a\u9996\u4e2a\u666e\u9002\u3001\u9ad8\u6548\u3001\u6613\u7528\u7684 TGNN \u89e3\u91ca\u6846\u67b6\uff0c\u6709\u671b\u63d0\u5347 TGNN \u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u7528\u6027\u3002"}}
{"id": "2512.22774", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22774", "abs": "https://arxiv.org/abs/2512.22774", "authors": ["Truong Son Nguyen"], "title": "Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization", "comment": null, "summary": "We introduce \\textbf{Schr\u00f6dinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.\n  Empirically, Schr\u00f6dinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.", "AI": {"tldr": "Schr\u00f6dinger AI proposes a physics-inspired ML framework integrating three components: a time-independent wave-energy solver for spectral perception via a learned Hamiltonian, a time-dependent dynamical solver for evolving semantic wavefunctions and context-aware reasoning, and a low-rank operator calculus learning quantum-like transition operators for symbolic transformations. Together, they form a physics-driven alternative to standard cross-entropy and attention-based models, aiming for robust generalization, interpretable semantics, and emergent topology.", "motivation": "To develop a unified, physics-inspired framework that transcends conventional deep learning training paradigms (e.g., cross-entropy, transformer attention), enabling interpretable semantic structure, robust generalization, and adaptive reasoning under changing environments.", "method": "Coupled architecture with three components: (1) a time-independent wave-energy solver performing spectral decomposition under a learned Hamiltonian to represent perception/classification; (2) a time-dependent dynamical solver governing temporal evolution of semantic wavefunctions for context-aware revision and routing; (3) a low-rank operator calculus learning symbolic quantum-like transformations, such as modular arithmetic, via learned transition operators, thereby capturing group actions.", "result": "Empirically, the approach yields emergent semantic manifolds aligned with human class relations without explicit supervision; dynamic reasoning adapting to environmental perturbations (e.g., navigation under potential-field changes); and exact operator generalization on modular arithmetic tasks, with the ability to compose learned actions across sequences beyond training length.", "conclusion": "Schr\u00f6dinger AI points to a foundational direction where learning is framed as discovering and navigating an underlying semantic energy landscape, offering a unified physics-driven alternative to traditional deep learning training and attention mechanisms with potential for interpretable semantics and robust generalization."}}
{"id": "2512.22777", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22777", "abs": "https://arxiv.org/abs/2512.22777", "authors": ["Kasra Jalaldoust", "Elias Bareinboim"], "title": "Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning", "comment": null, "summary": "Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \\textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u4f20\u8f93\u6027\u7684\u6846\u67b6Circuit-TR\uff0c\u7528\u4e8e\u96f6-shot\u7ec4\u5408\u6cdb\u5316\u548c\u5c11\u6837\u672c\u57df\u9002\u5e94\u3002\u901a\u8fc7\u6e90\u57df\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u5c40\u90e8\u9884\u6d4b\u6a21\u5757\uff0c\u5728\u5df2\u77e5\u56e0\u679c\u7ed3\u6784\u548c\u8de8\u57df\u5dee\u5f02 oracle \u7684\u7ea6\u675f\u4e0b\u8fdb\u884c\u4f20\u8f93\u4e0e\u7ec4\u5408\uff0c\u4ece\u800c\u6784\u5efa\u76ee\u6807\u57df\u7684\u9884\u6d4b\u7535\u8def\u3002\u8fd8\u7ed9\u51fa\u65e0\u9700\u663e\u5f0f\u56e0\u679c\u7ed3\u6784\u3001\u4ec5\u9760\u5c11\u91cf\u76ee\u6807\u6570\u636e\u5373\u53ef\u8fdb\u884c\u76d1\u7763\u57df\u9002\u914d\u7684\u7b56\u7565\u3002\u7406\u8bba\u4e0a\u5c06\u5c11-shot \u5b66\u4e60\u4efb\u52a1\u4e0e\u56fe\u7ed3\u6784\u5316\u7684\u7535\u8def\u4f20\u8f93\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5c06\u5176\u4e0e\u7535\u8def\u5927\u5c0f\u590d\u6742\u5ea6\u76f8\u8fde\uff1b\u5927\u91cf\u4eff\u771f\u5b9e\u9a8c\u5bf9\u7406\u8bba\u7ed3\u679c\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u8de8\u57df\u6cdb\u5316\u5728\u7f3a\u4e4f\u5bf9\u76ee\u6807\u57df\u7ed3\u6784\u8ba4\u77e5\u65f6\u51e0\u4e4e\u4e0d\u53ef\u80fd\u5b9e\u73b0\u3002\u5f15\u5165\u56e0\u679c\u4f20\u8f93\u6027\u6846\u67b6\uff0c\u5229\u7528\u57df\u5185\u56e0\u679c\u7ed3\u6784\u4e0e\u57df\u95f4\u673a\u5236\u5171\u4eab\u7684\u77e5\u8bc6\u7ea6\u675f\uff0c\u6765\u5b9e\u73b0\u96f6-shot \u7684\u7ec4\u5408\u6cdb\u5316\uff0c\u5e76\u5728\u6709\u9650\u76ee\u6807\u6570\u636e\u4e0b\u5b9e\u73b0\u76d1\u7763\u57df\u9002\u914d\u3002", "method": "\u4ece\u6e90\u6570\u636e\u5b66\u4e60\u4e00\u7ec4\u5c40\u90e8 predictor \u6a21\u5757\uff0c\u5229\u7528\u56e0\u679c\u7ed3\u6784\u8bb8\u53ef\u7684\u524d\u63d0\u5bf9\u8fd9\u4e9b\u6a21\u5757\u8fdb\u884c\u4f20\u8f93\u4e0e\u7ec4\u5408\uff0c\u5f62\u6210\u7528\u4e8e\u76ee\u6807\u57df\u9884\u6d4b\u7684\u7535\u8def\uff08Circuit-TR\uff09\u3002\u82e5\u65e0\u663e\u5f0f\u56e0\u679c\u7ed3\u6784\u4e5f\u53ef\u901a\u8fc7\u5e26\u6709\u5c11\u91cf\u76ee\u6807\u6570\u636e\u7684\u76d1\u7763\u57df\u9002\u914d\u5b9e\u73b0\u8de8\u57df\u4f20\u8f93\uff1b\u8de8\u57df\u5dee\u5f02\u901a\u8fc7\u201cdiscrepancies oracle\u201d\u8fdb\u884c\u673a\u5236\u5c42\u9762\u7684\u5171\u4eab\u3002\u7406\u8bba\u90e8\u5206\u7ed9\u51fa\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u7535\u8def\u4f20\u8f93\u6027\u51c6\u5219\u6765\u523b\u753b\u5c11-shot \u53ef\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u628a\u5c11-shot \u6cdb\u5316\u4e0e\u7535\u8def\u5927\u5c0f\u590d\u6742\u5ea6\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u7406\u8bba\u4e0a\u7ed9\u51fa\u5c11-shot \u5b66\u4e60\u4efb\u52a1\u7684\u56fe\u7ed3\u6784\u5316\u7535\u8def\u4f20\u8f93\u6027\u5206\u7c7b\u51c6\u5219\uff0c\u5e76\u5c06\u5176\u4e0e\u7535\u8def\u5927\u5c0f\u590d\u6742\u5ea6\u5173\u8054\uff1b\u901a\u8fc7\u53d7\u63a7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u8bba\uff0c\u663e\u793a\u5728\u7ed9\u5b9a\u56e0\u679c\u7ed3\u6784\u7ea6\u675f\u4e0b\u7684\u96f6-shot/\u5c11-shot \u6cdb\u5316\u80fd\u529b\u63d0\u5347\u3002", "conclusion": "Circuit-TR \u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56e0\u679c\u4f20\u8f38\u6846\u67b6\uff0c\u7528\u4e8e\u96f6-shot \u7ec4\u5408\u6cdb\u5316\u548c\u5c11\u6837\u672c\u57df\u9002\u914d\uff1b\u5728\u9700\u8981\u7ed3\u6784\u7ea6\u675f\u7684\u8de8\u57df\u4efb\u52a1\u4e2d\uff0c\u53ef\u901a\u8fc7\u5b66\u4e60\u5c40\u90e8\u6a21\u5757\u3001\u4f7f\u7528\u56e0\u679c\u56fe\u4e0e discrepancy oracle \u8fdb\u884c\u6709\u6548\u4f20\u8f93\u4e0e\u7ec4\u5408\uff0c\u5e76\u4e14\u7406\u8bba\u4e0e\u4eff\u771f\u5747\u652f\u6301\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.22802", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22802", "abs": "https://arxiv.org/abs/2512.22802", "authors": ["Amirhossein Tighkhorshid", "Zahra Dehghanian", "Gholamali Aminian", "Chengchun Shi", "Hamid R. Rabiee"], "title": "ReDiF: Reinforced Distillation for Few Step Diffusion", "comment": null, "summary": "Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u84b8\u998f\u8fc7\u7a0b\u89c6\u4e3a\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u5728\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u6570\u4e0b\u3001\u4ee5\u66f4\u957f\u7684\u6b65\u4f10\u903c\u8fd1\u6559\u5e08\u8f93\u51fa\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002\u8be5\u6846\u67b6\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\u7686\u53ef\u9002\u7528\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u7684\u6162\u91c7\u6837\u95ee\u9898\u3002\u73b0\u6709\u84b8\u998f\u591a\u4f9d\u8d56\u56fa\u5b9a\u7684\u91cd\u6784\u6216\u4e00\u81f4\u6027\u635f\u5931\uff0c\u96be\u4ee5\u5728\u66f4\u5c11\u4e00\u6b65\u7684\u6761\u4ef6\u4e0b\u4fdd\u7559\u9ad8\u8d28\u91cf\u6837\u672c\u3002\u672c\u6587\u63d0\u51fa\u7684RL\u9a71\u52a8\u84b8\u998f\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u5b66\u751f\u63a2\u7d22\u591a\u6761\u53bb\u566a\u8def\u5f84\uff0c\u5411\u9ad8\u6982\u7387\u6570\u636e\u533a\u57df\u79fb\u52a8\u3002", "method": "\u5c06\u84b8\u998f\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff1a\u5b66\u751f\u5728\u6559\u5e08\u8f93\u51fa\u7684\u5bf9\u9f50\u7a0b\u5ea6\u4e0a\u83b7\u5f97\u5956\u52b1\u4fe1\u53f7\uff0c\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u56de\u62a5\u3002\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u7b56\u7565\uff0c\u5b66\u751f\u80fd\u5728\u66f4\u957f\u7684\u53bb\u566a\u6b65\u957f\u4e0b\u641c\u7d22\u751f\u6210\u8def\u5f84\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u63a5\u8fd1\u6559\u5e08\u7684\u5206\u5e03\u3002\u8be5\u6846\u67b6\u5177\u5907\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u53ef\u63d0\u4f9b\u5408\u9002\u5956\u52b1\u51fd\u6570\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u6b65\u6570\u663e\u8457\u66f4\u5c11\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "RL\u9a71\u52a8\u7684\u6269\u6563\u84b8\u998f\u6846\u67b6\u4e3a\u9ad8\u6548\u6269\u6563\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u822c\u5316\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6269\u6563\u6a21\u578b\u7ed3\u6784\u4e2d\u901a\u8fc7\u8bbe\u8ba1\u5408\u9002\u7684\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u4e0e\u63a8\u7406\u3002"}}
{"id": "2512.22804", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22804", "abs": "https://arxiv.org/abs/2512.22804", "authors": ["Bor-Yiing Su", "Peter Dykas", "Mike Chrzanowski", "Jatin Chhugani"], "title": "MoR: Mixture Of Representations For Mixed-Precision Training", "comment": null, "summary": "Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.", "AI": {"tldr": "\u63d0\u51faMixture-of-Representations (MoR)\u6846\u67b6\uff0c\u57fa\u4e8e per-tensor \u548c sub-tensor \u7684\u52a8\u6001\u91cf\u5316\uff0c\u5728 FP8 \u4e0e BF16 \u4e4b\u95f4\u81ea\u9002\u5e94\u9009\u62e9\u8868\u793a\u5f62\u5f0f\uff0c\u4ee5\u5728\u4e0d\u540c\u91cf\u5316\u5206\u533a\u7b56\u7565\u4e0b\u5c3d\u91cf\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\u9ad8\u8fbe98.38%\u7684\u5f20\u91cf\u88ab\u91cf\u5316\u4e3a FP8\uff0c\u4e14 FP8 \u7cbe\u5ea6\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u65e0\u9700\u7ec6\u7c92\u5ea6\u5206\u533a\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u6027\uff0c\u5177\u5907\u4e0e NVFP4 \u7b49\u66f4\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "motivation": "\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u5bf9\u6269\u5c55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8981\u53d6\u5f97\u6210\u529f\uff0c\u9700\u8981\u8bc6\u522b\u5e76\u6b63\u786e\u5e94\u7528\u4e00\u7ec4\u8bad\u7ec3\u65b9\u6cd5\u3002\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u56fa\u5b9a\u8868\u793a\u5f62\u5f0f\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u5f20\u91cf\u7684\u6570\u503c\u5c5e\u6027\u4e0b\u901a\u7528\u5316\u3002\u63d0\u51fa\u7684MoR\u901a\u8fc7\u6309\u5f20\u91cf\u7ea7\u548c\u5b50\u5f20\u91cf\u7ea7\u7684\u52a8\u6001\u5206\u6790\uff0c\u9009\u62e9\u6700\u5408\u9002\u7684\u6570\u503c\u8868\u793a\u5f62\u5f0f\uff0c\u65e8\u5728\u5728\u4e0d\u540c\u5206\u533a\u7b56\u7565\u548c\u6570\u636e\u96c6\u4e0a\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u8d28\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u5f15\u5165MoR\u6846\u67b6\uff0c\u9488\u5bf9\u5f20\u91cf\u7684\u6570\u503c\u5c5e\u6027\u8fdb\u884c\u52a8\u6001\u5206\u6790\uff1b\u5f00\u53d1\u5728 per-tensor \u4e0e sub-tensor \u9897\u7c92\u5ea6\u4e0a\u5728 FP8 \u4e0e BF16 \u4e4b\u95f4\u5207\u6362\u7684\u5177\u4f53\u7b97\u6cd5\uff1b\u786e\u4fdd\u5728\u4e0d\u540c\u91cf\u5316\u5206\u533a\u7b56\u7565\u548c\u6570\u636e\u96c6\u4e0b\u5177\u6709\u666e\u9002\u6027\uff1b\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u670998.38%\u7684\u5f20\u91cf\u53ef\u91cf\u5316\u4e3a FP8\uff1b\u5728 FP8 \u4e0a\u7684\u7cbe\u5ea6\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u65e0\u9700\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u533a\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u6027\uff1b\u5e76\u4e14\u8be5\u65b9\u6cd5\u5177\u6709\u4e0e\u5176\u5b83\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u5229\u7528\u66f4\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff08\u5982 NVFP4\uff09\u7684\u6f5c\u529b\u3002", "conclusion": "Dynamic\u3001\u9762\u5411\u5c5e\u6027\u7684\u91cf\u5316\uff08MoR\uff09\u6709\u671b\u63d0\u5347\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u5206\u533a\u7b56\u7565\u4e0b\u6cdb\u5316\u5e76\u5728\u4e0d\u589e\u52a0\u7c92\u5ea6\u5206\u533a\u6210\u672c\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u4f4e\u7cbe\u5ea6\u8868\u793a\u3002\u8be5\u601d\u8def\u8fd8\u53ef\u4e0e\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\uff0c\u63a8\u52a8\u5728\u66f4\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e0a\u7684\u5e94\u7528\u3002"}}
{"id": "2512.22814", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.22814", "abs": "https://arxiv.org/abs/2512.22814", "authors": ["Scott A. Martin", "Noah Brenowitz", "Dale Durran", "Michael Pritchard"], "title": "Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models", "comment": null, "summary": "Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic \"student\" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive \"teacher\" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.", "AI": {"tldr": "\u63d0\u51fa\u957f\u8303\u56f4\u84b8\u998f\uff1a\u7528\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u5927\u91cf\u5408\u6210\u6570\u636e\uff0c\u8bad\u7ec3\u76f4\u63a5\u8fdb\u884c\u957f\u65f6\u9884\u6d4b\u7684\u5b66\u751f\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u957f\u8303\u56f4\u9884\u6d4b\u6280\u80fd\u5e76\u66ff\u4ee3\u591a\u6b65\u81ea\u56de\u5f52\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570AI\u5929\u6c14\u6a21\u578b\u662f\u81ea\u56de\u5f52\u7684\uff0c\u957f\u8303\u56f4\u9884\u62a5\u6613\u79ef\u7d2f\u9519\u8bef\u4e14\u5bf9\u6162\u6a21\u6001\u7684\u8bad\u7ec3\u6837\u672c\u6709\u9650\uff1b40\u5e74\u7684\u518d\u5206\u6790\u6570\u636e\u4e5f\u96be\u4ee5\u8986\u76d6\u6240\u9700\u6837\u672c\u3002\u9700\u8981\u901a\u8fc7\u6d77\u91cf\u6570\u636e\u63d0\u9ad8\u5bf9\u957f\u65f6\u5c3a\u5ea6\u7684\u5b66\u4e60\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u957f\u8303\u56f4\u84b8\u998f\uff1a\u4ee5\u77ed\u6b65\u81ea\u56de\u5f52\u6559\u5e08\u4e3a\u6570\u636e\u6e90\uff0c\u751f\u6210\u6d77\u91cf\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u5728\u957f\u65f6\u5c3a\u5ea6\u76f4\u63a5\u9884\u6d4b\u7684\u5b66\u751f\u6a21\u578b\u3002\u4ee5DLESyM\u4e3a\u6559\u5e08\uff0c\u751f\u6210>1\u4e07\u5e74\u6a21\u62df\u6c14\u5019\uff0c\u8bad\u7ec3\u9762\u5411S2S\u53ca\u66f4\u957f\u5c3a\u5ea6\u7684\u84b8\u998f\u5b66\u751f\u3002\u901a\u8fc7\u5b8c\u7f8e\u6a21\u578b\u4e0e\u771f\u5b9e\u6570\u636e\u8bc4\u4f30\u5176\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u7406\u60f3\u6a21\u578b\u4e2d\uff0c\u84b8\u998f\u5b66\u751f\u8d85\u8d8a\u57fa\u7ebf\u7684\u6c14\u5019\u5747\u503c\u5e76\u63a5\u8fd1\u81ea\u56de\u5f52\u6559\u5e08\u6280\u80fd\uff0c\u53ef\u7528\u5355\u4e00\u6b65\u9aa4\u66ff\u4ee3\u591a\u6b65\u81ea\u56de\u5f52\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u7ecf\u8fc7ERA5\u5fae\u8c03\uff0c\u84b8\u998f\u5b66\u751f\u8fbe\u5230\u4e0eECMWF\u96c6\u5408\u9884\u62a5\u76f8\u5f53\u7684S2S\u6280\u80fd\u3002\u968f\u7740\u5408\u6210\u6570\u636e\u91cf\u589e\u52a0\uff0c\u84b8\u998f\u6a21\u578b\u7684\u6280\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u9996\u6b21\u8bc1\u660eAI\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u53ef\u6269\u5c55\u957f\u8303\u56f4\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8bc1\u660e\u901a\u8fc7AI\u751f\u6210\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5e76\u6269\u5c55\u957f\u8303\u56f4\u5929\u6c14\u9884\u6d4b\u6280\u80fd\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u66ff\u4ee3\u591a\u6b65\u81ea\u56de\u5f52\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.22824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22824", "abs": "https://arxiv.org/abs/2512.22824", "authors": ["Gaurav Chaudhary", "Laxmidhar Behera"], "title": "TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b66\u751f-\u6559\u5e08\u5b66\u4e60\u8303\u5f0f\u5e76\u7ed3\u5408\u65f6\u95f4\u65b9\u5dee\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b89\u6392\uff0c\u7528\u4e8e\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u3002\u6559\u5e08\u52a8\u6001\u805a\u7126\u4e8e\u7b56\u7565\u7f6e\u4fe1\u5ea6\u5728\u65f6\u95f4\u4e0a\u65b9\u5dee\u6700\u9ad8\u7684\u76ee\u6807\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6837\u672c\u6548\u7387\uff0c\u7406\u8bba\u4e0a\u5c06Q\u503c\u7684\u65f6\u95f4\u65b9\u5dee\u4e0e\u7b56\u7565\u6f14\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u572811\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u76ee\u6807\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u6216\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5355\u4e00\u7684\u7b49\u6982\u7387\u76ee\u6807\u91c7\u6837\u5f80\u5f80\u5bfc\u81f4\u6837\u672c\u4f4e\u6548\u3002\u53d7\u5230\u751f\u7269\u7cfb\u7edf\u4e2d\u9002\u5e94\u6027\u4e0e\u7ed3\u6784\u5316\u5b66\u4e60\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u53ef\u81ea\u9002\u5e94\u3001\u805a\u7126\u6027\u7684\u5b66\u4e60\u4fe1\u53f7\u5206\u53d1\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u5f15\u5165Student-Teacher\u6846\u67b6\uff0c\u6559\u5e08\u6a21\u5757\u57fa\u4e8eQ\u503c\u7684\u65f6\u95f4\u65b9\u5dee\u6765\u52a8\u6001\u4f18\u5148\u9009\u62e9\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u76ee\u6807\uff0c\u5e76\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u4e0e\u5177\u4f53\u7684RL\u7b97\u6cd5\u65e0\u5173\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684RL\u6846\u67b6\u4e2d\u3002", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86Q\u503c\u65f6\u95f4\u65b9\u5dee\u4e0e\u7b56\u7565\u6f14\u5316\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u65b9\u6cd5\u63d0\u4f9b\u539f\u7406\u652f\u6491\u3002\u5b9e\u9a8c\u572811\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4e0e\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u73b0\u6709\u8bfe\u7a0b\u5b66\u4e60\u548c\u76ee\u6807\u9009\u62e9\u65b9\u6cd5\u4e0a\u5177\u6709\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u65b9\u5dee\u9a71\u52a8\u8bfe\u7a0b\u7684Student-Teacher\u6846\u67b6\u5177\u6709\u7b97\u6cd5\u65e0\u5173\u6027\u548c\u666e\u9002\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u591a\u76ee\u6807/\u76ee\u6807\u6761\u4ef6RL\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709RL\u6846\u67b6\u65e0\u7f1d\u5bf9\u63a5\u3002"}}
{"id": "2512.22897", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.22897", "abs": "https://arxiv.org/abs/2512.22897", "authors": ["S. Dai", "G. Sun", "F. Li", "X. Tang", "Q. Wang", "Y. Cong"], "title": "Federated Multi-Task Clustering", "comment": null, "summary": "Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.", "AI": {"tldr": "\u63d0\u51faFMTC\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e2a\u6027\u5316\u8054\u90a6\u591a\u4efb\u52a1\u805a\u7c7b\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u6620\u5c04\u5b9e\u73b0\u9c81\u68d2\u51fa\u6837\u672c\u63a8\u65ad\u3001\u670d\u52a1\u7aef\u5f20\u91cf\u4f4e\u79e9\u6b63\u5219\u5316\u6355\u83b7\u8de8\u5ba2\u6237\u7aef\u7ed3\u6784\uff0c\u91c7\u7528ADMM\u5206\u5e03\u5f0f\u4f18\u5316\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u96c6\u4e2d\u5f0f\u805a\u7c7b\u5728\u9690\u79c1\u4e0e\u8de8\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u73af\u5883\u4e2d\u7684\u5c40\u9650\uff0c\u514b\u670d\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u6027\u4e0e\u8de8\u5ba2\u6237\u7aef\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4e24\u5927\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5ba2\u6237\u7aef\u4e2a\u6027\u5316\u805a\u7c7b\u6a21\u5757\uff0c\u5b66\u4e60\u53c2\u6570\u5316\u6620\u5c04\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u51fa\u6837\u672c\u63a8\u65ad\uff0c\u907f\u514d\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff1b2) \u670d\u52a1\u7aef\u5f20\u91cf\u76f8\u5173\u6027\u6a21\u5757\uff0c\u5c06\u6240\u6709\u5ba2\u6237\u7aef\u6a21\u578b\u7ec4\u7ec7\u6210\u8054\u5408\u5f20\u91cf\u5e76\u901a\u8fc7\u4f4e\u79e9\u6b63\u5219\u5316\u5b66\u4e60\u5171\u4eab\u5b50\u7a7a\u95f4\u3002\u5168\u5c40\u95ee\u9898\u901a\u8fc7\u57fa\u4e8e\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u6c42\u89e3\uff0c\u5c06\u66f4\u65b0\u5206\u89e3\u4e3a\u672c\u5730\u5e76\u884c\u66f4\u65b0\u4e0e\u670d\u52a1\u7aef\u805a\u5408\u3002", "result": "\u5728\u591a\u7ec4\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cFMTC\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u548c\u73b0\u6709\u7684\u8054\u90a6\u805a\u7c7b\u7b97\u6cd5\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FMTC\u6709\u6548\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e2a\u4eba\u5316\u4e0e\u5171\u4eab\u77e5\u8bc6\u7684\u534f\u540c\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u591a\u4efb\u52a1\u805a\u7c7b\u7684\u6cdb\u5316\u4e0e\u6027\u80fd\u3002"}}
{"id": "2512.22903", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22903", "abs": "https://arxiv.org/abs/2512.22903", "authors": ["Chumeng Liang", "Zhanyang Jin", "Zahaib Akhtar", "Mona Pereira", "Haofei Yu", "Jiaxuan You"], "title": "Debugging Tabular Log as Dynamic Graphs", "comment": null, "summary": "Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.", "AI": {"tldr": "GraphLogDebugger \u5229\u7528\u5bf9\u8c61\u548c\u4e8b\u4ef6\u7684\u5f02\u8d28\u8282\u70b9\u3001\u52a8\u6001\u56fe\u7ed3\u6784\u53ca\u7b80\u5355\u7684\u52a8\u6001\u56feGNN\uff0c\u5bf9\u8868\u683c\u65e5\u5fd7\u8fdb\u884c\u8c03\u8bd5\uff0c\u80fd\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u6587\u672c\u589e\u5f3a\u7684\u8868\u683c\u65e5\u5fd7\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b58\u5728\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u65e5\u5fd7\u8c03\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u5f3a\u4f9d\u8d56\u6602\u8d35\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u7cfb\u7edf\u7ed3\u6784\u5e76\u53d1\u73b0\u4e0d\u4e00\u81f4\u3002", "method": "\u5c06\u5bf9\u8c61\u4e0e\u4e8b\u4ef6\u5efa\u6a21\u4e3a\u5f02\u8d28\u8282\u70b9\uff0c\u8282\u70b9\u95f4\u901a\u8fc7\u8fb9\u8fde\u63a5\uff0c\u5f62\u6210\u52a8\u6001\u56fe\u6765\u8868\u793a\u7cfb\u7edf\u968f\u65f6\u95f4\u6f14\u5316\uff1b\u91c7\u7528\u7b80\u5355\u7684\u52a8\u6001\u56feGNN\u5bf9\u56fe\u8fdb\u884c\u63a8\u7406\u548c\u8c03\u8bd5\uff1b\u5728\u771f\u5b9e\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u548c\u5b66\u672f\u8bba\u6587\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u76f8\u8f83\u4e8eLLMs\uff0c\u52a8\u6001\u56feGNN\u5728\u8c03\u8bd5\u8868\u683c\u65e5\u5fd7\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5177\u6709\u66f4\u597d\u7684\u541e\u5410\u4e0e\u7075\u6d3b\u6027\u3002", "conclusion": "\u52a8\u6001\u56fe\u5efa\u6a21\u4e3a\u8868\u683c\u65e5\u5fd7\u8c03\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0cGraphLogDebugger \u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.22904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22904", "abs": "https://arxiv.org/abs/2512.22904", "authors": ["Jin Wu", "Chanjin Zheng"], "title": "MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning", "comment": null, "summary": "Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22910", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22910", "abs": "https://arxiv.org/abs/2512.22910", "authors": ["\u00dcnver \u00c7ift\u00e7i"], "title": "Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning", "comment": null, "summary": "Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.22944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22944", "abs": "https://arxiv.org/abs/2512.22944", "authors": ["Vincent Herrmann", "Eric Alcaide", "Michael Wand", "J\u00fcrgen Schmidhuber"], "title": "Multiple Token Divergence: Measuring and Steering In-Context Computation Density", "comment": null, "summary": "Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.", "AI": {"tldr": "\u63d0\u51fa Multiple Token Divergence (MTD) \u4f5c\u4e3a\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u52aa\u529b\u7684\u7b80\u5355KL\u53d1\u6563\u5ea6\u6307\u6807\uff0c\u53ca\u5176\u884d\u751f\u89e3\u7801\u6cd5 Divergence Steering\uff0c\u53ef\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u5934\u9884\u6d4b\u4e2d\u5f97\u5230\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5e76\u5728\u4efb\u52a1\u96be\u5ea6\u4e0e\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u5efa\u7acb\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u6307\u6807\uff08\u5982\u4e0b\u4e00\u4e2a token \u635f\u5931\uff09\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u590d\u6742\u5ea6\uff1b\u57fa\u4e8e\u9690\u72b6\u6001\u53ef\u538b\u7f29\u6027\u7684\u8bc4\u4f30\u4e0d\u7a33\u5b9a/\u5177\u4fb5\u5165\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u3001\u7a33\u5b9a\u7684\u8861\u91cf\u548c\u63a7\u5236\u8ba1\u7b97\u52a8\u6001\u7684\u65b9\u6cd5\u3002", "method": "MTD = KL(\u6a21\u578b\u7684\u5b8c\u6574\u8f93\u51fa\u5206\u5e03 vs \u8f85\u52a9\u6d45\u5934\u9884\u6d4b\u7684\u8f93\u51fa\u5206\u5e03)\uff0c\u53ef\u76f4\u63a5\u4ece\u5e26\u6709\u591a\u9884\u6d4b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa Divergence Steering \u4ee5\u63a7\u5236\u751f\u6210\u6587\u672c\u7684\u8ba1\u7b97\u7279\u5f81\uff0c\u4e14\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u8bc1\u663e\u793a\uff0cMTD \u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u66f4\u80fd\u533a\u5206\u590d\u6742\u4efb\u52a1\u4e0e\u7b80\u5355\u4efb\u52a1\uff1b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cMTD \u4e0e\u95ee\u9898\u96be\u5ea6\u6b63\u76f8\u5173\uff1b\u8f83\u4f4e\u7684 MTD \u4e0e\u66f4\u9ad8\u7684\u63a8\u7406\u51c6\u786e\u6027\u76f8\u5173\uff1bMTD \u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u3001\u8f7b\u91cf\u7ea7\u7684\u5206\u6790\u4e0e\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u52a8\u6001\u7684\u5de5\u5177\u3002", "conclusion": "MTD \u4e3a\u5206\u6790\u4e0e\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u52a8\u6001\u7684\u5b9e\u7528\u5de5\u5177\uff0cDivergence Steering \u63d0\u4f9b\u4e00\u79cd\u53ef\u63a7\u7684\u89e3\u7801\u7b56\u7565\u6765\u8c03\u8282\u751f\u6210\u8fc7\u7a0b\u7684\u8ba1\u7b97\u7279\u5f81\u3002"}}
{"id": "2512.22953", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22953", "abs": "https://arxiv.org/abs/2512.22953", "authors": ["Wang Zixian"], "title": "APO: Alpha-Divergence Preference Optimization", "comment": null, "summary": "Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.", "AI": {"tldr": "\u63d0\u51fa Alpha-Divergence Preference Optimization (APO)\uff0c\u5728\u951a\u5b9a\u5750\u6807\u4e2d\u4f7f\u7528 Csiszar alpha-\u53d1\u6563\u7684\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u8fde\u7eed\u63d2\u503c\u524d\u5411\u4e0e\u53cd\u5411 KL\uff0c\u4ee5\u5b9e\u73b0\u6a21\u5f0f\u8986\u76d6\u4e0e\u6a21\u5f0f\u641c\u7d22\u7684\u6298\u4e2d\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94 alpha \u8c03\u5ea6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u5177\u7ade\u4e89\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u524d\u5411 KL\uff08q||pi) \u5bfc\u81f4\u7684\u6a21\u5f0f\u8986\u76d6\u4e0d\u8db3\u4e0e\u53cd\u5411 KL\uff08pi||q) \u9020\u6210\u7684\u6a21\u5f0f\u5d29\u6e83\u98ce\u9669\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u73b0\u6709\u951a\u5b9a\u65b9\u6cd5\uff08\u5982 ADPO\uff09\u5c3d\u7ba1\u7a33\u5b9a\uff0c\u4f46\u901a\u5e38\u7ed1\u5b9a\u5355\u4e00\u53d1\u6563\u3002\u9700\u8981\u5728\u540c\u4e00\u951a\u5b9a\u51e0\u4f55\u4e2d\u5b9e\u73b0\u5bf9\u591a\u79cd\u53d1\u6563\u7684\u8fde\u7eed\u63d2\u503c\u4ee5\u5b9e\u73b0\u8986\u76d6-\u5f00\u53d1\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa APO \u6846\u67b6\uff0c\u5728\u951a\u5b9a\u5750\u6807\u4e2d\u5f15\u5165 Csiszar alpha-\u53d1\u6563\uff0c\u5f97\u5230\u4e00\u4e2a\u4ee5 alpha \u4e3a\u53c2\u6570\u7684\u7edf\u4e00\u68af\u5ea6\u52a8\u529b\u5b66\uff1b\u5206\u6790\u68af\u5ea6\u65b9\u5dee\uff1b\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u4e0e\u7f6e\u4fe1\u5ea6\u7684 alpha \u8c03\u5ea6\u7b56\u7565\uff0c\u4f7f\u5f97\u5728\u7b56\u7565\u6539\u8fdb\u4e14\u7f6e\u4fe1\u5ea6\u63d0\u5347\u65f6\u4ece\u8986\u76d6\u8f6c\u5411\u5f00\u53d1\u3002", "result": "\u5728 Qwen3-1.7B\u3001math-level3 \u4efb\u52a1\u4e0a\uff0cAPO \u4e0e GRPO/GSPO \u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u5e76\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "APO \u63d0\u4f9b\u4e00\u4e2a\u7075\u6d3b\u4e14\u7a33\u5b9a\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u951a\u5b9a\u5750\u6807\u548c alpha \u63d2\u503c\u5b9e\u73b0\u5bf9\u8986\u76d6\u548c\u5f00\u53d1\u7684\u5e73\u8861\uff0c\u63d0\u5347\u5bf9\u9f50\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u4e0e\u9002\u7528\u6027\uff0c\u5177\u5907\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u5bf9\u9f50\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.22956", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22956", "abs": "https://arxiv.org/abs/2512.22956", "authors": ["Wafaa El Husseini"], "title": "FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing", "comment": "This paper introduces FLOW, a synthetic longitudinal dataset for modeling daily work-life balance and wellbeing. The dataset is publicly available on Kaggle under a CC BY-SA 4.0 license", "summary": "Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.\n  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.\n  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.", "AI": {"tldr": "FLOW is a synthetic longitudinal dataset and generation tool for modeling daily interactions among workload, lifestyle behaviors, and wellbeing, enabling reproducible experiments when real data are unavailable.", "motivation": "Access to real longitudinal, individual-level data is limited by privacy, ethical, and logistical constraints, which hampers reproducible research, methodological benchmarking, and education in domains like stress modeling, behavioral analysis, and machine learning.", "method": "A rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1,000 individuals over two years with daily resolution and is released publicly. A configurable data generation tool accompanies the dataset to enable reproducible experimentation under adjustable behavioral and contextual assumptions.", "result": "Public release of FLOW and its generator; demonstrates coherent temporal dynamics across modeled variables; supports exploratory analysis, methodological development, and benchmarking in the absence of real-world data; not a direct proxy for observed human populations.", "conclusion": "FLOW provides a controlled experimental environment for benchmarking, education, and methodological development when real-world data are inaccessible, serving as a testbed rather than a surrogate population."}}
{"id": "2512.22976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22976", "abs": "https://arxiv.org/abs/2512.22976", "authors": ["Amirali Vakili", "Salar Jahanshiri", "Armin Salimi-Badr"], "title": "A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging", "comment": null, "summary": "Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.", "AI": {"tldr": "\u9762\u5411\u5355\u901a\u9053\u8111\u7535\u4fe1\u53f7\u7684\u60c5\u5883\u611f\u77e5\u3001\u53ef\u89e3\u91ca\u7761\u7720\u5206\u671f\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u65f6\u5e8f\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347N1\u9636\u6bb5\u68c0\u6d4b\u3002SleepEDF\u6570\u636e\u96c6\u4e0a\u603b\u4f53\u51c6\u786e\u5ea689.72%\u3001 macro F1 85.46%\uff0cN1 F1\u8fbe61.7%\uff0c\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u5168\u7403\u7761\u7720\u969c\u788d\u7684\u9ad8\u53d1\u4e0e\u5bf9\u4fbf\u6377\u7761\u7720\u5206\u671f\u7684\u9700\u6c42\u4fc3\u4f7f\u7814\u7a76\u805a\u7126\u4e8e\u5355\u901a\u9053EEG\u7684\u81ea\u52a8\u7761\u7720\u5206\u671f\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u53d7\u9650\u611f\u53d7\u91ce\u5efa\u6a21\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u517c\u5177\u53ef\u89e3\u91ca\u6027\u548c\u826f\u597d\u9c81\u68d2\u6027\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u5355\u901a\u9053EEG\u7761\u7720\u5206\u671f\u6846\u67b6\u3002\u6838\u5fc3\u5728\u4e8e\u5c06\u7d27\u51d1\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u65f6\u5e8f\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u4ee5\u6355\u6349\u5c40\u90e8\u4e0e\u957f\u671f\u4f9d\u8d56\uff1b\u901a\u8fc7\u7c7b\u522b\u6743\u91cd\u635f\u5931\u4e0e\u6570\u636e\u589e\u5f3a\u7f13\u89e3N1\u7b49\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u5c06EEG\u4fe1\u53f7\u5206\u5272\u4e3a\u5b50\u65f6\u6bb5\u5757\uff0c\u5e76\u901a\u8fc7\u5bf9\u5757\u7684softmax\u6982\u7387\u53d6\u5e73\u5747\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\uff0c\u4ece\u800c\u63d0\u5347\u4e0a\u4e0b\u6587\u8868\u5f81\u4e0e\u9c81\u68d2\u6027\u3002\u5f3a\u8c03\u53ef\u89e3\u91ca\u6027\u7684\u7279\u5f81\u63d0\u53d6\u89d2\u8272\uff0c\u5e76\u517c\u987e\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u3002", "result": "\u5728SleepEDF\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u603b\u4f53\u51c6\u786e\u5ea689.72%\u3001\u5b8f\u5e73\u5747F1-score 85.46%\uff0c\u5176\u4e2dN1\u9636\u6bb5\u7684F1-score\u8fbe\u523061.7%\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002\u4e0a\u8ff0\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u7761\u7720\u5206\u671f\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u5355\u901a\u9053EEG\u7761\u7720\u5206\u671f\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0e\u65f6\u5e8f\u5efa\u6a21\u3001\u6570\u636e\u4e0d\u5e73\u8861\u5904\u7406\u53ca\u5206\u5757\u5e73\u5747\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u5907\u5728\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u573a\u666f\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.22991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22991", "abs": "https://arxiv.org/abs/2512.22991", "authors": ["Tillmann Rheude", "Roland Eils", "Benjamin Wild"], "title": "Fusion or Confusion? Multimodal Complexity Is Not All You Need", "comment": null, "summary": "Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.", "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0c\u590d\u6742\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u672a\u80fd\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\u3002\u63d0\u51fa SimBaMM\uff08\u665a\u878d\u5408 Transformer\uff09\u5728\u6807\u51c6\u5316\u4e14\u5145\u5206\u8c03\u53c2\u7684\u6761\u4ef6\u4e0b\u53ef\u4e0e\u6216\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\uff0c\u5f3a\u8c03\u65b9\u6cd5\u5b66\u7684\u4e25\u8c28\u6027\u4e0e\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u7ed9\u51fa\u53ef\u9760\u6027\u6e05\u5355\u4e0e\u5bf9\u6587\u732e\u7684\u65b9\u6cd5\u8bba\u53cd\u601d\u3002", "motivation": "\u591a\u6a21\u6001\u7814\u7a76\u5e38\u5047\u8bbe\u4e13\u95e8\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u5728\u516c\u5e73\u3001\u53ef\u91cd\u590d\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u5bf9\u6bd4\u3002\u9700\u8981\u901a\u8fc7\u6807\u51c6\u5316\u5b9e\u9a8c\u6761\u4ef6\u3001\u5145\u5206\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u6765\u68c0\u9a8c\u771f\u5b9e\u6536\u76ca\uff0c\u5e76\u5173\u6ce8\u65b0\u4efb\u52a1\u4e0e\u7f3a\u5931\u6a21\u6001\u7684\u6cdb\u5316\u6027\u3002", "method": "\u5bf919\u79cd\u9ad8\u5f71\u54cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u91cd\u65b0\u5b9e\u73b0\uff0c\u8986\u76d69\u4e2a\u591a\u6837\u6570\u636e\u96c6\uff0c\u6700\u591a\u652f\u630123\u79cd\u6a21\u6001\uff1b\u5728\u7edf\u4e00\u3001\u53d7\u63a7\u7684\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u8fdb\u884c\u6bd4\u8f83\u5e76\u5bf9\u6240\u6709\u65b9\u6cd5\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\uff1b\u8bc4\u4f30\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u7f3a\u5931\u6a21\u6001\u60c5\u666f\uff1b\u63d0\u51fa\u5e76\u9a8c\u8bc1\u7b80\u5355\u57fa\u7ebf SimBaMM\uff1b\u5e76\u9644\u5e26\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\u4e0e\u53ef\u91cd\u590d\u6027\u6e05\u5355\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0c\u590d\u6742\u65b9\u6cd5\u4e0e SimBaMM \u7684\u6027\u80fd\u5dee\u5f02\u901a\u5e38\u4e0d\u663e\u8457\uff0c\u4e14\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u5e76\u4e0d\u4f18\u4e8e\u5145\u5206\u8c03\u53c2\u540e\u7684\u5355\u6a21\u6001\u57fa\u7ebf\uff1b\u5c24\u5176\u5728\u5c0f\u6570\u636e\u60c5\u5f62\u4e0b\uff0c\u5dee\u5f02\u66f4\u5c0f\u751a\u81f3\u4e0d\u663e\u8457\u3002\u8fd8\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u6587\u732e\u4e2d\u7684\u65b9\u6cd5\u5b66\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u53ef\u63d0\u5347\u672a\u6765\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u6e05\u5355\u3002", "conclusion": "\u547c\u5401\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u67b6\u6784\u521b\u65b0\u8f6c\u5411\u65b9\u6cd5\u5b66\u4e25\u8c28\u3001\u53ef\u91cd\u590d\u6027\u4e0e\u7ed3\u679c\u53ef\u6bd4\u6027\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u5b66\u4e60\u9886\u57df\u7684\u53ef\u4fe1\u8bc4\u4f30\u4e0e\u5e94\u7528\u3002"}}
{"id": "2512.23017", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23017", "abs": "https://arxiv.org/abs/2512.23017", "authors": ["Fuli Qiao", "Mehrdad Mahdavi"], "title": "Merge before Forget: A Single LoRA Continual Learning via Continual Merging", "comment": null, "summary": "Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LoRA\u66f4\u65b0\u8fdb\u884c\u6b63\u4ea4\u521d\u59cb\u5316\u5e76\u987a\u5e8f\u5408\u5e76\u6210\u4e00\u4e2a\u7edf\u4e00\u7684LoRA\uff0c\u914d\u5408\u65f6\u95f4\u611f\u77e5\u5c3a\u5ea6\u5bf9\u65b0\u65e7\u77e5\u8bc6\u8fdb\u884c\u5e73\u8861\uff0c\u5b9e\u73b0\u4e0e\u4efb\u52a1\u6570\u91cf\u76f8\u5173\u7684\u5e38\u91cf\u5185\u5b58\u590d\u6742\u5ea6\u548c\u8f83\u4f4e\u5e72\u6270\uff0c\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8eLoRA\u7684\u6301\u7eed\u5b66\u4e60\u5728\u4efb\u52a1\u589e\u52a0\u65f6\u7684\u5185\u5b58\u589e\u957f\u548c\u4efb\u52a1\u95f4\u5e72\u6270\u95ee\u9898\uff0c\u671f\u671b\u901a\u8fc7\u5c06\u591a\u4e2aLoRA\u66f4\u65b0\u5408\u5e76\u4e3a\u4e00\u4e2a\u7edf\u4e00\u8868\u793a\u5e76\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "method": "\u4ece\u5df2\u5b66\u4e60\u7684LoRA\u4e2d\u63d0\u53d6\u6b63\u4ea4\u57fa\uff0c\u5229\u7528\u5176\u521d\u59cb\u5316\u65b0\u4efb\u52a1\u7684LoRA\u6743\u91cd\uff1b\u5c06\u65b0\u4efb\u52a1\u7684LoRA\u66f4\u65b0\u4e32\u884c\u5408\u5e76\u5230\u4e00\u4e2a\u7edf\u4e00\u7684LoRA\u4e2d\uff1b\u5f15\u5165\u65f6\u95f4\u611f\u77e5\u7684\u7f29\u653e\u673a\u5236\u4ee5\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\uff0c\u5229\u7528LoRA\u7ec4\u4ef6\u7684\u975e\u5bf9\u79f0\u6027\uff1b\u7ed9\u51fa\u7406\u8bba\u5206\u6790\u652f\u6491\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u53ca\u591a\u79cdLlama\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e0e\u4efb\u52a1\u6570\u91cf\u76f8\u5173\u7684\u5e38\u91cf\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u51cf\u5c11\u8fc7\u53bb\u4efb\u52a1\u4e0e\u65b0\u4efb\u52a1\u4e4b\u95f4\u7684\u5e72\u6270\uff1b\u5728\u76f8\u5bf9\u4e8e\u5bf9\u79f0/\u975e\u5bf9\u79f0\u5408\u5e76\u7684\u81ea\u9002\u5e94\u7f29\u653e\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1b\u5728\u591a\u9879\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u548cLlama\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u9ad8\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6b63\u4ea4\u521d\u59cb\u5316\u4e0e\u8fde\u7eed\u5408\u5e76\u7684LoRA\u65b9\u6cd5\u4e3aLoRA\u9a71\u52a8\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u6709\u636e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8f83\u4f4e\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u826f\u597d\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.23056", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.23056", "abs": "https://arxiv.org/abs/2512.23056", "authors": ["Min Zhu", "Jingmin Sun", "Zecheng Zhang", "Hayden Schaeffer", "Lu Lu"], "title": "PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations", "comment": null, "summary": "Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7269\u7406\u4fe1\u606f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b PI-MFM\uff0c\u7528\u7b26\u53f7\u5316 PDE \u8868\u8fbe\u5f0f\u81ea\u52a8\u7ec4\u88c5\u6b8b\u5dee\u635f\u5931\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u6c42\u5bfc\u5b9e\u73b0\u7edf\u4e00\u7684\u7269\u7406\u7ea6\u675f\u8bad\u7ec3\uff0c\u80fd\u5728\u5c11\u6807\u7b7e\u3001\u8de8\u65b9\u7a0b\u5bb6\u65cf\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684 PDE \u6c42\u89e3\uff0c\u5e76\u5177\u5907\u5bf9\u65b0 PDE \u5bb6\u65cf\u7684\u96f6-shot \u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u9ad8\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u5ffd\u7565\u7269\u7406\u7ea6\u675f\uff1b\u9700\u8981\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u53ef\u8fc1\u79fb\u7684 PDE \u6c42\u89e3\u6846\u67b6\uff0c\u80fd\u591f\u8de8\u65b9\u7a0b\u5bb6\u65cf\u76f4\u63a5\u5229\u7528\u7269\u7406\u89c4\u5f8b\u8fdb\u884c\u524d\u8bad\u7ec3\u4e0e\u5fae\u8c03\u3002", "method": "PI-MFM \u5c06 PDE \u7684\u7b26\u53f7\u5316\u8868\u793a\u4f5c\u4e3a\u8f93\u5165\uff0c\u81ea\u52a8\u4ece\u8868\u8fbe\u5f0f\u4e2d\u7ec4\u88c5 PDE \u6b8b\u5dee\u635f\u5931\uff0c\u5229\u7528\u5411\u91cf\u5316\u6c42\u5bfc\u8fdb\u884c\u5fae\u5206\u8ba1\u7b97\uff0c\u5f62\u6210\u8de8\u65b9\u7a0b\u5bb6\u65cf\u7684\u7edf\u4e00\u7269\u7406\u7ea6\u675f\u76ee\u6807\uff1b\u53ef\u7528\u4e8e\u4efb\u610f PDE \u7f16\u7801\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4fbf\u4e8e\u5728\u9884\u8bad\u7ec3\u6216\u9002\u914d\u9636\u6bb5\u5f15\u5165\u7269\u7406\u635f\u5931\u3002", "result": "\u5728 13 \u79cd\u53c2\u6570\u5316\u7684\u4e00\u7ef4\u65f6\u53d8 PDE \u5bb6\u65cf\u57fa\u51c6\u4e0a\uff0cPI-MFM \u76f8\u8f83\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u5728\u5c11\u91cf\u6807\u6ce8\u7684\u65f6\u7a7a\u70b9\u3001\u90e8\u5206\u89c2\u6d4b\u65f6\u57df\u6216\u5c11\u6807\u7b7e\u51fd\u6570\u5bf9\u7684\u60c5\u51b5\u4e0b\u66f4\u660e\u663e\uff1b\u7269\u7406\u635f\u5931\u63d0\u5347\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u91cd\u91c7\u6837 collocation \u70b9\u7b49\u7b56\u7565\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff1b\u5bf9\u81ea\u52a8\u5fae\u5206\u4e0e\u6709\u9650\u5dee\u5206\u5728\u5bfc\u6570\u8ba1\u7b97\u4e2d\u7684\u51c6\u786e\u6027\u3001\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u8fdb\u884c\u4e86\u5206\u6790\u3002\uff0c\u8fd8\u5c55\u793a\u4e86\u5bf9\u672a\u77e5 PDE \u5bb6\u65cf\u7684\u96f6-shot \u7269\u7406\u4fe1\u606f\u5fae\u8c03\uff1a\u5728\u4ec5\u7528 PDE \u6b8b\u5dee\u4e0e\u521d\u59cb/\u8fb9\u754c\u6761\u4ef6\u8fdb\u884c\u9002\u914d\u3001\u65e0\u6807\u7b7e\u89e3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u6d4b\u8bd5\u8bef\u5dee\u5feb\u901f\u964d\u81f3\u7ea6 1%\uff0c\u663e\u8457\u4f18\u4e8e\u4ece\u96f6\u5f00\u59cb\u7684\u7eaf\u7269\u7406\u8bad\u7ec3\u3002", "conclusion": "PI-MFM \u4e3a\u6570\u636e\u9ad8\u6548\u3001\u53ef\u8fc1\u79fb\u7684 PDE \u6c42\u89e3\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u652f\u6301\u8de8\u65b9\u7a0b\u5bb6\u65cf\u7684\u7edf\u4e00\u7269\u7406\u7ea6\u675f\u5b66\u4e60\u4e0e\u5fae\u8c03\uff0c\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u4e0e\u9002\u7528\u6027\u3002"}}
{"id": "2512.23070", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23070", "abs": "https://arxiv.org/abs/2512.23070", "authors": ["Boyang Zhang", "Xiaobing Chen", "Songyang Zhang", "Shuai Zhang", "Xiangwei Zhou", "Mingxuan Sun"], "title": "FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \\textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.", "AI": {"tldr": "FLEX-MoE\u901a\u8fc7\u5728\u8054\u90a6MoE\u4e2d\u8054\u5408\u4f18\u5316\u4e13\u5bb6\u5206\u914d\u4e0e\u8d1f\u8f7d\u5747\u8861\u6765\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0e\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u95ee\u9898\uff0c\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u53cd\u9988\u7684\u5ba2\u6237\u7aef-\u4e13\u5bb6\u9002\u914d\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u5728\u6700\u5927\u5316\u5ba2\u6237\u7aef-\u4e13\u5bb6\u4e13\u95e8\u5316\u7684\u540c\u65f6\u5b9e\u73b0\u7cfb\u7edf\u7ea7\u7684\u4e13\u5bb6\u5229\u7528\u5e73\u8861\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u3001\u5355\u4e2a\u5ba2\u6237\u7aef\u65e0\u6cd5\u5b58\u50a8\u5168\u90e8\u4e13\u5bb6\uff0c\u4ee5\u53ca\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5e26\u6765\u4e13\u5bb6\u8d1f\u8f7d\u4e25\u91cd\u4e0d\u5747\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u4e2a\u6027\u5316\u548c\u8d1f\u8f7d\u5747\u8861\u6765\u63d0\u5347\u8054\u90a6MoE\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef-\u4e13\u5bb6\u9002\u914d\u5206\u6570\u4ee5\u91cf\u5316\u672c\u5730\u6570\u636e\u96c6\u5bf9\u5404\u4e13\u5bb6\u7684\u9002\u7528\u6027\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u4f18\u5316\u7b97\u6cd5\u5728\u6700\u5927\u5316\u5ba2\u6237\u7aef-\u4e13\u5bb6\u4e13\u95e8\u5316\u7684\u540c\u65f6\u5f3a\u5236\u5b9e\u73b0\u7cfb\u7edf\u8303\u56f4\u5185\u7684\u4e13\u5bb6\u5229\u7528\u5e73\u8861\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4e13\u5bb6\u96c6\u5408\u7684\u9ad8\u6548\u5206\u914d\u4e0e\u8d1f\u8f7d\u5747\u8861.", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0cFLEX-MoE\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7ef4\u6301\u5e73\u8861\u7684\u4e13\u5bb6\u5229\u7528\u5206\u5e03\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u53ea\u5173\u6ce8\u4e2a\u6027\u5316\u7684\u8d2a\u5a6a\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "FLEX-MoE\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e2a\u6027\u5316\u4e0e\u8d1f\u8f7d\u5e73\u8861\u7684\u53cc\u91cd\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u66f4\u5747\u8861\u4e14\u6027\u80fd\u66f4\u4f18\u7684MoE\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2512.23087", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23087", "abs": "https://arxiv.org/abs/2512.23087", "authors": ["Yingru Li", "Jiawei Xu", "Jiacai Liu", "Yuxuan Tong", "Ziniu Li", "Tianle Cai", "Ge Zhang", "Qian Liu", "Baoxiang Wang"], "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning", "comment": null, "summary": "Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u573a\u666f\u4e2d\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5206\u5e03\u4e0d\u4e00\u81f4\u5bfc\u81f4\u8bad\u7ec3-\u63a8\u7406\u9519\u914d\u3002\u9519\u914d\u5bf9\u6982\u7387\u8f83\u4f4e\u7684\u5c3e\u90e8\u6807\u8bb0\u5f71\u54cd\u663e\u8457\uff0c\u4e14\u4f1a\u5728\u5e8f\u5217\u4e0a\u79ef\u7d2f\uff0c\u7834\u574f\u68af\u5ea6\u4f30\u8ba1\u3002\u63d0\u51fa\u901a\u8fc7\u52a8\u6001\u88c1\u526a\u7684\u201c\u5b89\u5168\u201d\u8bcd\u6c47\u8868\u6765\u7ea6\u675f RL \u76ee\u6807\uff0c\u6392\u9664\u6781\u7aef\u5c3e\u90e8\u8bcd\u4ee5\u964d\u4f4e\u504f\u5dee\uff0c\u540c\u65f6\u5e26\u6765\u53ef\u63a7\u7684\u4f18\u5316\u504f\u5dee\u3002\u5b9e\u8bc1\u4e0a\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u7ed9\u51fa\u88c1\u526a\u5f15\u5165\u7684\u504f\u5dee\u754c\u9650\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u9636\u6bb5\u7684\u4f18\u5316\u76ee\u6807\u4e0e\u9ad8\u541e\u5410\u63a8\u7406\u5f15\u64ce\u4e4b\u95f4\u7684\u5206\u5e03\u9519\u914d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u4f4e\u6982\u7387\u5c3e\u90e8\u6807\u8bb0\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4ee5\u53ca\u8fd9\u7c7b\u504f\u5dee\u5728\u5e8f\u5217\u7ea7\u68af\u5ea6\u4f30\u8ba1\u4e2d\u7684\u7d2f\u79ef\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u88c1\u526a\u7684\u201c\u5b89\u5168\u201d\u8bcd\u6c47\u8868\uff0c\u6392\u9664\u6781\u7aef\u5c3e\u90e8\u7684\u4f4e\u6982\u7387\u6807\u8bb0\uff0c\u4f5c\u4e3a RL \u76ee\u6807\u7684\u7ea6\u675f\u3002\u901a\u8fc7\u88c1\u526a\u6765\u5728\u8f83\u5927\u3001\u7cfb\u7edf\u6027\u504f\u5dee\u4e0e\u8f83\u5c0f\u4f46\u6709\u754c\u7684\u4f18\u5316\u504f\u5dee\u4e4b\u95f4\u6743\u8861\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u63d0\u5347\uff0c\u5e76\u5bf9\u8bcd\u6c47\u88c1\u526a\u5f15\u5165\u7684\u4f18\u5316\u504f\u5dee\u7ed9\u51fa\u7406\u8bba\u754c\u9650\u3002", "conclusion": "\u88c1\u526a\u5c3e\u90e8\u8bcd\u6c47\u4ee5\u521b\u5efa\u5b89\u5168\u8bcd\u6c47\u8868\uff0c\u80fd\u591f\u5728\u964d\u4f4e\u7cfb\u7edf\u6027\u9519\u914d\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u7ed9\u51fa\u53ef\u63a7\u7684\u4f18\u5316\u504f\u5dee\u754c\u9650\uff0c\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u7684 RL-LLM \u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2512.23097", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23097", "abs": "https://arxiv.org/abs/2512.23097", "authors": ["Yingru Li", "Ziniu Li", "Jiacai Liu"], "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms", "comment": null, "summary": "We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u5fae\u8c03\u6846\u67b6\uff0c\u5c06\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u901a\u8fc7\u5bf9\u5305\u542b\u8f68\u8ff9\u7ea7KL\u6563\u5ea6\u548c\u4efb\u52a1\u5956\u52b1\u7684\u590d\u5408\u76ee\u6807\u7684\u68af\u5ea6\u5206\u6790\uff0c\u5f97\u5230\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u4e00\u662f\u53ef\u89e3\u6790\u7684\u5bc6\u96c6\u68af\u5ea6\uff08Dense Gradient\uff09\u7528\u4e8e\u8bcd\u7ea7\u6a21\u4eff\uff0c\u4e8c\u662f\u7528\u4e8e\u957f\u5e8f\u5217\u5956\u52b1\u4f18\u5316\u7684\u7a00\u758f\u68af\u5ea6\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\uff1b\u5bc6\u96c6\u68af\u5ea6\u5728\u5bf9\u6570\u51e0\u7387\u7b49\u7ea7\uff08logit-level\uff09\u4e0a\u6709\u95ed\u5f0f\u516c\u5f0f\uff0c\u4fbf\u4e8e\u9ad8\u6548\u7684GPU\u5b9e\u73b0\u3002", "motivation": "\u5728LLM\u5fae\u8c03\u4e2d\u540c\u65f6\u517c\u987e\u6a21\u4eff\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u5bfc\u5411\u6027\uff0c\u5bfb\u6c42\u4e00\u79cd\u80fd\u5728\u8ba1\u7b97\u6548\u7387\u548c\u68af\u5ea6\u65b9\u5dee\u4e4b\u95f4\u83b7\u5f97\u5e73\u8861\u7684\u6846\u67b6\uff1b\u901a\u8fc7\u5bf9\u7ec4\u5408\u76ee\u6807\u7684\u68af\u5ea6\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u7684\u4e24\u5c42\u68af\u5ea6\u5206\u89e3\uff0c\u4ee5\u4fbf\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\u9ad8\u6548\u5b9e\u73b0\u3002", "method": "\u5bf9\u5305\u542b\u8f68\u8ff9\u7ea7KL\u6563\u5ea6\u548c\u4efb\u52a1\u5956\u52b1\u7684\u590d\u5408\u76ee\u6807\u8fdb\u884c\u68af\u5ea6\u5206\u6790\uff0c\u63a8\u5bfc\u51fa\u81ea\u7136\u7684\u5206\u89e3\uff1a1) \u63d0\u4f9b\u53ef\u89e3\u6790\u7684\u5bc6\u96c6\u68af\u5ea6\u7528\u4e8e token \u7ea7\u6a21\u4eff\uff08Dense Gradient\uff09\uff0c2) \u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5f97\u5230\u5bf9\u957f\u65f6\u5e8f\u5956\u52b1\u4f18\u5316\u6709\u7528\u7684\u7a00\u758f\u68af\u5ea6\uff08Sparse Gradient\uff09\u3002\u5176\u4e2d\u5bc6\u96c6\u68af\u5ea6\u5728\u5bf9\u6570\u51e0\u7387\uff08logit\uff09\u5c42\u9762\u5177\u6709\u95ed\u5f0f\u516c\u5f0f\uff0c\u4f7f\u5f97\u5728GPU\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u6548\u3002", "result": "\u63d0\u51fa\u7684\u5206\u89e3\u4e3a\u5b9e\u9645\u5b9e\u73b0\u63d0\u4f9b\u4e86\u4e24\u7c7b\u68af\u5ea6\u4fe1\u53f7\uff1a\u5bc6\u96c6\u7684\u4ee4\u724c\u7ea7\u6a21\u4eff\u68af\u5ea6\u548c\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u957f\u65f6\u5e8f\u5956\u52b1\u68af\u5ea6\u3002\u5bc6\u96c6\u68af\u5ea6\u5177\u6709\u95ed\u5f0f\u89e3\uff0c\u4fbf\u4e8e\u9ad8\u6548\u8ba1\u7b97\uff1b\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7528\u4e8e\u5904\u7406\u957f\u5e8f\u5217\u7684\u5956\u52b1\u4f18\u5316\uff0c\u6574\u4f53\u6846\u67b6\u5b9e\u73b0\u4e0a\u5177\u6709\u8ba1\u7b97\u6548\u7387\u7684\u4f18\u52bf\uff0c\u4f46\u5c1a\u672a\u5728\u6458\u8981\u4e2d\u7ed9\u51fa\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06 imitation learning \u4e0e reinforcement learning \u878d\u5408\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u590d\u5408\u76ee\u6807\u7684\u68af\u5ea6\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u7684 GPU \u5b9e\u73b0\u8def\u5f84\uff0c\u7279\u522b\u662f\u5bc6\u96c6\u68af\u5ea6\u7684\u95ed\u5f0f\u8868\u8fbe\u53ef\u80fd\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u957f\u5e8f\u5217\u5956\u52b1\u7684\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2512.23109", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23109", "abs": "https://arxiv.org/abs/2512.23109", "authors": ["Paul M. Thompson"], "title": "How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure", "comment": "13 pages, 2 figures", "summary": "Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.\n  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.\n  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.", "AI": {"tldr": "\u672c\u6587\u7ed9\u51fa\u57fa\u4e8eVLM\u8bf1\u5bfc\u5206\u7c7b\u5668\u7684\u6709\u9650\u6837\u672c\u7edf\u4e00\u6536\u655b\u4e0e\u6821\u51c6\u4fdd\u8bc1\uff1b\u6837\u672c\u590d\u6742\u5ea6\u7531\u5185\u5728\u7ef4\u5ea6\u4e0e\u8c31\u8870\u51cf\u51b3\u5b9a\uff0c\u652f\u6301\u751f\u7269\u533b\u5b66\u4e2d\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u7edf\u4e00\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u548c\u89c6\u89c9-\u6587\u672c\u6a21\u578b\u5728\u79d1\u5b66\u4e0e\u533b\u7597\u51b3\u7b56\u652f\u6301\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u9700\u786e\u4fdd\u9884\u6d4b\u5728\u8f93\u5165\u3001\u7c7b\u522b\u548c\u5b50\u7fa4\u4f53\u4e0a\u5177\u5907\u7edf\u4e00\u7684\u51c6\u786e\u6027\u548c\u826f\u597d\u6821\u51c6\u6027\u3002\u5355\u7eaf\u7684\u5e73\u5747\u6027\u80fd\u53ef\u80fd\u63a9\u76d6\u5c0f\u6982\u7387\u5b50\u7fa4\u4f53\u7684\u9ad8\u8bef\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u5728\u6709\u9650\u6837\u672c\u4e0b\u83b7\u5f97\u7a33\u5b9a\u7684\u7edf\u4e00\u4fdd\u8bc1\u3002", "method": "\u8003\u5bdf\u901a\u8fc7\u5728\u53d7\u9650\u8868\u793a\u7a7a\u95f4\u5185\u53d8\u52a8\u63d0\u793a\u6216\u8bed\u4e49\u5d4c\u5165\u5f97\u5230\u7684\u4e00\u65cf\u5206\u7c7b\u5668\uff1b\u5047\u8bbe\u8f93\u51fa\u5bf9\u4f4e\u7ef4\u8bed\u4e49\u8868\u793a\u5177\u6709\u5e73\u6ed1/ Lipschitz \u4f9d\u8d56\u6027\uff1b\u5229\u7528\u7ecf\u5178\u7684\u7edf\u4e00\u6536\u655b\u5de5\u5177\u7ed9\u51fa\u975e\u6e10\u8fdb\u7684\u754c\uff0c\u7ed9\u51fa\u4f9d\u8d56\u8c31\u7279\u5f81\u7684\u754c\u4ee5\u63ed\u793a\u6570\u636e\u9700\u6c42\u53d7\u7279\u5f81\u503c\u8870\u51cf\u5f71\u54cd\u7684\u673a\u5236\u3002", "result": "\u7ed9\u51faVLM\u8bf1\u5bfc\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u51fd\u6570\u4e0a\u7684\u6709\u9650\u6837\u672c\u7edf\u4e00\u6536\u655b\u754c\uff1b\u6837\u672c\u590d\u6742\u5ea6\u7531\u5185\u5728/\u6709\u6548\u7ef4\u5ea6\u51b3\u5b9a\uff0c\u800c\u975e\u5d4c\u5165\u7684\u603b\u7ef4\u5ea6\uff1b\u7ed9\u51fa\u8c31\u4f9d\u8d56\u8fb9\u754c\uff0c\u660e\u786e\u7279\u5f81\u503c\u8870\u51cf\u5982\u4f55\u5f71\u54cd\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u5bf9\u6570\u636e\u53d7\u9650\u7684\u751f\u7269\u533b\u5b66\u5efa\u6a21\u5177\u6709\u76f4\u63a5\u610f\u4e49\uff1a\u6307\u51fa\u5728\u4f55\u79cd\u6570\u636e\u89c4\u6a21\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u7edf\u4e00\u53ef\u9760\u7684\u9884\u6d4b\uff0c\u5e76\u89e3\u91ca\u4e3a\u4f55\u5e73\u5747\u6821\u51c6\u5ea6\u6307\u6807\u53ef\u80fd\u5ffd\u7565\u6700\u574f\u60c5\u51b5\u7684\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2512.23131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23131", "abs": "https://arxiv.org/abs/2512.23131", "authors": ["Yankang Li", "Changsheng Li"], "title": "SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals", "comment": "23 pages, 10 figures, 6 tables", "summary": "Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.", "AI": {"tldr": "SE-MLP \u5c06\u901a\u9053\u6ce8\u610f\u529b\u4e0e\u6b8b\u5dee\u8fde\u63a5\u7ed3\u5408\u7684\u591a\u5c42\u611f\u77e5\u5668\uff0c\u5feb\u901f\u9884\u6d4b\u7a7f\u900f\u52a0\u901f\u5ea6\u7279\u5f81\uff0c\u5b9e\u73b0\u4ece\u7269\u7406\u53c2\u6570\u5230\u7a7f\u900f\u7279\u5f81\u7684\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u5e76\u5728\u4e0e MLP\u3001XGBoost\u3001Transformer \u7684\u5bf9\u6bd4\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d56\u957f\u65f6\u95f4\u4eff\u771f\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u7a7f\u900f\u52a0\u901f\u5ea6\u5148\u9a8c\u7279\u5f81\u83b7\u53d6\u95ee\u9898\uff0c\u9700\u5feb\u901f\u3001\u7a33\u5b9a\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa SE-MLP \u67b6\u6784\uff0c\u5c06\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u4e0e\u6b8b\u5dee\u7ed3\u6784\u96c6\u6210\uff0c\u8f93\u5165\u7269\u7406\u5de5\u51b5\u53c2\u6570\uff0c\u8f93\u51fa\u5206\u5c42\u52a0\u901f\u5ea6\u7279\u5f81\uff0c\u5efa\u7acb\u7269\u7406\u53c2\u6570\u4e0e\u7a7f\u900f\u7279\u5f81\u7684\u975e\u7ebf\u6027\u6620\u5c04\uff1b\u8fdb\u884c\u4e0e\u5e38\u89c4 MLP\u3001XGBoost\u3001Transformer \u7684\u5bf9\u6bd4\uff0c\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u3002", "result": "SE-MLP \u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u6a21\u578b\uff1b\u901a\u9053\u6ce8\u610f\u529b\u548c\u6b8b\u5dee\u7ed3\u6784\u5bf9\u6027\u80fd\u63d0\u5347\u5747\u6709\u663e\u8457\u8d21\u732e\uff1b\u6570\u503c\u4eff\u771f\u4e0e\u8303\u56f4\u6062\u590d\u6d4b\u8bd5\u8868\u660e\u9884\u6d4b\u5cf0\u503c\u548c\u8109\u5bbd\u4e0e\u5b9e\u6d4b\u503c\u504f\u5dee\u5728\u5de5\u7a0b\u5bb9\u5fcd\u5ea6\u4e4b\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u884c\u6027\u548c\u5de5\u7a0b\u9002\u7528\u6027\u65b9\u9762\u5f97\u5230\u8bc1\u5b9e\uff0c\u4e3a\u5feb\u901f\u751f\u6210\u7a7f\u900f\u5f15\u4fe1\u7684\u5148\u9a8c\u7279\u5f81\u503c\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u4e0e\u5b9e\u9645\u57fa\u7840\u3002"}}
{"id": "2512.23133", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23133", "abs": "https://arxiv.org/abs/2512.23133", "authors": ["Anqi Mao", "Mehryar Mohri", "Yutao Zhong"], "title": "Principled Algorithms for Optimizing Generalized Metrics in Binary Classification", "comment": "ICML 2025", "summary": "In applications with significant class imbalance or asymmetric costs, metrics such as the $F_\u03b2$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.", "AI": {"tldr": "\u5728\u4e0d\u5e73\u8861\u6570\u636e\u548c\u4ee3\u4ef7\u4e0d\u5bf9\u79f0\u7684\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u57fa\u4e8e\u901a\u7528\u4ee3\u4ef7\u654f\u611f\u5b66\u4e60\u7684\u6307\u6807\u4f18\u5316\u6846\u67b6\uff0c\u8bbe\u8ba1 METRO \u7b97\u6cd5\u5e76\u7ed9\u51fa\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u8bc1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5982 F\u03b2\u3001AM\u3001Jaccard \u76f8\u4f3c\u7cfb\u6570\u548c\u52a0\u6743\u51c6\u786e\u5ea6\u5728\u4e0d\u5e73\u8861\u6570\u636e\u6216\u4ee3\u4ef7\u4e0d\u5bf9\u79f0\u7684\u5e94\u7528\u4e2d\u6bd4\u4f20\u7edf\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66f4\u5408\u9002\uff0c\u4f46\u76f4\u63a5\u4f18\u5316\u8fd9\u4e9b\u6307\u6807\u5b58\u5728\u8ba1\u7b97\u4e0e\u7edf\u8ba1\u6311\u6218\uff1b\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56 Bayes \u6700\u4f18\u5206\u7c7b\u5668\u7684\u8868\u5f81\uff0c\u4e14\u4ee5\u9608\u503c\u5206\u5272\u4e3a\u4e3b\uff0c\u672a\u80fd\u9488\u5bf9\u53d7\u9650\u5047\u8bbe\u96c6\u7ed9\u51fa\u6709\u9650\u6837\u672c\u7684\u6cdb\u5316\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u5728\u53d7\u9650\u5047\u8bbe\u96c6\u5185\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u76f4\u63a5\u6307\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u6307\u6807\u4f18\u5316\u91cd\u6784\u4e3a\u4e00\u4e2a\u5e7f\u4e49\u7684\u4ee3\u4ef7\u654f\u611f\u5b66\u4e60\u95ee\u9898\uff0c\u8bbe\u8ba1\u65b0\u7684\u66ff\u4ee3\u635f\u5931\u51fd\u6570\uff0c\u5e76\u7ed9\u51fa\u53ef\u8bc1\u660e\u7684 H-consistency\uff08H-\u4e00\u81f4\u6027\uff09\u4fdd\u8bc1\uff1b\u5728\u6b64\u6846\u67b6\u4e0b\u63d0\u51fa METRO\uff08Metric Optimization\uff09\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u5176\u7406\u8bba\u6027\u80fd\u754c\u9650\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u5bf9\u65b0\u66ff\u4ee3\u635f\u5931\u548c METRO \u7684 H-consistency \u4e0e\u6709\u9650\u6837\u672c\u6cdb\u5316\u754c\uff1b\u5728\u591a\u7ec4\u4e0d\u5e73\u8861/\u4ee3\u4ef7\u654f\u611f\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMETRO \u76f8\u8f83\u4e8e\u5148\u524d\u57fa\u7ebf\u5728\u4f18\u5316\u76ee\u6807\u6307\u6807\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6307\u6807\u4f18\u5316\u6846\u67b6\u53ca\u53ef\u5b9e\u73b0\u7684\u7b97\u6cd5 METRO\uff0c\u5177\u5907 H-consistency \u548c\u6709\u9650\u6837\u672c\u6cdb\u5316\u4fdd\u8bc1\uff0c\u80fd\u76f4\u63a5\u5728\u53d7\u9650\u5047\u8bbe\u96c6\u4e0a\u6709\u6548\u4f18\u5316\u5305\u62ec F\u03b2\u3001AM\u3001Jaccard \u7b49\u5728\u5185\u7684\u901a\u7528\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2512.23160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23160", "abs": "https://arxiv.org/abs/2512.23160", "authors": ["Xianqi Liu", "Xiangru Li", "Lefeng He", "Ziyu Fang"], "title": "A Weak Signal Learning Dataset and Its Baseline Method", "comment": null, "summary": "Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.", "AI": {"tldr": "\u63d0\u51fa\u4e13\u95e8\u7684\u5f31\u4fe1\u53f7\u5b66\u4e60\u6570\u636e\u96c6\u4e0ePDVFN\u6a21\u578b\uff0c\u805a\u7126\u4f4eSNR\u3001\u6570\u636e\u5206\u5e03\u504f\u659c\u548c\u53cc\u89c6\u56fe\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u5f31\u4fe1\u53f7\u7684\u5206\u7c7b\u4e0e\u56de\u5f52\u6027\u80fd\u5e76\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u5728\u566a\u58f0\u548c\u5e72\u6270\u906e\u853d\u4e0b\u63d0\u53d6\u5f31\u4fe1\u53f7\u4fe1\u606f\u662f\u8bb8\u591a\u9886\u57df\u7684\u6838\u5fc3\u96be\u9898\uff1b\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u9650\u5236\u65b9\u6cd5\u8bc4\u4f30\u4e0e\u6bd4\u8f83\u3002", "method": "\u6784\u5efa13,158\u6761\u5149\u8c31\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5411\u91cf\u548c\u65f6\u9891\u56fe\u7684\u53cc\u89c6\u56fe\u8868\u793a\uff1b\u63d0\u51faPDVFN\u6a21\u578b\uff0c\u5728\u4f4eSNR\u3001\u5206\u5e03\u504f\u659c\u548c\u53cc\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u5e76\u884c\u63d0\u53d6\u5c40\u90e8\u5e8f\u5217\u7279\u5f81\u4e0e\u5168\u5c40\u9891\u57df\u7ed3\u6784\uff0c\u5305\u542b\u5c40\u90e8\u589e\u5f3a\u3001\u5e8f\u5217\u5efa\u6a21\u3001\u566a\u58f0\u6291\u5236\u3001\u591a\u5c3a\u5ea6\u6355\u83b7\u3001\u9891\u7387\u63d0\u53d6\u4e0e\u5168\u5c40\u611f\u77e5\u7b49\u8bbe\u8ba1\u3002", "result": "\u5728\u4f4eSNR\u548c\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5efa\u7acb\u4e86\u5f31\u4fe1\u53f7\u5b66\u4e60\u7684\u57fa\u7ebf\u4e0e\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5982\u5929\u6587\u5149\u8c31\u7b49\u4efb\u52a1\u5177\u5907\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f31\u4fe1\u53f7\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e13\u7528\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u6a21\u578b\u4e0e\u7814\u7a76\u57fa\u7840\uff0c\u63a8\u52a8\u5f31\u4fe1\u53f7\u5b66\u4e60\u5728\u5206\u7c7b\u3001\u56de\u5f52\u53ca\u591a\u6e90\u6570\u636e\u878d\u5408\u65b9\u9762\u7684\u5e94\u7528\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2512.23165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23165", "abs": "https://arxiv.org/abs/2512.23165", "authors": ["Qingyu Yin", "Yulun Wu", "Zhennan Shen", "Sunbowen Li", "Zhilin Wang", "Yanshu Li", "Chak Tou Leong", "Jiale Kang", "Jinjin Gu"], "title": "Evaluating Parameter Efficient Methods for RLVR", "comment": "Preprint", "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f3012+\u79cdPEFT\u65b9\u6cd5\u5728RLVR\u6846\u67b6\u4e0b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u53d1\u73b0DoRA/AdaLoRA/MiSS\u7b49\u7ed3\u6784\u53d8\u4f53\u5e38\u4f18\u4e8eLoRA\uff1bSVD\u521d\u59cb\u5316\uff08PiSSA/MiLoRA\uff09\u51fa\u73b0\u8c31\u5d29\u6e83\uff1b\u6781\u7aef\u53c2\u6570\u524a\u51cf\uff08VeRA/Rank-1\uff09\u663e\u8457\u964d\u4f4e\u63a8\u7406\u80fd\u529b\uff0c\u7ed9\u51fa\u5728RLVR\u4e2d\u9009\u62e9PEFT\u7684\u5b9e\u7528\u6307\u5f15\u3002", "motivation": "RLVR\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u63a8\u52a8\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c1a\u65e0\u5173\u4e8e\u54ea\u79cdPEFT\u67b6\u6784\u6700\u4f18\u7684\u7cfb\u7edf\u7ed3\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\u5bf9\u591a\u79cdPEFT\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\uff0c\u4ee5\u4e3a\u672a\u6765\u65b9\u6cd5\u63d0\u4f9b\u6307\u5f15\u3002", "method": "\u5728DeepSeek-R1-Distill\u5bb6\u65cf\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u7cfb\u7edf\u8bc4\u4f30\u8d85\u8fc712\u79cdPEFT\u65b9\u6cd5\u7684\u6027\u80fd\uff1b\u5305\u542b\u5bf9LoRA\u7684\u5e38\u89c4\u5bf9\u6bd4\uff0c\u4ee5\u53caDoRA\u3001AdaLoRA\u3001MiSS\u3001PiSSA\u3001MiLoRA\u3001VeRA\u3001Rank-1\u7b49\u7ed3\u6784/\u521d\u59cb\u5316\u7b56\u7565\u7684\u6d88\u878d\u4e0e\u6269\u5c55\u5206\u6790\u3002", "result": "\u7ed3\u6784\u53d8\u4f53\uff08\u5982DoRA\u3001AdaLoRA\u3001MiSS\uff09\u5728\u5927\u591a\u6570\u57fa\u51c6\u4e0a\u4f18\u4e8eLoRA\uff1bSVD\u521d\u59cb\u5316\u7b56\u7565\uff08PiSSA\u3001MiLoRA\uff09\u8868\u73b0\u51fa\u8c31\u5d29\u6e83\uff0c\u663e\u793a\u4e3b\u6210\u5206\u66f4\u65b0\u4e0eRL\u4f18\u5316\u4e4b\u95f4\u5b58\u5728\u672c\u8d28\u4e0d\u5bf9\u9f50\uff1b\u6781\u7aef\u53c2\u6570\u538b\u7f29\uff08VeRA\u3001Rank-1\uff09\u663e\u8457\u964d\u4f4e\u63a8\u7406\u80fd\u529b\u5e76\u9650\u5236\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u5efa\u8bae\u5728RLVR\u573a\u666f\u4e0b\u66f4\u79ef\u6781\u5730\u63a2\u7d22\u591a\u79cdPEFT\u7ed3\u6784\uff0c\u800c\u975e\u9ed8\u8ba4\u91c7\u7528LoRA\uff1b\u9700\u8981\u66f4\u591a\u7684\u6d88\u878d\u4e0e\u6269\u5c55\u7814\u7a76\u6765\u5efa\u7acb\u9762\u5411RL\u4efb\u52a1\u7684PEFT\u9009\u578b\u6307\u5357\u3002"}}
{"id": "2512.23177", "categories": ["cs.LG", "cs.CE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23177", "abs": "https://arxiv.org/abs/2512.23177", "authors": ["Will Sebelik-Lassiter", "Evan Schubert", "Muhammad Alliyu", "Quentin Robbins", "Excel Olatunji", "Mustafa Barry"], "title": "Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR", "comment": "Won Best Undergraduate Research Paper at the 2025 Midwest Instruction & Computing Symposium (MICS)", "summary": "Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.", "AI": {"tldr": "ML-assisted VCUS analysis pipeline for vocal cord segmentation and VCP classification with high validation accuracy (segmentation 96%; classification 99%).", "motivation": "VCUS is noninvasive but operator-dependent; applying machine learning can automate detection/segmentation to improve diagnostic accuracy and consistency.", "method": "30 volunteers provided VCUS videos, converted to uniform-sized frames. Training data included healthy and simulated VCP images for both segmentation and classification models.", "result": "Segmentation model achieved 96% validation accuracy; best classification model (VIPRnet) achieved 99% validation accuracy.", "conclusion": "Machine learning\u2013assisted VCUS analysis shows strong potential to surpass operator-dependent interpretation in diagnostic accuracy."}}
{"id": "2512.23190", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23190", "abs": "https://arxiv.org/abs/2512.23190", "authors": ["Yi-Han Wang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization", "comment": null, "summary": "Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \\log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $\u03a9(d^\u03c9)$ arithmetic operations for bounded domains, even for the unit ball, where $\u03c9\\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\\tilde{O}(d^\u03c9T)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \\tilde{O}(d/\u03b5)$ rounds to achieve an excess risk of $\u03b5$, and thereby necessitates an $\\tilde{O}(d^{\u03c9+1}/\u03b5)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\\tilde{O}(d^{\u03c9+1}/\u03b5)$.\n  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^\u03c9\\sqrt{T \\log T})$ while preserving the optimal $O(d \\log T)$ regret. LightONS implies an SXO method with runtime $\\tilde{O}(d^3/\u03b5)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.", "AI": {"tldr": "LightONS \u662f\u5bf9 Online Newton Step \u7684\u4e00\u4e2a\u9ad8\u6548\u53d8\u4f53\uff0c\u901a\u8fc7\u9886\u57df\u53d8\u6362\u548c\u8fdf\u6ede\u673a\u5236\u5ef6\u8fdf\u6602\u8d35\u7684\u9a6c\u54c8\u62c9\u8bfa\u6bd4\u6295\u5f71\uff0c\u4ece\u800c\u5728\u4fdd\u6301 O(d log T) \u7ecf\u9a8c\u6027 regrets \u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u603b\u8fd0\u884c\u65f6\u95f4\u964d\u4f4e\u81f3 O(d^2 T + d^\u03c9 \u221a(T log T))\uff0c\u5e76\u7ed9\u51fa SXO \u7684\u8fd1\u4f3c\u65e0\u504f excess risk \u7684\u5b9e\u73b0\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a tilde O(d^3/\u03b5)\u3002", "motivation": "\u89e3\u51b3 ONS \u5728\u6709\u754c\u57df\uff08\u5982\u5355\u4f4d\u7403\uff09\u4e2d\u7684\u6295\u5f71\u6210\u672c \u03a9(d^\u03c9) \u4ee5\u53ca SXO \u573a\u666f\u4e0b\u7684\u9ad8\u8ba1\u7b97\u4ee3\u4ef7\u95ee\u9898\uff0c\u6ee1\u8db3\u5bf9\u9ad8\u7ef4\u3001\u65f6\u5e8f\u6570\u636e\u7684\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\u9700\u6c42\uff0c\u540c\u65f6\u56de\u5e94 COLT'13 \u5bf9 SXO \u8fd0\u884c\u65f6\u95f4\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u5728 ONS \u7684\u6846\u67b6\u5185\u5f15\u5165\u9886\u57df\u8f6c\u6362\uff08domain-conversion\uff09\u548c\u53c2\u6570\u81ea\u7531\u7684\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u8fdf\u6ede\uff08hysteresis\uff09\u673a\u5236\uff0c\u4f7f\u5f97\u6602\u8d35\u7684\u9a6c\u54c8\u62c9\u8bfa\u6bd4\u6295\u5f71\u4ec5\u5728\u5fc5\u8981\u65f6\u53d1\u751f\uff0c\u4ece\u800c\u51cf\u5c11\u6bcf\u8f6e\u7684\u8ba1\u7b97\u91cf\uff1b\u901a\u8fc7\u63a8\u5bfc\u8bc1\u660e\u4ecd\u4fdd\u6301 O(d log T) \u7684 regrets\uff1b\u7ed9\u51fa SXO \u573a\u666f\u4e0b\u7684\u5b9e\u73b0\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a tilde O(d^3/\u03b5)\u3002", "result": "\u63d0\u51fa LightONS\uff0c\u4f5c\u4e3a ONS \u7684\u9ad8\u6548\u66ff\u4ee3\uff0c\u5728\u591a\u79cd\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u4e2d\u5177\u6709\u66f4\u4f4e\u7684\u7406\u8bba\u548c\u5b9e\u9645\u8fd0\u884c\u6210\u672c\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u63d2\u4ef6\u5f0f\u6846\u67b6\u3002", "conclusion": "LightONS \u4ee5\u7b80\u6d01\u7684\u8bbe\u8ba1\u4fdd\u7559 ONS \u7684\u7ed3\u6784\u7f8e\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u53d8\u6362\u4e0e\u8fdf\u6ede\u7b56\u7565\u89e3\u51b3\u9ad8\u6210\u672c\u6295\u5f71\u95ee\u9898\uff0c\u4ece\u800c\u56de\u7b54 COLT'13 \u7684\u5f00\u653e\u95ee\u9898\u5e76\u62d3\u5c55\u5230\u66f4\u5e7f\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u68af\u5ea6\u8303\u6570\u81ea\u9002\u5e94\u540e\u6094\u754c\u3001\u53c2\u6570\u5316\u968f\u673a\u5e26\u5bbd\u3001\u4ee5\u53ca\u5185\u5b58\u53cb\u597d\u578b\u5728\u7ebf\u5b66\u4e60\u3002"}}
{"id": "2512.23192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23192", "abs": "https://arxiv.org/abs/2512.23192", "authors": ["Zhuo Zhang", "Xi Yang", "Yuan Zhao", "Canqun Yang"], "title": "PGOT: A Physics-Geometry Operator Transformer for Complex PDEs", "comment": "22 pages, 17 figures", "summary": "While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection\" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.", "AI": {"tldr": "\u63d0\u51fa Physics-Geometry Operator Transformer (PGOT)\uff0c\u901a\u8fc7 Spectrum-Preserving Geometric Attention \u548c physics slicing-geometry injection\uff0c\u5728\u4fdd\u7559\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\u7684\u540c\u65f6\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6 O(N) \u7684\u51e0\u4f55\u611f\u77e5 PDE \u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u81ea\u9002\u5e94\u8def\u7531\u5728\u5e73\u6ed1\u533a\u57df\u548c\u6fc0\u6ce2/\u4e0d\u8fde\u7eed\u5904\u91c7\u7528\u4e0d\u540c\u7684\u9ad8\u9636\u8def\u5f84\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u5de5\u4e1a\u4efb\u52a1\u4e0a\u8fbe\u5230\u72b6\u6001\u6700\u4f18\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u7f51\u683c\u548c\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e0b\u5bf9\u7269\u7406\u573a\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u9ad8\u6548\u67b6\u6784\u901a\u5e38\u91c7\u7528\u7279\u5f81\u964d\u7ef4\uff0c\u5bfc\u81f4\u51e0\u4f55\u6df7\u53e0\u5e76\u4e22\u5931\u8fb9\u754c\u4fe1\u606f\u3002\u9700\u5f00\u53d1\u663e\u5f0f\u51e0\u4f55\u611f\u77e5\u7684\u6a21\u578b\uff0c\u4ee5\u4fdd\u6301\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\u5e76\u9ad8\u6548\u5904\u7406\u6fc0\u6ce2\u4e0e\u4e0d\u8fde\u7eed\u5904\u3002", "method": "\u63d0\u51fa PGOT\uff0c\u5305\u62ec Spectrum-Preserving Geometric Attention\uff08SpecGeo-Attention\uff09\u4ee5\u53ca\u201cphysics slicing-geometry injection\u201d\u673a\u5236\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u51e0\u4f55\u7f16\u7801\u663e\u5f0f\u4fdd\u7559\u51e0\u4f55\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6 O(N)\uff1b\u5e76\u901a\u8fc7\u57fa\u4e8e\u7a7a\u95f4\u5750\u6807\u7684\u8def\u7531\uff0c\u5c06\u8ba1\u7b97\u5728\u5e73\u6ed1\u533a\u57df\u8d70\u4f4e\u9636\u7ebf\u6027\u8def\u5f84\uff0c\u5728\u6fc0\u6ce2/\u4e0d\u8fde\u7eed\u5904\u8d70\u9ad8\u9636\u975e\u7ebf\u6027\u8def\u5f84\uff0c\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u9ad8\u7cbe\u5ea6\u7269\u7406\u573a\u5efa\u6a21\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e00\u81f4\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u4efb\u52a1\uff08\u5982\u7ffc\u578b\u548c\u6c7d\u8f66\u8bbe\u8ba1\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PGOT \u901a\u8fc7\u663e\u5f0f\u51e0\u4f55\u611f\u77e5\u91cd\u5efa\u7269\u7406\u7279\u5f81\u5b66\u4e60\uff0c\u4fdd\u7559\u591a\u5c3a\u5ea6\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u4e14\u9ad8\u7cbe\u5ea6\u7684\u7269\u7406\u573a\u5efa\u6a21\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u7f51\u683c\u7684 PDE \u8868\u5f81\u3002"}}
{"id": "2512.23200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23200", "abs": "https://arxiv.org/abs/2512.23200", "authors": ["Ziru Niu", "Hai Dong", "A. K. Qin", "Tao Gu", "Pengcheng Zhang"], "title": "Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing", "comment": null, "summary": "Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.", "AI": {"tldr": "\u63d0\u51fa FedOLF\uff1a\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5bf9\u6a21\u578b\u5c42\u6309\u56fa\u5b9a\u987a\u5e8f\u8fdb\u884c\u9884\u51bb\u7ed3\u4ee5\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u540c\u65f6\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u5f20\u91cf\u8fd0\u7b97\u8fd1\u4f3c\uff08TOA\uff09\u4ee5\u51cf\u5c11\u901a\u4fe1\u548c\u80fd\u8017\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6/\u6a21\u578b\u7ec4\u5408\u4e0a\uff0cFedOLF \u80fd\u5728\u4fdd\u6301\u6216\u7565\u589e\u51c6\u786e\u7387\u7684\u540c\u65f6\u63d0\u5347\u80fd\u6548\u548c\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u5728\u7269\u8054\u7f51\u573a\u666f\u4e0b\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u7b97\u529b\u3001\u5185\u5b58\u4e0e\u5e26\u5bbd\u6210\u4e3a\u5236\u7ea6\u56e0\u7d20\uff0c\u73b0\u6709\u901a\u8fc7 dropout \u6216\u5c42\u51bb\u7ed3\u7684\u505a\u6cd5\u5e38\u4ee5\u727a\u7272\u51c6\u786e\u6027\u6216\u672a\u5145\u5206\u8003\u8651\u5185\u5b58\u7ea6\u675f\u4e3a\u4ee3\u4ef7\u3002\u9700\u8981\u4e00\u79cd\u517c\u987e\u51c6\u786e\u6027\u3001\u5185\u5b58/\u8ba1\u7b97\u6548\u7387\u4e0e\u901a\u4fe1\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5728 FL \u8bad\u7ec3\u524d\u5bf9\u6a21\u578b\u5c42\u6309\u9884\u5b9a\u987a\u5e8f\u8fdb\u884c\u51bb\u7ed3\uff08FedOLF\uff09\uff0c\u4ee5\u51cf\u5c11\u524d\u5411\u4e0e\u53cd\u5411\u8ba1\u7b97\u91cf\u5e76\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff1b\u540c\u65f6\u5f15\u5165 Tensor Operation Approximation\uff08TOA\uff09\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8fd1\u4f3c\u7b56\u7565\u66ff\u4ee3\u4f20\u7edf\u91cf\u5316\uff0c\u65e8\u5728\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u901a\u4fe1\u548c\u80fd\u8017\u3002", "result": "\u5728\u975eIID\u6570\u636e\u6761\u4ef6\u4e0b\uff0cFedOLF \u5728\u591a\u7ec4\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u6bd4\u57fa\u7ebf\u7684\u51c6\u786e\u7387\u63d0\u5347\uff1aEMNIST/CNN 0.3%\u3001CIFAR-10/AlexNet 6.4%\u3001CIFAR-100/ResNet20 5.81%\u3001CIFAR-100/ResNet44 4.4%\u3001CINIC-10/ResNet20 6.27%\u3001CINIC-10/ResNet44 1.29%\uff1b\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u524d\u5b9e\u73b0\u786e\u5b9a\u6027\u3001\u56fa\u5b9a\u987a\u5e8f\u7684\u5c42\u51bb\u7ed3\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7684 TOA\uff0cFedOLF \u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u5e76\u964d\u4f4e\u901a\u4fe1\u4e0e\u80fd\u8017\uff0c\u540c\u65f6\u5728\u591a\u9879\u975eIID\u6570\u636e\u573a\u666f\u4e0b\u7ef4\u6301\u6216\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.23235", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23235", "abs": "https://arxiv.org/abs/2512.23235", "authors": ["Zihao Zhou", "Shusen Yang", "Fangyuan Zhao", "Xuebin Ren"], "title": "FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs", "comment": null, "summary": "Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc6\u522b\u4e86\u5728\u56fe\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u5ba2\u6237\u7aef\u4e4b\u95f4\u91cd\u53e0\u5b50\u56fe\u4e0d\u5747\u8861\u5bfc\u81f4\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa FairGFL\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684\u91cd\u53e0\u6bd4\u4f30\u8ba1\u4e0e\u53ef\u89e3\u91ca\u7684\u52a0\u6743\u805a\u5408\u6765\u63d0\u5347\u8de8\u5ba2\u6237\u7aef\u516c\u5e73\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5728\u8054\u90a6\u6df7\u5408\u635f\u5931\u4e2d\u52a0\u5165\u6b63\u5219\u9879\u6765\u6539\u5584\u6a21\u578b\u6548\u7528\u4e0e\u516c\u5e73\u6027\u7684\u6743\u8861\uff0c\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u56db\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u56fe\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u62a4\u539f\u59cb\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u63d0\u53d6\u5206\u5e03\u5f0f\u5b50\u56fe\u4e2d\u7684\u9ad8\u9636\u4fe1\u606f\uff0c\u4f46\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u5b50\u56fe\u5b58\u5728\u91cd\u53e0\uff0c\u4e14\u91cd\u53e0\u7a0b\u5ea6\u4e0d\u5747\u8861\u3002\u5df2\u6709\u7814\u7a76\u53d1\u73b0\u91cd\u53e0\u5bf9\u7f13\u89e3\u6570\u636e\u5f02\u8d28\u6027\u6709\u76ca\uff0c\u4f46\u5bf9\u4e0d\u5747\u8861\u91cd\u53e0\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\uff08\u5c24\u5176\u662f\u8de8\u5ba2\u6237\u7aef\u7684\u516c\u5e73\u6027\u95ee\u9898\uff09\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5e76\u7f13\u89e3\u8fd9\u4e00\u4e0d\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa FairGFL\uff08FAIRness-aware subGraph Federated Learning\uff09\uff0c\u901a\u8fc7\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\u63d0\u5347\u8de8\u5ba2\u6237\u7aef\u516c\u5e73\u6027\uff0c\u4e14\u4ee5\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u5f0f\u4f30\u8ba1\u5ba2\u6237\u7aef\u7684\u91cd\u53e0\u6bd4\u3002\u8fdb\u4e00\u6b65\u5728\u8054\u90a6\u590d\u5408\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6b63\u5219\u9879\uff0c\u4ee5\u5728\u6a21\u578b\u6548\u7528\u4e0e\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u66f4\u4f18\u7684\u6743\u8861\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFairGFL\u5728\u6a21\u578b\u6548\u7528\u548c\u516c\u5e73\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u56db\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "FairGFL\u6709\u6548\u7f13\u89e3\u4e86\u7531\u4e0d\u5747\u8861\u91cd\u53e0\u5f15\u53d1\u7684\u8de8\u5ba2\u6237\u7aef\u4e0d\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5728\u56fe\u6570\u636e\u573a\u666f\u4e0b\u63d0\u5347\u516c\u5e73\u6027\u7684\u53ef\u884c\u9014\u5f84\u53ca\u672a\u6765\u5728\u66f4\u590d\u6742\u573a\u666f\u4e2d\u7684\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2512.23262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23262", "abs": "https://arxiv.org/abs/2512.23262", "authors": ["Tao Li", "Peilin Li", "Kui Lu", "Yilei Wang", "Junliang Shang", "Guangshun Li", "Huiyu Zhou"], "title": "PFed-Signal: An ADR Prediction Model based on Federated Learning", "comment": "IEEE International Conference on Bioinformatics and Biomedicine", "summary": "The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.", "AI": {"tldr": "PFed-Signal: a federated-learning ADR signal-prediction framework that identifies and removes biased FAERS data using Euclidean distance, training a Transformer-based ADR predictor on clean data; improves ROR/PRR and key metrics (ACC=0.887, F1=0.890, Recall=0.913, AUC=0.957).", "motivation": "FAERS contains biased reporting data that biases ADR signal measures (ROR/PRR) and harms diagnosis. A method that can identify and remove biased data is needed to improve predictive accuracy.", "method": "1) Pfed-Split: split dataset by ADR. 2) ADR-signal: federated-learning-based biased-data identification using Euclidean distance to produce a clean dataset. 3) Transformer-based ADR predictor trained on the clean data.", "result": "On the clean dataset, ROR/PRR perform better than traditional methods. PFed-Signal achieves accuracy 0.887, F1 0.890, recall 0.913, AUC 0.957, surpassing baselines.", "conclusion": "PFed-Signal effectively mitigates FAERS bias through federated identification and data cleaning, leading to improved ADR signal prediction."}}
{"id": "2512.23265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23265", "abs": "https://arxiv.org/abs/2512.23265", "authors": ["Alexander Korotin", "Gudmund Pammer"], "title": "On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases", "comment": null, "summary": "This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.", "AI": {"tldr": "\u5bf9 Flow Matching \u7684\u53cd\u95ee\u9898\u5728\u4e00\u7ef4\u548c\u9ad8\u65af\u60c5\u5f62\u4e0b\u6709\u552f\u4e00\u89e3\uff1b\u591a\u7ef4\u60c5\u5f62\u5c1a\u5f85\u7814\u7a76\u3002", "motivation": "\u53d7\u73b0\u4ee3\u751f\u6210\u5f0f AI \u5e94\u7528\u542f\u53d1\uff0c\u7814\u7a76\u6709\u9650\u6307\u6570\u77e9\u5206\u5e03\u4e4b\u95f4\u7684\u6d41\u52a8\u5339\u914d\u53cd\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u6d41\u52a8\u5339\u914d\u6a21\u578b\u84b8\u998f\u7b49\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u9002\u7528\u4e8e\u6709\u9650\u6307\u6570\u77e9\u5206\u5e03\u7684\u53cd\u95ee\u9898\u7684\u552f\u4e00\u6027\u6761\u4ef6\uff1b\u5206\u522b\u5728\u4e00\u7ef4\u548c\u9ad8\u65af\u5206\u5e03\u60c5\u5f62\u7ed9\u51fa\u8bc1\u660e\uff1b\u7ed9\u51fa\u5728\u591a\u7ef4\u60c5\u5f62\u672a\u89e3\u51b3\u7684\u7ed3\u8bba\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u5728\u4e00\u7ef4\u4e0e\u9ad8\u65af\u60c5\u5f62\u4e0b\u8bc1\u660e\u4e86\u95ee\u9898\u7684\u552f\u4e00\u6027\uff1b\u4e00\u822c\u591a\u7ef4\u60c5\u5f62\u5c1a\u672a\u89e3\u51b3\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u805a\u7126\u4e8e\u591a\u7ef4\u60c5\u5f62\u7684\u5b58\u5728\u6027\u4e0e\u552f\u4e00\u6027\uff0c\u4ee5\u53ca\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5206\u5e03\u5047\u8bbe\u3002"}}
{"id": "2512.23295", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.23295", "abs": "https://arxiv.org/abs/2512.23295", "authors": ["Yuchen Xie", "Honghang Chi", "Haopeng Quan", "Yahui Wang", "Wei Wang", "Yu Ma"], "title": "Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions", "comment": null, "summary": "Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\\tilde{u} = A + B \\cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.\n  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.\n  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.\n  This relationship demonstrates that the boundary function $B(\\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.\n  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.\n  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.\n  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.", "AI": {"tldr": "\u63d0\u51fa HC-PINN \u7684 NTK \u6846\u67b6\uff0c\u63ed\u793a\u8fb9\u754c\u51fd\u6570 B \u4f5c\u4e3a\u8c31\u6ee4\u6ce2\u5668\u91cd\u5851\u5185\u6838\u7279\u5f81\u5e76\u9884\u6d4b\u8bad\u7ec3\u6536\u655b\u7684\u6709\u6548\u79e9\uff0c\u5f3a\u8c03\u8fb9\u754c\u8bbe\u8ba1\u9700\u8fdb\u884c\u8c31\u4f18\u5316\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4f55\u786c\u7ea6\u675f\u7684 PINN \u8bad\u7ec3\u52a8\u6001\u5c1a\u672a\u88ab\u7406\u8bba\u63ed\u793a\uff0c\u5f25\u8865\u8f6f\u7ea6\u675f\u4e0e\u786c\u7ea6\u675f\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u4e3a\u51e0\u4f55\u786c\u7ea6\u675f\u7684\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u57fa\u77f3\u3002", "method": "\u6784\u5efa\u5e26\u6709 tilde u = A + B N \u7684\u8bd5\u89e3\u5f62\u5f0f\uff0c\u7ed9\u51fa HC-PINN \u7684\u663e\u5f0f\u6838\u7ec4\u5408\u5f8b\uff1b\u901a\u8fc7 Neural Tangent Kernel \u6846\u67b6\u4e0e\u8c31\u5206\u6790\uff0c\u63a8\u5bfc\u6b8b\u6838\u7684\u6709\u6548\u79e9\u53ca B \u7684\u8c31\u6ee4\u6ce2\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u8fb9\u754c\u51fd\u6570 B \u4f5c\u4e3a\u8c31\u6ee4\u6ce2\u5668\u91cd\u5851\u539f\u751f\u6838\u7684\u7279\u5f81\u8c31\uff0c\u51b3\u5b9a\u5b66\u4e60\u52a8\u529b\u5b66\uff1b\u6b8b\u6838\u7684\u6709\u6548\u79e9\u6210\u4e3a\u9884\u6d4b\u8bad\u7ec3\u6536\u655b\u7684\u786e\u5b9a\u6027\u6307\u6807\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6761\u4ef6\u6570\uff1b\u8b66\u793a\u5e38\u7528\u8fb9\u754c\u51fd\u6570\u53ef\u80fd\u5bfc\u81f4\u8c31\u584c\u9677\u548c\u4f18\u5316\u505c\u6ede\uff1b\u5728\u591a\u7ef4\u57fa\u51c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u6846\u67b6\u5c06\u8fb9\u754c\u8bbe\u8ba1\u4ece\u7ecf\u9a8c\u6027\u8f6c\u5316\u4e3a\u8c31\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u4e3a\u5305\u542b\u51e0\u4f55\u786c\u7ea6\u675f\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u8fb9\u754c\u51fd\u6570\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u8c31\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347 HC-PINN \u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.23340", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23340", "abs": "https://arxiv.org/abs/2512.23340", "authors": ["Dakuan Lu", "Jiaqi Zhang", "Cheng Yuan", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u201c\u591a\u6a21\u578b\u534f\u4f5c\u5b9a\u5f8b\u201d\uff0c\u7ed9\u51fa\u591a\u6a21\u578b\u96c6\u6210\u7684\u6027\u80fd\u4e0a\u9650\u968f\u53c2\u6570\u603b\u91cf\u7684\u5e42\u5f8b\u6269\u5c55\u89c4\u5f8b\uff0c\u5e76\u5f3a\u8c03\u6a21\u578b\u591a\u6837\u6027\u662f\u63d0\u5347\u534f\u4f5c\u6536\u76ca\u7684\u5173\u952e\u3002", "motivation": "\u5355\u4e00\u5927\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5f8b\u65e0\u6cd5\u5145\u5206\u89e3\u91ca\u591a\u6a21\u578b\u534f\u4f5c\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u591a\u6a21\u578b\u96c6\u5408\u7684\u6f5c\u5728\u4e0a\u9650\u53ca\u5176\u6536\u76ca\u3002", "method": "\u91c7\u7528\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5f62\u5f0f\u5316\u8868\u8ff0\uff0c\u8bbe\u5b9a\u7406\u60f3\u5316\u7684\u6574\u5408oracle\uff1a\u5bf9\u6bcf\u4e2a\u6837\u672c\uff0c\u603b\u635f\u5931\u4e3a\u6c60\u4e2d\u4efb\u4e00\u6a21\u578b\u635f\u5931\u7684\u6700\u5c0f\u503c\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u603b\u53c2\u6570\u91cf\u4e0a\u5448\u73b0\u5e42\u5f8b scaling\uff0c\u5e76\u6bd4\u8f83\u5f02\u6784\u4e0e\u540c\u8d28\u6a21\u578b\u5bb6\u65cf\u7684\u534f\u4f5c\u6548\u679c\u3002", "result": "\u591a\u6a21\u578b\u7cfb\u7edf\u6309\u53c2\u6570\u89c4\u6a21\u5448\u73b0\u5e42\u5f8b\u6269\u5c55\uff0c\u63d0\u5347\u8d8b\u52bf\u66f4\u660e\u663e\u3001\u7406\u8bba\u635f\u5931\u4e0b\u9650\u66f4\u4f4e\uff1b\u5f02\u8d28\u6a21\u578b\u5bb6\u65cf\u7684\u96c6\u6210\u5728\u6027\u80fd\u6269\u5c55\u4e0a\u4f18\u4e8e\u540c\u65cf\u6a21\u578b\uff0c\u8868\u660e\u6a21\u578b\u591a\u6837\u6027\u662f\u534f\u4f5c\u6536\u76ca\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u6a21\u578b\u534f\u4f5c\u4e3a\u6269\u5c55LLM\u667a\u80fd\u524d\u6cbf\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u7814\u7a76\u8f74\u7ebf\uff0c\u7406\u8bba\u4e0e\u5b9e\u9a8c\u5747\u652f\u6301\u591a\u6a21\u578b\u7cfb\u7edf\u7684\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2512.23347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23347", "abs": "https://arxiv.org/abs/2512.23347", "authors": ["Hai Duong Nguyen", "Xuan-The Tran"], "title": "ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling", "comment": null, "summary": "Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.", "AI": {"tldr": "ECG-RAMBA: a framework that disentangles morphology and rhythm for robust cross-domain ECG classification, using deterministic morphology via MiniRocket, HRV-based rhythm descriptors, and long-range context with a bi-directional Mamba backbone, powered by a stable Power Mean pooling. Achieves ~0.85 macro ROC-AUC on Chapman\u2013Shaoxing, zero-shot PR-AUC 0.708 on CPSC-2021, and good cross-dataset performance on PTB-XL; ablations highlight morphology as foundation and rhythm/context as drivers of cross-domain robustness.", "motivation": "Generalization across heterogeneous ECG acquisition settings is challenging due to entangled morphological patterns and rhythm dynamics that promote shortcut learning and distribution shift; separating morphology and rhythm and using context-aware fusion may improve cross-domain robustness and clinical deployment.", "method": "ECG-RAMBA builds three components: (i) deterministic morphological features extracted by MiniRocket; (ii) global rhythm descriptors from HRV; (iii) long-range context modeling via a bi-directional Mamba backbone. It introduces a numerically stable Power Mean pooling (Q=3) to emphasize high-evidence segments during windowed inference. Evaluation follows a protocol-faithful setting with subject-level cross-validation, fixed decision threshold, and no test-time adaptation.", "result": "On Chapman\u2013Shaoxing dataset, macro ROC-AUC \u2248 0.85. In zero-shot transfer, PR-AUC = 0.708 for atrial fibrillation on CPSC-2021, outperforming a comparable raw-signal Mamba baseline. Cross-dataset robustness observed on PTB-XL. Ablations indicate morphology as a strong foundation; rhythm modeling and long-range context crucial for cross-domain robustness.", "conclusion": "Deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are key drivers of cross-domain robustness."}}
{"id": "2512.23353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23353", "abs": "https://arxiv.org/abs/2512.23353", "authors": ["Nilin Abrahamsen"], "title": "ISOPO: Proximal policy gradients without pi-old", "comment": null, "summary": "This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.", "AI": {"tldr": "ISOPO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e00\u6b65\u8fd1\u4f3c\u81ea\u7136\u7b56\u7565\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u5e8f\u5217\u5bf9\u6570\u6982\u7387\u68af\u5ea6\u5728Fisher\u5ea6\u91cf\u4e0b\u5f52\u4e00\u5316\u518d\u4e0e\u4f18\u52bf\u76f8\u5377\u79ef\u6765\u8fd1\u4f3c\u81ea\u7136\u68af\u5ea6\uff1b\u76f8\u6bd4GRPO\u3001CISPO\u7b49\u9700\u8981\u591a\u6b21\u68af\u5ea6\u6b65\u9aa4\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0cISOPO\u5728\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u4e2d\u5b9e\u73b0\uff0c\u4e14\u5b9e\u73b0\u6210\u672c\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u76f8\u5bf9REINFORCE\u66f4\u9ad8\u6548\u3002", "motivation": "\u81ea\u7136\u7b56\u7565\u68af\u5ea6\u7684\u8ba1\u7b97\u901a\u5e38\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7684\u8fd1\u7aef\u7b56\u7565\u65b9\u6cd5\uff08\u5982GRPO\u3001CISPO\uff09\u4f9d\u8d56\u591a\u6b65\u68af\u5ea6\u4e0e\u91cd\u8981\u6027\u6bd4\u526a\u88c1\u6765\u63a5\u8fd1\u81ea\u7136\u68af\u5ea6\uff0c\u9020\u6210\u8ba1\u7b97\u4e0e\u6837\u672c\u6210\u672c\u8f83\u9ad8\uff0c\u6709\u5f85\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faISOPO\uff0c\u5728\u7b80\u5355\u5f62\u5f0f\u4e0b\uff0c\u5bf9\u6bcf\u4e2a\u5e8f\u5217\u7684\u5bf9\u6570\u6982\u7387\u68af\u5ea6\u5728Fisher\u5ea6\u91cf\u4e0b\u5f52\u4e00\u5316\u540e\u518d\u4e0e\u4f18\u52bf\u76f8 contracted\u3002\u53e6\u4e00\u79cd\u53d8\u4f53\u5728\u6bcf\u4e00\u5c42\u4f7f\u7528\u795e\u7ecf\u5f20\u91cf\u6838(NTK)\u5bf9\u5fae\u5c0f\u6279\u91cf\u7684\u4f18\u52bf\u8fdb\u884c\u53d8\u6362\uff0c\u5e76\u5728\u9010\u5c42\u8fdb\u884c\uff0c\u501f\u52a9\u5355\u6b21\u5411\u540e\u4f20\u9012\u5b9e\u73b0\uff0c\u7406\u8bba\u4e0e\u5b9e\u73b0\u4e0a\u4e0eREINFORCE\u76f8\u6bd4\u51e0\u4e4e\u65e0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u58f0\u79f0\u5728\u5355\u6b65\u68af\u5ea6\u4e0b\u5c31\u80fd\u8fd1\u4f3c\u81ea\u7136\u7b56\u7565\u68af\u5ea6\uff0c\u4e14\u65b9\u6cd5\u5bf9\u6bd4GRPO/CISPO\u7b49\u9700\u8981\u591a\u6b65\u66f4\u65b0\u7684\u65b9\u6848\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u4e14\u5728\u5b9e\u73b0\u4e0a\u5bf9\u6bd4REINFORCE\u51e0\u4e4e\u6ca1\u6709\u989d\u5916\u5f00\u9500\u3002\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u7684\u7ec6\u8282\u3002", "conclusion": "ISOPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u8fd1\u4f3c\u81ea\u7136\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b65\u68af\u5ea6\u5b9e\u73b0\uff0c\u4e14\u652f\u6301\u5c42\u7ea7\u5316\u7684NTK\u53d8\u6362\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u5347\u5b9e\u73b0\u901f\u7387\u3002"}}
{"id": "2512.23367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23367", "abs": "https://arxiv.org/abs/2512.23367", "authors": ["Yilun Luo", "HuaQing Zheng", "Haoqian Meng", "Wenyuan Liu", "Peng Zhang"], "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2", "comment": null, "summary": "Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.", "AI": {"tldr": "\u901a\u8fc7\u4f4e\u6bd4\u7279\u91cf\u5316\u5b9e\u73b0 openPangu-Embedded \u6a21\u578b\u5728\u5e26 CoT \u63a8\u7406\u4e2d\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u4e0d\u663e\u8457\u635f\u5931\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u901f\u5ea6\u548c\u964d\u4f4e\u5185\u5b58\u3002", "motivation": "\u89e3\u51b3\u4e09\u79cd CoT \u63a8\u7406\u5e26\u6765\u7684\u5185\u5b58\u4e0e\u5ef6\u8fdf\u5f00\u9500\uff0c\u4f7f big-LM/\u5d4c\u5165\u5f0f\u90e8\u7f72\u5728 Ascend NPUs \u4e0a\u5177\u5907 practicality\uff08\u5b9e\u7528\u6027\uff09\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4f4e\u6bd4\u7279\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301 INT8 (W8A8) \u4e0e W4A8 \u91cf\u5316\uff0c\u9488\u5bf9 openPangu-Embedded 1B/7B \u5728 Atlas A2 \u4e0a\u4f18\u5316\uff1b\u7ed3\u5408 slow_think\u3001auto_think\u3001no_think \u4e09\u79cd CoT \u601d\u8def\u8fdb\u884c\u8bc4\u4f30\uff0c\u57fa\u4e8e HumanEval \u4e0e MBPP \u7b49\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u3002", "result": "INT8 \u5728 FP16 \u57fa\u7ebf\u4fdd\u7559 >90% \u7cbe\u5ea6\uff0c\u5e76\u5728 Atlas A2 \u4e0a\u5b9e\u73b0\u7ea61.5x \u7684 prefill \u52a0\u901f\uff1bW4A8 \u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\uff0c\u5b58\u5728\u4e2d\u7b49\u7a0b\u5ea6\u7684\u7cbe\u5ea6\u6298\u4e2d\u3002", "conclusion": "\u4f4e\u6bd4\u7279\u91cf\u5316\u80fd\u5728 Ascend NPUs \u4e0a\u6709\u6548\u652f\u6491\u5e26 CoT \u7684\u63a8\u7406\uff0c\u4fdd\u6301\u9ad8\u6a21\u578b\u4fdd\u771f\u5ea6\u5e76\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u63a8\u52a8\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.23405", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23405", "abs": "https://arxiv.org/abs/2512.23405", "authors": ["Nathan Buskulic", "Luca Calatroni", "Lorenzo Rosasco", "Silvia Villa"], "title": "On the Sample Complexity of Learning for Blind Inverse Problems", "comment": null, "summary": "Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.", "AI": {"tldr": "\u5728\u4e00\u4e2a\u7ebf\u6027\u9ad8\u65af\u6846\u67b6\u4e0b\uff0c\u9488\u5bf9\u672a\u77e5\u524d\u5411\u7b97\u5b50\u7684\u76f2\u9006\u95ee\u9898\uff0c\u63d0\u51fa\u4ee5LMMSE\u4e3a\u6838\u5fc3\u7684\u5206\u6790\u6846\u67b6\uff0c\u7ed9\u51fa\u6700\u4f18\u4f30\u8ba1\u5668\u7684\u95ed\u5f0f\u8868\u8fbe\u5e76\u4e0e\u9002\u5f53\u7684Tikhonov\u6b63\u5219\u5316\u5f62\u5f0f\u7b49\u4ef7\uff0c\u63ed\u793a\u6b63\u5219\u5316\u9879\u5982\u4f55\u4f9d\u8d56\u4fe1\u53f7\u3001\u566a\u58f0\u548c\u968f\u673a\u524d\u5411\u7b97\u5b50\u5206\u5e03\uff1b\u5e76\u7ed9\u51fa\u6536\u655b\u6027\u3001\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\uff0c\u4ee5\u53ca\u5f53\u968f\u673a\u6027\u6d88\u5931\u65f6\u7684\u6536\u655b\u901f\u7387\uff0c\u6700\u540e\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u76f2\u9006\u95ee\u9898\u7684\u7406\u8bba\u7f3a\u53e3\uff1a\u5728\u524d\u5411\u7b97\u5b50\u672a\u77e5\u4e14\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e0e\u7406\u8bba\u4fdd\u969c\u7684\u80cc\u666f\u4e0b\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u5177\u7406\u8bba\u652f\u6491\u7684\u5206\u6790\u6846\u67b6\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "method": "\u4ee5\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u5668\uff08LMMSE\uff09\u4e3a\u6838\u5fc3\uff0c\u63a8\u5bfc\u51fa\u76f2\u9006\u95ee\u9898\u7684\u6700\u4f18\u4f30\u8ba1\u5668\u7684\u95ed\u5f0f\u89e3\uff1b\u5c06\u5176\u4e0e\u9009\u62e9\u5408\u9002\u7684\u6b63\u5219\u5316\uff08Tikhonov\uff09\u5f62\u5f0f\u7b49\u4ef7\u5316\uff0c\u6b63\u5219\u9879\u663e\u5f0f\u4f9d\u8d56\u672a\u77e5\u4fe1\u53f7\u3001\u566a\u58f0\u548c\u968f\u673a\u524d\u5411\u7b97\u5b50\u5206\u5e03\uff1b\u5728\u9002\u5f53\u7684\u6e90\u6761\u4ef6\u5047\u8bbe\u4e0b\u8bc1\u660e\u6536\u655b\u6027\uff1b\u63a8\u5bfc\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\uff0c\u523b\u753b\u566a\u58f0\u6c34\u5e73\u3001\u95ee\u9898\u6761\u4ef6\u6570\u548c\u6837\u672c\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u524d\u5411\u7b97\u5b50\u968f\u673a\u6027\u7684\u4f5c\u7528\u4e0e\u6d88\u5931\u65f6\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "\u5f97\u5230\u5173\u4e8e\u76f2\u9006\u95ee\u9898\u7684\u7406\u8bba\u7ed3\u679c\uff1a\u95ed\u5f0f\u6700\u4f18\u4f30\u8ba1\u5668\u3001\u4e0eTikhonov\u6b63\u5219\u5316\u7684\u7b49\u4ef7\u5173\u7cfb\u3001\u5728\u7ed9\u5b9a\u6e90\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\u3001\u4ee5\u53ca\u8003\u8651\u6837\u672c\u91cf\u4e0e\u566a\u58f0\u3001\u968f\u673a\u524d\u5411\u7b97\u5b50\u5206\u5e03\u7684\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u4e0e\u6536\u655b\u901f\u7387\uff1b\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\uff0c\u5e76\u5c55\u793a\u968f\u673a\u6027\u964d\u4f4e\u65f6\u6027\u80fd\u63d0\u5347\u7684\u8d8b\u52bf\u3002", "conclusion": "\u5728LMMSE\u6846\u67b6\u5185\u4e3a\u76f2\u9006\u95ee\u9898\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u91cf\u5316\u7684\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u968f\u673a\u524d\u5411\u7b97\u5b50\u5bf9\u4f30\u8ba1\u6027\u80fd\u7684\u5f71\u54cd\u53ca\u5176\u968f\u6837\u672c\u6570\u548c\u566a\u58f0\u6c34\u5e73\u7684\u6539\u5584\u8d8b\u52bf\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u4f46\u9700\u7406\u8bba\u4fdd\u969c\u7684\u65b9\u6cd5\u63d0\u4f9b\u91cd\u8981\u7684\u7406\u8bba\u652f\u6491\u4e0e\u6307\u5f15\u3002"}}
{"id": "2512.23406", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23406", "abs": "https://arxiv.org/abs/2512.23406", "authors": ["Ayushman Raghuvanshi", "Gonzalo Mateos", "Sundeep Prabhakar Chepuri"], "title": "Task-driven Heterophilic Graph Structure Learning", "comment": null, "summary": "Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.", "AI": {"tldr": "FgGSL\u901a\u8fc7\u5b66\u4e60\u5bf9\u79f0\u7684\u7279\u5f81\u9a71\u52a8\u63a9\u853d\uff0c\u8054\u5408\u5b66\u4e60\u540c\u8d28\u6027\u4e0e\u5f02\u8d28\u6027\u56fe\u7ed3\u6784\uff0c\u5e76\u914d\u5408\u8c31\u7f16\u7801\u5668\uff0c\u5229\u7528\u4f4e/\u9ad8\u901a\u8fc7\u6ee4\u5668\u5e26\u6765\u4e92\u8865\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u5f02\u8d28\u56fe\u4e0a\u7684\u8282\u70b9\u8868\u5f81\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5728\u5f02\u8d28\u6027\u56fe\u4e0a\uff0cGNN\u96be\u4ee5\u5b66\u4e60\u5177\u8fa8\u522b\u6027\u7684\u8282\u70b9\u8868\u793a\uff0c\u56e0\u4e3a\u8fde\u63a5\u7684\u8282\u70b9\u5f80\u5f80\u6807\u7b7e\u4e0d\u540c\u4e14\u7279\u5f81\u76f8\u4f3c\u6027\u63d0\u4f9b\u7684\u7ed3\u6784\u7ebf\u7d22\u8584\u5f31\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7ed3\u6784\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u5b66\u4e60\u6709\u5229\u4e8e\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5173\u7cfb\u7684\u56fe\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u9891\u57df\u4fe1\u606f\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u9891\u7387\u5f15\u5bfc\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\uff08FgGSL\uff09\uff0c\u901a\u8fc7\u4e00\u4e2a\u53ef\u5b66\u4e60\u3001\u5bf9\u79f0\u4e14\u57fa\u4e8e\u7279\u5f81\u7684\u63a9\u853d\u51fd\u6570\u6765\u63a8\u65ad\u4e24\u5f20\u4e92\u8865\u7684\u56fe\uff08\u540c\u8d28\u6027\u4e0e\u5f02\u8d28\u6027\uff09\uff0c\u5e76\u4f7f\u7528\u9884\u8bbe\u7684\u4f4e\u901a\u548c\u9ad8\u901a\u56fe\u6ee4\u6ce2\u5668\u5e26\u8fdb\u884c\u5904\u7406\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u6807\u7b7e\u7684\u7ed3\u6784\u635f\u5931\uff0c\u663e\u5f0f\u4fc3\u8fdb\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u8fb9\u7684\u6062\u590d\u3002\u7406\u8bba\u4e0a\u7ed9\u51fa\u7ed3\u6784\u635f\u5931\u7684\u7a33\u5b9a\u6027\u754c\uff0c\u4ee5\u53ca\u5728\u56fe\u6270\u52a8\u4e0b\u5bf9\u6ee4\u6ce2\u5668\u5e26\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "result": "\u5728\u516d\u4e2a\u5f02\u8d28\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cFgGSL\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GNN\u548c\u56fe\u91cd\u8fde\u65b9\u6cd5\uff0c\u4f53\u73b0\u4e86\u5c06\u9891\u7387\u4fe1\u606f\u4e0e\u6709\u76d1\u7763\u7684\u62d3\u6251\u63a8\u65ad\u76f8\u7ed3\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "\u5c06\u9891\u57df\u4fe1\u606f\u4e0e\u76d1\u7763\u7684\u62d3\u6251\u5b66\u4e60\u7ed3\u5408\uff0c\u53ef\u5728\u5f02\u8d28\u56fe\u4e0a\u83b7\u5f97\u66f4\u5177\u8fa8\u522b\u6027\u7684\u8282\u70b9\u8868\u5f81\uff0c\u5e76\u7ed9\u51fa\u7aef\u5230\u7aef\u3001\u9c81\u68d2\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2512.23407", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23407", "abs": "https://arxiv.org/abs/2512.23407", "authors": ["Huan Song", "Qingfei Zhao", "Ting Long", "Shuyu Tian", "Hongjun An", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Theoretical Foundations of Scaling Law in Familial Models", "comment": null, "summary": "Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks \"Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this \"one-run, many-models\" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the \"train once, deploy many\" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.", "AI": {"tldr": "Extends neural scaling laws to Familial models with multiple sub-models via granularity G, proposing L(N, D, G) and IsoFLOP design to quantify the trade-off between model size, data, and granularity under fixed compute; finds a small-exponent granularity penalty that enables train-once, deploy-many paradigms.", "motivation": "Current scaling laws assume a single dense model output, which cannot capture Familial models with early exits and relay-style inference. There is a need to generalize scaling to a one-run, many-models setting to enable deployable sub-models across heterogeneous compute hierarchies.", "method": "Introduce Granularity G as a fundamental scaling variable alongside model size N and training tokens D. Propose a unified functional form L(N, D, G) parametric in G, D, N. Employ IsoFLOP experimental design to isolate architectural impact from computational scale. Systematically sweep N and G under fixed budgets while dynamically adjusting D to decouple granularity cost from scaling benefits.", "result": "Empirical results indicate the granularity penalty follows a multiplicative power law with an extremely small exponent, tying fixed-compute training to dynamic architectures. This supports train-once, deploy-many by preserving compute-optimality of dense baselines while enabling flexible granular sub-model deployment.", "conclusion": "The study theoretically and empirically extends scaling laws to the one-run, many-models paradigm, validating the train-once deploy-many approach and providing a quantitative framework (L(N,D,G)) for predicting performance under granularity-aware architectures."}}
{"id": "2512.23441", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23441", "abs": "https://arxiv.org/abs/2512.23441", "authors": ["Taha Emre", "Arunava Chakravarty", "Thomas Pinetz", "Dmitrii Lachinov", "Martin J. Menten", "Hendrik Scholl", "Sobha Sivaprasad", "Daniel Rueckert", "Andrew Lotery", "Stefan Sacu", "Ursula Schmidt-Erfurth", "Hrvoje Bogunovi\u0107"], "title": "Stochastic Siamese MAE Pretraining for Longitudinal Medical Images", "comment": "Under review. Code is available in https://github.com/EmreTaha/STAMP", "summary": "Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.", "AI": {"tldr": "STAMP\u901a\u8fc7\u63a9\u853d\u81ea\u7f16\u7801\u4e0e\u968f\u673a\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u5728\u65f6\u5e8f\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u4e2d\u5b66\u4e60\u975e\u786e\u5b9a\u6027\u75be\u75c5\u8fdb\u5c55\u7684\u8868\u5f81\uff0c\u8d85\u8fc7\u73b0\u6709 temporal MAE \u548c\u57fa\u7840\u6a21\u578b\u5728AMD/AD\u8fdb\u5c55\u9884\u6d4b\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u9700\u8981\u57283D\u7eb5\u5411\u533b\u5b66\u6570\u636e\u4e2d\u83b7\u5f97\u5bf9\u75be\u75c5\u8fdb\u5c55\u7684\u65f6\u5e8f\u611f\u77e5\u8868\u793a\uff1b\u73b0\u6709\u7684\u786e\u5b9a\u6027Siamese MAE\u672a\u80fd\u5efa\u6a21\u75be\u75c5\u6f14\u5316\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa STAMP\uff08Siamese MAE + Masked Pretraining\uff09\uff0c\u5c06\u65f6\u95f4\u5dee\u4fe1\u606f\u4f5c\u4e3a\u6761\u4ef6\u53d8\u91cf\u5bf9\u4e24\u4e2a\u8f93\u5165\u4f53\u79ef\u8fdb\u884c\u7f16\u7801\uff0c\u901a\u8fc7\u5c06 MAE \u91cd\u6784\u635f\u5931\u6539\u5199\u4e3a\u6761\u4ef6\u53d8\u5206\u63a8\u65ad\u76ee\u6807\uff0c\u4ee5\u968f\u673a\u8fc7\u7a0b\u6765\u5b66\u4e60\u65f6\u5e8f\u52a8\u6001\u3002\u4ee5ViT\u4e3a backbone\uff0c\u5728\u4e24\u4e2aOCT\u6570\u636e\u96c6\u548c\u4e00\u4efdMRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728\u5305\u542b\u591a\u6b21\u5c31\u8bca\u7684\u4e24\u7ec4OCT\u6570\u636e\u548c\u4e00\u7ec4MRI\u6570\u636e\u4e0a\u8bc4\u4f30\uff0cSTAMP\u7684\u9884\u8bad\u7ec3ViT\u6a21\u578b\u5728AMD\u548cAD\u8fdb\u5c55\u9884\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u65f6\u5e8fMAE\u65b9\u6cd5\u4e0e\u57fa\u7840\u6a21\u578b\uff0c\u5c24\u5176\u5728\u9700\u8981\u6355\u6349\u975e\u786e\u5b9a\u6027\u7684\u65f6\u5e8f\u52a8\u6001\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u5c06\u968f\u673a\u6027\u6574\u5408\u5165 MAE \u6846\u67b6\u7684\u65f6\u5e8f\u5efa\u6a21\u53ef\u663e\u8457\u63d0\u5347\u957f\u671f\u75c5\u7a0b\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9700\u8981\u7406\u89e3\u75be\u75c5\u975e\u786e\u5b9a\u6027\u65f6\u95f4\u6f14\u5316\u7684\u573a\u666f\uff08\u5982AMD\u3001AD\uff09\u3002"}}
{"id": "2512.23448", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23448", "abs": "https://arxiv.org/abs/2512.23448", "authors": ["Vladimer Khasia"], "title": "Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion", "comment": null, "summary": "Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC", "AI": {"tldr": "DSC\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684MoE\u53d8\u4f53\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u57fa\u5411\u91cf\u5e93\u4e0a\u8fdb\u884c\u72b6\u6001\u76f8\u5173\u7684\u7a00\u758f\u6269\u5c55\u6765\u8fd1\u4f3c\u4e0a\u4e0b\u6587\u6743\u91cd\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u663e\u5b58\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u661f\u5f62\u57df\u548c\u5e45\u5ea6\u95e8\u63a7\u7684\u5355\u7eaf\u5f62\u63d2\u503c\u5b9e\u73b0\u4ece\u5355\u4f4d\u8eab\u4efd\u7684\u8fde\u7eed\u8fc7\u6e21\uff0c\u540c\u65f6\u7ed9\u51fa\u5e27\u7406\u8bba\u6b63\u5219\u5316\u548c\u8c31\u7ea6\u675f\u7684\u6700\u574f\u60c5\u51b5\u754c\u9650\u3002", "motivation": "\u73b0\u6709MoE\u4e0eMixture-of-LoRAs\u5728\u6269\u5c55\u80fd\u529b\u4e0a\u5b58\u5728\u8868\u5f81\u5d29\u584c\u548c\u68af\u5ea6\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff1b\u76f4\u63a5\u589e\u52a0\u7ec4\u4ef6\u5bfc\u81f4\u53c2\u6570\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5e26\u5bbd\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u6743\u91cd\u673a\u5236\u3002", "method": "\u63d0\u51faDynamic Subspace Composition (DSC)\uff1a\u5728\u5171\u4eab\u57fa\u5411\u91cf\u5e93\u4e0a\u5b9e\u73b0\u72b6\u6001\u76f8\u5173\u7684\u7a00\u758f\u6269\u5c55\uff0c\u901a\u8fc7\u6b8b\u5dee\u8f68\u8ff9\u5728\u661f\u5f62\u57df\u4e2d\u5efa\u6a21\u6743\u91cd\u66f4\u65b0\uff1b\u4f7f\u7528\u5e45\u5ea6\u95e8\u63a7\u7684\u7b80\u5355\u5f62\u63d2\u503c\u786e\u4fdd\u4ece\u5355\u4f4d\u8eab\u4efd\u7684\u8fde\u7eed\u8fc7\u6e21\uff1b\u4e0e\u6807\u51c6\u7684MoE-LO\u6bb4\u76f8\u6bd4\uff0cDSC\u7528\u89e3\u8026\u7684\u5355\u4f4d\u8303\u5411\u91cf\u6784\u6210\u7684\u79e9-K\u7ec4\u5408\u8fd1\u4f3c\uff0c\u964d\u4f4e\u53c2\u6570\u590d\u6742\u5ea6\u81f3O(M d)\uff0c\u5185\u5b58\u5e26\u5bbd\u81f3O(K d)\uff1b\u901a\u8fc7\u5e27\u7406\u8bba\u6b63\u5219\u5316\u548c\u8c31\u7ea6\u675f\u63d0\u4f9b\u5bf9\u52a8\u6001\u66f4\u65b0\u7684\u4e25\u683c worst-case \u8fb9\u754c\u3002", "result": "\u7406\u8bba\u5c42\u9762\u7ed9\u51fa\u53c2\u6570\u548c\u5185\u5b58\u590d\u6742\u5ea6\u51cf\u5c0f\uff0c\u4ee5\u53ca\u5bf9\u52a8\u6001\u66f4\u65b0\u7684\u8fb9\u754c\u7ea6\u675f\uff1b\u4ee3\u7801\u516c\u5f00\u5728GitHub\uff0c\u4fbf\u4e8e\u590d\u73b0\u4e0e\u6269\u5c55\u3002", "conclusion": "DSC\u901a\u8fc7\u5728\u5171\u4eab\u57fa\u5411\u91cf\u7684\u7a00\u758f\u7ec4\u5408\u548c\u7a33\u5065\u7684\u51e0\u4f55-\u8c31\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u4e0a\u4e0b\u6587\u6743\u91cd\u66f4\u65b0\uff0c\u7f13\u89e3\u4e86MoE\u4e2d\u7684\u8868\u793a\u5d29\u584c\u548c\u68af\u5ea6\u4e0d\u7a33\u95ee\u9898\u3002"}}
{"id": "2512.23461", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23461", "abs": "https://arxiv.org/abs/2512.23461", "authors": ["Zhuo Li", "Pengyu Cheng", "Zhechao Yu", "Feifei Tong", "Anningzhe Gao", "Tsung-Hui Chang", "Xiang Wan", "Erchao Zhao", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance", "comment": null, "summary": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \\textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \\textbf{D}ebiasing via \\textbf{I}nformation optimization for \\textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \\textit{response length}, \\textit{sycophancy}, and \\textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.", "AI": {"tldr": "\u63d0\u51fa DIR\uff08Debiasing via Information optimization for RM\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u7684 RM \u53bb\u504f\u65b9\u6cd5\u3002\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u601d\u60f3\uff0c\u5728 RLHF \u573a\u666f\u4e0b\u6700\u5927\u5316 RM \u5206\u6570\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u540c\u65f6\u6700\u5c0f\u5316 RM \u8f93\u51fa\u4e0e\u504f\u7f6e\u5c5e\u6027\uff08\u5982\u504f\u597d\u8f93\u5165\u7684\u957f\u5ea6\u3001\u5949\u627f\u98ce\u683c\u3001\u683c\u5f0f\u7b49\uff09\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u4ece\u800c\u5904\u7406\u66f4\u590d\u6742\u3001\u975e\u7ebf\u6027\u7684\u504f\u5dee\uff0c\u63d0\u5347 RM \u7684\u9c81\u68d2\u6027\u4e0e RLHF \u6027\u80fd\u3002", "motivation": " reward models \u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u666e\u904d\u8f83\u5dee\uff0c\u5b58\u5728\u6613\u8fc7\u62df\u5408\u3001\u5956\u52b1\u52ab\u6301\u7b49 inductive biases\u3002\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u591a\u9488\u5bf9\u5355\u4e00\u504f\u5dee\u6216\u4ec5\u5efa\u6a21\u7ebf\u6027\u76f8\u5173\uff08\u5982 Pearson \u7cfb\u6570\uff09\uff0c\u96be\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u504f\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e7f\u6cdb\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3001\u80fd\u5904\u7406\u590d\u6742\u504f\u5dee\u7684\u53bb\u504f\u65b9\u6cd5\u3002", "method": "\u672c\u5de5\u4f5c\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u53bb\u504f\u65b9\u6cd5 DIR\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u4fe1\u606f\u4f18\u5316\uff1a\u6700\u5927\u5316 RM \u5f97\u5206\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f I(R; P)\uff08\u6216 I(S; P) \u53d6\u51b3\u4e8e\u7b26\u53f7\u5b9a\u4e49\uff09\uff0c\u540c\u65f6\u6700\u5c0f\u5316 RM \u8f93\u51fa\u4e0e\u504f\u7f6e\u5c5e\u6027 A \u4e4b\u95f4\u7684\u4e92\u4fe1\u606f I(S; A)\uff0c\u4ee5\u6291\u5236\u7531\u504f\u7f6e\u5c5e\u6027\u5f15\u5165\u7684\u4fe1\u53f7\u3002\u7406\u8bba\u4e0a\u63d0\u4f9b\u4fe1\u606f\u8bba\u652f\u6491\uff0c\u80fd\u5904\u7406\u975e\u7ebf\u6027\u76f8\u5173\u7684\u590d\u6742\u504f\u5dee\u3002\u5b9e\u73b0\u4e0a\u53ef\u80fd\u5305\u62ec\u5bf9\u504f\u7f6e\u5c5e\u6027\u7684\u5b9a\u4e49\u3001\u53ef\u5fae\u4f30\u8ba1\u4e92\u4fe1\u606f\u3001\u4ee5\u53ca\u5728 RM \u8bad\u7ec3\u76ee\u6807\u4e2d\u5f15\u5165\u4fe1\u606f\u7ea6\u675f\u9879\u7b49\u3002", "result": "\u5728\u4e09\u7c7b\u504f\u89c1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1a\u54cd\u5e94\u957f\u5ea6\u3001\u5949\u627f\uff08sycophancy\uff09\u3001\u683c\u5f0f\u3002DIR \u80fd\u6709\u6548\u51cf\u5f31\u76ee\u6807\u504f\u89c1\uff0c\u5e76\u63d0\u5347 RLHF \u7684\u6574\u4f53\u8868\u73b0\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u8bad\u7ec3\u65b9\u6848\u516c\u5f00\u5728 GitHub\u3002", "conclusion": "\u5c06\u4fe1\u606f\u74f6\u9888\u601d\u60f3\u5f15\u5165 RM \u53bb\u504f\uff0cDIR \u80fd\u8986\u76d6\u66f4\u590d\u6742\u7684\u504f\u5dee\u7c7b\u578b\uff08\u975e\u7ebf\u6027\u76f8\u5173\uff09\uff0c\u62d3\u5c55 RM \u53bb\u504f\u7684\u5e94\u7528\u573a\u666f\uff0c\u5e76\u63d0\u5347 RLHF \u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.23485", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23485", "abs": "https://arxiv.org/abs/2512.23485", "authors": ["Guoan Wan", "Tianyu Chen", "Fangzheng Feng", "Haoyi Zhou", "Runhua Xu"], "title": "FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence", "comment": "The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.", "AI": {"tldr": "FRoD\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u7ea7\u8054\u5408\u5206\u89e3\u7ed3\u5408\u65cb\u8f6c\u81ea\u7531\u5ea6\uff0c\u4f7f\u7528\u5168\u5c40\u5171\u4eab\u57fa\u5e95\u548c\u7a00\u758f\u6270\u52a8\u5b9e\u73b0\u63a5\u8fd1\u5168\u91cf\u5fae\u8c03\u7684\u8868\u8fbe\u529b\uff0c\u4e14\u4ec5\u97001.72%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u572820\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u76f8\u540c\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709PEFT\u5982LoRA\u56e0\u4f4e\u79e9\u7ea6\u675f\u5bfc\u81f4\u6536\u655b\u6162\u548c\u9002\u5e94\u80fd\u529b\u53d7\u9650\uff0c\u6025\u9700\u63d0\u9ad8\u8868\u8fbe\u529b\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "method": "\u91c7\u7528\u5168\u5c40\u5171\u4eab\u57fa\u5e95\u7684\u5c42\u7ea7\u8054\u5408\u5206\u89e3\uff0c\u5728\u7f29\u653e\u56e0\u5b50\u5c42\u6ce8\u5165\u7a00\u758f\u53ef\u5b66\u4e60\u6270\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u8fd1\u4f3c\u5168\u79e9\u66f4\u65b0\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u3002", "result": "\u5728\u6db5\u76d6\u89c6\u89c9\u3001\u63a8\u7406\u548c\u8bed\u8a00\u7406\u89e3\u768420\u4e2a\u57fa\u51c6\u4e0a\uff0cFRoD\u8fbe\u5230\u4e0e\u5168\u91cf\u5fae\u8c03\u76f8\u540c\u7684\u51c6\u786e\u7387\uff0c\u4e14\u8bad\u7ec3\u9884\u7b97\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\u4ec5\u97001.72%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "FRoD\u63d0\u5347\u4e86PEFT\u7684\u8868\u8fbe\u529b\u4e0e\u6548\u7387\uff0c\u5b9e\u73b0\u66f4\u5feb\u66f4\u9c81\u68d2\u7684\u6536\u655b\uff0c\u517c\u5177\u9ad8\u6548\u4e0e\u5168\u79e9\u66f4\u65b0\u80fd\u529b\u3002"}}
{"id": "2512.23493", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23493", "abs": "https://arxiv.org/abs/2512.23493", "authors": ["Wei Gao", "Paul Zheng", "Peng Wu", "Yulin Hu", "Anke Schmeink"], "title": "Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization", "comment": "16 page,10 figures", "summary": "In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.", "AI": {"tldr": "\u5728CSI\u4e0d\u5b8c\u7f8e\u7684IIoT\u591a\u8bbe\u5907URLLC\u573a\u666f\u4e0b\uff0c\u63d0\u51fa\u4e00\u4e2a\u8054\u5408\u94fe\u8def\u81ea\u9002\u5e94\u4e0e\u8bbe\u5907\u8c03\u5ea6\uff08\u542b\u987a\u5e8f\uff09\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408BO\u9a71\u52a8\u7684TD3\u8fdb\u884c\u81ea\u9002\u5e94MCS\u4e0e\u670d\u52a1\u987a\u5e8f\u9009\u62e9\uff0c\u4ee5\u63d0\u9ad8\u541e\u5410\u4e0e\u6ee1\u8db3\u4e25\u683cBLER\uff0c\u4e14\u52a0\u5165\u57fa\u4e8eBO\u7684\u8bad\u7ec3\u673a\u5236\u4ee5\u6539\u5584\u6536\u655b\u548c\u5e94\u5bf9\u6837\u672c\u4e0d\u5e73\u8861\u3002", "motivation": "URRLC \u573a\u666f\u4e2dCSI\u4e0d\u5b8c\u7f8e\u3001\u6837\u672c\u4e0d\u5e73\u8861\u53ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5bf9\u53c2\u6570\u654f\u611f\u6027\u4f1a\u635f\u5bb3\u6536\u655b\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u9700\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5b66\u4e60\u65b9\u6cd5\u6765\u5b9e\u73b0\u9ad8\u541e\u5410\u4e0e\u4e25\u683cBLER\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aBO\u9a71\u52a8\u7684TD3\u6846\u67b6\uff0c\u8054\u5408\u8bbe\u5907\u7684\u670d\u52a1\u987a\u5e8f\u4e0eMCS\u9009\u62e9\uff0c\u5e76\u5728\u4e0d\u5b8c\u7f8eCSI\u6761\u4ef6\u4e0b\u8fdb\u884cLA\u4e0e\u8c03\u5ea6\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6536\u655b\u901f\u5ea6\u5e76\u7f13\u89e3\u6837\u672c\u5931\u8861\u95ee\u9898\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u4e0e\u603b\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9CSI\u4e0d\u5b8c\u7f8e\u4e0e\u6837\u672c\u4e0d\u5e73\u8861\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "BO\u9a71\u52a8\u7684TD3\u65b9\u6cd5\u53ef\u5728IIoT URLLC\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6548\u7387\u4e0e\u7cfb\u7edf\u541e\u5410\uff0c\u540c\u65f6\u66f4\u597d\u5730\u6ee1\u8db3\u4e25\u683cBLER\u7ea6\u675f\uff0c\u9002\u7528\u4e8e\u8054\u5408LA\u4e0e\u8bbe\u5907\u8c03\u5ea6\u7684\u573a\u666f\u3002"}}
{"id": "2512.23524", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23524", "abs": "https://arxiv.org/abs/2512.23524", "authors": ["Zhuo Huang"], "title": "Trustworthy Machine Learning under Distribution Shifts", "comment": "PhD Thesis", "summary": "Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \\textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.", "AI": {"tldr": "\u805a\u7126\u5206\u5e03\u79fb\u4f4d\u4e0b\u7684\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\uff0c\u5206\u6790\u6270\u52a8\u79fb\u4f4d\u3001\u9886\u57df\u79fb\u4f4d\u548c\u6a21\u6001\u79fb\u4f4d\uff0c\u5e76\u5728\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u9002\u5e94\u6027\u4e09\u4e2a\u7ef4\u5ea6\u63d0\u51fa\u6846\u67b6\u4e0e\u65b9\u6cd5\u3002", "motivation": "\u5206\u5e03\u79fb\u4f4d\u662fML\u7cfb\u7edf\u53ef\u9760\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u5347\u4fe1\u4efb\u5ea6\u5bf9\u793e\u4f1a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff1b\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u4e09\u7c7b\u79fb\u4f4d\u548c\u4e09\u5927\u4fe1\u4efb\u7ef4\u5ea6\uff0c\u65e8\u5728\u63d0\u9ad8AI\u7684\u9c81\u68d2\u6027\u3001\u901a\u7528\u6027\u4e0e\u8d23\u4efb\u6027\u3002", "method": "\u56f4\u7ed5\u4e09\u7c7b\u79fb\u4f4d\u573a\u666f\uff08\u6270\u52a8\u3001\u9886\u57df\u3001\u6a21\u6001\uff09\uff0c\u5728\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u9002\u5e94\u6027\u4e09\u4e2a\u7ef4\u5ea6\u5f00\u5c55\u7406\u8bba\u5206\u6790\u4e0e\u65b9\u6cd5\u7814\u7a76\uff0c\u63d0\u51fa\u76f8\u5e94\u89e3\u51b3\u65b9\u6848\u4e0e\u57fa\u672c\u6d1e\u89c1\uff0c\u5e76\u5173\u6ce8\u6548\u7387\u3001\u9002\u5e94\u6027\u4e0e\u5b89\u5168\u6027\u3002", "result": "\u539f\u6587\u6458\u8981\u672a\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u6216\u5b9a\u91cf\u7ed3\u8bba\uff0c\u9884\u8ba1\u5c06\u7ed9\u51fa\u65b9\u5411\u6027\u7ed3\u8bba\u3001\u6846\u67b6\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u6807\u51c6\uff0c\u4f9b\u540e\u7eed\u9a8c\u8bc1\u3002", "conclusion": "\u5728\u5206\u5e03\u79fb\u4f4d\u73af\u5883\u4e0b\u6784\u5efa\u66f4\u53ef\u4fe1\u7684\u673a\u5668\u5b66\u4e60\u4f53\u7cfb\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u5173\u6ce8\u9ad8\u6548\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u4e3aAI\u5728\u793e\u4f1a\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2512.23526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23526", "abs": "https://arxiv.org/abs/2512.23526", "authors": ["Maryam Mirzaei", "Farzaneh Shayegh", "Hamed Narimani"], "title": "EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition", "comment": "10 pages, 7 figures", "summary": "Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.", "AI": {"tldr": "\u63d0\u51faEGDA\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\u4ee5\u53ca\u56fe\u6b63\u5219\u5316\uff0c\u63d0\u5347\u8de8\u4f1a\u8bddEEG\u60c5\u611f\u8bc6\u522b\u9c81\u68d2\u6027\uff1b\u5728SEED-IV\u4e0a\u5b9e\u73b0\u7ea681\u201383%\u51c6\u786e\u7387\uff0c\u5e76\u6307\u660eGamma\u5e26\u53ca\u5173\u952e\u8111\u533a\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8de8\u8bb0\u5f55\u4f1a\u8bdd\u7684\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u56f0\u96be\uff1b\u9700\u8981\u540c\u65f6\u5bf9\u9f50\u8fb9\u9645\u4e0e\u6761\u4ef6\u5206\u5e03\u5e76\u4fdd\u6301\u6570\u636e\u7684\u5185\u5728\u7ed3\u6784\u3002", "method": "EGDA\u6846\u67b6\uff1a\u8054\u5408\u5bf9\u9f50\u5168\u5c40\uff08\u8fb9\u9645\uff09\u4e0e\u7c7b\u6761\u4ef6\u5206\u5e03\uff0c\u540c\u65f6\u5f15\u5165\u56fe\u6b63\u5219\u5316\u4ee5\u4fdd\u7559EEG\u6570\u636e\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "result": "\u5728SEED-IV\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u4f1a\u8bdd\u8f6c\u79fb\u4efb\u52a1\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a81.22%\u300180.15%\u300183.27%\uff0c\u4f18\u4e8e\u82e5\u5e72\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Gamma\u9891\u6bb5\u6700\u5177\u8fa8\u8bc6\u529b\uff0c\u4e2d\u592e-\u9876\u53f6\u4e0e\u524d\u989d\u53f6\u533a\u57df\u5bf9\u60c5\u611f\u8bc6\u522b\u5177\u6709\u5173\u952e\u4f5c\u7528\uff1b\u6240\u63d0\u65b9\u6cd5\u63d0\u5347\u8de8\u4f1a\u8bdd\u9c81\u68d2\u6027\u5e76\u63d0\u4f9b\u5bf9\u8111\u4fe1\u53f7\u7684\u89e3\u91ca\u6027\u5206\u6790\u3002"}}
{"id": "2512.23562", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23562", "abs": "https://arxiv.org/abs/2512.23562", "authors": ["Zhehao Huang", "Baijiong Lin", "Jingyuan Zhang", "Jingying Wang", "Yuhang Liu", "Ning Lu", "Tao Li", "Xiaolin Huang"], "title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing", "comment": null, "summary": "Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.", "AI": {"tldr": "VL-RouterBench \u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u6027\u3001\u53ef\u590d\u73b0\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8def\u7531\u57fa\u51c6\uff0c\u8986\u76d6\u5927\u89c4\u6a21\u6570\u636e\u4e0e\u591a\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u6210\u672c\u4e0e\u51c6\u786e\u5ea6\u7684\u8c03\u548c\u5e73\u5747\u6392\u540d\uff0c\u63ed\u793a\u73b0\u6709\u8def\u7531\u5668\u4e0e\u7406\u60f3Oracle\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8ba1\u5212\u5f00\u6e90\u6570\u636e\u4e0e\u5de5\u5177\u94fe\u4ee5\u4fc3\u8fdb\u7814\u7a76\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u7cfb\u7edf\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u6d4b\u57fa\u51c6\u6765\u8bc4\u4ef7vision-language\u6a21\u578b\u7684\u8def\u7531\u80fd\u529b\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u8def\u7531\u65b9\u6848\u4e4b\u95f4\u8fdb\u884c\u516c\u5e73\u3001\u53ef\u6bd4\u7684\u6bd4\u8f83\u3002", "method": "\u6784\u5efaVL-RouterBench\uff1a\u57fa\u4e8eVLM\u7684\u539f\u59cb\u63a8\u7406\u65e5\u5fd7\u4e0e\u8bc4\u5206\u65e5\u5fd7\uff0c\u5efa\u7acb\u6837\u672c-\u6a21\u578b\u5bf9\u7684\u8d28\u91cf\u4e0e\u6210\u672c\u77e9\u9635\uff1b\u8986\u76d614\u4e2a\u6570\u636e\u96c6\u30013\u7c7b\u4efb\u52a1\u7ec4\uff0c\u517130,540\u4e2a\u6837\u672c\u300115\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0e2\u4e2aAPI\u6a21\u578b\u3001\u603b\u8ba1519,180\u4e2a\u6837\u672c\u6a21\u578b\u5bf9\u4e0e\u7ea63,449\u4e07token\u8f93\u5165\u8f93\u51fa\u91cf\uff1b\u8bc4\u6d4b\u534f\u8bae\u540c\u65f6\u8861\u91cf\u5e73\u5747\u51c6\u786e\u5ea6\u3001\u5e73\u5747\u6210\u672c\u548c\u541e\u5410\u91cf\uff0c\u5e76\u4f7f\u7528\u7ecf\u5f52\u4e00\u5316\u7684\u6210\u672c\u4e0e\u51c6\u786e\u5ea6\u7684\u8c03\u548c\u5747\u503c\u6765\u6784\u5efa\u6392\u5e8f\u5206\u6570\uff1b\u8bc4\u4f3010\u79cd\u8def\u7531\u65b9\u6cd5\u53ca\u57fa\u7ebf\u3002", "result": "\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8def\u7531\u80fd\u529b\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u8def\u7531\u5668\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u7406\u60f3Oracle\uff0c\u63d0\u793a\u5728\u8def\u7531\u5668\u7ed3\u6784\u4e0a\u8fd8\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u66f4\u7ec6\u7684\u89c6\u89c9\u4fe1\u53f7\u4e0e\u6587\u672c\u7ed3\u6784\u5efa\u6a21\u65b9\u9762\u3002", "conclusion": "\u5c06\u5f00\u6e90\u5b8c\u6574\u7684\u6570\u636e\u6784\u5efa\u8fc7\u7a0b\u548c\u8bc4\u6d4b\u5de5\u5177\u94fe\uff0c\u4fc3\u8fdb\u53ef\u6bd4\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u5728\u591a\u6a21\u6001\u8def\u7531\u7814\u7a76\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.23602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23602", "abs": "https://arxiv.org/abs/2512.23602", "authors": ["Christopher Burger"], "title": "Distribution-Free Process Monitoring with Conformal Prediction", "comment": "9 pages, 4 figures", "summary": "Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.", "AI": {"tldr": "\u5c06\u65e0\u5206\u5e03\u5047\u8bbe\u7684Conformal Prediction\u4e0e\u4f20\u7edf\u63a7\u5236\u56fe\u7ed3\u5408\uff0c\u63d0\u51faConformal-Enhanced\u63a7\u5236\u56fe\u4e0eConformal-Enhanced\u8fc7\u7a0b\u76d1\u63a7\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\uff08SPC\uff09\u4f9d\u8d56\u82e5\u5e72\u7edf\u8ba1\u5047\u8bbe\uff0c\u5f80\u5f80\u5728\u73b0\u4ee3\u590d\u6742\u5236\u9020\u73af\u5883\u4e2d\u88ab\u8fdd\u53cd\uff0c\u5bfc\u81f4\u76d1\u63a7\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u5206\u5e03\u81ea\u7531\u3001\u6a21\u578b\u65e0\u5173\u4e14\u6613\u4e8e\u89e3\u91ca\u7684\u65b0\u6846\u67b6\u6765\u63d0\u5347\u8d28\u91cf\u63a7\u5236\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728SPC\u6846\u67b6\u4e2d\u5d4c\u5165Conformal Prediction\u7684\u5206\u5e03\u81ea\u7531\u3001\u6a21\u578b\u65e0\u5173\u4fdd\u969c\uff0c\u63d0\u51fa\u4e24\u79cd\u5e94\u7528\uff1a1) Conformal-Enhanced Control Charts\uff0c\u901a\u8fc7\u9884\u6d4b\u533a\u95f4\u4e0e\u4e0d\u786e\u5b9a\u6027\u5c16\u5cf0\u53ef\u89c6\u5316\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u5e76\u53d1\u51fa\u4e3b\u52a8\u4fe1\u53f7\uff1b2) Conformal-Enhanced Process Monitoring\uff0c\u5c06\u591a\u53d8\u91cf\u63a7\u5236\u95ee\u9898\u8f6c\u5316\u4e3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u5229\u7528p\u503c\u56fe\u8fdb\u884c\u76f4\u89c2\u76d1\u63a7\u3002", "result": "\u6846\u67b6\u5728\u7406\u8bba\u4e0a\u63d0\u4f9b\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u4e0e\u7edf\u8ba1\u4e25\u683c\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6613\u7528\u6027\uff0c\u9002\u5408\u5728\u590d\u6742\u5236\u9020\u73af\u5883\u4e2d\u63d0\u9ad8\u76d1\u63a7\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06Conformal Prediction\u6574\u5408\u5230SPC\u4e2d\uff0c\u672c\u6587\u63d0\u51fa\u7684\u4e24\u79cd\u5e94\u7528\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u4e14\u6613\u4e8e\u89e3\u8bfb\u7684\u8d28\u91cf\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u671b\u63d0\u5347\u5b9e\u9645\u751f\u4ea7\u8fc7\u7a0b\u7684\u76d1\u63a7\u6548\u679c\u3002"}}
{"id": "2512.23617", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23617", "abs": "https://arxiv.org/abs/2512.23617", "authors": ["Deniz Akdemir"], "title": "Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning", "comment": null, "summary": "Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing \"negative transfer\" that can be catastrophic in safety-critical applications [Wang et al., 2019].\n  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $\u03b4(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66ff\u4ee3\u5bf9\u79f0\u4e0d\u53d8\u6027\uff08UDA\uff09\u7684\u65b9\u6cd5\uff1a\u901a\u8fc7\u52d2\u5361\u59c6\uff08Le Cam\uff09\u7406\u8bba\u7684\u65b9\u5411\u6027\u53ef\u6a21\u62df\u6027\u6765\u5b9e\u73b0\u6709\u98ce\u9669\u8f6c\u79fb\u7684\u53ef\u63a7\u6027\uff0c\u7528Deficiency Distance\u8861\u91cf\u8fc1\u79fb\u98ce\u9669\uff0c\u907f\u514d\u56e0\u4fe1\u606f\u4e0d\u5bf9\u79f0\u5bfc\u81f4\u7684\u8d1f\u8fc1\u79fb\u3002\u5e76\u7ed9\u51fa\u4e00\u4e2a\u901a\u8fc7\u5b66\u4e60\u6e90\u5230\u9776\u7684\u6838\u6620\u5c04\u5b9e\u73b0\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u6f02\u79fb\u4f7f\u5f97\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u4e2d\u7684\u5bf9\u79f0\u4e0d\u53d8\u6027\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u60c5\u5f62\u4e0b\u4f1a\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u548c\u8d1f\u8fc1\u79fb\u3002\u9700\u8981\u4e00\u4e2a\u98ce\u9669\u53d7\u63a7\u3001\u975e\u5bf9\u79f0\u4e14\u53ef\u89e3\u91ca\u7684\u8fc1\u79fb\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u6e90\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u57df\u4f20\u9012\u3002", "method": "\u4ee5Le Cam\u7edf\u8ba1\u5b9e\u9a8c\u7406\u8bba\u4e3a\u57fa\u7840\uff0c\u63d0\u51faLe Cam Distortion\u53caDeficiency Distance\u8861\u91cf\u6e90/\u9776\u4e4b\u95f4\u7684\u53ef\u6a21\u62df\u6027\u3002\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u6838\u6620\u5c04\u5b9e\u73b0\u4ece\u6e90\u5230\u9776\u7684\u5355\u5411\u6a21\u62df\uff0c\u66ff\u4ee3\u5bf9\u79f0\u4e0d\u53d8\u6027\u7684\u4e25\u683c\u6700\u5c0f\u5316\uff0c\u7ed9\u51fa\u8fc1\u79fb\u98ce\u9669\u7684\u4e0a\u754c\u3002", "result": "\u5728\u4e94\u4e2a\u9886\u57df\uff08\u57fa\u56e0\u7ec4\u5b66\u3001\u89c6\u89c9\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u4e0a\u5b9e\u73b0\u4e86\u6709\u663e\u8457\u6539\u8fdb\u7684\u8fc1\u79fb\u6548\u679c\uff1a\u5982HLA\u57fa\u56e0\u7ec4\u7684\u9891\u7387\u4f30\u8ba1\u76f8\u5173\u6027r=0.999\uff0cCIFAR-10\u4fdd\u630181.2%\u7684\u51c6\u786e\u7387\uff08\u76f8\u6bd4CycleGAN\u4e0b\u964d34.7%\uff09\uff0c\u4ee5\u53ca\u5728RL\u63a7\u5236\u4e2d\u5b9e\u73b0\u5b89\u5168\u7684\u7b56\u7565\u8f6c\u79fb\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u4e0d\u53d8\u6027\u7684\u5d29\u6e83\u3002", "conclusion": "\u4e3a\u8d1f\u8fc1\u79fb\u4e0d\u53ef\u63a5\u53d7\u7684\u9886\u57df\uff08\u5982\u533b\u5b66\u6210\u50cf\u3001\u81ea\u52a8\u7cfb\u7edf\u3001\u7cbe\u51c6\u533b\u5b66\uff09\u63d0\u4f9b\u9996\u4e2a\u57fa\u4e8e\u98ce\u9669\u63a7\u5236\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5f3a\u8c03\u65b9\u5411\u6027\u53ef\u6a21\u62df\u6027\u800c\u975e\u5bf9\u79f0\u4e0d\u53d8\u6027\u3002"}}
{"id": "2512.23670", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.23670", "abs": "https://arxiv.org/abs/2512.23670", "authors": ["Francesco Piatti", "Thomas Cass", "William F. Turner"], "title": "Random Controlled Differential Equations", "comment": null, "summary": "We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.\n  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.23675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23675", "abs": "https://arxiv.org/abs/2512.23675", "authors": ["Arnuv Tandon", "Karan Dalal", "Xinhao Li", "Daniel Koceja", "Marcel R\u00f8d", "Sam Buchanan", "Xiaolong Wang", "Jure Leskovec", "Sanmi Koyejo", "Tatsunori Hashimoto", "Carlos Guestrin", "Jed McCaleb", "Yejin Choi", "Yu Sun"], "title": "End-to-End Test-Time Training for Long Context", "comment": "Code: https://github.com/test-time-training/e2e", "summary": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.23707", "categories": ["cs.LG", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23707", "abs": "https://arxiv.org/abs/2512.23707", "authors": ["Shashwat Goel", "Rishi Hazra", "Dulhan Jayalath", "Timon Willi", "Parag Jain", "William F. Shen", "Ilias Leontiadis", "Francesco Barbieri", "Yoram Bachrach", "Jonas Geiping", "Chenxi Whitehouse"], "title": "Training AI Co-Scientists Using Rubric Rewards", "comment": "11 pages in the main paper, total 119 including sample outputs in the Appendix", "summary": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e0e\u81ea\u6211\u8bc4\u5206\u673a\u5236\uff0c\u5229\u7528\u5927\u89c4\u6a21\u8bba\u6587\u8bed\u6599\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u5408\u89c4\u7684\u7814\u7a76\u8ba1\u5212\uff0c\u5e76\u5728\u8de8\u57df\u573a\u666f\uff08\u5305\u62ec\u533b\u5b66\u4e0e\u65b0\u9884\u5370\u672c\uff09\u5c55\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u548c\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u7ea6\u675f\u548c\u9690\u6027\u8981\u6c42\u65f6\u7684\u4e0d\u8db3\uff0c\u5229\u7528\u8bba\u6587\u7ea7\u522b\u7684\u7814\u7a76\u76ee\u6807\u4e0e\u8bc4\u5206\u6807\u51c6\uff0c\u6784\u5efa\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ee5\u63d0\u5347AI\u5408\u4f5c\u8005\u5728\u79d1\u7814\u8ba1\u5212\u751f\u6210\u65b9\u9762\u7684\u8d28\u91cf\u4e0e\u53ef\u6267\u884c\u6027\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u4ece\u8bba\u6587\u81ea\u52a8\u62bd\u53d6\u7814\u7a76\u76ee\u6807\u53ca\u76ee\u6807\u7279\u5b9a\u8bc4\u5206\u91cf\u5c3a\u7684\u53ef\u6269\u5c55\u6570\u636e\u6e90\uff1b\u901a\u8fc7\u81ea\u6211\u6253\u5206\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5bf9\u521d\u59cb\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\uff0c\u51bb\u7ed3\u521d\u59cb\u7b56\u7565\u4f5c\u4e3a\u8bc4\u5206\u5668\u4ee5\u5f62\u6210\u751f\u6210-\u9a8c\u8bc1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1b\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5ba1\u548c\u8de8\u57df\u5bf9\u9886\u57df\u7684\u8bc4\u4f30\u4e0b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8986\u76d6\u533b\u5b66\u9886\u57df\u53ca\u65b0arXiv\u9884\u5370\u672c\uff0c\u8bc4\u4f30\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u7684\u8868\u73b0\u5728\u8de8\u57df\u6cdb\u5316\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u5fae\u8c03\u540e\u6a21\u578b\u5728\u751f\u6210\u7814\u7a76\u76ee\u6807\u65b9\u9762\u76f8\u5bf9\u521d\u59cb\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4f18\u4e8e70%\u7684\u76ee\u6807\uff1b\u81ea\u52a8\u63d0\u53d6\u7684\u76ee\u6807\u8bc4\u5206\u91cf\u5c3a\u83b7\u5f9784%\u7684\u8ba4\u53ef\uff1b\u5728\u533b\u5b66\u548c\u65b0\u9884\u5370\u672c\u7b49\u8de8\u57df\u573a\u666f\u4e5f\u5b9e\u73b0\u4e8612-22%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u81ea\u52a8\u5316\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347AI\u5408\u4f5c\u8005\u5728\u751f\u6210\u7814\u7a76\u8ba1\u5212\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5bf9\u8de8\u9886\u57df\u901a\u7528\u6027\u4e0e\u540e\u7eed\u65e0\u76d1\u7763\u6216\u534a\u76d1\u7763\u5fae\u8c03\u7684\u53ef\u80fd\u6027\u7ed9\u51fa\u79ef\u6781\u4fe1\u53f7\u3002"}}
