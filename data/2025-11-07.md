<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 6]
- [cs.LG](#cs.LG) [Total: 60]
- [eess.SP](#eess.SP) [Total: 10]
- [eess.SY](#eess.SY) [Total: 18]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas](https://arxiv.org/abs/2511.03820)
*Zhiguo Ding,Robert Schober,H. V. Poor*

Main category: cs.IT

TL;DR: 提出了环境划分多址EDMA，通过pinching天线重新配置传播环境实现多用户干扰抑制与信号增强，降低对复杂信号处理的需求，并给出两种上下行的低复杂度算法及理论分析。


<details>
  <summary>Details</summary>
Motivation: 在多用户无线场景中，干扰与信号衰落限制系统容量，本文通过利用物理环境的可控性来提升性能。

Method: 通过在特定位置布置pinching天线来有目的地阻断干扰链路、重配LoS通道，形成EDMA框架，推导出闭式表达的ergodic sum-rate增益及达到对比基准的概率；提出两种上/下行的低复杂度算法，并与穷举搜索进行对比评估。

Result: 给出EDMA相对于传统多址的ergodic sum-rate增益的闭式表达，给出实现更大瞬时和的概率；对pinching天线位置进行优化；仿真表明所提算法与穷举搜索接近最优甚至优于部分场景。

Conclusion: 在不依赖复杂信号处理的前提下，利用环境可控性实现的EDMA对多用户通信具有显著潜力，pinching天线位置优化是成败的关键。

Abstract: This paper exploits the dynamic features of wireless propagation environments
as the basis for a new multiple access technique, termed environment division
multiple access (EDMA). In particular, with the proposed
pinching-antenna-assisted EDMA, the multi-user propagation environment is
intelligently reconfigured to improve signal strength at intended receivers and
simultaneously suppress multiple-access interference, without requiring complex
signal processing, e.g., precoding, beamforming, or multi-user detection. The
key to creating a favorable propagation environment is to utilize the
capability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,
pinching antennas are placed at specific locations, such that interference
links are blocked on purpose. Based on a straightforward choice of
pinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional
multiple access and the probability that EDMA achieves a larger instantaneous
sum rate than the considered benchmarking scheme are derived in closed form.
The obtained analytical results demonstrate the significant potential of EDMA
for supporting multi-user communications. Furthermore, pinching antenna
location optimization is also investigated, since the locations of pinching
antennas are critical for reconfiguring LoS links and large-scale path losses.
Two low-complexity algorithms are developed for uplink and downlink
transmission, respectively, and simulation results are provided to show their
optimality in comparison to exhaustive searches.

</details>


### [2] [Age of Job Completion Minimization with Stable Queues](https://arxiv.org/abs/2511.04630)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出两种基于状态感知的调度策略，用以在中心服务器-马尔可夫机模型的时隙作业系统中最小化作业完成的年龄并降低采样成本，同时给出使作业队列保持稳定的充分条件。


<details>
  <summary>Details</summary>
Motivation: 在存在马尔可夫性可用性以及为了减少过时完成的作业的前提下，设计能尽量缩短作业完成时间的调度策略，同时控制对机器状态的采样开销。引入“作业完成年龄”这一新度量来衡量系统性能。

Method: 建立一个时隙化的多用户-中心服务器系统，机器状态服从二元对称马尔可夫链，服务器对每个用户维护独立队列；通过对机器空闲状态的采样来触发调度，并提出两种调度策略，分析在给定策略下队列的稳定性条件，并通过数值仿真比较性能。

Result: 给出两种策略在若干情形下的性能评估，获得使队列稳定的充分条件，并展示在一定条件下可显著降低作业完成年龄与采样成本的潜力。

Conclusion: 所提出的两种策略能够在给定的马尔可夫机模型和采样框架下实现较低的作业完成年龄，并在满足特定稳定性条件时保持队列稳定；未来工作可进一步收紧性能界限、优化采样成本与扩展到更复杂的系统。

Abstract: We consider a time-slotted job-assignment system with a central server, N
users and a machine which changes its state according to a Markov chain (hence
called a Markov machine). The users submit their jobs to the central server
according to a stochastic job arrival process. For each user, the server has a
dedicated job queue. Upon receiving a job from a user, the server stores that
job in the corresponding queue. When the machine is not working on a job
assigned by the server, the machine can be either in internally busy or in free
state, and the dynamics of these states follow a binary symmetric Markov chain.
Upon sampling the state information of the machine, if the server identifies
that the machine is in the free state, it schedules a user and submits a job to
the machine from the job queue of the scheduled user. To maximize the number of
jobs completed per unit time, we introduce a new metric, referred to as the age
of job completion. To minimize the age of job completion and the sampling cost,
we propose two policies and numerically evaluate their performance. For both of
these policies, we find sufficient conditions under which the job queues will
remain stable.

</details>


### [3] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: LCR 与 VS 在捕捉相似性信息方面往往差异显著且可互补；VS 对若干 Renyi-Hill 阶段给出对 LCR 的上界，并且两者的结果受“半距离”参数化影响。总体而言，若目标是通过相似性编码提取丰富信息，LCR 更具优势；在特定半距离或具备 ur 元素/量子特征的数据中，VS 也有价值。


<details>
  <summary>Details</summary>
Motivation: 比较两种相似性敏感熵（LCR 与 VS）的性质、相互关系及使用场景，澄清何时应选用哪一种。

Method: 通过概念分析、解析证明以及对 53 个机器学习数据集的实证实验，揭示两者的关系；引入半距离概念来参数化相似性缩放；给出 VS 相对于 LCR 的上界，以及对该上界是否对所有参数成立的猜想。

Result: LCR 与 VS 可能差异达数量级，提供互补的的信息维度，除非处于极限情形；相似性尺度由半距离参数控制；在若干 Renyi-Hill 阶段，VS 对 LCR 具有上界，并且实验数据支持该结论；作者还提出在特定半距离下两者可互补。

Conclusion: 总体而言，若仅关注通过相似性编码的丰富信息，LCR 更为合适；在某些半距离条件下，VS 与 LCR 可互补。若将元素视为对更基元“ur 元素”的线性组合，或数据具有量子特征，则 VS 更具解释力。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


### [4] [Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing](https://arxiv.org/abs/2511.04088)
*Pranav Joshi,Daniel McMorrow,Yihan Zhang,Amitalok J. Budkuley,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 在对抗性信道（q进制，ρ比例符号可被任意篡改）下，给定极低速的反馈条件，首次给出对任意q≥2可实现的接近信息论最优率的编码方案，具备可行的编码/解码时间、存储和误差概率保证。


<details>
  <summary>Details</summary>
Motivation: 在信息论中，如何在对抗性噪声下实现接近容量的可靠传输，同时让接收机将消息限制在一个“较小集合”内，是一个核心难题。本研究尝试在极低速的反馈下逼近理论极限。

Method: 提出一个最小反馈方案的编码/解码框架，覆盖任意q≥2，目标实现率1 − H_q(ρ) − ε、可控的list-size、计算与存储复杂度，以及误差概率，并使反馈率趋于0。

Result: 给出可实现的参数：对任意ε>0且ρ ∈ (1−1/q−Θ(√ε))，达到的率为1−H_q(ρ)−ε，list-size为exp(O(ε^{-3/2} log^2(1/ε)))，编码/解码时间复杂度n^{O(ε^{-1} log(1/ε))}，存储复杂度O(n^{η+1} log n)（η>1），误差概率O(n^{-η})，且反馈率为O(1/ log n)。

Conclusion: 结论表明，使用极低速的反馈即可在任意q≥2的对抗性信道上实现接近信息论极限的性能，并给出详细的资源开销与误差界，为鲁棒编码设计提供新路径，同时指向在list-size与复杂度之间的进一步优化空间。

Abstract: Given a channel with length-$n$ inputs and outputs over the alphabet
$\{0,1,\ldots,q-1\}$, and of which a fraction $\varrho \in (0,1-1/q)$ of
symbols can be arbitrarily corrupted by an adversary, a fundamental problem is
that of communicating at rates close to the information-theoretically optimal
values, while ensuring the receiver can infer that the transmitter's message is
from a ``small" set. While the existence of such codes is known, and
constructions with computationally tractable encoding/decoding procedures are
known for large $q$, we provide the first schemes that attain this performance
for any $q \geq 2$, as long as low-rate feedback (asymptotically negligible
relative to the number of transmissions) from the receiver to the transmitter
is available. For any sufficiently small $\varepsilon > 0$ and $\varrho \in
(1-1/q-\Theta(\sqrt{\varepsilon})$ our minimal feedback scheme has the
following parameters: Rate $1-H_q(\varrho) - \varepsilon$ (i.e.,
$\varepsilon$-close to information-theoretically optimal -- here $H_q(\varrho)$
is the $q$-ary entropy function), list-size
$\exp(\mathcal{O}(\varepsilon^{-3/2}\log^2(1/\varepsilon))$, computational
complexity of encoding/decoding
$n^{\mathcal{O}(\varepsilon^{-1}\log(1/\varepsilon))}$, storage complexity
$\mathcal{O}(n^{\eta+1}\log n)$ for a code design parameter $\eta>1$ that
trades off storage complexity with the probability of error. The error
probability is $\mathcal{O}(n^{-\eta})$, and the (vanishing) feedback rate is
$\mathcal{O}(1/ \log n)$.

</details>


### [5] [List Decoding of Folded Reed-Solomon Codes Over Galois Ring](https://arxiv.org/abs/2511.04135)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 将list decoding从有限字段拓展到Galois环上的RS及折叠RS码，取得与有限字段等价的解码界限与列表大小。


<details>
  <summary>Details</summary>
Motivation: 迫切需要把list decoding的理论扩展到代数结构更一般的Galois环，以支撑基于环的零知识证明系统并提升近似间距的理解。

Method: 将Guruswami–Sudan的list decoding框架推广到Galois环上的Reed–Solomon码；研究Galois环上折叠RS码的list decoding；并把最近在Srivastava工作中的列表大小优化思路扩展到Galois环。

Result: 对于Galois环上的RS码，能够在码率r下List解码到半径1−sqrt(r)；对于Galois环上的折叠RS码，List解码半径达到单射界（Singleton bound）；并将折叠RS码的列表大小改进至O(1/ε^2)。

Conclusion: 证明Galois环上RS及折叠RS码具有与有限字段情形相近的List解码性能，推动环结构下的编码理论及其在零知识证明等应用中的潜在实现。

Abstract: List decoding of codes can be seen as the generalization of unique decoding
of codes While list decoding over finite fields has been extensively studied,
extending these results to more general algebraic structures such as Galois
rings remains an important challenge. Due to recent progress in zero knowledge
systems, there is a growing demand to investigate the proximity gap of codes
over Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and
coauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely
related to the decoding capability of codes. It was shown in Eli Ben-Sasson and
coauthors(2020) that the proximity gap for RS codes over finite field can be
improved to $1-\sqrt{r}$ if one consider list decoding instead of unique
decoding. However, we know very little about RS codes over Galois ring which
might hinder the development of zero knowledge proof system for ring-based
arithmetic circuit. In this work, we first extend the list decoding procedure
of Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows
that RS codes with rate $r$ can be list decoded up to radius $1-\sqrt{r}$.
Then, we investigate the list decoding of folded Reed-Solomon codes over Galois
rings. We show that the list decoding radius of folded Reed-Solomon codes can
reach the Singlton bound as its counterpart over finite field. Finally, we
improve the list size of our folded Reed-Solomon code to
$O(\frac{1}{\varepsilon^2})$ by extending recent work in Shashank
Srivastava(2025) to Galois Rings.

</details>


### [6] [Affine Frequency Division Multiplexing: From Communication to Sensing](https://arxiv.org/abs/2511.04471)
*Ali Bemani,Nassar Ksairi,Marios Kountouris*

Main category: cs.IT

TL;DR: AFDM在ISAC中展现出降低接收端复杂度与能耗、支持大带宽下的低采样率以及在多雷达场景中的干扰抑制潜力，同时利用DAFT在资源分配上的灵活性提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统在大带宽条件下的接收端复杂度与能耗挑战，以及多雷达干扰对感知/通信耦合的影响；通过AFDM的延迟-多普勒全局表征来实现更高效的传感与通信协同。

Method: 在AFDM框架下，利用低复杂度的自干扰消除（SIC）和模拟去啮（analog dechirping）实现低采样率的单基地感知；在双基地场景中实现亚奈奎斯特采样且不需要硬件改动，同时保持延迟分辨率；通过DAFT的资源分配灵活性来提升在多雷达环境中的干扰抑制能力。

Result: 证明单基地ISAC系统中AFDM接收机可通过SIC和模拟去啮实现低复杂度与低采样率；双基地情形下AFDM可在不改硬件的前提下实现亚奈奎斯特采样并保持延迟分辨率；并利用DAFT的资源分配灵活性来缓解多雷达干扰。

Conclusion: AFDM为ISAC在降低实现成本、提升采样效率及抗干扰能力方面提供有效途径，且DAFT的灵活性为多雷达环境中的资源管理带来显著优势。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been proposed as an
effective waveform for achieving the full diversity of doubly-dispersive
(delay-Doppler) channels. While this property is closely related to range and
velocity estimation in sensing, this article focuses on other AFDM features
that are particularly relevant for addressing two challenges in integrated
sensing and communication (ISAC) systems: (1) maintaining receiver complexity
and energy consumption at acceptable levels while supporting the large
bandwidths required for high delay/range resolution, and (2) mitigating
interference in multiradar environments. In monostatic sensing, where direct
transmitter-receiver leakage is a major impairment, we show that AFDM-based
ISAC receivers can address the first challenge through their compatibility with
low-complexity self-interference cancellation (SIC) schemes and reduced
sampling rates via analog dechirping. In bistatic sensing, where such analog
solutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist
sampling without requiring hardware modifications while preserving delay
resolution. Finally, we show that the second challenge can be addressed by
leveraging the resource-assignment flexibility of the discrete affine Fourier
transform (DAFT) underlying the AFDM waveform.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习的隐私保护ECG分类框架。通过将1D ECG信号转化为2D Gramian Angular Field图像，利用CNN特征提取，在服务器、笔记本和树莓派等异质IoT设备上实现联邦学习，达到95.18%的多客户端分类准确率，并在准确率和训练时间方面显著优于单客户端基线，同时保持可控的通信开销与资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在物联网医疗场景中，需保护病人隐私且数据需在本地处理。通过将ECG信号转化为2D图像并使用联邦学习，可以在不直接共享原始数据的前提下实现高性能分类，同时在异质设备上实现端到端的边缘云协同。

Method: 将1D ECG信号转换为2D的GAF图像以便CNN提取特征；在多设备(服务器、笔记本、树莓派4等资源受限设备)上进行联邦学习训练，评估在真实IoT设置中的可行性、性能及通信开销；对比单客户端基线。

Result: 多客户端下的FL-GAF模型实现95.18%的高准确率；明显优于单客户端基线，体现在分类性能与训练时间上。尽管GAF变换增加了计算负担，但整体资源使用与通信开销仍保持高效，适合边缘部署。

Conclusion: FL-GAF展现出在物联网医疗监测中实现可扩展、隐私保护AI的潜力，支持轻量级边缘部署与端到云一体化的智能医疗系统。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [8] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 提出 Common-O 基准，用以评估多模态语言模型在真实世界场景中的跨场景推理能力；在野图像、无训练数据污染、涉及共性推理的任务，现有模型在跨场景推理上表现极弱，最佳模型仅达 35%，Complex 场景仅 1%，多图输入/多图训练有显著改善；基准公开以推动研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界推理能力显著落后于在感知基准上的表现，无法仅通过单图像感知来理解场景，因此需要超越感知能力，评估模型在跨场景“找共同点”的推理能力，并揭示幻觉与对象共现偏差等问题。

Method: 构建 Common-O 基准，包含 10.5k 例，使用全新图像避免网页数据污染；设计跨场景推理任务“what's in common?”，评估主流多模态语言模型及专门进行链式推理的模型；分析幻觉、对象共现、尺度与多图输入对性能的影响。

Result: 感知单图像的对象识别对大多数模型可行，但跨场景推理极具挑战；Best 模型在 Common-O 上仅 35%，在 Complex 场景上仅 1%；相似对象的多发增加幻觉概率，提示模型依赖训练中的对象共现；尺度带来温和提升，显式的多图输入训练带来更大提升。

Conclusion: 基准揭示跨场景推理的重大挑战与幻觉风险，明确了多图输入训练的重要性，为提升模型在跨场景推理中的鲁棒性提供研究方向与数据资源。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [9] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 提出基于多模态语义扰动的污染检测方法，用于识别受污染的视觉-语言模型，鲁棒性强，适用于多种污染策略，并计划公开代码与数据集。


<details>
  <summary>Details</summary>
Motivation: 互联网规模且常含专有数据的预训练语料可能导致测试集泄露，从而抬高模型表现；现有研究多聚焦于去污染训练数据与基准重设计，而检测受污染的VLM的方向尚不充分。

Method: 系统性地在开源VLM和常用基准上注入污染，评估现有检测方法，提出基于多模态语义扰动的简单但有效检测策略，测试不同污染策略的鲁棒性，最终在公开数据和代码上进行验证。

Result: 现有检测方法在污染情境下要么失效要么表现不稳定；所提扰动检测对污染模型表现出较强的泛化与鲁棒性，跨多种污染策略有效。

Conclusion: 本文证明对抗性污染检测是可行的，提供一种简单有效的检测框架，并计划公开实现以促进领域研究。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [10] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: 融合式DP：通过 foundation 模型对敏感特征进行缺失值推断，再结合修改后的 DP-SGD，在保留原始敏感特征隐私的同时提升模型效用，适用于特征级隐私保护。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护中，往往只有部分特征具有高度隐私风险（如人口统计信息），而原始实验数据如实验室结果相对不敏感。传统的 DP-SGD 对所有特征施加同等隐私噪声，导致显著的效用下降。需要在特征层面进行隐私保护并提升效用。

Method: 1) 使用大模型对敏感特征在非敏感特征条件下进行推断，作为在训练时的外部先验，避免直接访问真正的敏感值；2) 引入对原始敏感特征隐私进行形式化保护的修改版 DP-SGD，对原始与推断特征共同训练，但确保对原始敏感特征的隐私约束。

Result: 在 PhysioNet 的败血症预测（表格数据）和 MIMIC-III 的临床笔记分类两个任务上，与隐私保护基线相比，FusionDP 显著提升了模型性能，且在特征级隐私保护方面保持严格约束。改善效果在多模态场景中体现出 foundation 模型驱动的推断有助于提升隐私-效用权衡。

Conclusion:  foundation 模型驱动的特征推断可以有效缓解特征级差分隐私下的效用损失，促进不同模态下的隐私保护与模型性能的折中，为未来在更多医学场景中的应用提供思路。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [11] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: CAA (Complexity as Advantage) defines complexity as observer-relative predictive regret, not an intrinsic property. A system is complex if it yields low regret for some observers and high regret for others, creating an information advantage. This framework unifies emergent notions (multiscale entropy, predictive information, observer-dependent structure) and grounds why complexity can be functionally valuable. Demonstrated on simple dynamical models; discusses implications for learning, evolution, and artificial agents.


<details>
  <summary>Details</summary>
Motivation: To quantify and explain complexity as a property emerging from the interaction between a system and a family of observers, rather than as an intrinsic attribute, thereby providing a quantitative basis for the functional value of complexity.

Method: Propose a formal framework (CAA) that evaluates predictive regret across different observer families attempting to model the system. Show that differences in regret across observers encapsulate various emergent phenomena. Validate with simple dynamical models and discuss implications for learning, evolution, and AI.

Result: A unifying perspective where complexity emerges from observer-dependent predictive difficulty. The framework accounts for multiscale entropy, predictive information, and observer-dependent structure, and explains the functional utility of complexity via information advantage for certain observers. Demonstrative models illustrate concepts and potential applications in learning, evolution, and artificial agents.

Conclusion: CAA provides a quantitative grounding for complexity as an observer-relative construct, clarifying when and why complex behavior is valuable. It suggests design and analysis strategies for systems and agents to leverage observer-dependent advantages.

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [12] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出一种基于难度路由的多模型分组策略，将问题分配给最小且可能解决该问题的模型，以降低推理成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模推理语言模型性能优越但成本高，需在保证准确性前提下降低计算资源消耗，并实现更高效的部署策略。

Method: 使用来自 s1.1-32B 的中间表示，训练轻量级预测器来估计题目难度或模型正确性，并据此在一个包含多种推理模型的池中进行路由分配。

Result: 在多样化的数学基准上，路由策略在效率上优于随机分配，并达到 s1.1-32B 的性能，同时显著降低计算量。

Conclusion: 难度感知路由是成本有效部署推理模型的有效方法，具有广泛的应用潜力。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [13] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [14] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出 Energy-Guided Diffusion Stratification (StratDiff) 的离线到在线强化学习方法，通过扩散模型学习离线数据的先验并用能量函数微调，利用 KL 散度将样本分成离线-like 与在线-like 子集，从而分别采用离线和在线目标进行更新，在 D4RL 上与 Cal-QL、IQL 等方法结合实现更稳定、适应性更强的性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线数据和在线学习中策略分布的分布差异导致性能下降，且当前研究较少利用离线数据的分布结构来自适应学习策略。通过对离线数据的分布特征进行显式建模与分层训练，降低从离线到在线的迁移成本与不稳定性。

Method: 利用扩散模型从离线数据中学习先验知识，并通过能量-基函数对其进行精炼，以提升模仿能力并在在线微调阶段生成离线风格的动作。对每个样本计算生成动作与采样动作的 KL 散度，将训练批次按离线-like 与在线-like 子集划分。离线-like 样本使用离线目标更新，在线-like 样本采用在线学习策略。将 StratDiff 与 Cal-QL、IQL 等现有方法结合并评估。

Result: 在 D4RL 基准上，StratDiff 显著优于现有方法，提供更好的适应性和在多样RL设置中的稳定性表现，且与现有离线到在线方法的整合表现良好。

Conclusion: StratDiff 通过显式建模离线数据分布并在训练中对样本进行分层处理，有效缓解离线到在线迁移过程中的分布漂移，提升离线到在线 RL 的稳定性与性能。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [15] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 提出将因果添加模型扩展到包含高阶交互的有向无环超图，给出理论可识别性与学习算法的扩展，并在合成数据中显示潜在的经验优势。


<details>
  <summary>Details</summary>
Motivation: 现实世界过程常呈现高阶交互，但现有的因果发现多聚焦于一阶模块化，CAM 在复杂结构下存在局限，因此需要新的结构表示、理论工具与学习算法以捕捉高阶因果关系。

Method: 引入有向无环超图来表示高阶交互的因果结构，建立新的定义与理论工具，推导超图的可识别性结果，扩展贪心 CAM 算法以在更复杂的超图搜索空间中进行学习。

Result: 给出超 DAG 的可识别性结果，理论上讨论相较于传统马尔可夫等价类的扩展，并分析在受 CAM 限制的前提下，学习更复杂结构可能带来更好的有限样本表现；在合成数据上验证所提出方法的实用性。

Conclusion: 高阶交互下的因果结构可以通过超 DAG 表示，若在 CAM 的约束下也能实现可识别性与可学习性，则可能获得更佳的推断效果；未来工作可在真实数据集上检验并优化算法效率。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [16] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出 SADQ，通过引入一个随机转移模型对后继状态分布进行建模，将后继状态信息融入 Q 值估计，从而降低训练方差、提高稳定性和学习效率，同时保持无偏估计，实验上优于 DQN 变体。


<details>
  <summary>Details</summary>
Motivation: DQN 的目标更新常来自过去策略产生的转移，可能产生高方差且与当前策略不对齐，导致学习信号稀疏或无效。需要更具信息性且与当前策略对齐的学习信号。

Method: 引入一个随机转移模型来显式建模环境动力学；在 Q 值估计中整合后继状态分布；提供更高效的动作选择策略，利用模型的转移结构；给出理论保证使估值保持无偏、方差降低。

Result: 在标准 RL 基准和真实向量化控制任务上，SADQ 在稳定性和学习效率方面持续优于 DQN 变体；理论分析证明无偏估计且方差下降。

Conclusion: SADQ 为 DQN 的改进，利用环境动力学建模提升稳定性和样本效率，具有广泛适用性。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [17] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 提出 Lead-Lag Forecasting (LLF) 的统一研究框架，并提供两大规模基准数据集（arXiv=访问→引文，GitHub=推送/收藏→分叉），以探索跨时间尺度的前导-滞后关系；并对数据清洗、统计验证和回归基线进行了基准评估，建立数据门户。


<details>
  <summary>Details</summary>
Motivation: 在社交与使用数据中，早期交互事件往往在后续很长时间后影响更高影响结果；缺乏统一数据集与基准使得 LLF 未成为主流时序预测的统一研究对象。

Method: 构建并公开高容量的 LLF 基准数据集（arXiv、GitHub），进行数据清洗、验证 lead-lag 动态、并在回归任务中评估参数化与非参数基线，讨论其他领域的同类现象，提供可复现的数据门户。

Result: 确立 LLF 作为新预测范式，提供跨年尺度的长时程数据，验证存在 lead-lag 动态，并对基线做了系统评估，具备可复现性与扩展性。

Conclusion: LLF 为统一的预测范式与数据基础，为社会与使用数据领域的跨时间关系研究奠定基础，数据门户可供社区下载与复现实验。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [18] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出一种基于分数估计的快速变化检测框架，利用从样本对学习条件分数以估计转移核变化，通过 score-based CUSUM 实现高维马尔可夫过程的变化检测，给出渐进上界和鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 在未知转移核条件下进行快速变化检测具有现实意义，但直接最大似然估计在高维数据中不可行。通过直接学习条件分数∇_y log p(y|x)，避免显式似然评估，提供一个可操作且理论上有保障的检测方法。

Method: 从样本对 (x, y) 学习条件分数 ∇_y log p(y|x)，构建基于 Hyvarinen 分数差的条件 score-CUSUM；引入截断以确保增量有界；在 uniformly ergodic Markov 过程下，利用 Hoeffding 不等式推导误报时间的指数下界；给出检测延迟的渐近上界。

Result: 提出可操作的 score-based 变更检测框架，理论上给出误报时间的指数下界和检测延迟的渐近上界，显示在高维马尔可夫模型中的实际可行性。

Conclusion: 该工作将条件 Hyvarinen 分数作为核心，提供未知转移核条件下的快速检测方法，并给出理论保证和可行性分析。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [19] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 提出基于有向移动图的高阶移动描述符来从移动数据推断社会人口属性，辅以不确定性可视化工具以及多任务学习以提升跨情境的泛化与样本效率。


<details>
  <summary>Details</summary>
Motivation: 从被动收集的移动数据中推断人口统计特征可帮助交通规划，但由于移动模式与人口属性之间的关系弱且不稳定、且跨情境泛化困难，亟需兼具可解释性、准确性和泛化能力的方法。

Method: 1) 提出基于有向移动图的高阶、以行为为驱动的移动描述符，捕捉 trips 序列、出行方式与共同出行的结构模式；2) 引入评估指标与可视诊断工具，确保模型置信度与预测准确性的一致性并量化不确定性；3) 构建多任务学习框架，在共享表示上同时预测多项社会人口属性，提升样本效率并增强跨时间段的泛化能力。

Result: 高阶移动描述符显著提升了年龄、性别、收入和家庭结构等属性的预测准确性，相比基线特征更具解释性。多任务学习在训练数据有限或测试分布与训练分布不同（跨时间段）时，优于单任务模型，显示出更强的泛化与样本利用效率。

Conclusion: 该方法在移动数据中的推断任务上实现了更高的预测精度、可解释性、以及不确定性量化能力，并通过多任务学习提升跨情境的泛化性能，为交通规划中的数据驱动决策提供更稳健的工具。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [20] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 提出一个差分隐私的上下文学习框架，在检索阶段引入隐私保护的最近邻检索并跟踪隐私预算；在文本分类和文档问答任务上显著优于基线，兼顾隐私与效用。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型的检索-上下文工作流存在隐私风险，现有的差分隐私ICL忽略了检索阶段对上下文数据的暴露。需要将最近邻检索与隐私保护相结合，确保在给定隐私预算下的效用最大化。

Method: 提出一个以最近邻检索为核心的DP-ICL框架；从上下文数据数据库中进行最近邻检索，并引入隐私筛选器，记录已选择样本的累计隐私成本，以确保遵守中心差分隐私预算。

Result: 在文本分类和文档问答任务上，该方法相较于现有基线在多数评估指标上表现显著优越，获得更有利的隐私-效用权衡。

Conclusion: 将隐私感知的检索整合到DP-ICL中可以显著提升在实际LMM管道中的性能与隐私合规性，未来工作可将该框架推广到更广泛的LLM检索-上下文场景。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [21] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL是Nemotron系列的最新多模态模型，在视觉和文本领域对比前代显著提升，特别在长文档与长视频场景下的推理能力和吞吐量。通过架构、数据集和训练策略的全面改进，实现更高的推理效率，并提供BF16/FP8/FP4等格式的模型检查点，同时公开数据与训练代码。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界的文档理解、长时视频理解与推理任务的挑战，提升在长序列、多模态场景中的推理吞吐量和准确性，推动实际部署的可能性。

Method: 在Nemotron Nano V2的基础上进行架构升级与令牌（token）压缩/缩减技术的引入，结合混合Mamba-Transformer LLM；扩充和公开化数据集、训练配方与代码，并提供BF16、FP8、FP4等格式的模型检查点，针对长文档与长视频场景优化推理速度。

Result: 在视觉与文本域均实现显著提升，相较于前代模型Llama-3.1-Nemotron-Nano-VL-8B，长文档/长视频场景下的推理吞吐量更高，整体性能更强。

Conclusion: Nemotron Nano V2 VL在多模态理解和长序列推理方面取得进展，提供对实际部署更友好的推理效率，并开放数据、训练配方与代码，推动未来的研究与应用。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [22] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出 LogHD：在超维计算（HDC）中用对数类轴缩减，把 C 个原型替换为约 log_k C 个捆绑超向量，解码在 n 维激活空间，从而把内存降到 O(D log_k C) 并保持性能，同时提升鲁棒性与能效。


<details>
  <summary>Details</summary>
Motivation: 传统的一类原型设计在内存上需要 O(CD)，对内存、能量和可靠性敏感的系统受限。已有的特征轴压缩在降低 D 的同时削弱鲁棒性，因此需要在减小内存与保持准确性之间取得平衡。

Method: 提出 LogHD：将分类轴上的 C 个原型替换为 n ≈ ⌈log_k C⌉ 个捆绑超向量（字母表大小为 k），在 n 维激活空间完成解码；引入容量感知的码本与基于配置/轮廓的解码，并可与特征轴稀疏化相结合。

Result: 在多数据集和随机位翻转注入下，LogHD 在同等内存条件下能达到相当的准确性，同时实现更高的对位错鲁棒性；在等内存条件下，仍能维持目标准确度，耐受约 2.5-3.0× 的位翻转率提升。ASIC 实现相比 AMD Ryzen 9 9950X 提升 498× 能效、62.6× 速度提升；相较 NVIDIA RTX 4090 提升 24.3×/6.58×，且比基于特征轴的 HDC ASIC 基线高 4.06× 能效与 2.19× 速度。

Conclusion: LogHD 实现了对数级别的分类轴缩减，在不牺牲准确性和鲁棒性的前提下显著降低内存与能耗开销，并且与特征轴稀疏化等方法兼容，具备在低功耗高效能硬件实现的潜力。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [23] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 对 RLHF 的最新进展进行综述，聚焦多模态对齐、文化公平与低时延优化，系统地比较 PPO、DPO、GRPO 等方法，并给出未来研究的路线图。


<details>
  <summary>Details</summary>
Motivation: RLHF 已成为对齐大语言模型的标准，但现有工作多停留在文本维度，尚需梳理跨模态对齐、文化公平与低延迟优化等新兴领域的方法与挑战。

Method: 对相关文献进行系统综述，首先回顾基础算法（PPO、DPO、GRPO），再分析最新创新，进行方法的对比综合，提出开放性问题。

Result: 提供一个可操作的综述性路线图，总结现有方法的优劣与适用场景，明确尚待解决的关键挑战与研究方向。

Conclusion: 为研究者构建更鲁棒、效率更高、更加公平的 AI 系统提供指导与方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [24] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 提出针对认知诊断(CD)模型的数据移除/遗忘问题的层次化重要性导向遗忘(HIF)算法，在三个真实数据集上显著优于基线，首次实现CD模型的高效隐私保护数据unlearning。


<details>
  <summary>Details</summary>
Motivation: 个人“被遗忘权”等隐私需求推动数据移除，但现有CD模型多缺乏隐私保护及有效数据unlearning机制，直接使用通用算法难以兼顾移除完整性、模型效用与效率，需针对CD模型的异构结构进行专门设计。

Method: 提出层次化重要性导向遗忘(HIF)算法。通过对CD模型参数在不同层级呈现的特征进行分析，设计平滑机制将个体和层级重要性结合起来，从而更精准地区分与待移除数据相关的参数，实现高效数据unlearning。

Result: 在三个真实数据集上，HIF在数据移除相关指标和保持模型性能方面均显著优于基线方法，证明了其在CD模型中的有效性与实用性。

Conclusion: 首次对CD模型的数据unlearning进行系统研究，提出高效的HIF算法，为在隐私需求驱动下的CD模型应用提供可行解并促进隐私保护AI系统的部署。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [25] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: PETRA uses evolutionary-trajectory transformers based on phylogenetic trees to predict SARS-CoV-2 mutations, outperforming baselines by large margins, and addresses sequencing noise and data imbalance; code available on GitHub.


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2迅速进化且具免疫逃逸性，原始RNA序列容易受测序噪声影响；需利用进化轨迹与系统发生树信息来预测未来突变，并解决全球地理和时间分布不均的问题。

Method: 提出基于进化轨迹的变换器模型PETRA，将进化轨迹来自系统发育树的特征用于训练，而非直接使用原始RNA序列；引入加权训练框架以应对显著的地理与时间不均衡。

Result: 在加权条件下，PETRA实现对核苷酸突变的加权召回率1（weighted recall@1）为9.45%，对刺突蛋白氨基酸突变为17.10%，显著高于最佳基线的0.49%和6.64%；还能实现对24F(XEC)、25A(LP.8.1)等主要谱系的实时突变预测。

Conclusion: PETRA在预测未来 SARS-CoV-2 突变方面表现出显著优势，能辅助公共卫生监测和疫苗开发，代码开源于GitHub。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [26] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 提出一个可组合的微调框架，将图结构先验与模块化适配器结合，用于大规模预训练模型的多任务微调，提升效率、稳定性和预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决多任务微调中高计算成本与结构不稳定性的问题；需要一种高效且可复用的跨任务适配机制，并在结构约束下实现权重分配和路径选择。

Method: 引入任务关系矩阵，将任务之间的相关性及节点/路径的关系编码到图先验中；通过低秩映射和可插拔机制将模块化适配器嵌入不同层，实现基于先验的跨任务组装与复用；设定路由温度、门控阈值和关系矩阵正则化来引导路径与权重分配。

Result: 实验表明在超参数、环境和数据敏感性分析中，该框架在任务预测准确性、适配器权重分配精度和整体计算效率方面优于 baseline，同时保持模型轻量化，显著缓解路径冲突与冗余计算。

Conclusion: 将图先验与模块化机制结合的可组合微调框架在多任务场景中具有显著优势，提升参数效率、训练稳定性和跨任务重用性，展现了结构约束下的高效协同效应。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [27] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: CGM 与可穿戴技术推动动态代谢表型分型，揭示以胰岛素抵抗、β细胞功能和促胰岛素分泌不足为核心的 dysglycemia 异质性；机器学习可利用在家OGTT的高分辨率数据预测肌肉 IR 与β细胞功能，并以PPGR及个体日常行为实现个体化干预。


<details>
  <summary>Details</summary>
Motivation: 现有糖尿病及前糖尿病诊断依赖静态血糖阈值，未能捕捉早期 dysglycemia 的生理异质性，需要非侵袭、动态的代谢表型来实现精准预防。

Method: 对CGM、可穿戴技术、在家OGTT数据以及PPGR等进行综述；利用机器学习从高分辨率 glucose 数据推断肌肉 IR、β细胞功能，并分析日常饮食、睡眠、体力活动等因素与代谢表型的关系，评估不同表型下的干预效果。

Result: 证据显示，机器学习模型可利用家用CGM-OGTT数据预测 gold-standard 的肌肉 IR 与β细胞功能；个体对标准化餐的后餐血糖反应（PPGR）可作为代谢亚型生物标志；可穿戴数据揭示饮食、睡眠、活动及其节律与特定代谢功能相关；饮食干预对 PPGR 的效力因表型而异。

Conclusion: CGM 可把早期 dysglycemia 拆解为可操作的子表型，超越单纯血糖控制，为个体化的营养、行为和药物干预铺平道路，推动精准糖尿病预防的新阶段。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [28] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出一种可扩展的合成预训练数据生成方法，用于决策树的元学习，通过近最优树的合成采样和 MetaTree Transformer 实现，与真实数据或昂贵的最优树相比性能相当，显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 决策树在高风险领域的重要性与可解释性需求，以及缺乏大规模真实数据/昂贵的最优树训练所带来的挑战，需要一种更高效的合成数据生成方案来支撑元学习。

Method: 通过对齐目标近似最优的决策树进行合成采样，生成大规模、真实感强的数据集；采用 MetaTree Transformer 架构进行元学习训练。

Result: 在与真实数据预训练等价的性能方面接近，且与昂贵的最优树训练相比具有显著的计算成本节省；数据生成的灵活性得到提升，方法具备可扩展性。

Conclusion: 该策略推动可解释性决策树的可扩展元学习，降低计算成本并提升数据生成灵活性。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [29] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 提出一个用于科学与工程的通用任务框架（CTF），通过一组挑战性数据集与客观指标，统一评估在有限数据和噪声测量条件下的预测、状态重建、泛化与控制等科学目标的ML/AI算法。


<details>
  <summary>Details</summary>
Motivation: 当前多学科应用中，难以在不同任务和数据条件下公平比较算法的表现；需要标准化的基准和评价指标来促进ML/AI在科学与工程领域的快速、可重复的发展。

Method: 提出CTF及其增长中的挑战数据集集合，覆盖预测、状态重建、泛化、控制等多种目标，强调在有限数据与噪声条件下的鲁棒性评估，并借助客观指标进行跨任务比较。

Result: CTF被描述为一种关键赋能技术，已推动ML/AI算法在语音、语言处理、计算机视觉等传统应用中的快速发展；在科学与工程领域，CTF为比较不同算法提供统一的平台和度量标准。

Conclusion: 强调需要建立并采用CTF的客观指标以便在当今快速发展的领域中对算法进行横向比较，从而促进科学与工程中的ML/AI进步；并呼吁社区广泛采用CTF并持续扩展数据集。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [30] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出面向边缘设备的自回归感知分割计算框架，通过一点分割压缩(OPSC)、两阶段中间压缩(阈值切分TS+token-wise自适应位量化TAB-Q)和统一优化来在保持精度的同时降低通信并提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限的物联网设备上部署大语言模型的难题，尤其在自回归推理中需迭代生成token并管理扩展的KV缓存。

Method: （1）OPSC：前后端以不同精度混合定点，避免OOM；（2）两阶段中间压缩：阈值切分TS与TAB-Q实现激活的精度关键部位保留同时减少传输量；（3）统一优化框架：联合搜索最优分割点、量化设置与序列长度以满足内存与延迟约束。

Result: 在多种LLM与硬件平台上对比Smoo thQuant、OmniQuant、Atom等方法，实现约1.49x推理加速和显著的通信开销降低，同时保持或提升精度。

Conclusion: 为边缘端部署LLM提供自回归感知的分割计算方案，缓解内存瓶颈、降低通信成本，并维持模型性能。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [31] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant 提出一种分布感知的旋转标定方法和 QR-Orth 优化，显著加速大模型量化并降低内存需求，使在单卡 GPU 上对 70B 模型的量化成为可能。


<details>
  <summary>Details</summary>
Motivation: 端到端旋转优化的高计算成本和易过拟合问题阻碍大规模模型的高效量化，需要更高效且稳健的量化校准方法。

Method: 通过约束旋转后激活的分布，实现对旋转优化的复杂度约束；引入 QR-Orth 优化，替代昂贵的交替优化，提供更高效的求解方案；减少对任务特定损失的依赖以降低过拟合风险。

Result: 在多种模型量化实验中，DartQuant 相较于现有方法实现 47 倍加速和 10 倍内存节省的旋转优化，在 70B 模型上实现单卡 3090 的量化可行性，首次完成对 70B 的旋转校准。

Conclusion: DartQuant 使大语言模型的量化在资源受限环境下成为可行，且提供了可复现的实现（代码见项目仓库）。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [32] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 基于预训练的 ResNet 的超声图像自动检测阑尾炎，针对儿科病例的 Regensburg 数据集，达到高于90%的准确率与良好精确率/召回率。


<details>
  <summary>Details</summary>
Motivation: 儿科阑尾炎诊断因症状重叠、影像质量变化大而挑战性高，需借助深度学习提升超声诊断的客观性与稳定性。

Method: 在 Regensburg 小儿阑尾炎数据集上对 ResNet 进行微调，针对 右下腹、阑尾及相关结构的多视图超声图像进行二分类（阑尾炎/非阑尾炎）。图像预处理包括归一化、缩放与数据增强，以提高泛化。模型利用了多视图信息，尽量克服低对比度、散射噪声与解剖变异。

Result: 模型在总体上达到准确率 93.44%、精确率 91.53%、召回率 89.8%，在异质超声视图下对阑尾炎的识别表现良好，体现了学习到的判别空间特征对挑战性影像的鲁棒性。

Conclusion: 所提出的 ResNet 模型对儿科超声影像中的阑尾炎检测具有较强的诊断潜力，尤其在多视图、多变的解剖背景下表现出对关键空间特征的有效学习，未来可结合临床数据进一步验证与部署。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [33] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: Learned distance-weight trade-off for filtered ANN via constrained linear optimization, improving accuracy by 5-10% over fixed-penalty methods.


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤感知方法依赖固定的惩罚项，难以在不同数据集的标签和向量分布上泛化。需要数据驱动的权重来更好反映过滤分布。

Method: 将问题建模为受限线性优化，学习反映过滤分布的权重；将学习到的权重用于搜索过程和索引构造，使图结构更好地捕捉过滤语义。

Result: 实验表明，将距离函数自适应于数据能显著提升准确性，提升幅度为5-10%，相较固定惩罚方法具有更好的灵活性和泛化性。

Conclusion: 学习得到的距离函数能提高精度并在不同数据集上具有更好的泛化性，提供一个更灵活、可扩展的有过滤约束的ANN搜索框架。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [34] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: 提出 DeNoise，针对带污染训练数据的无监督图级异常检测（UGAD）提供鲁棒框架。通过对抗训练联合优化图级编码器、属性解码器和结构解码器；引入编码器锚对齐去噪机制，将高信息的普通图节点嵌入融合入所有图嵌入；再通过对比学习在潜在空间紧凑正常样本、排斥异常样本。实验证明在8个真实数据集上对噪声强度具鲁棒性，显著优于现有 UGAD 基线。


<details>
  <summary>Details</summary>
Motivation: 在图结构数据快速增长的场景下，UGAD 需要利用未标注的正常图样本进行学习。然而训练数据常常混入异常图，导致表示学习被污染并显著下降性能，因此需要在污染条件下仍能学习稳健的图级表示。

Method: 提出 DeNoise 框架：其核心是对图级编码器、属性解码器和结构解码器之间的对抗目标进行联合优化，以学习对噪声不敏感的嵌入。创新点包括：1) 编码器锚对齐去噪机制，将高信息的来自正常图的节点嵌入注入到所有图的嵌入中，提升表示质量并抑制异常干扰；2) 引入对比学习，聚集正常图嵌入、分离异常图嵌入。

Result: 通过在8个真实数据集上的广泛实验，DeNoise 在不同噪声强度下都能学习到可靠的图级表示，并显著优于现有的 UGAD 基线方法。

Conclusion: DeNoise 能在污染的训练数据条件下有效学习抗噪声的图级表示，提升无监督图级异常检测的性能，展示了对抗学习、锚对齐去噪及对比学习在 UGAD 的有效性与潜力。

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [35] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: KoTaP is a long-term panel dataset of non-financial firms listed on KOSPI and KOSDAQ (2011-2024) that treats corporate tax avoidance as a predictor linked to multiple firm dimensions (earnings management, profitability, stability, growth, governance). It uses CETR, GETR, and book-tax differences (TSTA, TSDA) and yields 12,653 firm-year observations from 1,754 firms, enabling benchmarking, AI applications, and policy/investment analyses.


<details>
  <summary>Details</summary>
Motivation: There is a need for a standardized, internationally comparable panel dataset on corporate tax avoidance that can be linked to diverse corporate outcomes while reflecting unique Korean institutional features, to support research benchmarking, external validity checks, and policy implications.

Method: Constructed from publicly listed Korean firms (excluding financials), with 2011-2024 data. Excludes firms with non-December fiscal year ends, capital impairment, and negative pre-tax income. Tax avoidance is measured with CETR, GETR, and book–tax differences (TSTA, TSDA) with interpretability adjustments. The dataset is a balanced panel with standardized variables and aligned with international literature on distributions and correlations of core indicators, while capturing Korea-specific features (concentrated ownership, high foreign ownership, liquidity).

Result: Final KoTaP dataset comprises 12,653 firm-year observations across 1,754 firms. It links tax avoidance to multiple domains (earnings management, profitability, stability, growth, governance) and supports benchmarking, econometric and deep-learning analyses, external validity checks, and explainable AI. It serves as a resource for policy evaluation, audit planning, and investment analysis, offering international comparability and contextual Korean features.

Conclusion: KoTaP provides a critical open resource for accounting, finance, and interdisciplinary research, enabling analyses of tax avoidance as a predictor across multiple corporate dimensions and supporting policy, governance, and investment decision-making.

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [36] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 提出一种可解释的分解式符号回归方法，融合多集合Transformer、遗传算法和遗传编程，从黑箱回归模型中提取并组合成易于理解的多变量数学表达式，在噪声条件下实现与对比方法相当甚至更优的拟合，同时保持原始数学结构。


<details>
  <summary>Details</summary>
Motivation: 现有的符号回归往往以最小化预测误差为目标，容易产生复杂或不准确的表达式，缺乏对系统真实驱动规律的揭示，因此需要一种能够可解释地还原潜在方程的分解式方法。

Method: 提出可解释的分解式SR：先将训练好的“黑箱”回归模型蒸馏成数学表达；通过一个多集合Transformer为每个变量生成多条单变量骨架，刻画该变量对模型输出的影响；用GA评估并选取高质量的骨架候选；通过GP的级联过程增量地合并骨架，同时保持各自的骨架结构；最后对合成表达的系数进行GA优化，得到最终的多变量表达式。

Result: 在带有可控与不同水平噪声的问题上，与两种GP方法、三种神经SR方法和一种混合方法相比，所提方法在插值和外推误差上具有更低或相当的表现；并且其骨架-级联策略能更稳定地学习到与原始数学结构相匹配的表达式。

Conclusion: 该分解式SR框架在可解释性与对原方程结构保留方面具有明显优势，对噪声数据具有鲁棒性，能够从黑箱模型中提取出可解释、正确的多变量表达式，提升对复杂系统的透明建模能力。

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [37] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: LLMs具备成为端到端编译器的潜力，但现阶段仍存在较低的编译成功率；通过提示优化、扩大模型规模与推理方法等手段，能显著提升生成的汇编代码质量。研究设计了CompilerEval数据集与框架，用以评估主流LLMs在源代码理解与汇编代码生成方面的能力，并给出未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究背景是端到端的大语言模型在多个领域展现出强大能力，但将其应用于编译器这一关键系统软件尚未充分探索。本工作目标在于评估LLM作为编译器（LaaC）的可行性、潜力与局限。

Method: 设计并使用CompilerEval数据集与框架来评估主流LLMs在源码理解与汇编代码生成方面的能力；分析产生的错误类型；探索多种提升LLM生成代码质量的方法；并评估跨平台编译能力。

Result: 实验结果显示，LLMs具有基本的编译能力，但当前的编译成功率仍偏低；通过优化提示、扩大模型规模和引入推理方法等策略，LLMs生成的汇编代码质量可以显著提升。

Conclusion: 对LaaC保持乐观态度，提出实用的体系结构设计与未来研究方向。通过定向训练、知识丰富的提示以及专门的基础设施支持，LaaC有潜力生成高质量的汇编代码，并推动编译领域的范式变革。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [38] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了一种名为EPO的半无限安全强化学习框架，通过对约束集合的扩/删实现有限约束子问题迭代求解，兼顾性能和有界安全。


<details>
  <summary>Details</summary>
Motivation: 在需要跨越整个连续参数空间保证安全的场景（如在每个空间位置确保资源分配等）中，直接处理无限约束难以实现。

Method: 在每次迭代中求解带有限约束的安全RL子问题；通过容忍度阈值将违反的约束加入活跃集；对拉格朗日乘子为零的约束删除；通过这样的约束扩/删策略控制工作集规模并促进训练。

Result: 理论分析表明，在 mild assumptions 下，EPO训练的策略在全局约束下的最优解的性能近似，且全局约束违反严格限定在一个界内。

Conclusion: EPO为SI-safe RL提供一个可理论支撑且实用的框架，能在保持安全性界限的同时实现接近最优的长期性能，并避免工作集的无限膨胀。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [39] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 在航空轨迹数据稀缺场景中，通过在数据丰富机场的预训练并在数据稀缺机场微调，可以用扩散与流对齐等生成模型实现高效迁移。5% Dublin 数据即可达到竞争性性能，20% 达到基线，整体优于从头训练。


<details>
  <summary>Details</summary>
Motivation: 中小机场缺乏历史轨迹数据，限制机器学习和大规模仿真；利用迁移学习将数据丰富机场的模型迁移至数据稀缺环境具有潜在价值。

Method: 将扩散与潜在流对齐（latent flow matching/latent diffusion）等生成模型应用于航空领域，在苏黎世（源）预训练，再在都柏林（目标）微调，数据按 Dublin 数据占比从0%到100%变化以评估迁移效果。

Result: 扩散模型在仅使用 5% 的 Dublin 数据时就具备竞争性表现，约在 20% 数据时达到基线，整体优于从零开始训练的模型，且在多项指标和可视化评估上领先。潜在流对齐与潜在扩散也能从预训练中受益，但增益更为波动，流对齐模型的泛化能力较弱。捕捉罕见轨迹模式仍存在挑战。

Conclusion: 迁移学习能够显著降低轨迹生成的数据需求，为在历史记录受限的环境中生成真实感合成轨迹提供可行路径。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [40] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的纵向建模方法，能够处理异构EHR数据中的不规则时间、模态差异和语义结构，通过特征嵌入、可学习的时间编码、多头自注意力以及语义加权池化，最终线性映射得到个体风险评分；在多源EHR数据上的实验优于传统机器学习与时间序列深度模型。


<details>
  <summary>Details</summary>
Motivation: 在多源异构EHR数据中进行临床风险分类时，面临不规则采样、模态差异和复杂语义结构等挑战，需要统一的特征表示与对长期与短期动态的建模能力。

Method: 输入多源医疗特征并进行特征嵌入以统一表征结构化和非结构化数据；引入可学习的时间编码来捕捉不均匀采样下的动态演化；使用多头自注意力对纵向序列进行全局依赖建模，聚合长期趋势和短期波动；设计语义加权池化以自适应为关键医疗事件赋权，提高特征判别力；最后使用线性映射输出个体风险分数。

Result: 实验结果表明该模型在准确率、召回率、精确率和F1分数等指标上优于传统机器学习和时间序列深度学习模型，在多源异构EHR环境下实现稳定且精确的风险识别，为临床智能决策提供高效可靠的框架。

Conclusion: 所提出的框架在异构EHR纵向数据中的风险识别方面提供了一个高效、鲁棒的解决方案，能够整合多模态信息、捕捉时间动态并给出可解释的风险评分，适用于临床智能决策支持。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [41] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 联合调优深度集成的超参（权重衰减、温度缩放、早停）通常提升预测与不确定性校准，提出部分重叠留出法作为训练数据充分利用与联合评估的折中方案，并给出实践指南与开源实现。


<details>
  <summary>Details</summary>
Motivation: 研究在深度集成中通过联合调优超参数来提升整体性能与不确定性估计的潜力，并探索一个兼顾评估需求与训练数据利用的实际留出策略。

Method: 在深度集成训练中同时调优权重衰减、温度缩放和早停；提出部分重叠的留出策略以实现联合评估与数据利用之间的折中；对比单独优化与联合优化的效果，评估在不同任务和指标上的差异。

Result: 联合调优普遍能达到或超过单独调优的表现，但效果大小在任务与指标间差异显著；重叠留出法提供了一个实用的折中方案，代码可用以复现实验并指导实践。

Conclusion: 研究揭示了深度集成训练中个体与联合优化之间的权衡，为实践者优化深度集成模型提供有力洞见，并给出一个可操作的留出策略与开源代码。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [42] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: ScaleDL: a nonlinear layer-wise runtime predictor with graph neural network–based cross-layer interactions and D-optimal data collection, delivering high accuracy and generalizability with reduced data costs.


<details>
  <summary>Details</summary>
Motivation: Accurate DNN runtime prediction is critical for efficient resource allocation. Traditional additive models are limited in accuracy and generalizability, while graph-based approaches, though accurate, incur high data collection costs.

Method: ScaleDL combines nonlinear layer-wise modeling with a graph neural network–based cross-layer interaction mechanism and employs D-optimal design to reduce data collection costs.

Result: On five popular DNN models, ScaleDL achieves about 6× lower mean relative error (MRE) and 5× lower RMSE compared with baseline models, demonstrating improved prediction accuracy and hierarchical generalizability.

Conclusion: ScaleDL offers accurate, generalizable DNN runtime prediction with reduced data collection requirements, effectively balancing accuracy, generalizability, and data collection costs.

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [43] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 在 MXFP4 格式下对后训练量化（PTQ）方法进行全面基准评估，发现 GPTQ 等方法表现良好，而基于旋转的量化在 MXFP4 下存在严重不兼容性；通过分析根源并提出块级旋转策略来改造旋转法，使其在 MXFP4 上显著提升精度并为未来低精度格式的 PTQ 研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模迅速增长，部署成本（内存、计算和能耗）成为瓶颈。PTQ 为高效部署提供潜在解法，但在新兴的 FP4 (MXFP4) 格式下，现有方法的兼容性与有效性尚未明确，亟需基准测试与机制分析。

Method: 对 MXFP4 下的多种 PTQ 方法进行系统化基准评估，覆盖不同规模的语言模型。通过分析发现旋转基类方法在 MXFP4 下与 PoT(block scaling) 的关系存在本质冲突，随后提出一种简单的块级旋转策略，使旋转方法能够适配 MXFP4，并在多种模型上验证性能提升。

Result: GPTQ 等方法在 MXFP4 下保持稳定的量化性能；基于旋转的主流方法因与 MXFP4 的权量分布和能量重分配不兼容而表现受限；引入的块级旋转策略显著提升旋转法在不同大模型中的准确性。

Conclusion: 为从业者在新兴低精度格式下开展 PTQ 提供明确指引，并为未来在 MXFP4 等格式上的 PTQ 研究奠定理论与方法基础，尤其是通过块级旋转实现对旋转法的有效迁移。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [44] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: seqme 是一个开源的 Python 库，用于评估生物序列设计方法的表现，提供基于序列、嵌入和属性的度量，支持多种序列类型，并可用于一击式和迭代式设计方法。


<details>
  <summary>Details</summary>
Motivation: 在序列设计的目标分布拟合与属性达成方面，需要一个统一、模块化且可扩展的评估工具；目前缺乏一个单一的软件库来实现这类度量。

Method: 提出 seqme，作为一个模块化、可扩展的开源库，提供模型无关的度量、嵌入与属性模型，以及诊断和可视化工具，覆盖序列基于、嵌入基于和属性基于的评估，并适用于小分子、DNA、ncRNA、mRNA、肽和蛋白等多种生物序列。支持一击式和迭代式设计方法。

Result: 提供一组度量和工具，包含嵌入和属性模型、诊断与可视化，能够评估多种生物序列设计任务，并能应用于不同序列类型及设计方法。

Conclusion: seqme 为生物序列设计方法的评估提供统一、可扩展的解决方案，增强对目标分布拟合与属性达成的评估能力，适用范围广。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [45] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: STELLE是一种神经符号混合框架，将时间序列嵌入到时序逻辑概念的空间中，通过类似STL的核将原始序列与预定义公式对齐，从而在提高分类准确性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的应用中，时间序列分类的可解释性尤为重要，而现有的深度学习方法通常为黑箱，难以理解预测依据。

Method: 提出STELLE框架：将轨迹直接嵌入到时序逻辑概念的空间，使用一个受STL启发的核将时间序列映射到与预定义的STL公式的对齐程度，模型在训练时同时优化准确性与可解释性，以便每个预测都伴随最相关的逻辑概念。

Result: 实验表明STELLE在竞争性准确性方面具有优势，同时提供逻辑上可信的局部与全局解释：局部解释为可读的STL条件，支持单个预测；全局解释为类级别表征的公式，帮助理解各类别的特征。

Conclusion: STELLE实现了预测性能与可解释性之间的折中，提供与直观、可验证的逻辑解释相结合的时间序列分类方法。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [46] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出一个将 acquisition-driven 模块并入 RLHF 流程的混合框架，以提升对偏好数据的采样效率并保持可扩展性。


<details>
  <summary>Details</summary>
Motivation: 从人类偏好中学习的成本高，RLHF 能扩展到高维任务但样本效率不足；PBO 在主动查询方面更高效但规模化能力有限。两者各有优点，需结合以实现高维任务的高效偏好学习。

Method: 在 RLHF 流程中整合一个 acquisition-driven 模块，进行主动查询的偏好数据采集，形成一个对偏好数据采样更友好的混合框架。通过在高维偏好优化与 LLM 微调两类任务上的验证，评估该框架的样本效率与性能。

Result: 实验结果在这两个任务上均显示样本效率和总体性能的持续提升，证明了所提出混合框架的有效性。

Conclusion: 该工作成功将 RLHF 的规模化能力与 PBO 的样本效率结合起来，提供了一种在高维偏好学习场景中更高效的学习范式，适用于高维偏好优化和 LLM 微调等任务。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [47] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: SCER是一种嵌入表示层正则化方法，通过减少模型对虚假相关的依赖来提升最差子群鲁棒性，在理论上将最差组误差与对虚假方向/核心方向的依赖联系起来，并通过对嵌入层施加约束实现对虚假信号的抑制；在视觉和语言任务上实验证明其优于现有方法的最差组准确率，且给出开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型容易学习到与任务无关的虚假相关，导致在分布转移和子人群场景下性能下降，尤其在低样本的子群上。当前研究缺乏一个将嵌入空间表示与最差组误差建立严格理论联系的框架，因此需要一种从嵌入层出发的鲁棒性正则化方法。

Method: 提出SCER（Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness）。在理论分析中，将最差组误差与分类器对虚假方向和核心方向的依赖程度联系起来，该依赖由跨域与类别的分组均值嵌入差异所揭示。通过在嵌入层施加正则化约束，使模型更关注核心特征，降低对虚假模式的敏感性。实验在多种视觉和语言任务上验证，SCER在最差组准确率上优于先前方法，且提供代码实现。

Result: 理论上揭示最差组误差受 classifier 对虚假方向与核心方向依赖的影响，且该依赖可通过分组均值嵌入差异来量化；在实践中，嵌入层正则化有效抑制虚假信号，提升了最差组鲁棒性，实验结果在视觉和语言任务中优于现有SOTA。

Conclusion: SCER提供了一个从嵌入层出发的理论-实践框架，明确了最差组误差的来源并给出嵌入级正则化的有效性，证明通过强调核心特征可显著提升在分布转移与子群情形下的鲁棒性，且代码开源。

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [48] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出了在存在歧义的情境下LLMs的不确定性量化（UQ）能力的局限，提出了带有 ground-truth answer distributions 的 AmbigQA 数据集，并在多种估计范式下表现出接近随机的性能下降，进而理论解释其局限性。


<details>
  <summary>Details</summary>
Motivation: 真实语言具有歧义性导致的 aleatoric 不确定性在问答场景中的重要性，以及现有 UQ 方法在有歧义数据上的不足。

Method: 提出 MAQA* 和 AmbigQA* 两个带有 Ground-truth 分布的歧义问答数据集；在 predictive distribution、模型内部表征、以及模型集成等多种不确定性估计范式上评估；给出理论分析说明预测分布与集成估计在存在歧义时的固有局限。

Result: 现有 UQ 方法在歧义数据上性能接近随机；MAQA*、AmbigQA* 提供 ground-truth 分布并用于评估；不同估计范式的性能下降具有一致性；理论分析揭示预测分布和集成估计在存在歧义时的局限性。

Conclusion: 当前 LLM 的 UQ 方法在有歧义的真实语言情境中存在显著不足，需要重新思考建模范式。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [49] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 在联邦最小极大优化中针对重尾梯度噪声，提出两种鲁棒算法并给出理论收敛率，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的梯度噪声往往呈重尾分布，传统有界方差假设不现实；在非凸-PL 框架下的联邦最小极大优化需要具备对重尾噪声的鲁棒性与理论保障。

Method: 提出两种新算法：Fed-NSGDA-M，通过归一化梯度抑制重尾噪声对梯度的影响；FedMuon-DA，在局部更新中引入 Muon 优化器，结合联邦学习框架实现鲁棒更新。两者在较弱的假设条件下获得理论收敛。

Result: 理论上两算法给出收敛率 O(1/(TNp)^{(s-1)/(2s)})，其中 s>1 为尾部参数，表明收敛性受重尾噪声影响且随尾部程度调整；这是在重尾噪声存在下的首个具有严格理论保证的联邦最小极大优化算法。大量实验验证其有效性。

Conclusion: 两种算法在理论收敛性和实验表现上均体现对重尾梯度噪声的鲁棒性，填补了在联邦极值优化下对重尾噪声的理论与实践空缺。未来可拓展至更广的非凸结构、数据异质性及通信效率等方面。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [50] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出将时间序列因果模型与VAE结合的TNCM-VAE，用以生成具因果约束的后验金融时序数据，并支持压力测试和情景分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的金融合成数据缺乏因果推理能力，难以进行反事实分析与风险评估；需要能够同时保持时序依赖与因果结构的生成模型。

Method: 提出时间序列神经因果模型VAE（TNCM-VAE），在解码器架构中通过有向无环图 enforcing causal constraints，并采用因果Wasserstein距离进行训练；将变分自编码器与结构化因果模型结合，实现对因果机制约束下的反事实金融时序生成。

Result: 在受OU过程启发的合成自回归模型上验证，TNCM-VAE在反事实概率估计方面表现更好，L1距离达到0.03-0.10，显著优于 ground truth。该模型可用于金融压力测试、情景分析与基于更贴近因果机制的回测。

Conclusion: 该方法成功将时序数据的因果关系嵌入生成过程，兼顾时间依赖性与因果约束，提升了对反事实分析与风险评估的能力。

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [51] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: SynthKGQA提出一个通用框架，将任意知识图谱转化为高质量的合成KGQA数据集，包含每个问题需要推理的完整KG事实，既可用于更有信息量的基准评测，又可用于提升模型训练。以Wikidata为例生成GTSQA，用于评估KG检索器的零-shot泛化及对KG增强的LLM的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏具有ground-truth目标的挑战性图检索问答数据集，难以对KG检索器与LLM的事实性进行系统对比与评估；需要一个可扩展的、覆盖不同结构和关系类型的测试床。

Method: 提出SynthKGQA框架，可以从任意KG生成带有完整底层事实的KGQA数据集；对Wikidata应用以生成GTSQA，作为评估零-shot泛化的基准数据集。

Result: 数据集具备较高的信息量和可用于训练的特性；在GTSQA上对现有KG增强的LLM解法进行基线评估，展示了对KG检索器基准的改进潜力并揭示零-shot泛化能力。

Conclusion: SynthKGQA提供一个通用、可扩展的KGQA数据生成框架，能提升对KG检索器和KG-augmented LLMs的评估与训练效果，GTSQA为未见结构和关系类型的零-shot泛化提供测试平台。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [52] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: We propose Q3R, a quadratic reweighted rank regularizer for low-rank training, enabling prescribed low-rank weight matrices with competitive accuracy and low overhead. It bridges the gap where low-rank pre-training is challenging, validated on vision and language Transformers, including aggressive pruning (60–80%) with small CIFAR-10 accuracy drops (1.3–4%).


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient low-rank methods perform well for fine-tuning but struggle for low-rank pre-training, where maintaining the low-rank structure and the objective is difficult. There is a need for a training strategy that enforces low-rank structure during pre-training with minimal performance loss and overhead.

Method: Introduce Quadratic Reweighted Rank Regularizer (Q3R) based on a quadratic regularizer that majorizes a smoothed log-determinant surrogate of the rank. The approach draws from iteratively reweighted least squares (IRLS) to enforce a prescribed low target rank while remaining compatible with standard architectures.

Result: Empirical evidence across Transformers for vision and language tasks shows that aggressive pruning can preserve performance: e.g., truncating 60% and 80% of parameters in ViT-Tiny yields CIFAR-10 accuracy drops of about 1.3% and 4%, respectively, indicating effective low-rank training with small overhead.

Conclusion: Q3R provides a practical and architecture-compatible low-rank training strategy that achieves competitive accuracy with significant parameter reduction, validating its applicability for low-rank fine-tuning and pre-training across domains.

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [53] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 在刑事司法领域提出一种兼顾可行性与公平性的风险评估新准则：以加权错误损失最小化为目标，同时将虚假阴性率的差异控制在小容忍度内，改善可操作性与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决算法公平性研究中的核心概念不统一，特别是在刑事司法场景中；需要在群体公平、个体公平和过程公平之间建立清晰关系并给出可部署的框架。

Method: 对群体公平进行理论梳理并提出改进：不追求严格的群体平等，而是在最小化加权错误损失的同时，对不同受保护群体的虚假阴性率差异设定小容忍度。并把讨论放在数据偏见、潜在有利行动与子群约束爆炸等三类批评之下，提出实际部署框架。

Result: 提出一个简单、可实现的公平性标准，可能提高预测准确性并揭示错误成本的伦理权衡；为在公部决策系统中使用风险评估工具提供一个更易落地的框架。

Conclusion: 通过将技术设计与正当性联系起来，提供一个面向机构的、以需要为基础、透明度与问责制、以及严格界定的定义与解决方案为核心的部署框架，指向实际可操作的政策与系统实现。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [54] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 本研究探讨线性模态连通性在数据漂移下的行为及其在训练效率与集成多样性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 揭示数据偏移作为额外梯度噪声对LMC的影响，以及如何通过调整学习率和批量大小来控制收敛到的区域和泛化。

Method: 在包含数据偏移的实验设置中，系统性地改变学习率和批量大小，分析模型是否收敛到同一极小值，以及LMC下误差分布与多样性之间的关系。

Result: 将数据偏移视作额外的梯度噪声；较小的学习率和较大的批量有助于模型收敛到相同极小值或到具不同平滑性与泛化特征的区域；LMC在提高集成多样性的同时保持训练效率，并提供一个折中的优化路径。

Conclusion: 在数据漂移条件下，LMC仍具价值，需通过合适的学习率与批量尺寸实现对收敛景观与集成收益的平衡。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [55] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [56] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 提出一个后验、模型无关的置信区间预测框架，利用 conformal prediction 对潜在空间 ROM 的多环节输出进行不确定性量化，并在云微物理潜在空间动力学模型上进行验证。


<details>
  <summary>Details</summary>
Motivation: ROM 虽然高效，但缺乏鲁棒的不确定性量化；现有方法往往依赖具体架构/训练，难以实现通用化。

Method: 提出一个 post hoc、模型无关的框架，在不修改原始架构或训练的前提下，基于 conformal prediction 对潜在动力学、重建与端到端预测输出给出统计预测区间。

Result: 在一个潜在空间动态模型用于云滴径分布演化的任务上，所提出的方法能准确预测演变并对整个 ROM 流程的不确定性进行量化。

Conclusion: 该框架提供灵活、普适的 ROM 不确定性量化方法，显著提升 ROM 的可信度，无需改动原有模型结构或训练过程。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [57] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出一种基于时间子图采样的关系图变换器，利用跨注意力潜在瓶颈实现结构与时间信息的融合，并通过灵活的解码器实现多任务学习，达到在多任务关系数据上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有关系图模型多聚焦于空间结构，将时间信息仅视为过滤未来事件的约束，难以在跨实体类型、跨任务的场景中捕获长程时序依赖。需要一个能在多实体类型和多任务设置下，整合长期时序依赖与全局上下文的建模框架。

Method: 引入时间子图采样以扩展全局上下文，获取超出近邻的 temporally relevant 节点。提出 Relational Graph Perceiver (RGP)，利用跨注意力的潜在瓶颈在结构与时间上下文之间进行高效信息整合，并将不同节点/边类型信号投影到统一潜在空间以构建全局关系系统；引入灵活的跨注意解码器以在单模型中实现对具有不相干标签空间的多任务学习。

Result: 在 RelBench、SALT、CTU 数据集上达到当前最优性能，展示出对关系深度学习的通用、可扩展解决方案，适用于多任务、多类型关系数据的预测。

Conclusion: RGP提供了一般化且可扩展的关系深度学习框架，成功将时间维度融入全局上下文建模，并通过跨任务解码与潜在瓶颈实现对多任务目标的协同学习。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [58] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: ARETE R 包通过大语言模型自动从非结构化文献中提取物种发生数据，验证并扩展分布范围，为保育研究提供更快速的可用数据。


<details>
  <summary>Details</summary>
Motivation: 现有关键物种数据不足且信息更新速度受人为活动推动，文献数据难以机器可读，需要大量人工整理。

Method: ARETE R 包整合 OCR、异常值检测、输出表格，利用 ChatGPT API 进行数据抽取并进行模型与人类标注者对比验证。

Result: 与基于 GBIF 的范围图对比，100 物种蜘蛛的自动提取扩展了分布的 Occurrence 范围平均三个数量级；新数据可扩展 Extent of Occurrence；有助于空间保育规划和灭绝风险评估。

Conclusion: ARETE 提供更快的获取从未被利用的发生数据的能力，使研究者在自动化提取大部分数据的同时进行少量人工核查，便于资源优先排序和项目规划中的文献数据预测。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [59] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 在线性近似的 Dec-MASSP（去中心化多智能体最短路径问题）中，给出首个遗憾下界 Ω(√K)（K 为回合数），并通过对称性分析揭示最优策略结构，揭示去中心化学习的本质难度。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统需要去中心化协同，SSP 提供自然的随机最短路框架来建模分布式控制；在单智能体设置下的学习已被广泛研究，但去中心化多智能体情形的学习研究仍十分有限。本研究试图填补 Dec-MASSPs 在线性近似下的理论空白，阐明结构与学习难度。

Method: 采用线性函数近似表示转移动力学和成本，并通过新颖的对称性推理来刻画最优策略的结构；构造对任意智能体数 n 均适用的困难学习实例，以推导回合数 K 下的遗憾下界。

Result: 首次给出 Dec-MASSPs 的遗憾下界 Ω(√K)，说明在多智能体去中心化学习中的固有难度；下界及对称性分析为后续设计高效学习算法提供理论基石。

Conclusion: 这些结果深化了对去中心化控制中的学习复杂性的理解，并可为多智能体系统中的高效学习算法设计提供指引。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [60] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: This work investigates whether causal interventions used in mechanistic interpretability produce out-of-distribution representations and how this affects explanations; it categorizes divergences into harmless and pernicious, and proposes a modification to the Counterfactual Latent (CL) loss to keep interventions closer to the natural distribution, aiming for more reliable interpretability methods.


<details>
  <summary>Details</summary>
Motivation: There is concern that interventions used to probe model representations may push them away from the model's natural distribution, potentially undermining the faithfulness of explanations. The paper seeks to empirically detect distribution shifts, theoretically classify divergences, and mitigate pernicious cases.

Method: Empirical analysis showing that common causal interventions shift representations away from the natural distribution; theoretical analysis of two divergence classes—harmless (null-space of weights and covariance within decision boundaries) and pernicious (activate hidden pathways and induce dormant behavior); propose modification to Grant (2025) Counterfactual Latent (CL) loss to regularize interventions to stay closer to natural distributions, reducing harmful divergences while preserving explanatory power.

Result: Demonstrates that interventions often shift internal representations away from the natural distribution; provides a taxonomy of divergences (harmless vs pernicious); shows that the modified CL loss reduces pernicious divergences and maintains interpretability.

Conclusion: Points toward more reliable interpretability methods by ensuring interventions stay within the model's natural distribution, thereby improving the faithfulness and reliability of causal explanations.

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [61] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 聚焦于将多种扩展的流匹配范式用于预测由偏微分方程描述的动力系统，以减少采样步数，并探索直接预测大型三维仿真的二维切片以生成高效的求解器输入。


<details>
  <summary>Details</summary>
Motivation: 解决高成本的采样问题，在复杂PDE系统的概率预测中提升采样效率与可扩展性；比较直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GANs 和 rectified flows 等方法在流匹配中的表现；并探究直接预测2D切片的可行性以支持大尺度仿真的入流场生成。

Method: 系统性比较多种扩展的流匹配方法（直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GANs、rectified flows），在一组挑战性动力系统上进行实验评估，并尝试直接预测大尺度3D仿真的2D切片以便为求解器提供高效的入流场。

Result: 在多种系统上比较了不同方法的采样效率、稳定性与保真度，各方法各有优劣，部分扩展方法在减少采样步数方面显示出显著改进；直接预测2D切片的策略被评估为具有潜力但仍面临高维、时序相关性等挑战，给出未来改进方向。

Conclusion: 扩展的流匹配方法有望提升PDE驱动动力系统的概率预测效率，尤其是在对大型3D仿真切片进行快速入流场生成方面展现出前景。但要在高维与长期预测中达到稳健应用，还需解决稳定性、误差积累及泛化能力等问题。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [62] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 本论文给出对扩散式语言模型中“取消掩码”并行采样的误差的精确表征，并将其与一元函数逼近理论联系起来，提出一组下界和上界，揭示在某些分布下可实现对数级别的并行采样而几乎不损失性能。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型固有的逐步推理瓶颈，量化在不显著降级的前提下可以并行采样多少，以及在何种分布下接近最优并行策略。现有界限（如 Li & Cai）对很多分布并不紧致，因此需要一个普适且更精确的框架。

Method: 给出对任意分布和任意未遮掩（unmasking）计划下，真实分布与采样分布之间的期望散度（如KL/总变差等）的精确表达，并建立与一元函数逼近的优雅联系；在此基础上导出对该散度的下界与上界，并提出基于基分布信息量性质（总相关性 TC 和对偶总相关性 DTC）的新上界和新的采样计划，分析在何种条件下可实现 O(log n) 步并行采样而几乎不牺牲性能。

Result: 给出一组严格的分布无关与分布相关的界限，证明理论上可为任意分布选出最优未遮掩计划，但在没有强先验信息时通常难以超越该理论上界；在以TC/DTC等信息量为基础的设定下，给出新的上界和采样计划，指出在自然设置中可达成对数级并行采样（O(log n)）且性能损失可忽略。

Conclusion: 该工作建立了分析扩散语言模型并行采样的系统框架，揭示了把分布性质（TC/DTC）引入采样策略的有效性与局限性；虽然在一般情形下要达到理论最优需要先验知识，但在某些常见设置下，理论与实践都指向可实现显著的并行性提升（对数级步数），从而缓解推理成本。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [63] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 在无线时间触发的联邦学习TT-Fed框架中，提出自适应模型剪枝并联邦带宽分配的联合优化，以在给定延迟约束下最小化训练损失，同时显著降低通信开销（约40%）且保持性能。


<details>
  <summary>Details</summary>
Motivation: FL在隐私保护方面的机会，TT-Fed作为异步/同步的通用形式，但设备增多与带宽受限导致延迟和通信开销显著上升；需通过剪枝等手段降低通信负担并保持收敛性。

Method: 对TT-Fed模型在剪枝下的梯度L2范数收敛进行分析；基于收敛上界，建立剪枝比率与无线带宽的联合优化问题，目标是在给定延迟阈值下最小化训练损失；通过KKT条件导出带宽和剪枝比的闭式解。

Result: 理论上给出收敛上界并推导出最优带宽与剪枝比的闭式解；仿真实验显示剪枝可将通信成本降低约40%，且模型性能保持在同一水平。

Conclusion: 在无线TT-Fed系统中引入自适应剪枝是一种有效的降低通信成本、控制延迟并维持模型性能的可行方法，为大规模分布式学习提供了一个可行的优化框架。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [64] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出一种灰箱化的三维雷达现在预报框架，直接处理体积雷达反射率，耦合物理约束的神经算子与数据驱动学习，显式建模垂直变化的3D平流与参数化扩散，加入Brownian-运动启发的随机项以表示未解算的运动，残差分支捕捉微尺度对流与微物理变率，能在三小时前景中更准确，在160名气象学家盲评中57%的案例排名第一；恢复完整3D动态并具物理一致性，具备可扩展性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的降水今早预报（nowcasting）方法在时空保真度和预报时效上受限。NWP及其深度学习同侪要么过慢、要么分辨率不足以捕捉快速演变的对流；单纯外推和数据驱动方法易积累误差且易造成过度平滑；2D雷达方法忽略竖直信息，难以重构高度相关的动力学。迫切需要一种能在全3D尺度上保持物理一致性、且可扩展的现在预报框架。

Method: 提出一个灰箱、全三维的nowcasting框架，直接处理体积雷达反射率并耦合物理约束的神经算子与数据驱动学习。模型在守恒平流算子下学习垂直变化的3D平流场、参数化空间变扩散，并引入受Brownian运动启发的随机项以表示未解析的运动。残差分支用于捕获小尺度对流起始及微物理变异，扩散型随机模块用于估计不确定性。

Result: 在多种降水情形下实现更准确的三小时及更长时延的预测；在160名气象学家进行的盲评中，所提出框架在57%的案例中排名第一；通过恢复完整3D动力学并保持物理一致性，提供一个可扩展、鲁棒的现在预报途径。

Conclusion: 通过将全3D动力学与物理约束结合，并引入不确定性表示，该框架为极端降水的技能化、可靠的现在预报提供了可扩展的解决方案。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [65] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 提出以自洽性缺失与预测信息损失为核心的遗忘理论，并通过跨领域实验验证其普适性与对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 长期缺乏一个统一的定义来解释学习中的遗忘动力学，需要一种普适且任务无关的理论来揭示遗忘的本质。

Method: 提出一个以预测分布的自洽性缺失为核心的理论，将遗忘定义为预测信息的损失，并给出一个普适的遗忘倾向度量；通过广泛的分类、回归、生成建模和强化学习实验来验证理论。

Result: 理论自然导出一个普适的遗忘倾向度量；实证显示遗忘在所有学习设置中普遍存在并显著影响学习效率。

Conclusion: 为遗忘提供了一个 principled 的理解框架，并为分析与提升通用学习算法的信息保持能力奠定基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [66] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 多方法评估一个40道题的数学选拔考试，结合经典测验理论、机器学习与无监督聚类，揭示项诊断效度差异、关键题目、模型预测能力及潜在两阶段选拔的必要性。


<details>
  <summary>Details</summary>
Motivation: 在高等数学选拔中寻求证据驱动的优化策略，通过多方法整合提高选拔的有效性、公平性与透明度。

Method: 采用经典测验理论分析项诊断指数（如D，0.20–0.40等区分水平），使用随机森林与梯度提升等机器学习模型评估跨验证准确率，应用K-means识别潜在的能力结构并用自助法评估聚类稳定性（ARI）与聚类纯度。

Result: 55%的题目具有优秀区分度，30%区分度差需更换。题6为最强区分因子，D=1.000，F=4609.1，RF特征重要性0.206。随机森林与梯度提升在跨验证中的准确率分别为97.5%与96.0%。K-means识别出42.5%的分界点形成二元能力结构，与55%的机构阈值不同，触发对入门级别的再分类倾向；两聚类解的鲁棒性高（bootstrap ARI=0.855），下簇几乎纯净。

Conclusion: 多方法集成提供了一个稳健的证据基础，用于数学选拔的优化：替换低效项、采用两阶段评估、将随机森林预测与透明机制结合。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [67] [Correlation and Temporal Consistency Analysis of Mono-static and Bi-static ISAC Channels](https://arxiv.org/abs/2511.03837)
*Saúl Fenollosa,Narcis Cardona,Wenfei Yang,Jian Li*

Main category: eess.SP

TL;DR: ISAC测量显示77-79 GHz的单静态与双静态通道在瞬时相关性上显著不同（几何发射/接收分布导致低相关），但在时间维度上呈现统一的趋同行为，能在动态场景中预测性演化。


<details>
  <summary>Details</summary>
Motivation: 填补ISAC场景下通道模型的空白，系统地刻画单静态与双静态传感配置在动态城市微区环境中的关系与演化规律。

Method: 在动态城市微区（UMi）环境中，使用79 GHz FMCW通道健动器进行实测，覆盖七个真实场景与移动目标/收发端，比较单静态与双静态通道的瞬时相关性及时间演化特征。

Result: 发现两点核心结论：1) 由于几何传播不同，单静态与双静态通道具有持续低水平的瞬时相关性；2) 尽管瞬时相关性低，两种通道在时间上却呈现统一的连续性，与环境运动共同驱动的可预测演化。该结论在七个现实场景中得到验证。

Conclusion: 为ISAC系统设计与标准化提供参考：需在建模中区分几何导致的瞬时解相关性与共同的时间演化规律，以便在动态环境中实现更鲁棒的感知-通信一体化。

Abstract: Integrated Sensing and Communication (ISAC) is critical for efficient
spectrum and hardware utilization in future wireless networks like 6G. However,
existing channel models lack comprehensive characterization of ISAC-specific
dynamics, particularly the relationship between mono-static (co-located Tx/Rx)
and bi-static (separated Tx/Rx) sensing configurations. Empirical measurements
in dynamic urban microcell (UMi) environments using a 79-GHz FMCW channel
sounder help bridge this gap. Two key findings are demonstrated: (1)
mono-static and bi-static channels exhibit consistently low instantaneous
correlation due to divergent propagation geometries; (2) despite low
instantaneous correlation, both channels share unified temporal consistency,
evolving predictably under environmental kinematics. These insights, validated
across seven real-world scenarios with moving targets/transceivers, inform
robust ISAC system design and future standardization.

</details>


### [68] [Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals](https://arxiv.org/abs/2511.03967)
*Wuxia Chen,Sean Moushegian,Vahid Tarokh,Taposh Banerjee*

Main category: eess.SP

TL;DR: 提出 min-SCUSUM：一种基于 Hyvarinen 分数的多流快速变化检测与故障隔离方法，避免显式对数似然比，适用于高维数据和复杂模型。


<details>
  <summary>Details</summary>
Motivation: 在高维或基于机器学习的场景中，直接计算预变更/后变更分布及对数似然比成本高甚至不可行，因此需要一种无须显式分布的增量检测方法。

Method: 利用 Hyvarinen 分数的差值替代对数似然比，构建 min-SCUSUM；在多流场景下进行故障隔离分析，给出延迟与误报警分析；理论结果表明渐近性能取决于预变更与后变更分布之间的 Fisher 散度；给出区分受影响流与未受影响流的误识别概率上界。

Result: 理论性结果：渐近性能由 Fisher 散度决定；给出故障误识别概率上界；验证在理论层面的可行性与鲁棒性。

Conclusion: 基于分数的最小 SCUSUM 提供计算友好的多流快速变化检测与故障隔离框架，适用于高维或复杂模型场景，克服传统对数似然比的计算难题，并具备理论保证。

Abstract: This paper introduces an approach to multi-stream quickest change detection
and fault isolation for unnormalized and score-based statistical models.
Traditional optimal algorithms in the quickest change detection literature
require explicit pre-change and post-change distributions to calculate the
likelihood ratio of the observations, which can be computationally expensive
for higher-dimensional data and sometimes even infeasible for complex machine
learning models. To address these challenges, we propose the min-SCUSUM method,
a Hyvarinen score-based algorithm that computes the difference of score
functions in place of log-likelihood ratios. We provide a delay and false alarm
analysis of the proposed algorithm, showing that its asymptotic performance
depends on the Fisher divergence between the pre- and post-change
distributions. Furthermore, we establish an upper bound on the probability of
fault misidentification in distinguishing the affected stream from the
unaffected ones.

</details>


### [69] [Joint Beamforming and Position Design for Movable Antenna Assisted LEO ISAC Systems](https://arxiv.org/abs/2511.03984)
*Hanfu Zhang,Erwu Liu*

Main category: eess.SP

TL;DR: 提出可移动天线（MA）辅助的LEO ISAC系统，通过联合波束赋形与MA位置优化，在SINR约束和功率约束下最小化SPEB；采用SDR和AO方法，将原问题分解为若干凸子问题求解，结果显示收敛且在 sensing 性能上相对于基线至少提升25%。


<details>
  <summary>Details</summary>
Motivation: LEO卫星在集成感知与通信(ISAC)中面临严重信号衰减与有限发射功率，需要新的系统设计来提升感知与通信的性能；可移动天线（MA）提供位置灵活性，可改善信道与探测精度的权衡。

Method: 推导ISAC系统的通信SINR和感知的平方位置误差界(SPEB)表达式；在SINR约束、总发射功率约束及MA阵列物理约束下，联合优化发射波束赋形和MA位置以最小化SPEB。首先利用半正定松弛（SDR）对原问题进行简化；随后给出一种交替优化（AO）基准算法，将原问题分解为两个子问题，逐步凸化并求解。

Result: 算法表现出良好的收敛性和有效性；在通信和感知之间取得更优的权衡，且相较于基线，感知性能至少提升25%。

Conclusion: 可移动天线辅助的LEO ISAC系统通过SDR+AO的优化框架，能够在限制条件下显著提升感知性能，并实现鲁棒的通信与感知权衡，展示出潜在的实际应用价值。

Abstract: Low earth orbit (LEO) satellite-assisted integrated sensing and
communications (ISAC) systems have been extensively studied to achieve
ubiquitous connectivity. However, the severe signal attenuation and limited
transmit power at LEO satellites can degrade ISAC performance. To address this
issue, this paper investigated movable antenna (MA)-assisted LEO ISAC systems.
We derive the communication signal-to-interference-plus-noise ratio (SINR) and
the sensing squared position error bound (SPEB) for evaluating the ISAC
performance. Then, we jointly optimize the transmit beamforming and the MA
positions to minimize the SPEB under the SINR constraints, total transmit power
constraint, and several inherent physical constraints of the MA array. We first
simplify the complex problem using the semidefinite relaxation (SDR). Then, we
present a novel alternating optimization (AO)-based algorithm to decouple the
original problem into two subproblems, consequently convexified and solved.
Simulations demonstrate the convergence and effectiveness of the proposed
algorithm. Better trade-off between communication and sensing performance, and
at least 25% gain in sensing performance are achieved, compared to the
benchmarks.

</details>


### [70] [Optimal RIS Placement in a Multi-User MISO System with User Randomness](https://arxiv.org/abs/2511.03998)
*Abhishek Rajasekaran,Mehdi Karbalayghareh,Xiaoyan Ma,David J. Love,Christopher G. Brinton*

Main category: eess.SP

TL;DR: 提出一种在用户分布不确定的情况下优化 RIS 位置的递归粗糙到细化方法，通过基于障碍物配置构建候选位置集并在多次用户分布实例上评估，以确定最终最优部署区域，实验结果支持该方法有效性。


<details>
  <summary>Details</summary>
Motivation: RIS 的性能高度依赖于放置位置。以往工作要么只做覆盖最大化，要么在已知 BS、RIS、用户位置的前提下联合优化定位、波束与反射系数；但在实际场景中仅能知晓障碍物配置和用户密度的空间变化，因此需要在不完美位置信息下对 RIS 进行鲁棒部署，以提高期望最小 SINR。

Method: 将问题建模为一个非凸优化问题，目标是在用户随机性下对 RIS 位置进行期望最小 SINR 的优化。提出递归的粗糙到细化方法：基于障碍物配置构造一组候选 RIS 放置位置，在多次用户分布实例上对其进行评估；在每一阶段确定的最优区域内递归细化搜索，得到最终的部署区域。

Result: 给出了数值仿真结果，证实了所提方法在鲁棒 RIS 部署区域定位上的有效性。

Conclusion: 所提出的递归粗糙到细化方法为在不确定用户分布和已知障碍物配置的情形下确定 RIS 部署区域提供了一种实用途径，并能在多实例评估中获得稳定的性能提升。

Abstract: It is well established that the performance of reconfigurable intelligent
surface (RIS)-assisted systems critically depends on the optimal placement of
the RIS. Previous works consider either simple coverage maximization or
simultaneous optimization of the placement of the RIS along with the
beamforming and reflection coefficients, most of which assume that the location
of the RIS, base station (BS), and users are known. However, in practice, only
the spatial variation of user density and obstacle configuration are likely to
be known prior to deployment of the system. Thus, we formulate a non-convex
problem that optimizes the position of the RIS over the expected minimum
signal-to-interference-plus-noise ratio (SINR) of the system with user
randomness, assuming that the system employs joint beamforming after
deployment. To solve this problem, we propose a recursive coarse-to-fine
methodology that constructs a set of candidate locations for RIS placement
based on the obstacle configuration and evaluates them over multiple
instantiations from the user distribution. The search is recursively refined
within the optimal region identified in each stage to determine the final
optimal region for RIS deployment. Numerical results are presented to
corroborate our findings.

</details>


### [71] [Tiny-WiFo: A Lightweight Wireless Foundation Model for Channel Prediction via Multi-Component Adaptive Knowledge Distillation](https://arxiv.org/abs/2511.04015)
*Haotian Zhang,Shijian Gao,Xiang Cheng*

Main category: eess.SP

TL;DR: 通过MCAKD实现轻量化的无线基础模型蒸馏：CA-KS进行关键特征选择，AL-PL平衡知识传输与自主学习，Tiny-WiFo 5.5M在边缘达到1.6ms推理，保持>98%性能与零-shot泛化。


<details>
  <summary>Details</summary>
Motivation: 为解决无线基础模型在边缘设备上的实时部署困难，面对超大规模模型带来的高计算与能源成本，需通过高效知识蒸馏与自适应特征选择来降低推理代价，同时尽量保留模型的关键能力与泛化性。

Method: 提出跨注意力知识选择（CA-KS）模块，从教师模型中选择对任务最关键的特征；引入自治学习-被动学习（AL-PL）策略，在独立学习与知识传输之间进行权衡，从而提高训练效率并控制计算成本。将该框架应用于WiFo无线FM，蒸馏出Tiny-WiFo（5.5M参数）。

Result: Tiny-WiFo在边缘设备上实现1.6毫秒推理时间，参数极小（5.5M），仍保留WiFo原模型约98%以上的性能，并维持关键的零-shot泛化能力。

Conclusion: MCAKD框架有望实现无线基础模型在边缘设备上的高效落地，兼顾性能与计算成本，促进实时无线FM的实际部署。

Abstract: The massive scale of Wireless Foundation Models (FMs) hinders their real-time
deployment on edge devices. This letter moves beyond standard knowledge
distillation by introducing a novel Multi-Component Adaptive Knowledge
Distillation (MCAKD) framework. Key innovations include a Cross-Attention-Based
Knowledge Selection (CA-KS) module that selectively identifies critical
features from the teacher model, and an Autonomous Learning-Passive Learning
(AL-PL) strategy that balances knowledge transfer with independent learning to
achieve high training efficiency at a manageable computational cost. When
applied to the WiFo FM, the distilled Tiny-WiFo model, with only 5.5M
parameters, achieves a 1.6 ms inference time on edge hardware while retaining
over 98% of WiFo's performance and its crucial zero-shot generalization
capability, making real-time FM deployment viable.

</details>


### [72] [Ambiguity Function Analysis of AFDM Under Pulse-Shaped Random ISAC Signaling](https://arxiv.org/abs/2511.04200)
*Yuanhan Ni,Fan Liu,Haoran Yin,Yanqun Tang,Zulin Wang*

Main category: eess.SP

TL;DR: AFDM在ISAC场景下的模糊度函数(AF)分析揭示：无脉冲整形时平均平方DPAF由参数c1和随机数据的尖度决定，与c2无关；与OFDM、OCDM对比，三者在旁瓣的常规凹陷数量相同，但AFDM可通过c1调控凹陷位置以缓解强目标对弱目标的影响；脉冲整形会在延迟轴塑形主瓣、在多普勒轴快速衰减旁瓣；给出脉冲整形后AF的闭式表达，数值验证理论与设计方法。


<details>
  <summary>Details</summary>
Motivation: 揭示AF在ISAC中的作用机理，比较AFDM/OFDM/OCDM的AF特性，为AFDM参数设计提供理论依据，以提升弱目标检测与估计性能。

Method: 对AFDM无脉冲整形情形推导平均平方DPAF的闭式解，分析c1与数据峰度对AF的影响，证明c2不影响；比较AFDM、OFDM、OCDM的AF形状与凹陷分布，给出三者等同的凹陷数量；给出脉冲整形随机AFDM的平均平方DPAF闭式表达，研究脉冲整形对主瓣/旁瓣的影响。

Result: 得到无脉冲整形下的闭式DPAF表达式，表明AF依赖c1及数据的峰度、与c2无关；三种波形具有相同数量的旁瓣凹陷；AFDM可通过c1调控凹陷位置；脉冲整形后得到主瓣沿延迟轴、旁瓣在多普勒轴快速下降的特征，且给出对应的闭式表达；数值结果验证分析和设计方法的有效性。

Conclusion: AFDM提供可调参数以缓解强目标对弱目标的影响，且脉冲整形进一步优化主瓣和旁瓣特性；该工作为AFDM在ISAC中的参数设计提供理论基础，且揭示OFDM/OCDM在凹陷数量上的相似性。

Abstract: This paper investigates the ambiguity function (AF) of the emerging affine
frequency division multiplexing (AFDM) waveform for Integrated Sensing and
Communication (ISAC) signaling under a pulse shaping regime. Specifically, we
first derive the closed-form expression of the average squared discrete period
AF (DPAF) for AFDM waveform without pulse shaping, revealing that the AF
depends on the parameter $c_1$ and the kurtosis of random communication data,
while being independent of the parameter $c_2$. As a step further, we conduct a
comprehensive analysis on the AFs of various waveforms, including AFDM,
orthogonal frequency division multiplexing (OFDM) and orthogonal chirp-division
multiplexing (OCDM). Our results indicate that all three waveforms exhibit the
same number of regular depressions in the sidelobes of their AFs, which incurs
performance loss for detecting and estimating weak targets. However, the AFDM
waveform can flexibly control the positions of depressions by adjusting the
parameter $c_1$, which motivates a novel design approach of the AFDM parameters
to mitigate the adverse impact of depressions of the strong target on the weak
target. Furthermore, a closed-form expression of the average squared DPAF for
pulse-shaped random AFDM waveform is derived, which demonstrates that the pulse
shaping filter generates the shaped mainlobe along the delay axis and the rapid
roll-off sidelobes along the Doppler axis. Numerical results verify the
effectiveness of our theoretical analysis and proposed design methodology for
the AFDM modulation.

</details>


### [73] [RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding](https://arxiv.org/abs/2511.04351)
*Hasan Akgul,Mari Eplik,Javier Rojas,Akira Yamamoto,Rajesh Kumar,Maya Singh*

Main category: eess.SP

TL;DR: RCMCL 通过跨模态对比学习、模态内自蒸馏和降解建模实现跨模态对齐和对模态缺失鲁棒性，并以自适应模态门控进行鲁棒融合，在 NTU RGB+D 120 (CS/CV) 与 UWA3D-II 上达到标准设置的最新准确率，并在强模态缺失下具显著鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态人类动作识别通常需要大量带标签数据，且在传感器失效或噪声时性能剧烈下降，现有自监督或监督方法对跨模态对齐与降解鲁棒性欠缺。

Method: 提出 RC-MCL：,(i) 跨模态对比目标以对齐异构模态；(ii) 模态内自蒸馏以提升视图不变性与降低冗余；(iii) 降解建模目标在训练中显式让模型从被遮挡或损坏的输入中恢复。推理时通过自适应模态门控分配模态权重进行鲁棒融合。

Result: 在 NTU RGB+D 120（CS/CV）和 UWA3D-II 数据集上达到标准设置的最新准确率，并且在严重双模态断线情况下退化仅 11.5%，显著优于强基线。

Conclusion:  自监督跨模态对齐结合显式降解建模和自适应融合，是实现可部署的多模态 HAR 的关键。

Abstract: Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,
point cloud) can achieve high accuracy but typically relies on large labeled
datasets and degrades sharply when sensors fail or are noisy. We present Robust
Cross-Modal Contrastive Learning (RCMCL), a self-supervised framework that
learns modality-invariant representations and remains reliable under modality
dropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive
objective that aligns heterogeneous streams, (ii) an intra-modal
self-distillation objective that improves view-invariance and reduces
redundancy, and (iii) a degradation simulation objective that explicitly trains
models to recover from masked or corrupted inputs. At inference, an Adaptive
Modality Gating (AMG) network assigns data-driven reliability weights to each
modality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL
attains state-of-the-art accuracy in standard settings and exhibits markedly
better robustness: under severe dual-modality dropout it shows only an 11.5%
degradation, significantly outperforming strong supervised fusion baselines.
These results indicate that self-supervised cross-modal alignment, coupled with
explicit degradation modeling and adaptive fusion, is key to deployable
multi-modal HAR.

</details>


### [74] [High-Resolution Forest Mapping from L-Band Interferometric SAR Time Series using Deep Learning over Northern Spain](https://arxiv.org/abs/2511.04362)
*Chiara Telli,Oleg Antropov,Anne Lönnqvist,Marco Lavalle*

Main category: eess.SP

TL;DR: 本研究评估基于L波段干涉SAR时间序列的高分辨率森林高度反演，结合UNet家族的深度学习模型，探索输入特征的不同组合对精度的影响。结果表明引入模型基于InSAR特征及相干性、以及注意力与嵌套连接，显著提升预测准确度，最优RMSE约1.95–2.2 m，R2约0.45–0.55，适用于NISAR/ROSE-L等未来任务。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率森林高度反演的挑战，充分利用L波段干涉SAR时间序列及多源输入特征以提升森林结构参数估计精度。

Method: 以九帧ALOS-2 PALSAR-2双极化干涉SAR时间序列、近零基线，基于UNet家族的Vanilla、SeU-Net（带Squeeze-Excitation注意力）与嵌套UNet，比较不同输入特征组合（强度、双极化干涉观测、基于模型的InSAR特征、InSAR相干性等）的森林高度反演。以空气激光扫描为参考数据，评估RMSE和R2。

Result: 引入模型基InSAR特征与InSAR相干性、以及注意力与嵌套连接，促成比单纯强度更高的预测精度；20 m分辨率下RMSE约3.1–3.8 m，60 m分辨率下可降至约2.2 m（全部特征时约1.95 m），R2约0.45–0.55。

Conclusion: 将强度与干涉特征结合的混合方法及高级UNet变体对L波段森林高度反演具有显著优势，适用于NISAR和未来ROSE-L等任务。

Abstract: In this study, we examine the potential of high-resolution forest mapping
using L-band interferometric time series datasets and deep learning modeling.
Our SAR data are represented by a time series of nine ALOS-2 PALSAR-2 dual-pol
SAR images acquired at near-zero spatial baseline over a study site in
Asturias, Northern Spain. Reference data are collected using airborne laser
scanning. We examine the performance of several candidate deep learning models
from UNet-family with various combinations of input polarimetric and
interferometric features. In addition to basic Vanilla UNet, attention
reinforced UNet model with squeeze-excitation blocks (SeU-Net) and advanced
UNet model with nested structure and skip pathways are used. Studied features
include dual pol interferometric observables additionally incorporating
model-based derived measures. Results show that adding model-based inverted
InSAR features or InSAR coherence layers improves retrieval accuracy compared
to using backscatter intensity only. Use of attention mechanisms and nested
connection fusion provides better predictions than using Vanilla UNet or
traditional machine learning methods. Forest height retrieval accuracies range
between 3.1-3.8 m (R2 = 0.45--0.55) at 20 m resolution when only intensity data
are used, and improve to less than 2.8 m when both intensity and
interferometric coherence features are included. At 40 m and 60 m resolution,
retrieval performance further improves, primarily due to higher SNR in both the
intensity and interferometric layers. When using intensity at 60 m resolution,
best achieved RMSE is 2.2 m, while when using all suitable input features the
achieved error is 1.95 m. We recommend this hybrid approach for L-band SAR
retrievals also suitable for NISAR and future ROSE-L missions.

</details>


### [75] [A Lightweight Framework for Integrated Sensing and Communications with RIS](https://arxiv.org/abs/2511.04448)
*Chu Li,Kevin Weinberger,Aydin Sezgin*

Main category: eess.SP

TL;DR: 提出一种轻量级的 RIS 相位设计框架，给出闭式解，权衡通信与感知，分两阶段配置，前段优化通信，后段微扰产生多束用于多目标感知，复杂度显著低于 SDR，性能接近 SDR。


<details>
  <summary>Details</summary>
Motivation: 解决 RIS 优化在 ISAC 系统中的高复杂度和可扩展性不足问题，避免 SDR 的高计算成本与迭代算法的初值敏感。

Method: 把 RIS 配置分成两部分：第一部分最大化通信性能，第二部分对相位施加小扰动以产生多束用于多目标感知；给出闭式解并考虑通信与感知之间的折衷及对多目标的定比分布。

Result: 仿真结果显示所提方法在性能上与 SDR 相当，但计算复杂度显著降低。

Conclusion: 提出的框架实现了低复杂度的 RIS 相位设计，同时兼顾通信和感知性能，适用于未来6G ISAC 系统。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have been recognized as a promising
technology to enhance both communication and sensing performance in integrated
sensing and communication (ISAC) systems for future 6G networks. However,
existing RIS optimization methods for improving ISAC performance are mainly
based on semidefinite relaxation (SDR) or iterative algorithms. The former
suffers from high computational complexity and limited scalability, especially
when the number of RIS elements becomes large, while the latter yields
suboptimal solutions whose performance depends on initialization. In this work,
we introduce a lightweight RIS phase design framework that provides a
closed-form solution and explicitly accounts for the trade-off between
communication and sensing, as well as proportional beam gain distribution
toward multiple sensing targets. The key idea is to partition the RIS
configuration into two parts: the first part is designed to maximize the
communication performance, while the second introduces small perturbations to
generate multiple beams for multi-target sensing. Simulation results validate
the effectiveness of the proposed approach and demonstrate that it achieves
performance comparable to SDR but with significantly lower computational
complexity.

</details>


### [76] [An Area-Efficient 20-100-GHz Phase-Invariant Switch-Type Attenuator Achieving 0.1-dB Tuning Step in 65-nm CMOS](https://arxiv.org/abs/2511.04635)
*Qingbin Li,Jian Pang*

Main category: eess.SP

TL;DR: 20-100 GHz 的开关型衰减器，采用电容补偿以减小相位误差，金属线实现小电阻以降低寄生电容并减小幅度/相位误差，具连续调谐衰减单元，基于65nm CMOS 制造，达到7.5 dB 的衰减范围，插入损耗 1.6-3.8 dB，回波损耗 >11.5 dB，RMS 振幅误差 <0.15 dB，相位误差 <1.6°。


<details>
  <summary>Details</summary>
Motivation: 需要在高频（20-100 GHz）范围内实现宽带、低误差的衰减器，并便于与 CMOS 集成的紧凑方案。

Method: 提出一种开关型衰减器，使用电容补偿来降低相位误差；通过金属线替代小电阻以降低寄生电容并缩小芯片面积；引入连续调谐衰减单元以提升衰减精度；在标准65nm CMOS 工艺上设计并实现。

Result: 测量结果显示在20-100 GHz范围内具7.5 dB 的相对衰减范围，连续调谐步长内的衰减，插入损耗1.6-3.8 dB，所有状态的回波损耗>11.5 dB，RMS 振幅误差<0.15 dB，相位误差<1.6°。

Conclusion: 该工作展示了一种在极高频段实现宽带、低误差衰减的可集成方案，利用金属线减少寄生电容与面积，并通过电容补偿实现较低相位误差，适合CMOS 嵌入式射频前端。

Abstract: This paper presents a switch-type attenuator working from 20 to 100 GHz. The
attenuator adopts a capacitive compensation technique to reduce phase error.
The small resistors in this work are implemented with metal lines to reduce the
intrinsic parasitic ca?pacitance, which helps minimize the amplitude and phase
errors over a wide frequency range. Moreover, the utilization of metal lines
also reduces the chip area. In addition, a continuous tuning attenuation unit
is employed to improve the overall attenuation accuracy of the attenuator. The
passive attenuator is designed and fabricated in a standard 65nm CMOS. The
measurement results reveal a relative attenuation range of 7.5 dB with a
continuous tuning step within 20-100 GHz. The insertion loss is 1.6-3.8 dB
within the operation band, while the return losses of all states are better
than 11.5 dB. The RMS amplitude and phase errors are below 0.15 dB and
1.6{\deg}, respectively.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [77] [On excitation of control-affine systems and its use for data-driven Koopman approximants](https://arxiv.org/abs/2511.03734)
*Philipp Schmitz,Lea Bold,Friedrich M. Philipp,Mario Rosenfelder,Peter Eberhard,Henrik Ebel,Karl Worthmann*

Main category: eess.SY

TL;DR: 提出一个数据拟合框架，针对控制-仿射系统获得鲁棒性更强的双线性EDMD近似，通过基于子空间角度的输入选择和最小奇异值阈值来提升鲁棒性，并给出最大化最小奇异值的必要充分条件，且在具备约束的非完整机器人场景中给出示例。


<details>
  <summary>Details</summary>
Motivation: Koopman算子及扩展的EDMD作为数据驱动的近似工具在建模、分析和控制复杂动力系统方面具有重要意义。然而，向控制-仿射系统扩展，导致的双线性替代模型往往需要大量数据，使得应用变得困难。本文提出一个数据拟合框架，以提高相关系统辨识问题的鲁棒裕度，从而提供更可靠的双线性EDMD方案。

Method: 提出针对控制-仿射映射的数据拟合框架，并给出基于子空间角度的输入选择准则，以确保最小奇异值达到所需阈值。此外，推导最大化最小奇异值的必要充分条件。

Result: 使用带控制的双线性EDMD在非完整约束的机器人实例中演示，验证所提出方法可以提高鲁棒性并提供更可靠的双线性EDMD近似。

Conclusion: 所提出的输入选取策略和最优性条件为控制-仿射系统的双线性EDMD提供了一套鲁棒且可验证的框架，提升了数据驱动辨识在此类系统中的可靠性与适用性。

Abstract: The Koopman operator and extended dynamic mode decomposition (EDMD) as a
data-driven technique for its approximation have attracted considerable
attention as a key tool for modeling, analysis, and control of complex
dynamical systems. However, extensions towards control-affine systems resulting
in bilinear surrogate models are prone to demanding data requirements rendering
their applicability intricate. In this paper, we propose a framework for
data-fitting of control-affine mappings to increase the robustness margin in
the associated system identification problem and, thus, to provide more
reliable bilinear EDMD schemes. In particular, guidelines for input selection
based on subspace angles are deduced such that a desired threshold with respect
to the minimal singular value is ensured. Moreover, we derive necessary and
sufficient conditions of optimality for maximizing the minimal singular value.
Further, we demonstrate the usefulness of the proposed approach using bilinear
EDMD with control for non-holonomic robots.

</details>


### [78] [Hybrid ILM-NILM Smart Plug System](https://arxiv.org/abs/2511.03737)
*Dániel István Németh,Kálmán Tornai*

Main category: eess.SY

TL;DR: 通过扩展智能插座以同一个插座连接多负载，形成混合负载分类方法，在降低安装成本的同时牺牲一定的控制粒度。


<details>
  <summary>Details</summary>
Motivation: 在负载分类中平衡控制粒度与安装成本的矛盾，结合侵入式与非侵入式方法；现实场景中家庭常以扩展线将多个负载接入单一智能插座。

Method: 提出一种混合方法：将智能插座扩展到单个插座可连接多负载，通过扩展线实现多设备连接，分析其对控制粒度和分类能力的影响，并讨论在实际家庭环境中的可行性。

Result: 通过多负载接入同一智能插座，降低总体安装成本，但相应地降低了对单个设备的独立控制粒度；该混合方案能够覆盖常见的家庭场景，具有实际可行性。

Conclusion: 混合负载分类方法为侵入式与非侵入式的折中方案，提升了应用范围，尤其在希望降低安装成本但又需一定控制能力的场景中具有实用性。

Abstract: Electrical load classification is generally divided into intrusive and
non-intrusive approaches, both having their limitations and advantages. With
the non-intrusive approach, controlling appliances is not possible, but the
installation cost of a single measurement device is cheap. In comparison,
intrusive, smart plug-based solutions offer individual appliance control, but
the installation cost is much higher. There have been very few approaches
aiming to combine these methods. In this paper we show that extending a smart
plug-based solution to multiple loads per plug can reduce control granularity
in favor of lowering the system's installation costs. Connecting various loads
to a Smart Plug through an extension cord is seldom considered in the
literature, even though it is common in households. This scenario is also
handled by the hybrid load classification solution presented in this paper.

</details>


### [79] [Electric Vehicle Charging Load Modeling: A Survey, Trends, Challenges and Opportunities](https://arxiv.org/abs/2511.03741)
*Xiachong Lin,Arian Prabowo,Imran Razzak,Hao Xue,Matthew Amos,Sam Behrens,Flora D. Salim*

Main category: eess.SY

TL;DR: 对过去五年电动汽车充电负荷建模的系统性综述，按统计、仿真、数据驱动三类方法分类，并分析信息融合的三个自下而上的层级及未来挑战与研究方向。


<details>
  <summary>Details</summary>
Motivation: EV充电行为的准确预测对基础设施规划与优化至关重要；充电负荷受不确定性和随机性影响显著，现有综述缺乏对信息融合在建模中的系统分析。

Method: 对最近五年的文献进行综述，按统计、仿真、数据驱动三大类对建模方法进行归类，系统分析信息融合的三种自下而上的层级（数据层、特征层、决策层）及其在EV充电负荷建模中的应用与优缺点。

Result: 总结各类别方法的优点与局限；分析信息融合三个层级在不同场景中的适用性；给出对该领域存在的挑战与机遇的综合评估。

Conclusion: 提出未来研究方向与实践建议，推动对EV充电负荷建模的理论理解与实际落地。

Abstract: The evolution of electric vehicles (EVs) is reshaping the automotive
industry, advocating for more sustainable transportation practices. Accurately
predicting EV charging behavior is essential for effective infrastructure
planning and optimization. However, the charging load of EVs is significantly
influenced by uncertainties and randomness, posing challenges for accurate
estimation. Furthermore, existing literature reviews lack a systematic analysis
of modeling approaches focused on information fusion. This paper
comprehensively reviews EV charging load models from the past five years. We
categorize state-of-the-art modeling methods into statistical, simulated, and
data-driven approaches, examining the advantages and drawbacks of each.
Additionally, we analyze the three bottom-up level operations of information
fusion in existing models. We conclude by discussing the challenges and
opportunities in the field, offering guidance for future research endeavors to
advance our understanding and explore practical research directions.

</details>


### [80] [A Model-Based Approach to Automated Digital Twin Generation in Manufacturing](https://arxiv.org/abs/2511.03742)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: eess.SY

TL;DR: 一种基于AutomationML工厂计划的数字孪生平台，通过GAI驱动的场景生成器和自动化线体重新配置实现数字孪生的快速生成、仿真和部署，提升制造业的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代制造对灵活性和可重配置性提出高要求。基于模型的工程（MBE）虽能辅助快速的产线设计，但最终的重配置仍依赖仿真与验证。数字孪胎（DT）通过实时监控、仿真和重配置来简化该过程。本文提出一个新颖的平台，自动化生成并部署DT，基于AutomationML工厂计划，并通过GAI驱动的仿真场景生成器与自动物理线体重配置闭环，提升效率与适应性。

Method: 平台采用基于AutomationML的工厂计划自动生成并部署DT；集成GAI驱动的仿真场景生成器以产生生产情景；实现对实际线体的自动物理重配置；形成设计-仿真-运行的闭环。

Result: 该平台实现了DT的自动化生成与部署，并通过实时监控、仿真和自动重配置提高制造过程的效率与适应性。

Conclusion: 该方法将模型驱动工程与数字孪生深度整合，缩短从设计到实际部署的周期，降低停机时间并提升对变化生产需求的响应能力。

Abstract: Modern manufacturing demands high flexibility and reconfigurability to adapt
to dynamic production needs. Model-based Engineering (MBE) supports rapid
production line design, but final reconfiguration requires simulations and
validation. Digital Twins (DTs) streamline this process by enabling real-time
monitoring, simulation, and reconfiguration. This paper presents a novel
platform that automates DT generation and deployment using AutomationML-based
factory plans. The platform closes the loop with a GAI-powered simulation
scenario generator and automatic physical line reconfiguration, enhancing
efficiency and adaptability in manufacturing.

</details>


### [81] [Predictive Compensation in Finite-Horizon LQ Games under Gauss-Markov Deviations](https://arxiv.org/abs/2511.03744)
*Navid Mojahed,Mahdis Rabbani,Shima Nazari*

Main category: eess.SY

TL;DR: 在有限时域的离散时间线性二次型动态博弈中，针对来自反馈纳什策略的Gauss-Markov偏差，提出一种预测性补偿框架，给出均值与协方差的闭式递推，并评估预计成本的敏感性以量化性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决在动态博弈中对偏差的鲁棒性问题，尤其是在扰动呈自回归相关且对一方产生显著影响的情况下，通过前瞻性补偿提高策略性能。

Method: 建立有限时域离散时间LQ动态博弈模型，假设一名玩家的偏差遵循一阶自回归过程；另一名玩家采用预测性策略，预先考虑未来相关性的影响；推导均值与协方差的闭式传播递推；对期望成本的灵敏度进行分析。

Result: 给出均值和协方差的闭式递推公式；分析并量化预测补偿对成本的提升，显示在相关噪声存在时的性能改进。

Conclusion: 引入的预测性补偿框架可在带有Gauss-Markov偏差的有限-horizon离散LQ动态博弈中实现成本改进，且闭式递推便于直接评估性能增益。

Abstract: This paper presents a predictive compensation framework for finite-horizon
discrete-time linear quadratic dynamic games in the presence of Gauss-Markov
deviations from feedback Nash strategies. One player experiences correlated
stochastic deviations, modeled via a first-order autoregressive process, while
the other compensates using a predictive strategy that anticipates the effect
of future correlation. Closed-form recursions for mean and covariance
propagation are derived, and the resulting performance improvement is analyzed
through the sensitivity of expected cost.

</details>


### [82] [InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters](https://arxiv.org/abs/2511.03745)
*Osama A. Marzouk*

Main category: eess.SY

TL;DR: 提出一个通用6-DOF固定翼飞行力学InvSim逆仿真框架，通过符号计算、RK4和有限差分法，推导并计算实现给定轨迹所需的控制输入。


<details>
  <summary>Details</summary>
Motivation: 解决在已知目标轨迹条件下逆向求取发动机推力和舵面控制量的问题，提升仿真与控制设计的灵活性与适用性。

Method: 建立包含体坐标、惯性坐标、风向坐标，以及方位角、仰角、攻角与侧滑角等在内的完整EOM框架；导出适用于InvSim的离散化与求解算法，结合符号数学、四阶RK4数值积分和有限差分法，得到整段机动时间上的发动机推力、副翼/襟翼、升降舵与方向舵的控制输入序列。

Result: 提出并实现InvSim数值过程，能够在给定目标三维惯性坐标和滚转角的情况下输出控制输入序列，以实现指定轨迹，并给出相应的仿真实例。

Conclusion: 该工作验证了在通用6-DOF框架下的InvSim方法的可行性与应用潜力，为复杂飞行轨迹控制的逆向仿真提供新工具。

Abstract: In this work, we start with a generic mathematical framework for the
equations of motion (EOM) in flight mechanics with six degrees of freedom
(6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This
mathematical framework incorporates (1) body axes (fixed in the airplane at its
center of gravity), (2) inertial axes (fixed in the earth/ground at the
take-off point), wind axes (aligned with the flight path/course), (3) spherical
flight path angles (azimuth angle measured clockwise from the geographic north,
and elevation angle measured above the horizon plane), and (4) spherical flight
angles (angle of attack and sideslip angle). We then manipulate these equations
of motion to derive a customized version suitable for inverse simulation flight
mechanics, where a target flight trajectory is specified while a set of
corresponding necessary flight controls to achieve that maneuver are predicted.
We then present a numerical procedure for integrating the developed inverse
simulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2)
explicit fourth-order Runge-Kutta (RK4) numerical integration technique, and
(3) expressions based on the finite difference method (FDM); such that the four
necessary control variables (engine thrust force, ailerons' deflection angle,
elevators' deflection angle, and rudder's deflection angle) are computed as
discrete values over the entire maneuver time, and these calculated control
values enable the airplane to achieve the desired flight trajectory, which is
specified by three inertial Cartesian coordinates of the airplane, in addition
to the Euler's roll angle. We finally demonstrate the proposed numerical
procedure of flight mechanics inverse simulation (InvSim).

</details>


### [83] [A Dynamic Recurrent Adjacency Memory Network for Mixed-Generation Power System Stability Forecasting](https://arxiv.org/abs/2511.03746)
*Guang An Ooi,Otavio Bertozzi,Mohd Asim Aftab,Charalambos Konstantinou,Shehab Ahmed*

Main category: eess.SY

TL;DR: DRAMN: A dynamic recurrent adjacency memory network that fuses physics-informed analysis with graph-based deep learning to forecast power-system stability in real time, using sliding-window dynamic mode decomposition to build time-varying adjacency matrices, achieving state-of-the-art accuracies on IEEE test systems and HVDC networks with reduced feature dimensionality and enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional stability assessment methods struggle with scalability and generalization in power systems with high penetration of inverter-based resources; there is a need for real-time, accurate, and interpretable stability forecasting that captures evolving dynamics.

Method: DRAMN integrates graph convolution within recurrent gating mechanisms to model both spatial (graph) and temporal dependencies. It constructs time-varying, multi-layer adjacency matrices via sliding-window dynamic mode decomposition from PMU and sensor data, capturing dynamics such as modal participation factors, coupling strengths, phase relationships, and spectral energy. The framework also performs feature selection, reducing dimensionality by 82% without losing performance.

Result: Validation on modified IEEE 9-bus, 39-bus, and multi-terminal HVDC networks yields average accuracies of 99.85%, 99.90%, and 99.69%, respectively, surpassing classical ML and recent graph-based models. The approach identifies optimal measurement subsets, and correlation analyses of dominant measurements across small-signal and transient stability demonstrate generalizability across different stability phenomena.

Conclusion: DRAMN achieves state-of-the-art accuracy with enhanced interpretability, enabling real-time deployment in modern control centers.

Abstract: Modern power systems with high penetration of inverter-based resources
exhibit complex dynamic behaviors that challenge the scalability and
generalizability of traditional stability assessment methods. This paper
presents a dynamic recurrent adjacency memory network (DRAMN) that combines
physics-informed analysis with deep learning for real-time power system
stability forecasting. The framework employs sliding-window dynamic mode
decomposition to construct time-varying, multi-layer adjacency matrices from
phasor measurement unit and sensor data to capture system dynamics such as
modal participation factors, coupling strengths, phase relationships, and
spectral energy distributions. As opposed to processing spatial and temporal
dependencies separately, DRAMN integrates graph convolution operations directly
within recurrent gating mechanisms, enabling simultaneous modeling of evolving
dynamics and temporal dependencies. Extensive validations on modified IEEE
9-bus, 39-bus, and a multi-terminal HVDC network demonstrate high performance,
achieving 99.85\%, 99.90\%, and 99.69\% average accuracies, respectively,
surpassing all tested benchmarks, including classical machine learning
algorithms and recent graph-based models. The framework identifies optimal
combinations of measurements that reduce feature dimensionality by 82\% without
performance degradation. Correlation analysis between dominant measurements for
small-signal and transient stability events validates generalizability across
different stability phenomena. DRAMN achieves state-of-the-art accuracy while
providing enhanced interpretability for power system operators, making it
suitable for real-time deployment in modern control centers.

</details>


### [84] [Analytical modelling of a stop-less modular bus service with an application to charging strategies comparison](https://arxiv.org/abs/2511.03754)
*Haoran Zhao,Neema Nassir,Andres Fielbaum*

Main category: eess.SY

TL;DR: 提出结合SLAM公交与车对车充电的分析优化模型，比较无充电与有充电策略下的最优设计及可行性，揭示在需求增长过程中的阶段性运营序列与能量限制下的频率变化与不可行性。


<details>
  <summary>Details</summary>
Motivation: 提高大都市公交系统的运行效率，降低停靠时间(dwell time)并减低能源与排放成本，同时应对充电约束对运营灵活性的影响。

Method: 开发并比较面向SLAM公交的分析优化模型，整合车对车充电(V2V)技术，评估无充电与移动/静态充电策略下的可行设计，并在需求增长过程中推导出一系列运营阶段。

Result: 在需求增长序列中揭示：低需求时为闲置容量、随后转为全量小巴、再为全量大巴，最后出现仅扩充容量的频率受限 regime。在移动充电策略下，增加了能量受限情形，频率下降，且在高需求时最终不可行。

Conclusion: 为运营者提供在不同充电策略与需求水平下的最优部署与切换点，提升服务效率并指明实现SLAM与V2V充电耦合的可行性边界。

Abstract: Buses are a vital component of metropolitan public transport, yet
conventional bus services often struggle with inefficiencies including extended
dwelling time, which increases in-vehicle travel time for non-alighting
passengers. A stop-less autonomous modular (SLAM) bus service has emerged as a
solution, enabling dynamic capacity to reduce dwelling time. Meanwhile, the
electrification of buses is advancing as a strategy to mitigate greenhouse gas
emissions and reduces operators' costs, but introduces new operational
constraints due to charging requirements. This study develops analytical
optimization models for SLAM bus service that integrates vehicle-to-vehicle
(V2V) charging technology. By comparing the optimal designs and their
feasibility across non-charging case and charging strategies, we identify a
sequence of operational stages as ridership grows: from idle capacity under low
demand, to full small buses, full large buses, and a proposed frequency-capped
regime where only bus capacity expands. Under the mobile charging strategy,
this progression further includes an energy-limited regime, in which frequency
declines, and ultimately infeasibility under high demand. These findings enable
operators to deliver more efficient services.

</details>


### [85] [Removing Time-Scale Separation in Feedback-Based Optimization via Estimators](https://arxiv.org/abs/2511.03903)
*Niloufar Yousefi,John W. Simpson-Porco*

Main category: eess.SY

TL;DR: 提出一种基于估计器的 FBO 修改，使得不再需要传统 FBO 的时标分离约束，闭环收敛速率受开环系统的主特征值控制，并可推广至近似模型与奇异摄动情形，应用于快速功率系统频率控制。


<details>
  <summary>Details</summary>
Motivation: FBO 在存在外部扰动且对植物动力信息了解有限时，需让闭环稳定地将系统状态收敛到约束优化问题的解。然而传统 FBO 要求控制器运行在比植物慢的时间尺度，显著限制了性能。

Method: 提出基于估计器的 FBO 修改，利用动态植物模型信息消除时间尺度分离的要求，使闭环的收敛速率仅受开环系统的主特征值支配。进一步给出在仅有近似植物模型时的设计扩展，尤其在原系统呈现显著奇异摄动时的情形。

Result: 理论上证实闭环收敛速率受主特征值控制，且设计可在近似植物模型条件下工作；通过对快速功率系统频率控制（ inverter-based 资源） 的应用示例，展示了提案在实际系统中的有效性。

Conclusion: 该方法消除了传统 FBO 的时间尺度分离约束，提升了闭环性能与响应速度，并扩展到近似模型和奇异摄动情形，尤其适用于需要快速响应的能源系统等场景。

Abstract: Feedback-based optimization (FBO) provides a simple control framework for
regulating a stable dynamical system to the solution of a constrained
optimization problem in the presence of exogenous disturbances, and does so
without full knowledge of the plant dynamics. However, closed-loop stability
requires the controller to operate on a sufficiently slower timescale than the
plant, significantly constraining achievable closed-loop performance. Motivated
by this trade-off, we propose an estimator-based modification of FBO which
leverages dynamic plant model information to eliminate the time-scale
separation requirement of traditional FBO. Under this design, the convergence
rate of the closed-loop system is limited only by the dominant eigenvalue of
the open-loop system. We extend the approach to the case of design based on
only an approximate plant model when the original system is singularly
perturbed. The results are illustrated via an application to fast power system
frequency control using inverter-based resources.

</details>


### [86] [A Co-simulation Framework for Quadrotor Control System Design using ROS 2 and MATLAB/Simulink](https://arxiv.org/abs/2511.03969)
*Hangyu Teng*

Main category: eess.SY

TL;DR: 提出一个将 ROS 2 与 MATLAB/Simulink 融合的协同仿真框架，用于四旋翼无人机控制的设计与验证，涵盖六自由度动力学建模、分层控制与跨平台数据交换。


<details>
  <summary>Details</summary>
Motivation: 在复杂的协同系统中，跨工具协同仿真可提升开发效率并降低成本；通过高保真建模与仿真验证提升控制设计的可靠性。

Method: 建立六自由度非线性动力学模型（牛顿-欧拉法则）；设计分层控制：姿态用 LQR，位置用 PID；实现 ROS 2 与 MATLAB/Simulink 的跨平台数据交换框架；通过软硬件在环/软件在环（SIL）进行仿真验证及快速原型开发。

Result: 仿真结果证明该框架能够高效、标准化地进行 UAV 控制算法的快速原型设计与 SIL 验证，显示跨工具协同仿真的有效性。

Conclusion: 提供一个可重复使用的协同仿真架构，提升 UAV 控制设计的开发效率与成本效益，为未来更复杂模块集成打下基础。

Abstract: Co-simulation is a critical approach for the design and analysis of complex
cyber-physical systems. It will enhance development efficiency and reduce
costs. This paper presents a co-simulation framework integrating ROS 2 and
MATLAB/Simulink for quadrotor unmanned aerial vehicle (UAV) control system
design and verification. First, a six-degree-of-freedom nonlinear dynamic model
of the quadrotor is derived accurately that based on Newton-Euler equations.
Second, within the proposed framework, a hierarchical control architecture was
designed and implemented: LQR controller for attitude control to achieve
optimal regulation performance, and PID controller for position control to
ensure robustness and practical applicability. Third, elaborated the
architecture of the framework, including the implementation details of the
cross-platform data exchange mechanism. Simulation results demonstrate the
effectiveness of the framework, highlighting its capability to provide an
efficient and standardized solution for rapid prototyping and
Software-in-the-Loop (SIL) validation of UAV control algorithms.

</details>


### [87] [Necessary and Sufficient Conditions for the Optimization-Based Concurrent Execution of Learned Robotic Tasks](https://arxiv.org/abs/2511.04054)
*Sheikh A. Tahmid,Gennaro Notomista*

Main category: eess.SY

TL;DR: 研究在优化框架下并行执行多任务价值函数的条件，给出必要且充分条件，以及在折扣因子情况下的扩展。


<details>
  <summary>Details</summary>
Motivation: 解决在强化学习所得价值函数能否被并行执行的核心问题，提供理论准则以判断何时可行、何时不可行，并提升与标准RL实践的兼容性。

Method: 提出关于在状态子集内并发执行多任务的必要且充分条件的定理，使用此前提出的最小范数控制器，并将优化框架扩展以处理带折扣因子的价值函数。

Result: 给出判定并发可执行性的准则，指明何时可能并行执行、何时天然可并行、以及何时不可能通过所述方法实现；并将该框架扩展以兼容带折扣因子的价值函数的学习结果。

Conclusion: 理论上揭示了 learned-control 任务并发执行的边界与条件，并在实践层面对 RL 训练的价值函数提供了更高的可并行性与适配性。

Abstract: In this work, we consider the problem of executing multiple tasks encoded by
value functions, each learned through Reinforcement Learning, using an
optimization-based framework. Prior works develop such a framework, but left
unanswered a fundamental question of when learned value functions can be
concurrently executed. The main contribution of this work is to present
theorems which provide necessary and sufficient conditions to concurrently
execute sets of learned tasks within subsets of the state space, using a
previously proposed min-norm controller. These theorems provide insight into
when learned control tasks are possible to be made concurrently executable,
when they might already inherently be concurrently executable and when it is
not possible at all to make a set of learned tasks concurrently executable
using the previously proposed methods. Additional contributions of this work
include extending the optimization-based framework to execute multiple tasks
encoded by value functions to also account for value functions trained with a
discount factor, making the overall framework more compatible with standard RL
practices.

</details>


### [88] [Differential Flatness of Quasi-Static Slider-Pusher Models with Applications in Control](https://arxiv.org/abs/2511.04246)
*Sander De Witte,Tom Lefebvre,Thomas Neve,Andras Retzler,Guillaume Crevecoeur*

Main category: eess.SY

TL;DR: The paper models planar slider-pusher dynamics quasi-statically, analyzes differential flatness, and proposes two control strategies; validated in simulations and experiments for polygon sliders with circular pushers.


<details>
  <summary>Details</summary>
Motivation: Enable reliable manipulation using slider-pusher primitives by deriving a tractable differential model and exploiting flatness for control and planning.

Method: Derive differential kinematic model from limit surface under quasi-static and negligible friction; analyze differential flatness; identify flat outputs; design cascaded quasi-static feedback and dynamic feedback linearization; validate with simulations and experiments using a finger-like pusher and vision-based sensing.

Result: The system exhibits differential flatness with COM as flat output for polygon sliders and circular pushers; two controllers achieve trajectory tracking; simulation with perturbations and noise shows robustness; experiments corroborate simulation gains.

Conclusion: The proposed framework offers viable motion primitive-based manipulation with tractable control and planning, enabling accurate trajectory tracking in planar slider-pusher tasks and motivating further development.

Abstract: This paper investigates the dynamic properties of planar slider-pusher
systems as a motion primitive in manipulation tasks. To that end, we construct
a differential kinematic model deriving from the limit surface approach under
the quasi-static assumption and with negligible contact friction. The
quasi-static model applies to generic slider shapes and circular pusher
geometries, enabling a differential kinematic representation of the system.
From this model, we analyze differential flatness - a property advantageous for
control synthesis and planning - and find that slider-pusher systems with
polygon sliders and circular pushers exhibit flatness with the centre of mass
as a flat output. Leveraging this property, we propose two control strategies
for trajectory tracking: a cascaded quasi-static feedback strategy and a
dynamic feedback linearization approach. We validate these strategies through
closed-loop simulations incorporating perturbed models and input noise, as well
as experimental results using a physical setup with a finger-like pusher and
vision-based state detection. The real-world experiments confirm the
applicability of the simulation gains, highlighting the potential of the
proposed methods for

</details>


### [89] [ComEMS4Build: Comfort-Oriented Energy Management System for Residential Buildings using Hydrogen for Seasonal Storage](https://arxiv.org/abs/2511.04293)
*Jovana Kovačević,Felix Langner,Erfan Tajalli-Ardekani,Marvin Dorn,Simon Waczowicz,Ralf Mikut,Jörg Matthes,Hüseyin K. Çakmak,Veit Hagenmeyer*

Main category: eess.SY

TL;DR: 提出了一种基于模糊逻辑的家庭能源管理系统（ComEMS4Build），集成光伏、BESS和氢气存储，使用燃料电池（FC）与热泵（HP）互补，评估了相较于基于规则的RBC和模型预测控制MPC的性能，基于德国家庭住宅的半合成冬季12周仿真。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源波动带来的供需错配，利用氢储能实现季节性调峰，并通过将FC与HP耦合降低氢系统初始投资；在用户舒适、成本和系统利用率等方面寻找折衷的能源管理策略。

Method: 设计了基于模糊逻辑的ComEMS4Build，并与两种基准控制策略RBC（规则控制，低输入要求）和MPC（理想预测下的成本最优）进行对比评估。评估在德国一个家庭住宅的12周冬季半合成建模场景中进行，比较指标包括热舒适度、每周电费、HESS利用率、与主网的能源交换，以及FC运行的切换次数与工作时长。

Result: 结果表明，ComEMS4Build在10/12周未违反热舒适要求，与MPC在舒适性方面相近；RBC的中位数不舒适度较高，达到0.68 Kh。相比MPC，ComEMS4Build的每周电费增加了12.06欧元，RBC则增加30.14欧元。与RBC相比，ComEMS4Build提高了HESS的利用率和与主网的能源交换效率；在FC运行方面，RBC通过降低切换次数3.48%和工作时长7.59%相较于MPC具有优势。

Conclusion: 与基于MPC的理想最优解相比，基于模糊逻辑的ComEMS4Build在舒适性和系统利用方面接近最优，同时对成本有一定提升但优于RBC。将FC与HP耦合的H2存储策略在降低系统体积方面具有效果，但仍存在成本权衡；RBC在某些FC运行指标上更有优势，但整体能耗和舒适性表现不及ComEMS4Build与MPC。

Abstract: Integrating flexible loads and storage systems into the residential sector
contributes to the alignment of volatile renewable generation with demand.
Besides batteries serving as a short-term storage solution, residential
buildings can benefit from a Hydrogen (H2) storage system, allowing seasonal
shifting of renewable energy. However, as the initial costs of H2 systems are
high, coupling a Fuel Cell (FC) with a Heat Pump (HP) can contribute to the
size reduction of the H2 system. The present study develops a Comfort-Oriented
Energy Management System for Residential Buildings (ComEMS4Build) comprising
Photovoltaics (PV), Battery Energy Storage System (BESS), and H2 storage, where
FC and HP are envisioned as complementary technologies. The fuzzy-logic-based
ComEMS4Build is designed and evaluated over a period of 12 weeks in winter for
a family household building in Germany using a semi-synthetic modeling
approach. The Rule-Based Control (RBC), which serves as a lower benchmark, is a
scheduler designed to require minimal inputs for operation. The Model
Predictive Control (MPC) is intended as a cost-optimal benchmark with an ideal
forecast. The results show that ComEMS4Build, similar to MPC, does not violate
the thermal comfort of occupants in 10 out of 12 weeks, while RBC has a
slightly higher median discomfort of 0.68 Kh. The ComEMS4Build increases the
weekly electricity costs by 12.06 EUR compared to MPC, while RBC increases the
weekly costs by 30.14 EUR. The ComEMS4Build improves the Hybrid Energy Storage
System (HESS) utilization and energy exchange with the main grid compared to
the RBC. However, when it comes to the FC operation, the RBC has an advantage,
as it reduces the toggling counts by 3.48% and working hours by 7.59% compared
to MPC...

</details>


### [90] [Data-Driven Modeling of Photosynthesis Regulation Under Oscillating Light Condition - Part I: In-Silico Exploration](https://arxiv.org/abs/2511.04330)
*Christian Portilla,Arviandy G Aribowo,Ramachandran Anantharaman,César A Gómez-Pérez,Leyla Özkan*

Main category: eess.SY

TL;DR: 利用数据驱动的频域建模，在光照波动条件下获得简化、以控制为导向的光合作用调控模型，并给出基于直流光强的LPV表征。


<details>
  <summary>Details</summary>
Motivation: 解决在光照扰动下对光合作用调控机制进行建模的难题，目标提供可用于控制设计的简化模型并提升对系统动态的理解。

Method: 通过在合成数据集（基于基础DREAM模型BDM）上使用包含直流(DC)与交流(AC)分量的光强输入、叶绿素荧光ChlF输出，应用最佳线性近似(BLA)来估计在不同DC水平与调制频率下的二阶LTI传递函数；在局部模型基础上构建以DC光强为调参变量的线性参数变模型(LPV)，给出紧凑的状态空间表示。

Result: 得到覆盖不同DC水平和调制频率的局部LTI传递函数模型，并据此构建以DC值为调度参数的LPV表示，实现对系统动态的紧凑描述。

Conclusion: 证明基于数据驱动的频域建模在光照波动下的光合作用调控研究中是可行的，提供了一个有用的LPV框架用于控制设计与系统理解；未来需要在实验数据上进行进一步验证与鲁棒性评估。

Abstract: This paper explores the application of data-driven system identification
techniques in the frequency domain to obtain simplified, control-oriented
models of photosynthesis regulation under oscillating light conditions.
In-silico datasets are generated using simulations of the physics-based Basic
DREAM Model (BDM) Funete et al.[2024], with light intensity signals --
comprising DC (static) and AC (modulated) components as input and chlorophyll
fluorescence (ChlF) as output. Using these data, the Best Linear Approximation
(BLA) method is employed to estimate second-order linear time-invariant (LTI)
transfer function models across different operating conditions defined by DC
levels and modulation frequencies of light intensity. Building on these local
models, a Linear Parameter-Varying (LPV) representation is constructed, in
which the scheduling parameter is defined by the DC values of the light
intensity, providing a compact state-space representation of the system
dynamics.

</details>


### [91] [Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay](https://arxiv.org/abs/2511.04451)
*Patrik Valábek,Marek Wadinger,Michal Kvasnica,Martin Klaučo*

Main category: eess.SY

TL;DR: 提出一个基于LSTM增强的Deep Koopman模型，用于对带时间延迟的非线性系统进行Koopman算子近似，从而实现潜在线性化表示。该模型字典自由，不依赖预定义字典；在未知动力学的仿真系统中，预测性能显著优于扩展eDMD，在已知动力学时与eDMD表现相当。


<details>
  <summary>Details</summary>
Motivation: 非线性系统的输入时滞给预测、估计和控制带来挑战，传统线性控制方法在此类问题上往往失效，需要新的、更具表达力的近似框架来捕捉时滞与非线性耦合。

Method: 在深Koopman框架中引入LSTM层以捕获历史依赖和时间延迟，将时滞系统动力学编码到潜在空间，并实现一个字典自由的近似 Koopman 运算的模型，与传统的eDMD不同之处在于不依赖预定义字典。

Result: 与扩展eDMD在一个仿真系统上的比较显示：当真实非线性动力学未知时，预测准确性有显著提升；在动力学已知的情形下，结果与eDMD相当。

Conclusion: 该工作提供了一种在未知动力学和带时滞的非线性系统中实现近似Koopman表示的有效途径，字典自由特性减轻了对系统动力学先验的要求，提升了对时滞性系统的预测能力。

Abstract: Nonlinear dynamical systems with input delays pose significant challenges for
prediction, estimation, and control due to their inherent complexity and the
impact of delays on system behavior. Traditional linear control techniques
often fail in these contexts, necessitating innovative approaches. This paper
introduces a novel approach to approximate the Koopman operator using an
LSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear
systems with time delays. By incorporating Long Short-Term Memory (LSTM)
layers, the proposed framework captures historical dependencies and efficiently
encodes time-delayed system dynamics into a latent space. Unlike traditional
extended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined
dictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which
mitigates the problems with the underlying dynamics being known and
incorporated into the dictionary. Quantitative comparisons with extended eDMD
on a simulated system demonstrate highly significant performance gains in
prediction accuracy in cases where the true nonlinear dynamics are unknown and
achieve comparable results to eDMD with known dynamics of a system.

</details>


### [92] [Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition](https://arxiv.org/abs/2511.04461)
*Giorgio Palma,Andrea Serani,Matteo Diez*

Main category: eess.SY

TL;DR: 提出并验证一种基于Hankel HDMD with control的集成方法，用于高速度猫形船的不确定性认知海况预测；在Delft 372模型上，频率域的FHDMDc优于确定性模型并提供稳健的不确定性估计，BHDMDc在该测试案例中收益有限。


<details>
  <summary>Details</summary>
Motivation: 海洋工程中的海况预测需同时具备高精度与不确定性量化；现有模型对非线性和记忆效应的捕捉有限，且计算成本需兼顾设计与运营支撑。

Method: 基于HDMDc构建一个不依赖显式方程的线性降阶模型，通过对状态与输入进行时滞扩增以捕捉非线性与记忆效应。提出两种集成策略：BHDMDc（贝叶斯，利用先验分布对超参数采样，给出后验均值与置信区间）与FHDMDc（频率学派，基于数据子集的多模型聚合）。在1:33.3比例 Delft 372模型、海况5、Fr=0.425 的不规则波浪实验数据上进行训练、验证、测试并与实验及URANS对比。

Result: FHDMDc相较确定性模型提高了预测精度并提供稳健的不确定性估计；BHDMDc在本测试用例中并未表现出比确定性模型的额外收益；FHDMDc导出的概率密度函数与实验及URANS结果高度吻合。

Conclusion: 基于HDMDc的频率学派集成策略可实现对海况的快速、可靠且具不确定性量化的预测，适用于海险设计与运营决策；对于此用例，贝叶斯集成未显示明显优势。

Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode
Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions
of a high-speed catamaran, namely the Delft 372 model. Experimental
measurements (time histories) of wave elevation at the longitudinal center of
gravity, heave, pitch, notional flight-deck velocity, notional bridge
acceleration, and total resistance were collected from irregular wave basin
tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5
conditions at Fr = 0.425, and organized into training, validation, and test
sets. The HDMDc algorithm constructs an equation-free linear reduced-order
model of the seakeeping vessel by augmenting states and inputs with their
time-lagged copies to capture nonlinear and memory effects. Two ensembling
strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters
considered stochastic variables with prior distribution to produce posterior
mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which
aggregates multiple model obtained over data subsets, are compared in providing
seakeeping prediction and uncertainty quantification. The FHDMDc approach is
found to improve the accuracy of the predictions compared to the deterministic
counterpart, also providing robust uncertainty estimation; whereas the
application of BHDMDc to the present test case is not found beneficial in
comparison to the deterministic model. FHDMDc-derived probability density
functions for the motions closely match both experimental data and URANS
results, demonstrating reliable and computationally efficient seakeeping
prediction for design and operational support.

</details>


### [93] [Synchronous Observer Design for Landmark-Inertial SLAM with Almost-Global Convergence](https://arxiv.org/abs/2511.04531)
*Arkadeep Saha,Pieter van Goor,Antonio Franchi,Ravi Banavar*

Main category: eess.SY

TL;DR: 提出在连续时间框架下的非线性观测器用于LI-SLAM，并在基空间中对观测状态的误差动力学给出局部指数稳定性和近全局渐近稳定性证明，且通过数值仿真验证。


<details>
  <summary>Details</summary>
Motivation: LI-SLAM 同时估计环境中的地标位置和机器人姿态，且需要对观测到的地标位置和IMU数据进行鲁棒、稳定的状态估计。

Method: 构建一个连续时间的非线性观测器，将LI-SLAM的可观测状态编码到一个基空间；在基空间中对误差动力学进行理论分析，证明局部指数稳定性和近全局渐近稳定性；通过仿真实验验证理论结论。

Result: 给出误差动力学在基空间的稳定性结论，并通过仿真验证其有效性。

Conclusion: 该工作提供了在基空间上对LI-SLAM观测器的稳定性保证，并通过仿真验证，增强了连续时间LI-SLAM观测器的理论基础和应用潜力。

Abstract: Landmark Inertial Simultaneous Localisation and Mapping (LI-SLAM) is the
problem of estimating the locations of landmarks in the environment and the
robot's pose relative to those landmarks using landmark position measurements
and measurements from Inertial Measurement Unit (IMU). This paper proposes a
nonlinear observer for LI-SLAM posed in continuous time and analyses the
observer in a base space that encodes all the observable states of LI-SLAM. The
local exponential stability and almost-global asymptotic stability of the error
dynamics in base space is established in the proof section and validated using
simulations.

</details>


### [94] [Control Affine Hybrid Power Plant Subsystem Modeling for Supervisory Control Design](https://arxiv.org/abs/2511.04644)
*Stephen Ampleman,Himanshu Sharma,Sayak Mukherjee,Sonja Glavaski*

Main category: eess.SY

TL;DR: 将风电场、光伏发电厂与电池储能组成的混合电站（HPP）建模并设计控制框架，抽象为控制仿射形式，便于上位控制设计。通过非线性控制与控制边界函数，给出风机转矩与电池电流的控制律以跟踪上层指令并确保安全与稳定；并通过含需量信号、时变风速/辐照度数据及规则型上位控制的测试用例验证可行性。


<details>
  <summary>Details</summary>
Motivation: 解决HPP中多能互补与储能协调的建模与控制设计问题，以提高对电网需求的响应能力、可操作性与安全性。

Method: 将风电场、光伏发电与电池模型整合为控制仿射形式；在风机转矩和电池电流层面提出基于非线性控制与控制边界函数的控制律，用以实现对上层指令的跟踪并保持关键变量在安全区域；设计基于规则的上位控制以实现对需求信号的追踪。

Result: 通过一个综合测试：使用公用事业需求信号进行跟踪、引入时变风速与辐照度数据，以及规则型上位控制，证明所提框架在跟踪性和安全性方面具有性能。

Conclusion: 所提出的建模与控制框架可实现风/光伏/储能的协同控制，为HPP的高效、可靠电力输出提供可行路径，并展示在实际需求跟踪中的潜在应用与扩展方向。

Abstract: Hybrid power plants (HPPs) combine multiple power generators
(conventional/variable) and energy storage capabilities to support generation
inadequacy and grid demands. This paper introduces a modeling and control
design framework for hybrid power plants (HPPs) consisting of a wind farm,
solar plant, and battery storage. Specifically, this work adapts established
modeling paradigms for wind farms, solar plants and battery models into a
control affine form suitable for control design at the supervisory level. In
the case of wind and battery models, generator torque and cell current control
laws are developed using nonlinear control and control barrier function
techniques to track a command from a supervisory control law while maintaining
safe and stable operation. The utility of this modeling and control framework
is illustrated through a test case using a utility demand signal for tracking,
time varying wind and irradiance data, and a rule-based supervisory control
law.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [Design and Detection of Covert Man-in-the-Middle Cyberattacks on Water Treatment Plants](https://arxiv.org/abs/2511.03971)
*Victor Mattos,João Henrique Schmidt,Amit Bhaya,Alan Oliveira de Sá,Daniel Sadoc Menasché,Gaurav Srivastava*

Main category: cs.CR

TL;DR: 本研究提出对水处理等关键基础设施中潜在隐蔽中间人攻击的系统建模与评估框架；通过系统辨识设计隐蔽控制器，并基于 PASAD 异常检测评估检测能力，利用带时延的二阶线性时不变模型对水处理动力学进行仿真，揭示噪声和攻击者植物模型不精度对攻击隐蔽性的影响，强调需要更鲁棒的检测策略。


<details>
  <summary>Details</summary>
Motivation: 保护关键基础设施免受隐蔽攻击对公共健康、安全和环境的潜在威胁；需要能系统化地建模攻击、评估其隐蔽性并评估检测方法。

Method: 建立带时延的二阶线性时不变模型来近似水处理系统；通过系统辨识设计潜在的隐蔽中间人攻击并部署隐蔽控制器；以 PASAD 作为监测手段对异常进行检测；在仿真中设计并评估攻击的 stealthiness，考察系统噪声和攻击者植物模型不精度对检测效果的影响

Result: 仿真结果表明，系统噪声和攻击者对植物模型的误差会显著影响攻击的隐蔽性；在某些条件下攻击可以维持隐蔽性，从而暴露出 PASAD 等检测方法的鲁棒性不足；需要更强健的检测策略以应对此类协同比攻击。

Conclusion: 本文提供一个可系统化评估潜在隐蔽攻击及其检测能力的框架，强调对工业控制环境中对抗隐蔽 MitM 攻击的鲁棒检测策略的必要性。

Abstract: Cyberattacks targeting critical infrastructures, such as water treatment
facilities, represent significant threats to public health, safety, and the
environment. This paper introduces a systematic approach for modeling and
assessing covert man-in-the-middle (MitM) attacks that leverage system
identification techniques to inform the attack design. We focus on the
attacker's ability to deploy a covert controller, and we evaluate
countermeasures based on the Process-Aware Stealthy Attack Detection (PASAD)
anomaly detection method. Using a second-order linear time-invariant with time
delay model, representative of water treatment dynamics, we design and simulate
stealthy attacks. Our results highlight how factors such as system noise and
inaccuracies in the attacker's plant model influence the attack's stealthiness,
underscoring the need for more robust detection strategies in industrial
control environments.

</details>


### [96] [Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback](https://arxiv.org/abs/2511.03995)
*Shiyin Lin*

Main category: cs.CR

TL;DR: Hybrid fuzzing with static/dynamic analysis and LLM-guided mutation, using semantic feedback to improve vulnerability discovery; implemented on AFL++; evaluated on libpng, tcpdump, sqlite; faster time-to-first-bug and higher semantic diversity with competitive bug counts.


<details>
  <summary>Details</summary>
Motivation: Existing mutation strategies in fuzzing are semantically unaware, leading to redundant inputs and slow exploration of deep program states.

Method: Integrate static analysis (control-flow and data-flow) to prompt LLM to produce semantically diverse inputs; runtime semantic feedback signals from program state changes, exception types, and output semantics; combine with AFL++ instrumentation and embedding-based semantic similarity for seed selection.

Result: Evaluation on real-world targets (libpng, tcpdump, sqlite) shows faster time-to-first-bug, higher semantic diversity, and competitive number of unique bugs compared to state-of-the-art fuzzers.

Conclusion: LLM-guided, semantic-aware fuzzing can accelerate and deepen vulnerability discovery; demonstrates potential of combining reasoning with semantic feedback.

Abstract: Software fuzzing has become a cornerstone in automated vulnerability
discovery, yet existing mutation strategies often lack semantic awareness,
leading to redundant test cases and slow exploration of deep program states. In
this work, I present a hybrid fuzzing framework that integrates static and
dynamic analysis with Large Language Model (LLM)-guided input mutation and
semantic feedback. Static analysis extracts control-flow and data-flow
information, which is transformed into structured prompts for the LLM to
generate syntactically valid and semantically diverse inputs. During execution,
I augment traditional coverage-based feedback with semantic feedback
signals-derived from program state changes, exception types, and output
semantics-allowing the fuzzer to prioritize inputs that trigger novel program
behaviors beyond mere code coverage. I implement our approach atop AFL++,
combining program instrumentation with embedding-based semantic similarity
metrics to guide seed selection. Evaluation on real-world open-source targets,
including libpng, tcpdump, and sqlite, demonstrates that our method achieves
faster time-to-first-bug, higher semantic diversity, and a competitive number
of unique bugs compared to state-of-the-art fuzzers. This work highlights the
potential of combining LLM reasoning with semantic-aware feedback to accelerate
and deepen vulnerability discovery.

</details>


### [97] [Automated and Explainable Denial of Service Analysis for AI-Driven Intrusion Detection Systems](https://arxiv.org/abs/2511.04114)
*Paul Badu Yakubu,Lesther Santana,Mohamed Rahouti,Yufeng Xin,Abdellah Chehri,Mohammed Aledhari*

Main category: cs.CR

TL;DR: 提出一个基于 TPOT 自动化模型与特征选择，结合 SHAP 解释的 DDoS 检测框架，实现高准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着 DDoS 攻击日益频繁且日趋复杂，需要高效且具可解释性的检测方法，以提升实时响应能力和对攻击向量的理解。

Method: 利用 Tree-based Pipeline Optimization Tool (TPOT) 自动化选择和优化机器学习模型与特征；引入 SHAP 提供模型可解释性，揭示各特征对检测的贡献。

Result: 将 TPOT 的自动化管道搜索与 SHAP 解释结合，提升检测准确性与透明度；实验指出关键特征如 mean backward packet length 与 minimum forward packet header length 对 DDoS 检测尤为关键。

Conclusion: TPOT + SHAP 为 DDoS 检测提供可扩展且可解释的解决方案，具备更高的可理解性和潜在的实时应用价值。

Abstract: With the increasing frequency and sophistication of Distributed Denial of
Service (DDoS) attacks, it has become critical to develop more efficient and
interpretable detection methods. Traditional detection systems often struggle
with scalability and transparency, hindering real-time response and
understanding of attack vectors. This paper presents an automated framework for
detecting and interpreting DDoS attacks using machine learning (ML). The
proposed method leverages the Tree-based Pipeline Optimization Tool (TPOT) to
automate the selection and optimization of ML models and features, reducing the
need for manual experimentation. SHapley Additive exPlanations (SHAP) is
incorporated to enhance model interpretability, providing detailed insights
into the contribution of individual features to the detection process. By
combining TPOT's automated pipeline selection with SHAP interpretability, this
approach improves the accuracy and transparency of DDoS detection. Experimental
results demonstrate that key features such as mean backward packet length and
minimum forward packet header length are critical in detecting DDoS attacks,
offering a scalable and explainable cybersecurity solution.

</details>


### [98] [A Parallel Region-Adaptive Differential Privacy Framework for Image Pixelization](https://arxiv.org/abs/2511.04261)
*Ming Liu*

Main category: cs.CR

TL;DR: 提出一种并行、区域自适应的差分隐私像素化框架，通过区域复杂度自适应网格与噪声尺度，在GPU并行下实现高效运行，并通过轻量存储方案显著降低空间开销；在PETS、Venice-2和PPM-100数据集上展现良好的隐私-效用权衡、显著的运行时与存储提升，并在CelebA的人脸再识别攻击中验证对身份推断的有效防护。


<details>
  <summary>Details</summary>
Motivation: 在高分辨率视频传感和基础模型崛起背景下，视频数据存在明显的隐私风险。差分隐私像素化提供严格数学保障，但在保持任务相关保真度、扩展性与实时性方面仍面临挑战。本文提出一种并行、区域自适应的像素化方法以缓解这些问题。

Method: 提出一种并行、区域自适应的像素化框架：根据区域复杂度自适应调整网格尺寸和噪声尺度，利用GPU并行性实现显著的运行时加速；引入轻量化存储方案，仅保留关键的带噪声统计信息以降低空间开销；在拉普拉斯机制与并行组合定理的基础上给出形式化隐私分析。

Result: 在PETS、Venice-2、PPM-100数据集上实现了更优的隐私-效用折中，同时在运行时间和存储需求上相较经典基线有显著降低；在CelebA上进行的人脸再识别攻击实验证明该方法有效阻止身份推断。

Conclusion: 该方法适用于实时隐私关键应用场景，如养老照护、智能家居监控、驾驶行为分析与人群行为监测等，具有良好的理论保障与工程可实现性。

Abstract: The widespread deployment of high-resolution visual sensing systems, coupled
with the rise of foundation models, has amplified privacy risks in video-based
applications. Differentially private pixelization offers mathematically
guaranteed protection for visual data through grid-based noise addition, but
challenges remain in preserving task-relevant fidelity, achieving scalability,
and enabling efficient real-time deployment. To address this, we propose a
novel parallel, region-adaptive pixelization framework that combines the
theoretical rigor of differential privacy with practical efficiency. Our method
adaptively adjusts grid sizes and noise scales based on regional complexity,
leveraging GPU parallelism to achieve significant runtime acceleration compared
to the classical baseline. A lightweight storage scheme is introduced by
retaining only essential noisy statistics, significantly reducing space
overhead. Formal privacy analysis is provided under the Laplace mechanism and
parallel composition theorem. Extensive experiments on the PETS, Venice-2, and
PPM-100 datasets demonstrate favorable privacy-utility trade-offs and
significant runtime/storage reductions. A face re-identification attack
experiment on CelebA further confirms the method's effectiveness in preventing
identity inference. This validates its suitability for real-time
privacy-critical applications such as elderly care, smart home monitoring,
driver behavior analysis, and crowd behavior monitoring.

</details>


### [99] [Data Certification Strategies for Blockchain-based Traceability Systems](https://arxiv.org/abs/2511.04409)
*Giacomo Zonneveld,Giulia Rafaiani,Massimo Battaglioni,Marco Baldi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The use of blockchains for data certification and traceability is now well
established in both the literature and practical applications. However, while
blockchain-based certification of individual data is clear and straightforward,
the use of blockchain to certify large amounts of data produced on a nearly
continuous basis still poses some challenges. In such a case, in fact, it is
first necessary to collect the data in an off-chain buffer, and then to
organize it, e.g., via Merkle trees, in order to keep the size and quantity of
certification data to be written to the blockchain small. In this paper, we
consider a typical system for blockchain-based traceability of a production
process, and propose and comparatively analyze some strategies for certifying
the data of such a process on blockchain, while maintaining the possibility of
verifying their certification in a decentralized way.

</details>


### [100] [Large Language Models for Cyber Security](https://arxiv.org/abs/2511.04508)
*Raunak Somani,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: LLMs can augment cybersecurity tools by providing intelligent, scalable, and context-aware capabilities; encrypted prompting helps defend against prompt injection; a four-layer architecture and decoupled models contribute to more accurate IDS integration.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and signature-based security systems struggle with AI-powered, adaptive cyber threats; there is a need for scalable, context-aware, and intelligent defenses.

Method: Literature analysis of LLM integration into cybersecurity workflows, including encrypted prompts for security, a four-layer integration architecture, and exploration of IDS enhancements via LLMs and decoupled models.

Result: Encrypted prompts with LLMs mitigate prompt injection attacks; LLM-enhanced cybersecurity tools show higher accuracy, scalability, and adaptability than traditional models; decoupled model approach yields the best accuracy for IDS integration.

Conclusion: LLMs offer meaningful improvements for cybersecurity toolchains when designed with secure prompting and decoupled architectures; careful architectural choices are crucial to maximize detection capabilities while maintaining security against prompt-based threats.

Abstract: This paper studies the integration off Large Language Models into
cybersecurity tools and protocols. The main issue discussed in this paper is
how traditional rule-based and signature based security systems are not enough
to deal with modern AI powered cyber threats. Cybersecurity industry is
changing as threats are becoming more dangerous and adaptive in nature by
levering the features provided by AI tools. By integrating LLMs into these
tools and protocols, make the systems scalable, context-aware and intelligent.
Thus helping it to mitigate these evolving cyber threats. The paper studies the
architecture and functioning of LLMs, its integration into Encrypted prompts to
prevent prompt injection attacks. It also studies the integration of LLMs into
cybersecurity tools using a four layered architecture. At last, the paper has
tried to explain various ways of integration LLMs into traditional Intrusion
Detection System and enhancing its original abilities in various dimensions.
The key findings of this paper has been (i)Encrypted Prompt with LLM is an
effective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber
security tools are more accurate, scalable and adaptable to new threats as
compared to traditional models, (iii) The decoupled model approach for LLM
integration into IDS is the best way as it is the most accurate way.

</details>


### [101] [Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments](https://arxiv.org/abs/2511.04550)
*Dhruv Deepak Agarwal,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: TEEs (如 Intel SGX、ARM TrustZone) 为云端“数据在使用中”提供保护，通过硬件隔离的执行环境提升数据安全，论文对 TEEs 的架构、安全特性、部署策略、性能及应用进行了综述与评估，探讨了实现中的挑战与局限性，并强调 TEEs 在 confidential computing 中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的发展，数据安全面临新的挑战，传统的“静态/传输中的加密”无法完全保护数据在使用中的安全；需要在计算阶段提供可信执行环境来数据保护。

Method: 通过系统性文献综述，分析 Intel SGX、ARM TrustZone 等 TEEs 的架构与安全特性，比较不同部署策略、性能指标及实际应用；讨论部署、可扩展性、集成等挑战；总结 TEEs 在云安全中的作用与局限。

Result:  TEEs 提供受保护的处理上下文，能在受损软件或操作系统下保持数据保密性与完整性，从而推动 confidential computing 的实现；但仍存在部署复杂性、可扩展性、性能开销和集成等问题，需要进一步研究和规范化。

Conclusion: TEEs 对加强云安全基础设施具有中枢地位，是 confidential computing 的关键支撑；要充分发挥其作用需解决部署、可扩展性与互操作性等挑战，并在设计中权衡安全与性能。

Abstract: The growth of cloud computing has revolutionized data processing and storage
capacities to another levels of scalability and flexibility. But in the
process, it has created a huge challenge of security, especially in terms of
safeguarding sensitive data. Classical security practices, including encryption
at rest and during transit, fail to protect data in use and expose it to
various possible breaches. In response to this problem , Confidential Computing
has been a tool ,seeking to secure data in processing by usage of
hardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's
Software Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts
within the processor, where data is kept confidential ,intact and secure , even
with malicious software or compromised operating systems. In this research, we
have explored the architecture and security features of TEEs like Intel SGX and
ARM TrustZone, and their effectiveness in improving cloud data security. From a
thorough literature survey ,we have analyzed the deployment strategies,
performance indicators, and practical uses of these TEEs for the same purpose.
In addition, we have discussed the issues regarding deployment, possible
weaknesses, scalability issues, and integration issues. Our results focuses on
the central position of TEEs in strengthening and advancing cloud security
infrastructures, pointing towards their ability to create a secure foundation
for Confidential Computing.

</details>
