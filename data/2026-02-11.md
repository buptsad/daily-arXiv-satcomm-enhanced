<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 8]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.LG](#cs.LG) [Total: 78]
- [eess.SP](#eess.SP) [Total: 14]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [Shaping Energy Exchange with Gyroscopic Interconnections: a geometric approach](https://arxiv.org/abs/2602.09144)
*Jasper Juchem,Mia Loccufier*

Main category: eess.SY

TL;DR: 本文量化陀螺耦合对子系统能量交换的影响，提出内插半径指标和无仿真方法，发现低阶共振限制能量耗散，而高阶共振可恢复保守能量界限，并提供耦合设计方案。


<details>
  <summary>Details</summary>
Motivation: 陀螺耦合在受控拉格朗日方法（C-L）和能量导入-能量平衡控制（IDA-PBC）中起核心作用，但其对子系统瞬态能量交换与性能的定量影响尚未得到充分表征。

Method: 研究保守力学系统，分析其常数反对称速度耦合下的可积动力学；在极点平面上用投影获得几何图形；定义并计算子系统轨迹的内插半径作为性能指标；推导共振条件，提出无时域仿真的半径求解与判定方法。

Result: 揭示低阶共振显著限制能量衰减，导致子系统能量停滞；高阶共振时可恢复先前的保守能量边界；同时提出利用内插半径进行无时域分析的高效算法，并给出针对能量吸收与封存的明确耦合设计指引。

Conclusion: 本文通过几何分析量化了陀螺耦合对瞬态能量转移的影响，并提出了基于内插半径的子系统性能度量与趋同解剖；得出了低阶共振下能量耗散受限、阶高共振恢复保守界限的结论；同时给出了明确的耦合设计法则，为能量吸收与封存控制提供了可执行框架。

Abstract: Gyroscopic interconnections enable redistribution of energy among degrees of freedom while preserving passivity and total energy, and they play a central role in controlled Lagrangian methods and IDA-PBC. Yet their quantitative effect on transient energy exchange and subsystem performance is not well characterised. We study a conservative mechanical system with constant skew-symmetric velocity coupling. Its dynamics are integrable and evolve on invariant two-tori, whose projections onto subsystem phase planes provide geometric description of energy exchange. When the ratio of normal-mode frequencies is rational, these projections become closed resonant Lissajous curves, enabling structured analysis of subsystem trajectories. To quantify subsystem behaviour, we introduce the inscribed-radius metric: the radius of the largest origin-centred circle contained in a projected trajectory. This gives a lower bound on attainable subsystem energy and acts as an internal performance measure. We derive resonance conditions and develop an efficient method to compute or certify the inscribed radius without time-domain simulation. Our results show that low-order resonances can strongly restrict energy depletion through phase-locking, whereas high-order resonances recover conservative bounds. These insights lead to an explicit interconnection-shaping design framework for both energy absorption and containment control strategies, while taking responsiveness into account.

</details>


### [2] [Dynamic Passivity Multipliers for Plug-and-Play Stability Certificates of Converter-Dominated Grids](https://arxiv.org/abs/2602.09150)
*Andrey Gorbunov,Youhong Chen,Petr Vorobev,Jin Ma,Gregor Verbic*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring small-signal stability in power systems with a high share of inverter-based resources (IBRs) is hampered by two factors: (i) device and network parameters are often uncertain or completely unknown, and (ii) brute-force enumeration of all topologies is computationally intractable. These challenges motivate plug-and-play (PnP) certificates that verify stability locally yet hold globally. Passivity is an attractive property because it guarantees stability under feedback and network interconnections; however, strict passivity rarely holds for practical controllers such as Grid Forming Inverters (GFMs) employing P-Q droop. This paper extends the passivity condition by constructing a dynamic, frequency-dependent multiplier that enables PnP stability certification of each component based solely on its admittance, without requiring any modification to the controller design. The multiplier is parameterised as a linear filter whose coefficients are tuned under a passivity goal. Numerical results for practical droop gains confirm the PnP rules, substantially enlarging the certified stability region while preserving the decentralised, model-agnostic nature of passivity-based PnP tests.

</details>


### [3] [Real-time Load Current Monitoring of Overhead Lines Using GMR Sensors](https://arxiv.org/abs/2602.09213)
*Md Mahfuzur Rahman Chy,Md Rifat Al Amin Khan,Md Sultan Mahamud,Anwarul Islam Sifat,Fiona J. Stevens McFadden*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Non-contact current monitoring has emerged as a prominent research focus owing to its non-intrusive characteristics and low maintenance requirements. However, while they offer high sensitivity, contactless sensors necessitate sophisticated design methodologies and thorough experimental validation. In this study, a Giant Magneto-Resistance (GMR) sensor is employed to monitor the instantaneous currents of a three-phase 400-volt overhead line, and its performance is evaluated against that of a conventional contact-based Hall effect sensor. A mathematical framework is developed to calculate current from the measured magnetic field signals. Furthermore, a MATLAB-based dashboard is implemented to enable real-time visualization of current measurements from both sensors under linear and non-linear load conditions. The GMR current sensor achieved a relative accuracy of 64.64% to 91.49%, with most phases above 80%. Identified improvements over this are possible, indicating that the sensing method has potential as a basis for calculating phase currents.

</details>


### [4] [Escaping Local Minima: A Finite-Time Markov Chain Analysis of Constant-Temperature Simulated Annealing](https://arxiv.org/abs/2602.09398)
*Hansini Ramachandran,Bhaskar Krishnamachari*

Main category: eess.SY

TL;DR: 本文为常温SA提供了有限时分析框架，利用线性基底解析求得逃逸时间表达式，证明其可通过方差匹配与连续SA对应，并以此设计两温度切换策略。


<details>
  <summary>Details</summary>
Motivation: 现有SA理论主要集中于渐进收敛或光谱界限，缺乏有限时间的分析；迫切需要能够预测和控制SA探索行为的实用工具。

Method: 采用离散状态Markov链模型，求解线性成本函数的期望逃逸时间；将结果扩展到二维基底，获得起始于局部最优到全局最优所需时间表达式；通过实验验证连续状态的缩放因子，并利用这些结果设计两温度切换策略。

Result: 获得单基底逃逸时间的闭式表达式，并发现连续状态SA的逃逸时间可按√3比例匹配；在双基底场景中给出从局部最优到全局最优的期望时间公式；此结果被用于指导两温度切换策略的设计。

Conclusion: 本研究通过对线性分段成本函数的精确解析，推导出常温SA在单一线性基底和双基底环境中逃逸所需期望时间的闭式公式，并证明该公式可以通过方差匹配来调整，使其同连续状态SA在特定范围内具有相同的收敛因子，为SA的有限时分析提供了实用工具。

Abstract: Simulated Annealing (SA) is a widely used stochastic optimization algorithm, yet much of its theoretical understanding is limited to asymptotic convergence guarantees or general spectral bounds. In this paper, we develop a finite-time analytical framework for constant-temperature SA by studying a piecewise linear cost function that permits exact characterization. We model SA as a discrete-state Markov chain and first derive a closed-form expression for the expected time to escape a single linear basin in a one-dimensional landscape. We show that this expression also accurately predicts the behavior of continuous-state searches up to a constant scaling factor, which we analyze empirically and explain via variance matching, demonstrating convergence to a factor of sqrt(3) in certain regimes.
  We then extend the analysis to a two-basin landscape containing a local and a global optimum, obtaining exact expressions for the expected time to reach the global optimum starting from the local optimum, as a function of basin geometry, neighborhood radius, and temperature. Finally, we demonstrate how the predicted basin escape time can be used to guide the design of a simple two-temperature switching strategy.

</details>


### [5] [Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors](https://arxiv.org/abs/2602.09414)
*Nazanin S. Hashkavaei,Abhijit Dongare,Neon Srinivasu,Amit K. Sanyal*

Main category: eess.SY

TL;DR: 提出基于SE(3)的有限时稳定姿态估计器，快速收敛、鲁棒、不受奇异影响，已在仿真和实时实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 迫切需要一种能够在三维刚体运动中快速、鲁棒、无奇异、可直接应用于无人系统的姿态估计方法。

Method: 通过Lyapunov分析在SE(3)的切丛上构造全状态观测器，直接利用位姿测量向量和线性/角速度，实现有限时稳定估计；同时提出一种仅使用点云和角速度的变体，并使用几何力学框架离散化实现数值与硬件实现。

Result: 数值仿真显示FTS-PE在有限时间内收敛且对噪声具有鲁棒性，优于双四元数扩展卡尔曼滤波器和之前的变分姿态估计器；实验结果同样确认了其稳定性与鲁棒性。

Conclusion: FTS-PE实现了在有限时间内稳定、鲁棒的姿态估计，避免了局部坐标系的奇异和翻滚现象，能够直接在SE(3)上全状态观测，并已在仿真和基于Zed 2i深度相机的实验中得到验证。

Abstract: This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities. The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations. Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained. It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor. These results validate the stability and robustness of the FTS-PE.

</details>


### [6] [Lateral tracking control of all-wheel steering vehicles with intelligent tires](https://arxiv.org/abs/2602.09427)
*Luigi Romano,Ole Morten Aamo,Jan Åslund,Erik Frisk*

Main category: eess.SY

TL;DR: 提出一种基于ODE‑PDE模型的侧向跟踪控制，利用智能轮胎信息消除微振并提升全轮转向汽车的路径跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 准实时精确表征轮胎动力学对自动驾驶车辆控制至关重要；现有经验式或机器学习方法需大量标定且对工况变化敏感，需要更稳健的模型基方法。

Method: 利用ODE-PDE系统框架构建输出反馈侧向跟踪控制器与观测器，融合分布式轮胎动力学与智能轮胎传感数据，估计轮胎滑角、车辆运动学以及侧向轮胎力。

Result: 控制策略实现了低速微振消除并通过力控实现路径跟随，首次在结合分布式轮胎模型与智能轮胎技术的车辆系统上得到严格控制方案。

Conclusion: 所提出的控制器在全轮转向车辆中验证了通过分布式轮胎模型与智能轮胎技术相结合，实现了微振消除和轨迹跟踪的鲁棒性。

Abstract: The accurate characterization of tire dynamics is critical for advancing control strategies in autonomous road vehicles, as tire behavior significantly influences handling and stability through the generation of forces and moments at the tire-road interface. Smart tire technologies have emerged as a promising tool for sensing key variables such as road friction, tire pressure, and wear states, and for estimating kinematic and dynamic states like vehicle speed and tire forces. However, most existing estimation and control algorithms rely on empirical correlations or machine learning approaches, which require extensive calibration and can be sensitive to variations in operating conditions. In contrast, model-based techniques, which leverage infinite-dimensional representations of tire dynamics using partial differential equations (PDEs), offer a more robust approach. This paper proposes a novel model-based, output-feedback lateral tracking control strategy for all-wheel steering vehicles that integrates distributed tire dynamics with smart tire technologies. The primary contributions include the suppression of micro-shimmy phenomena at low speeds and path-following via force control, achieved through the estimation of tire slip angles, vehicle kinematics, and lateral tire forces. The proposed controller and observer are based on formulations using ODE-PDE systems, representing rigid body dynamics and distributed tire behavior. This work marks the first rigorous control strategy for vehicular systems equipped with distributed tire representations in conjunction with smart tire technologies.

</details>


### [7] [First-order friction models with bristle dynamics: lumped and distributed formulations](https://arxiv.org/abs/2602.09429)
*Luigi Romano,Ole Morten Aamo,Jan Åslund,Erik Frisk*

Main category: eess.SY

TL;DR: 基于物理的一级动力摩擦模型（特例类似鲁格模型）经过稳定性、被动性分析，并扩展为分布式PDE，用实验验证其与鲁格模型的相似与差别，为控制器设计提供物理支持。


<details>
  <summary>Details</summary>
Motivation: 现有许多速率相关摩擦模型主要依赖经验，缺乏物理解释；通过物理推导可实现更高可解释性与更好的控制设计。

Method: 从基本物理原理出发，采用刷毛元件的逆摩擦特性及简单流变方程推导动态摩擦模型；随后将其扩展为分布式超平面偏微分方程用于滚动接触情形，并对其稳定性与被动性进行严格分析。

Result: 与经典鲁格模型比较，所提出模型在实验验证中表现出显著相似性，同时揭示关键差异，证明其在模拟滚动接触摩擦方面的有效性。

Conclusion: 本文提出了一类基于物理原理的一级动力摩擦模型，能够逼近刷毛元件动力学，并在鲁格模型附近得到特例。该模型在稳定性与被动性方面可被严谨分析，为观测器与控制器设计奠定基础。

Abstract: Dynamic models, particularly rate-dependent models, have proven effective in capturing the key phenomenological features of frictional processes, whilst also possessing important mathematical properties that facilitate the design of control and estimation algorithms. However, many rate-dependent formulations are built on empirical considerations, whereas physical derivations may offer greater interpretability. In this context, starting from fundamental physical principles, this paper introduces a novel class of first-order dynamic friction models that approximate the dynamics of a bristle element by inverting the friction characteristic. Amongst the developed models, a specific formulation closely resembling the LuGre model is derived using a simple rheological equation for the bristle element. This model is rigorously analyzed in terms of stability and passivity -- important properties that support the synthesis of observers and controllers. Furthermore, a distributed version, formulated as a hyperbolic partial differential equation (PDE), is presented, which enables the modeling of frictional processes commonly encountered in rolling contact phenomena. The tribological behavior of the proposed description is evaluated through classical experiments and validated against the response predicted by the LuGre model, revealing both notable similarities and key differences.

</details>


### [8] [Community-Centered Resilience Enhancement of Urban Power and Gas Networks via Microgrid Partitioning, Mobile Energy Storage, and Data-Driven Risk Assessment](https://arxiv.org/abs/2602.09673)
*Arya Abdollahi*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Urban energy systems face increasing challenges due to high penetration of renewable energy sources, extreme weather events, and other high-impact, low-probability disruptions. This project proposes a community-centered, open-access framework to enhance the resilience and reliability of urban power and gas networks by integrating microgrid partitioning, mobile energy storage deployment, and data-driven risk assessment. The approach involves converting passive distribution networks into active, self-healing microgrids using distributed energy resources and remotely controlled switches to enable flexible reconfiguration during normal and emergency operations. To address uncertainties from intermittent renewable generation and variable load, an adjustable interval optimization method combined with a column and constraint generation algorithm is developed, providing robust planning solutions without requiring probabilistic information. Additionally, a real-time online risk assessment tool is proposed, leveraging 25 multi-dimensional indices including load, grid status, resilient resources, emergency response, and meteorological factors to support operational decision-making during extreme events. The framework also optimizes the long-term sizing and allocation of mobile energy storage units while incorporating urban traffic data for effective routing during emergencies. Finally, a novel time-dependent resilience and reliability index is introduced to quantify system performance under diverse operating conditions. The proposed methodology aims to enable resilient, efficient, and adaptable urban energy networks capable of withstanding high-impact disruptions while maximizing operational and economic benefits.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Probabilistic Fair Ordering of Events](https://arxiv.org/abs/2602.09148)
*Muhammad Haseeb,Jinkun Geng,Aurojit Panda,Radhika Mittal,Nirav Atre,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: 本文提出Tommy，一种利用时钟同步误差统计模型的概率公平排序器，解决不传递比较问题，将排序转化为社会选择中的排名，从而实现更公平的事件顺序。


<details>
  <summary>Details</summary>
Motivation: 在不同客户端生成事件时，因同步误差导致时间戳无法可靠排序。

Method: 构建一个基于统计模型的概率公平排序器Tommy，并将排序问题映射到社会选择理论中的经典排名问题，以处理比较的不传递性。

Result: Tommy能够生成部分有序事件集合，在公平性方面显著优于Spanner TrueTime基线方法。

Conclusion: 通过采用概率化比较和社会选择理论方法，可以在存在时钟同步误差的环境下实现更公平的事件排序。

Abstract: A growing class of applications depends on fair ordering, where events that occur earlier should be processed before later ones. Providing such guarantees is difficult in practice because clock synchronization is inherently imperfect: events generated at different clients within a short time window may carry timestamps that cannot be reliably ordered. Rather than attempting to eliminate synchronization error, we embrace it and establish a probabilistically fair sequencing process. Tommy is a sequencer that uses a statistical model of per-clock synchronization error to compare noisy timestamps probabilistically. Although this enables ordering of two events, the probabilistic comparator is intransitive, making global ordering non-trivial. We address this challenge by mapping the sequencing problem to a classical ranking problem from social choice theory, which offers principled mechanisms for reasoning with intransitive comparisons. Using this formulation, Tommy produces a partial order of events, achieving significantly better fairness than a Spanner TrueTime-based baseline approach.

</details>


### [10] [Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains](https://arxiv.org/abs/2602.09188)
*Mahir Rahman,Samuel Joseph,Nihar Kodkani,Behnaz Arzani,Vamsi Addanki*

Main category: cs.NI

TL;DR: Harvest 通过动态规划和拓扑优化优化硅光子互连重构，显著缩短 AllReduce 等集体算法的完成时间。


<details>
  <summary>Details</summary>
Motivation: 硅光子互连在带宽和能效上表现优异，但其电路开关特性导致在集体通信中如何重构互连至关重要。

Method: 提出 Harvest 方法，将互连重构调度问题降为动态规划并嵌入拓扑优化子问题；针对递归加倍 AllReduce 能够无需优化器直接求解最优重构时序。

Result: 在包级与流级仿真及 GPU 硬件验证中，Harvest 合成的调度显著降低集体通信完成时间，优于静态互连和每步重构基线。

Conclusion: Harvest 为多种光子技术提供可调节的重构策略，提升集体通信效率。

Abstract: As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, their circuit-switched nature raises a fundamental question for collective communication: when and how should the interconnect be reconfigured to realize these benefits? Establishing direct optical paths can reduce congestion and propagation delay, but each reconfiguration incurs non-negligible overhead, making naive per-step reconfiguration impractical.
  We present Harvest, a systematic approach for synthesizing topology reconfiguration schedules that minimize collective completion time in photonic interconnects. Given a collective communication algorithm and its fixed communication schedule, Harvest determines how the interconnect should evolve over the course of the collective, explicitly balancing reconfiguration delay against congestion and propagation delay. We reduce the synthesis problem into a dynamic program with an underlying topology optimization subproblem and show that the approach applies to arbitrary collective communication algorithms. Furthermore, we exploit the algorithmic structure of a well-known AllReduce algorithm (Recursive Doubling) to synthesize optimal reconfiguration schedules without using any optimizers. By parameterizing the formulation using reconfiguration delay, Harvest naturally adapts to various photonic technologies. Using packet-level and flow-level evaluations, as well as hardware emulation on commercial GPUs, we show that the schedules synthesized by Harvest significantly reduce collective completion time across multiple collective algorithms compared to static interconnects and reconfigure-every-step baselines.

</details>


### [11] [XLB: A High Performance Layer-7 Load Balancer for Microservices using eBPF-based In-kernel Interposition](https://arxiv.org/abs/2602.09473)
*Yuejie Wang,Chenchen Shou,Jiaxu Qian,Guyue Liu*

Main category: cs.NI

TL;DR: XLB是一种将L7负载均衡放入内核、基于eBPF的架构，通过减少中间层负担，在微服务场景下实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 微服务架构中L7负载均衡需求更高性能、更严格隔离，传统基于侧车的方案无法满足，导致显著性能下降。

Method: 采用eBPF在内核层实现核心负载均衡逻辑，并通过独特的套接字层重定向与嵌套eBPF映射处理连接管理与状态维护。

Result: 在50个微服务实例下，XLB相较于Istio和Cilium实现了高达1.5倍的吞吐量提升与60%更低的端到端延迟。

Conclusion: XLB通过在内核的socket层实现L7负载均衡，消除了传统侧车方案的调度、通信与数据迁移开销，显著提升微服务环境下的吞吐量与延迟表现。

Abstract: L7 load balancers are a fundamental building block in microservices as they enable fine-grained traffic distribution. Compared to monolithic applications, microservices demand higher performance and stricter isolation from load balancers. This is due to the increased number of instances, longer service chains, and the necessity for co-location with services on the same host. Traditional sidecar-based load balancers are ill-equipped to meet these demands, often resulting in significant performance degradation.
  In this work, we present XLB, a novel architecture that reshapes L7 load balancers as in-kernel interposition operating on the socket layer. We leverage eBPF to implement the core load balancing logic in the kernel, and address the connection management and state maintenance challenges through novel socket layer redirection and nested eBPF maps designs. XLB eliminates the extra overhead of scheduling, communication, and data movement, resulting in a more lightweight, scalable, and efficient L7 load balancer architecture. Compared to the widely used microservices load balancers (Istio and Cilium), over 50 microservice instances, XLB achieves up to 1.5x higher throughput and 60% lower end-to-end latency.

</details>


### [12] [ISO FastLane: Faster ISO 11783 with Dual Stack Approach as a Short Term Solution](https://arxiv.org/abs/2602.09633)
*Timo Oksanen*

Main category: cs.NI

TL;DR: ISO FastLane：无需新硬件、无需改动旧设备，即用以太网提升 ISOBUS 带宽，实验提升上传速度 8 倍，任务控制速率 100 倍以上。


<details>
  <summary>Details</summary>
Motivation: 过去十余年，农业行业迫切需要替代 ISO11783 中的 250 kbit/s CAN 总线以满足现代播种机、喷雾机以及 Virtual Terminal 对带宽的需求，但尚未有协议层面的标准化方案。

Method: 采用无网关双栈方案，将点对点的 ISOBUS 流量上行至以太网，同时保持广播消息仍保留在旧的 CAN 总线上。核心改进包括基于 Layer 3 的路由决策以及轻量级的同伴发现机制 Augmented Address Claim (AACL)。

Result: 实验显示，ISO FastLane 能使 Virtual Terminal 对象池上传速度提升 8 倍，并将 Task Controller 消息速率维持在远高于当前规范 100 倍的水平。

Conclusion: ISO FastLane 能在无需更改应用层或增加中间件的情况下，显著提升农业设备的通信效率；它在现有硬件基础上实现高性能、可即插即用的解决方案，已证明能在数周内完成部署。

Abstract: The agricultural industry has been searching for a high-speed successor to the 250~kbit/s CAN bus backbone of ISO~11783 (ISOBUS) for over a decade, yet no protocol-level solution has reached standardization. Meanwhile, modern planters, sprayers, and Virtual Terminals are already constrained by the bus bandwidth. This paper presents ISO FastLane, a gateway-less dual-stack approach that routes point-to-point ISOBUS traffic over Ethernet while keeping broadcast messages on the existing CAN bus. The solution requires no new state machines, no middleware, and no changes to application layer code: only a simple Layer~3 routing decision and a lightweight peer discovery mechanism called Augmented Address Claim (AACL). Legacy devices continue to operate unmodified and unaware of FastLane traffic. Preliminary tests reported on the paper demonstrate that ISO FastLane accelerates Virtual Terminal object pool uploads by factor of 8 and sustains Task Controller message rates over 100 times beyond the current specification limit. Because ISO FastLane builds entirely on existing J1939 and ISO~11783 conventions, it can be implemented by ISOBUS engineers in a matter of weeks. This is delivering tangible performance gains today, without waiting for the long-term High Speed ISOBUS solution.

</details>


### [13] [6G NTN Waveforms: A Comparison of OTFS, AFDM and OCDM in LEO Satellite Channels](https://arxiv.org/abs/2602.09834)
*Baidyanath Mandal,Aniruddha Chandra,Rastislav Roka,Jarosław Wojtun,Jan Kelner,Cezary Ziołkowski*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sixth generation (6G) physical layer (PHY) is evolving beyond the legacy orthogonal frequency division multiplexing (OFDM)-based waveforms. In this paper, we compare the bit error rate (BER) performance of three beyond-OFDM waveforms, namely, orthogonal time-frequency-space (OTFS) modulation, affine frequency division multiplexing (AFDM), and orthogonal chirp division multiplexing (OCDM), which are particularly suitable for the highly mobile non-terrestrial network (NTN) vertical of 6G. In order to characterize the effect of mobility and Doppler shift in low Earth orbit (LEO) satellites, we performed BER comparisons over four different NTN tapped-delay-line (TDL) models, TDL-A, TDL-B, TDL-C, and TDL-D, as specified in the 3rd generation partnership project (3GPP) technical report TR 38.811. After channel equalization, a minimum mean squared error with successive detection (MMSE-SD) algorithm was used to enhance the BER performance. It was found that AFDM and OTFS consistently outperformed OCDM across all TDL models, while AFDM performed better than OTFS in TDL-B and TDL-C, in the high signal-to-noise ratio (SNR) regime. The complete simulation framework is made available as an open-source code for quick validation and further development.

</details>


### [14] [SCOPE: A Training-Free Online 3D Deployment for UAV-BSs with Theoretical Analysis and Comparative Study](https://arxiv.org/abs/2602.09971)
*Chuan-Chi Lai*

Main category: cs.NI

TL;DR: SCOPE：一种无训练、在线3D UAV-BS布置方法，靠周长提取+最小包围圆在多项式时间内完成部署，性能接近DRL而更节能。


<details>
  <summary>Details</summary>
Motivation: 在热点场景下灵活布置无人机基站，克服传统DRL方法在训练开销、拓扑泛化和计算复杂度上的不足。

Method: 基于周长提取和最小包围圆算法的在线3D部署框架；通过分析用户分布，动态优化无人机位置，时间复杂度为O(N^2 log N)。

Result: 与主流DRL基线（如PPO）相比，SCOPE实现了相近的用户满意度，计算时延从小时级训练降至毫秒级，且能耗更低。

Conclusion: SCOPE提供了一个快速、便捷、能耗低、训练无关的3D无人机基站部署方案，在应急部署场景中与DRL方法相比能保持相当的用户满意度，同时大幅降低计算时延。

Abstract: Unmanned Aerial Vehicle (UAV)-mounted Base Stations (UAV-BSs) offer a flexible solution for serving ground users in temporary hotspot scenarios. However, efficiently deploying UAV-BSs to satisfy heterogeneous user distributions remains a challenging optimization problem. While recent data-driven approaches, particularly Deep Reinforcement Learning (DRL), have shown promise in dynamic environments, they often suffer from prohibitive training overhead, poor generalization to topology changes, and high computational complexity. To address these limitations, this paper proposes Satisfaction-driven Coverage Optimization via Perimeter Extraction (SCOPE), a training-free and online 3D deployment framework. Unlike heuristic baselines that rely on fixed-altitude assumptions, SCOPE integrates a perimeter extraction mechanism with the Smallest Enclosing Circle (SEC) algorithm to dynamically optimize 3D UAV positions. Theoretically, we provide a rigorous convergence proof of the proposed algorithm and derive its polynomial time complexity of $O(N^2 \log N)$. Experimentally, we conduct a comprehensive comparative study against state-of-the-art DRL baselines (e.g., PPO). Simulation results demonstrate that SCOPE achieves comparable user satisfaction to DRL methods but significantly lower computational latency (milliseconds vs. hours of training) and superior energy efficiency, making it an ideal solution for real-time, on-demand emergency deployment.

</details>


### [15] [ORCHID: Fairness-Aware Orchestration in Mission-Critical Air-Ground Integrated Networks](https://arxiv.org/abs/2602.09994)
*Chuan-Chi Lai,Chi Jai Choy*

Main category: cs.NI

TL;DR: ORCHID采用GBS感知划分与Reset‑and‑Finetune两步提升多UAV DRL学习稳定性，最大最小公平方案兼顾能效与公平，实验显示在多基准中占优。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在多无人机编排中常因多主体环境的非平稳性导致不稳定，且难以在能耗效率与服务公平之间取得平衡。

Method: 提出两阶段框架ORCHID：第一阶段利用基站感知的拓扑划分缓解探索冷启动；第二阶段在MAPPO中引入Reset‑and‑Finetune机制，同步学习率衰减并重置优化器状态以抑制梯度方差，提升学习稳定性。

Result: 发现最大最小公平设计在满足边缘用户服务的同时，能耗效率优于传统比例公平，并且在多场景实验中ORCHID在Pareto集上占优，表现出更稳健的收敛与连通性。

Conclusion: ORCHID通过拓扑划分和Reset‑and‑Finetune有效提升多无人机编排的学习稳定性和系统鲁棒性，实现了能耗效率与服务公平的协同优化，在任务关键场景中表现卓越。

Abstract: In the era of 6G Air-Ground Integrated Networks (AGINs), Unmanned Aerial Vehicles (UAVs) are pivotal for providing on-demand wireless coverage in mission-critical environments, such as post-disaster rescue operations. However, traditional Deep Reinforcement Learning (DRL) approaches for multi-UAV orchestration often face critical challenges: instability due to the non-stationarity of multi-agent environments and the difficulty of balancing energy efficiency with service equity. To address these issues, this paper proposes ORCHID (Orchestration of Resilient Coverage via Hybrid Intelligent Deployment), a novel stability-enhanced two-stage learning framework. First, ORCHID leverages a GBS-aware topology partitioning strategy to mitigate the exploration cold-start problem. Second, we introduce a Reset-and-Finetune (R\&F) mechanism within the MAPPO architecture that stabilizes the learning process via synchronized learning rate decay and optimizer state resetting. This mechanism effectively suppresses gradient variance to prevent policy degradation, thereby ensuring algorithmic resilience in dynamic environments. Furthermore, we uncover a counter-intuitive efficiency-fairness synergy: contrary to the conventional trade-off, our results demonstrate that the proposed Max-Min Fairness (MMF) design not only guarantees service for cell-edge users but also achieves superior energy efficiency compared to Proportional Fairness (PF), which tends to converge to suboptimal greedy equilibria. Extensive experiments confirm that ORCHID occupies a superior Pareto-dominant position compared to state-of-the-art baselines, ensuring robust convergence and resilient connectivity in mission-critical scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [PICASSO: Scaling CHERI Use-After-Free Protection to Millions of Allocations using Colored Capabilities](https://arxiv.org/abs/2602.09131)
*Merve Gülmez,Ruben Sturm,Hossam ElAtali,Håkan Englund,Jonathan Woodruff,N. Asokan,Thomas Nyman*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While the CHERI instruction-set architecture extensions for capabilities enable strong spatial memory safety, CHERI lacks built-in temporal safety, particularly for heap allocations. Prior attempts to augment CHERI with temporal safety fall short in terms of scalability, memory overhead, and incomplete security guarantees due to periodical sweeps of the system's memory to individually revoke stale capabilities. We address these limitations by introducing colored capabilities that add a controlled form of indirection to CHERI's capability model. This enables provenance tracking of capabilities to their respective allocations via a hardware-managed provenance-validity table, allowing bulk retraction of dangling pointers without needing to quarantine freed memory. Colored capabilities significantly reduce the frequency of capability revocation sweeps while improving security. We realize colored capabilities in PICASSO, an extension of the CHERI-RISC-V architecture on a speculative out-of-order FPGA softcore (CHERI-Toooba). We also integrate colored-capability support into the CheriBSD OS and CHERI-enabled Clang/LLVM toolchain. Our evaluation shows effective mitigation of use-after-free and double-free bugs across all heap-based temporal memory-safety vulnerabilities in NIST Juliet test cases, with only a small performance overhead on SPEC CPU benchmarks (5% g.m.), less latency, and more consistent performance in long-running SQLite, PostgreSQL, and gRPC workloads compared to prior work.

</details>


### [17] [One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning](https://arxiv.org/abs/2602.09182)
*Kotekar Annapoorna Prabhu,Andrew Gan,Zahra Ghodsi*

Main category: cs.CR

TL;DR: RNGGuard通过静态+运行时拦截。不安全随机调用被替换成安全实现，降低了攻击面。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统对随机数的高度依赖，而伪随机数生成器的实现差异和缺乏统计验证导致隐藏的攻击向量，需提升随机源的安全性。

Method: 使用静态源代码分析识别随机函数调用点，在运行时拦截并替换为符合安全规范的实现，形成安全执行路径。

Result: 实验评估表明，RNGGuard在常用机器学习框架中能低成本、可行地修复安全缺口，显著提升系统的随机性安全保障。

Conclusion: RNGGuard通过静态和运行时分析，更换不安全随机函数调用，填补了机器学习框架随机数生成器安全性的空白，有效降低了攻击面。

Abstract: Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.

</details>


### [18] [MUZZLE: Adaptive Agentic Red-Teaming of Web Agents Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2602.09222)
*Georgios Syros,Evan Rose,Brian Grinstead,Christoph Kerschbaumer,William Robertson,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM) based web agents are increasingly deployed to automate complex online tasks by directly interacting with web sites and performing actions on users' behalf. While these agents offer powerful capabilities, their design exposes them to indirect prompt injection attacks embedded in untrusted web content, enabling adversaries to hijack agent behavior and violate user intent. Despite growing awareness of this threat, existing evaluations rely on fixed attack templates, manually selected injection surfaces, or narrowly scoped scenarios, limiting their ability to capture realistic, adaptive attacks encountered in practice. We present MUZZLE, an automated agentic framework for evaluating the security of web agents against indirect prompt injection attacks. MUZZLE utilizes the agent's trajectories to automatically identify high-salience injection surfaces, and adaptively generate context-aware malicious instructions that target violations of confidentiality, integrity, and availability. Unlike prior approaches, MUZZLE adapts its attack strategy based on the agent's observed execution trajectory and iteratively refines attacks using feedback from failed executions. We evaluate MUZZLE across diverse web applications, user tasks, and agent configurations, demonstrating its ability to automatically and adaptively assess the security of web agents with minimal human intervention. Our results show that MUZZLE effectively discovers 37 new attacks on 4 web applications with 10 adversarial objectives that violate confidentiality, availability, or privacy properties. MUZZLE also identifies novel attack strategies, including 2 cross-application prompt injection attacks and an agent-tailored phishing scenario.

</details>


### [19] [Atlas: Enabling Cross-Vendor Authentication for IoT](https://arxiv.org/abs/2602.09263)
*Sanket Goutam,Omar Chowdhury,Amir Rahmati*

Main category: cs.CR

TL;DR: Atlas利用ACME+DNS为IoT设备颁发X.509证书，实现跨域mTLS，部署快、延迟低、可扩展。


<details>
  <summary>Details</summary>
Motivation: 云托管的IoT架构让跨厂商设备间身份认证分散在供应商孤岛中，造成延迟和可用性瓶颈。

Method: Atlas框架扩展Web公钥基础设施，利用供应商运营的ACME客户端和DNS域名为设备颁发X.509证书，直接在行政域间建立相互TLS通道，解耦运行时身份验证与云可达性。

Result: 在ESP32与Raspberry Pi原型中，证书配置<6s，互通TLS延迟≈17ms，CPU开销轻量，系统在智能家居与智能城市工作负载中保持低且可预测的延迟；相较云托管基准，表现更佳。

Conclusion: Atlas能立即部署，主要厂商已使用ACME兼容CA，可在不改造硬件或大规模基础设施的情况下实现跨域安全通信。

Abstract: Cloud-mediated IoT architectures fragment authentication across vendor silos and create latency and availability bottlenecks for cross-vendor device-to-device (D2D) interactions. We present Atlas, a framework that extends the Web public-key infrastructure to IoT by issuing X.509 certificates to devices via vendor-operated ACME clients and vendor-controlled DNS namespaces. Devices obtain globally verifiable identities without hardware changes and establish mutual TLS channels directly across administrative domains, decoupling runtime authentication from cloud reachability. We prototype Atlas on ESP32 and Raspberry Pi, integrate it with an MQTT-based IoT stack and an Atlas-aware cloud, and evaluate it in smart-home and smart-city workloads. Certificate provisioning completes in under 6s per device, mTLS adds only about 17ms of latency and modest CPU overhead, and Atlas-based applications sustain low, predictable latency compared to cloud-mediated baselines. Because many major vendors already rely on ACME-compatible CAs for their web services, Atlas is immediately deployable with minimal infrastructure changes.

</details>


### [20] [Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation](https://arxiv.org/abs/2602.09319)
*Zhisheng Qi,Utkarsh Sahu,Li Ma,Haoyu Han,Ryan Rossi,Franck Dernoncourt,Mahantesh Halappanavar,Nesreen Ahmed,Yushun Dong,Yue Zhao,Yu Zhang,Yu Wang*

Main category: cs.CR

TL;DR: 推出一套统一评估框架，系统评估RAG中的知识提取攻击与防御，使研究更可复现、可比较


<details>
  <summary>Details</summary>
Motivation: RAG系统易受知识提取攻击，导致知识产权和隐私泄露，现有研究碎片化，缺乏统一基准

Method: 构建首个统一的检索增强生成系统知识提取攻防基准，涵盖多种攻击/防御、检索嵌入、生成器，并使用标准化实验框架与多数据集评测

Result: 基准为可复制、可比的评估提供了可操作的洞见，帮助研发更安全的RAG系统

Conclusion: 该基准统一实验范式，推动RAG隐私保护技术的系统性进步

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.

</details>


### [21] [Privacy Amplification for BandMF via $b$-Min-Sep Subsampling](https://arxiv.org/abs/2602.09338)
*Andy Dong,Arun Ganesh*

Main category: cs.CR

TL;DR: 一种新的$b$-min-sep采样方案，提升了BandMF的隐私放大效果，在不同噪声水平下表现更优，并可支持多属性用户级隐私。


<details>
  <summary>Details</summary>
Motivation: 在带相关噪声的BandMF（DP-SGD）中，实现更强的隐私放大效果仍存在挑战，现有的采样方案在不同噪声水平下的隐私保证有限。

Method: 提出$b$-min-sep子采样，融合Poisson采样与balls-in-bins方法，利用迭代采样的马尔可夫结构，通过基于Monte Carlo的动态规划实现近似精确的隐私分析。

Result: 在高噪声下$b$-min-sep与循环Poisson采样保持一致，在中低噪声下严格优于后者；实验验证其性能，并且能够自然扩展至多属性用户级隐私。

Conclusion: $b$-min-sep提供了一种在保持结构分析优势的同时实现更强隐私放大的采样策略，可广泛应用于BandMF及多赋属性用户隐私场景。

Abstract: We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.

</details>


### [22] [Timing and Memory Telemetry on GPUs for AI Governance](https://arxiv.org/abs/2602.09369)
*Saleh K. Monfared,Fatemeh Ganji,Dan Holcomb,Shahin Tajik*

Main category: cs.CR

TL;DR: 论文提出四种GPU利用率测量原语，实验验证可通过时序与内存驻留估计GPU使用情况，为部署后治理提供可靠遥测。


<details>
  <summary>Details</summary>
Motivation: GPU-accelerated computing的快速扩张推动了大规模人工智能的发展，但当前GPU缺乏可信的遥测功能，导致部署后可能被隐蔽地转用于训练模型、规避使用政策或逃离法律监管，亟需治理机制。

Method: 提出一套利用现代GPU微架构特性的测量框架，包含四类互补原语：1）类似PoW的概率性工作驱动机制；2）基于可验证延迟函数(VDF)的顺序延迟敏感工作负载；3）基于GEMM的张量核心测量；4）利用VRAM驻留测试区分设备内存与外部访问的哈希方法。

Result: 通过对竞争、架构一致性、内存压力和功耗的评估，结果证明时序偏移与驻留延迟能够表征GPU的计算活动，并可在缺乏可信固件或供应商计数器的情况下获得可观测信号。

Conclusion: 计算基础遥测能够补充未来的可追溯机制，为GPU部署后治理提供可观测的架构信号。

Abstract: The rapid expansion of GPU-accelerated computing has enabled major advances in large-scale artificial intelligence (AI), while heightening concerns about how accelerators are observed or governed once deployed. Governance is essential to ensure that large-scale compute infrastructure is not silently repurposed for training models, circumventing usage policies, or operating outside legal oversight. Because current GPUs expose limited trusted telemetry and can be modified or virtualized by adversaries, we explore whether compute-based measurements can provide actionable signals of utilization when host and device are untrusted. We introduce a measurement framework that leverages architectural characteristics of modern GPUs to generate timing- and memory-based observables that correlate with compute activity. Our design draws on four complementary primitives: (1) a probabilistic, workload-driven mechanism inspired by Proof-of-Work (PoW) to expose parallel effort, (2) sequential, latency-sensitive workloads derived via Verifiable Delay Functions (VDFs) to characterize scalar execution pressure, (3) General Matrix Multiplication (GEMM)-based tensor-core measurements that reflect dense linear-algebra throughput, and (4) a VRAM-residency test that distinguishes on-device memory locality from off-chip access through bandwidth-dependent hashing. These primitives provide statistical and behavioral indicators of GPU engagement that remain observable even without trusted firmware, enclaves, or vendor-controlled counters. We evaluate their responses to contention, architectural alignment, memory pressure, and power overhead, showing that timing shifts and residency latencies reveal meaningful utilization patterns. Our results illustrate why compute-based telemetry can complement future accountability mechanisms by exposing architectural signals relevant to post-deployment GPU governance.

</details>


### [23] [LLMAC: A Global and Explainable Access Control Framework with Large Language Model](https://arxiv.org/abs/2602.09392)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: LLMAC 使用 LLM 将 RBAC、ABAC、DAC 合并为一体化系统，准确率达98.5%，远超传统方法，同时提供可解释决策，部署可行。


<details>
  <summary>Details</summary>
Motivation: 现代商业组织需要能够处理复杂、动态、情境依赖工作流的访问控制系统，而传统方法无法满足此需求。

Method: 提出LLMAC，一种统一方法，利用大型语言模型（LLMs）将RBAC、ABAC、DAC等不同访问控制模型合并为一个可解释的系统，并在覆盖所有权验证、版本管理、工作流流程和动态角色分离等场景的合成数据集上训练模型。

Result: 使用Mistral 7B训练的LLM在实验中达到了98.5%的准确率，明显优于传统方法（RBAC 14.5%、ABAC 58.5%、DAC 27.5%），并为每个决策提供了人类可读的解释，性能测试表明系统能够在合理的响应时间和计算资源下实用部署。

Conclusion: LLMAC 展示了大型语言模型能够统一多种访问控制模型，并在保持高性能的同时提供可解释性，满足现代动态业务环境的安全需求。

Abstract: Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.

</details>


### [24] [Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models](https://arxiv.org/abs/2602.09431)
*Xinwei Zhang,Li Bai,Tianwei Zhang,Youqian Zhang,Qingqing Ye,Yingnan Zhao,Ruochen Du,Haibo Hu*

Main category: cs.CR

TL;DR: LVLM易受攻击，现有编码器攻击转移差。作者发现两大瓶颈，提出SGMA提升转移。实验表明SGMA更强，凸显安全隐患。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在多模态任务表现突出，但其对视觉输入的依赖使其易受对抗攻击。已有的编码器级攻击仅优化视觉编码器，在性能上具有优势，但在不同模型间的转移效果不明，需系统研究。

Method: 1) 对八款多样化大型视觉-语言模型进行大规模基准测试；2) 通过对成功与失败案例的对比，定位攻击转移受限的根本原因；3) 设计SGMA框架，利用语义关键区域和跨模态对齐破坏机制，指导扰动生成。

Result: SGMA在不同受害模型与任务上显著提升转移效果，优于现有攻击方法，并通过实验验证了其有效性。

Conclusion: 本研究揭示了现有编码器攻击在不同大型视觉-语言模型中的转移能力有限，并通过对转移瓶颈的深度剖析，阐明了视觉对齐不一致与语义标记冗余是主要原因。基于此，提出的语义引导多模态攻击（SGMA）在多种任务和模型上实现了更高的转移效果，警示了部署中的安全风险，亟需强化多模态防御。

Abstract: Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>


### [25] [A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors](https://arxiv.org/abs/2602.09434)
*Zhenyu Xu,Victor S. Sheng*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Protecting the intellectual property of large language models (LLMs) is a critical challenge due to the proliferation of unauthorized derivative models. We introduce a novel fingerprinting framework that leverages the behavioral patterns induced by safety alignment, applying the concept of refusal vectors for LLM provenance tracking. These vectors, extracted from directional patterns in a model's internal representations when processing harmful versus harmless prompts, serve as robust behavioral fingerprints. Our contribution lies in developing a fingerprinting system around this concept and conducting extensive validation of its effectiveness for IP protection. We demonstrate that these behavioral fingerprints are highly robust against common modifications, including finetunes, merges, and quantization. Our experiments show that the fingerprint is unique to each model family, with low cosine similarity between independently trained models. In a large-scale identification task across 76 offspring models, our method achieves 100\% accuracy in identifying the correct base model family. Furthermore, we analyze the fingerprint's behavior under alignment-breaking attacks, finding that while performance degrades significantly, detectable traces remain. Finally, we propose a theoretical framework to transform this private fingerprint into a publicly verifiable, privacy-preserving artifact using locality-sensitive hashing and zero-knowledge proofs.

</details>


### [26] [ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance](https://arxiv.org/abs/2602.09548)
*Gianluca Capozzi,Anna Paola Giancaspro,Fabio Petroni,Leonardo Querzoni,Giuseppe Antonio Di Luna*

Main category: cs.CR

TL;DR: ReSIM在二进制函数相似度检索中加入重排序模块，显著提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 现有的bi-encoder架构只能独立编码函数，无法捕捉跨函数的关联信息，限制了相似度评估的准确性，亟需改进以提升安全、版权和恶意软件分析中的检索质量。

Method: 引入ReSIM体系结构，利用双向encoder嵌入基础向量，再通过联合编码的重排序模型对查询-候选对进行互相表示，从而计算更精准的相似度评分并对检索结果进行重排序。

Result: 在七种embedding模型和两种基准数据集上，ReSIM实现了平均21.7%的nDCG提升和27.8%的Recall提升。

Conclusion: ReSIM通过在基于embedding的检索结果上加入神经重排序模块，显著提升了二进制函数相似度检索效果，实验表明平均nDCG提升21.7%，Recall提升27.8%。

Abstract: Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.
  Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.
  We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.

</details>


### [27] [Parallel Composition for Statistical Privacy](https://arxiv.org/abs/2602.09627)
*Dennis Breutigam,Rüdiger Reischuk*

Main category: cs.CR

TL;DR: 本文对受限对手的统计隐私进行多查询组合分析，提出分采样随机划分机制，获得无数据库限制的上限隐私界，并在实际情境中提升隐私与精度。


<details>
  <summary>Details</summary>
Motivation: DP假设对手几乎掌握所有数据库信息，可能高估了实时隐私威胁；研究真正的少知识对手环境，关注分布熵与机制失真之间的相互作用。

Method: 采用分批采样与随机数据库划分技术，降低查询间的依赖，从而获得对有限背景知识对手的上限隐私界；随后利用分布熵约束估计隐私与效用损失。

Result: 在同一隐私参数与效用损失下，SP可容纳显著更多查询，相比DP提供显著更高的查询次数与更小的精度损失。

Conclusion: 在考虑受限对手时，基于分采样与随机划分的机制能够无额外数据库约束地提供上限隐私保证，并在实际场景中实现更优的隐私与精度双重提升。

Abstract: Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.
  This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.
  These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.

</details>


### [28] [Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks](https://arxiv.org/abs/2602.09629)
*Hayfa Dhabhi,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: 提出四检查点框架和 WASR 指标，针对 13 种检查点逃避技术对三大 LLM 进行测试，发现输出阶段防御薄弱，Claude 仍然最安全，其余两款安全性不如预期。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM的安全机制可被“jailbreak”攻击，也就是说攻击能导致模型生成有害内容，但缺乏对安全机制失败点的系统化分析。研究者需求一个可分离、可评估的框架，帮助定位系统中易被攻击的环节，从而有针对性地提升安全性。

Method: 构建四检查点框架（Four‑Checkpoint Framework），将安全措施划分为输入/输出两个处理阶段以及文字/意图两个检测层级，形成 CP1–CP4 四个独立的防御检查点。随后设计 13 种针对单个检查点的逃避技术，利用 LLM-judge 自动分类，并提出加权攻击成功率（Weighted Attack Success Rate，WASR）作为新的评价指标。

Result: 对 GPT‑5、Claude Sonnet 4、Gemini 2.5 Pro 共计 3,312 条单轮黑盒测试案例分析。传统二元 ASR 仅报告 22.6% 的攻击成功率，但 WASR 则揭示 52.7% 的成功率，显著提高了可被利用的漏洞比例。输出阶段（CP3、CP4）最弱（72–79% WASR），输入文字级（CP1）最强（13% WASR）。Claude 在安全性上表现最佳（42.8% WASR），其次为 GPT‑5（55.9%）和 Gemini（59.5%）。

Conclusion: 当今 LLM 的安全防护在输入文字级却在意图层和输出阶段仍显脆弱。四检查点框架为拆解与评估安全层次提供了结构化方法，帮助开发者精准定位弱点并策略性强化保护措施。

Abstract: Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \textit{where} defenses fail or \textit{why}.
  To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\ output) and detection level (literal vs.\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.
  Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.
  Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\% attack success. However, WASR reveals 52.7\%, a 2.3$\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\% WASR, while input-literal defenses (CP1) are strongest at 13\% WASR. Claude achieves the strongest safety (42.8\% WASR), followed by GPT-5 (55.9\%) and Gemini (59.5\%).
  These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.

</details>


### [29] [PiTPM: Partially Interactive Signatures for Multi-Device TPM Operations](https://arxiv.org/abs/2602.09707)
*Yunusa Simpa Abdulsalam,Mustapha Hedabou*

Main category: cs.CR

TL;DR: PiTPM：利用Schnorr签名与预共享随机种子实现无交互式多重签名，减少通信成本，保持常数签名大小，提供官方安全证明与实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于TPM的多重签名需要交互，导致同步瓶颈、通信复杂度随参与者增大而呈二次增长，且难以处理设备失效，尤其适用于跨设备加密场景，对此需要一种更高效、容错性更好的方案。

Method: 构建了基于Schnorr签名的聚合器框架，利用安全存储在聚合器中的预共享随机种子，实现无交互的全局承诺计算，从而实现非交互式多主体签名。

Result: 提出的PiTPM实现了常数规模签名，显著减少通信开销，并通过实验验证了在TPM环境下与传统方案相比提升的性能，同时给出了基于离散对数假设的随机预言机模型下的EU-CMA安全证明。

Conclusion: PiTPM消除了传统TPM多重签名中的交互需求，实现了常数大小签名并显著降低通信复杂度，展示了混合信任架构在TPM系统中的高效性与安全性。

Abstract: Trusted Platform Module (TPM) 2.0 devices provide efficient hardware-based cryptographic security through tamper-resistant key storage and computation, making them ideal building blocks for multi-party signature schemes in distributed systems. However, existing TPM-based multi-signature constructions suffer from a fundamental limitation, they require interactive protocols where all participants must coordinate during the commitment phase, before any signature can be computed. This interactive requirement creates several critical problems, such as synchronization bottlenecks, quadratic communication complexity, and aborted protocols as a result of participant failure. These limitations become particularly heightened for applications that require cross-device cryptographic operations. This paper presents PiTPM, an Aggregator Framework built upon Schnorr's digital signature. Our protocol eliminates the interactive requirement using a hybrid trust architecture. The proposed framework uses pre-shared randomness seeds stored securely in an Aggregator, enabling deterministic computation of global commitments without inter-participant communication. The resulting signatures of the proposed framework are of constant size regardless of signer count. Our experimental results show a possible paradigm shift in TPM-based cryptographic system design, demonstrating that hybrid trust architectures can achieve significant performance improvements while maintaining rigorous security guarantees. We provide a comprehensive formal security analysis proving EU-CMA security under the discrete logarithm assumption in the random oracle model.

</details>


### [30] [QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery](https://arxiv.org/abs/2602.09774)
*George Tsigkourakos,Constantinos Patsakis*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.

</details>


### [31] [From Multi-sig to DLCs: Modern Oracle Designs on Bitcoin](https://arxiv.org/abs/2602.09822)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 自 2015 年后，比特币 Layer 1 oracle 从多签名转向 DLC 等证明基础设计，获得社区支持并用于实际投注、预测市场；但学术研究仍有限。


<details>
  <summary>Details</summary>
Motivation: 解决比特币原生可编程性不足的问题，评估自 2015 年以太坊主导智能合约时代以来，Bitcoin Layer 1 是否出现了新的 oracle 设计以及改进提案的影响。

Method: 文献检索主要依赖 Scopus 与 Web of Science，辅以 Google Scholar 以捕捉协议提案，构建索引语料库进行定量与定性分析。

Result: 检索结果显示学术覆盖较低，论文多数在期刊外流传；主流发展方向由多签名类 oracle 向基于证明（DLC）迈进，后者获得更广泛社区接受、工具支撑，并已在现实投注与预测市场中得到实现。

Conclusion: 论文表明，随着时间推移，Bitcoin Layer 1 的 oracle 设计从多重签名式演进为基于证明的模型（如 DLC），显示出更强的社区合规性、工具支持以及在下注和预测市场等真实场景中的实践应用；然而学术界对此的关注仍有限，许多贡献发表于非期刊渠道。

Abstract: Unlike Ethereum, which was conceived as a general-purpose smart-contract platform, Bitcoin was designed primarily as a transaction ledger for its native currency, which limits programmability for conditional applications. This constraint is particularly evident when considering oracles, mechanisms that enable Bitcoin contracts to depend on exogenous events. This paper investigates whether new oracle designs have emerged for Bitcoin Layer 1 since the 2015 transition to the Ethereum smart contracts era and whether subsequent Bitcoin improvement proposals have expanded oracles' implementability. Using Scopus and Web of Science searches, complemented by Google Scholar to capture protocol proposals, we observe that the indexed academic coverage remains limited, and many contributions circulate outside journal venues. Within the retrieved corpus, the main post-2015 shift is from multisig-style, which envisioned oracles as co-signers, toward attestation-based designs, mainly represented by Discreet Log Contracts (DLCs), which show stronger Bitcoin community compliance, tool support, and evidence of practical implementations in real-world scenarios such as betting and prediction-market mechanisms.

</details>


### [32] [Spinel: A Post-Quantum Signature Scheme Based on SLn(Fp) Hashing](https://arxiv.org/abs/2602.09882)
*Asmaa Cherkaoui,Faraz Heravi,Delaram Kahrobaei,Siamak F. Shahandashti*

Main category: cs.CR

TL;DR: Spinel：结合SPHINCS+与基于大图拼贴的代数哈希函数的新型后量子数字签名方案，既安全又实用。


<details>
  <summary>Details</summary>
Motivation: 在量子计算出现的背景下，硬币需要一种安全性超越经典困难假设的数字签名方案；本文通过引入新的基于Tillich-Zemor的代数哈希函数以及SPHINCS+框架来解决这一需求。

Method: ①设计了一种基于SL_n(F_p)幂等图的代数哈希函数，并给出了其理论安全性和经验实验证据。②将该哈希函数嵌入SPHINCS+签名框架，完成Spinel签名方案的构建。③对方案的安全退化进行建模分析，阐明参数选择依据。④实现哈希函数及Spinel签名器，并对性能进行实验评测。

Result: Spinel在理论分析与实验评测中都表现出可行性，安全性符合量子抗性要求，并在实验中展示了可接受的性能。该方案为代数哈希签名提供了实验和理论支持。

Conclusion: Spinel证明了将基于Tillich-Zemor的代数哈希函数与SPHINCS+结合能够实现实用且量子安全的数字签名，为后量子密码学提供了新的工具。

Abstract: The advent of quantum computation compels the cryptographic community to design digital signature schemes whose security extends beyond the classical hardness assumptions. In this work, we introduce Spinel, a post-quantum digital signature scheme that combines the proven security of SPHINCS+ (CCS 2019) with a new family of algebraic hash functions (Adv. Math. Commun. 2025) derived from the Tillich-Zemor paradigm (Eurocrypt 2008) with security rooted in the hardness of navigating expander graphs over SL_n(F_p), a problem believed to be hard even for quantum adversaries. We first provide empirical evidence of the security of this hash function, complementing the original theoretical analysis. We then show how the hash function can be integrated within the SPHINCS+ framework to give a secure signature scheme. We then model and analyze the security degradation of the proposed scheme, which informs the parameter selection we discuss next. Finally, we provide an implementation of the hash function and the proposed signature scheme Spinel as well as detailed empirical results for the performance of Spinel showing its feasibility in practice. Our approach lays the foundations for the design of algebraic hash-based signature schemes, expanding the toolkit of post-quantum cryptography.

</details>


### [33] [The Need for Standardized Evidence Sampling in CMMC Assessments: A Survey-Based Analysis of Assessor Practices](https://arxiv.org/abs/2602.09905)
*Logan Therrien,John Hastings*

Main category: cs.CR

TL;DR: CMMC评估员的证据抽样缺乏统一标准，导致结果不一致，研究建议制定风险驱动的标准化抽样方法。


<details>
  <summary>Details</summary>
Motivation: 当前CMMC对证据抽样缺乏正式指导，分析现状以评估是否需要统一、风险信息化的抽样规范。

Method: 匿名问卷收集17名CMMC评估员及资深评估员的数据，探索性分析评估人员在证据抽样中的做法与一致性。

Result: 证据抽样主要由评估员主观判断、感知风险和环境复杂度驱动，形式化统计模型使用低；多次评估间出现不一致，受访者支持制定统一指导但反对硬性百分比要求。

Conclusion: 缺乏统一的证据抽样框架导致评估结果可靠性受影响，建议制定风险信息化标准抽样方法以提高一致性与信任度。

Abstract: The Cybersecurity Maturity Model Certification (CMMC) framework provides a common standard for protecting sensitive unclassified information in defense contracting. While CMMC defines assessment objectives and control requirements, limited formal guidance exists regarding evidence sampling, the process by which assessors select, review, and validate artifacts to substantiate compliance. Analyzing data collected through an anonymous survey of CMMC-certified assessors and lead assessors, this exploratory study investigates whether inconsistencies in evidence sampling practices exist within the CMMC assessment ecosystem and evaluates the need for a risk-informed standardized sampling methodology. Across 17 usable survey responses, results indicate that evidence sampling practices are predominantly driven by assessor judgment, perceived risk, and environmental complexity rather than formalized standards, with formal statistical sampling models rarely referenced. Participants frequently reported inconsistencies across assessments and expressed broad support for the development of standardized guidance, while generally opposing rigid percentage-based requirements. The findings support the conclusion that the absence of a uniform evidence sampling framework introduces variability that may affect assessment reliability and confidence in certification outcomes. Recommendations are provided to inform future CMMC assessment methodology development and further empirical research.

</details>


### [34] [CAPID: Context-Aware PII Detection for Question-Answering Systems](https://arxiv.org/abs/2602.10074)
*Mariia Ponomarenko,Sepideh Abedini,Masoumeh Shafieinejad,D. B. Emerson,Shubhankar Mohapatra,Xi He*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user's question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, but due to their closed source nature and lack of privacy guarantees, they are unsuitable for sensitive data processing. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively. To fill this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and relevance levels. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving significantly higher downstream utility under anonymization.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Entropy-Based Evidence for Bitcoin's Discrete Time Mechanism](https://arxiv.org/abs/2602.09027)
*Bin Chen,Pan Feng*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bitcoin derives a verifiable temporal order from probabilistic block discovery and cumulative proof-of-work rather than from a trusted global clock. We show that block arrivals exhibit stable exponential behavior across difficulty epochs, and that the proof-of-work process maintains a high-entropy search state that collapses discretely upon the discovery of a valid block. This entropy-based interpretation provides a mechanistic account of Bitcoin's non-continuous temporal structure. In a distributed network, however, entropy collapse is not completed instantaneously across all participants. Using empirical observations of temporary forks, we show that collapse completion unfolds over a finite propagation-bounded interval, while remaining rapid in practice.

</details>


### [36] [Non-existence of Information-Geometric Fermat Structures: Violation of Dual Lattice Consistency in Statistical Manifolds with $L^n$ Structure](https://arxiv.org/abs/2602.09028)
*Kanta Tochigi*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper reformulates Fermat's Last Theorem as an embedding problem of information-geometric structures. We reinterpret the Fermat equation as an $n$-th moment constraint, constructing a statistical manifold $\mathcal{M}_n$ of generalized normal distributions via the Maximum Entropy Principle. By Chentsov's Theorem, the natural metric is the Fisher information metric ($L^2$); however, the global structure is governed by the $L^n$ moment constraint. This reveals a discrepancy between the local quadratic metric and the global $L^n$ structure. We axiomatically define an "Information-Geometric Fermat Solution," postulating that the lattice structure must maintain "dual lattice consistency" under the Legendre transform. We prove the non-existence of such structures for $n \ge 3$. Through the Poisson Summation Formula and Hausdorff-Young Inequality, we demonstrate that the Fourier transform induces an alteration of the function family ($L^n \to L^q$, where $1/n + 1/q = 1$), rendering dual lattice consistency analytically impossible. This identifies a geometric obstruction where integer and energy structures are incompatible within a dually flat space. We conclude by discussing the correspondence between this model and elliptic curves.

</details>


### [37] [On the Subpacketization Level of the Banawan-Ulukus Multi-Message PIR Scheme](https://arxiv.org/abs/2602.09417)
*Anoosheh Heidarzadeh*

Main category: cs.IT

TL;DR: 给出PIR方案中递推式的显式多项式解，最高次项为N^{K-D+1}/D，说明分包层级随N指数增长且系数为非负。


<details>
  <summary>Details</summary>
Motivation: 在多消息私有信息检索（PIR）协议设计中，分包层级直接影响实现的存储和通信成本，因而需要精确评估其增长趋势和上界。Banawan和Ulukus方案中出现的递推式没有闭式解，限制了对参数适用范围和性能评价的深入研究。

Method: 分析递推关系，利用归纳与矩阵法推导L的显式表达式；随后将所得公式化简为关于N、K、D的多项式形式。

Result: 得到了分包层级L的完全解析：L是N的非负系数多项式，其首项为N^{K-D+1}/D。该公式可用于快速计算任何（N,K,D）组合下的分包层级，帮助调节PIR协议中的资源分配。

Conclusion: 本文给出了Banawan与Ulukus多消息PIR方案中频繁出现的线性递推的显式解析。计算出的标准化分包层级L是关于服务器数N的多项式，且其系数均为非负。L的最高次项为N^{K-D+1}/D，证明了分包层级随N呈多项式增长，并提供了可直接用于设计更高效PIR方案的闭式形式。

Abstract: This note analyzes a linear recursion that arises in the computation of the subpacketization level for the multi-message PIR scheme of Banawan and Ulukus. We derive an explicit representation for the normalized subpacketization level $L$, whose smallest integer multiple yields the subpacketization level of the scheme, in terms of the number of servers $N$, the total number of messages $K$, and the number of demand messages $D$. The resulting formula shows that $L$ is a polynomial in $N$ with nonnegative coefficients, and its leading term is $N^{K-D+1}/D$.

</details>


### [38] [Directed Information: Estimation, Optimization and Applications in Communications and Causality](https://arxiv.org/abs/2602.09711)
*Dor Tsur,Oron Sabag,Navin Kashyap,Haim Permuter,Gerhard Kramer*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Directed information (DI) is an information measure that attempts to capture directionality in the flow of information from one random process to another. It is closely related to other causal influence measures, such as transfer entropy, Granger causality, and Pearl's causal framework. This monograph provides an overview of DI and its main application in information theory, namely, characterizing the capacity of channels with feedback and memory. We begin by reviewing the definitions of DI, its basic properties, and its relation to Shannon's mutual information. Next, we provide a survey of DI estimation techniques, ranging from classic plug-in estimators to modern neural-network-based estimators. Considering the application of channel capacity estimation, we describe how such estimators numerically optimize DI rate over a class of joint distributions on input and output processes. A significant part of the monograph is devoted to techniques to compute the feedback capacity of finite-state channels (FSCs). The feedback capacity of a strongly connected FSC involves the maximization of the DI rate from the channel input process to the output process. This maximization is performed over the class of causal conditioned probability input distributions. When the FSC is also unifilar, i.e., the next state is given by a time-invariant function of the current state and the new input-output symbol pair, the feedback capacity is the optimal average reward of an appropriately formulated Markov decision process (MDP). This MDP formulation has been exploited to develop several methods to compute exactly, or at least estimate closely, the feedback capacity of a unifilar FSC. This monograph describes these methods, starting from the value iteration algorithm, to Q-graph methods, and reinforcement learning algorithms that can handle large input and output alphabets.

</details>


### [39] [On the generalization of $g$-circulant MDS matrices](https://arxiv.org/abs/2602.10028)
*Atif Ahmad Khan,Shakir Ali,Bhupendra Singh*

Main category: cs.IT

TL;DR: 本文构造 consta‑g‑circulant 矩阵，给出可逆性/ MDS 条件与计数式，并列举 3、4 阶 MDS 矩阵的完整表征，提供新的变体与实例。


<details>
  <summary>Details</summary>
Motivation: 在密码学与编码理论中 MDS 矩阵因其高鲁棒性而重要，但构造与判定往往复杂；本文通过扩展 g‑circulant 矩阵至 consta‑g‑circulant，提供了更易分析的结构。

Method: 首先定义基于多项式 h(x) 的线性变换构造 consta‑g‑circulant 矩阵；随后利用 x^m－λ 的分解和 λ 的乘法阶，推导可逆矩阵个数与条件；最后针对 g‑circulant 矩阵的 MDS 性进行阶为 3、4 的完整分类，并引入受偏置多项式环启发的变体。

Result: 给出了 consta‑g‑circulant 矩阵可逆性的上界与精确计数公式，并在 3、4 阶情况下给出了 MDS 矩阵的完整描述，此外还提出了新的变体并给出示例。

Conclusion: 论文通过构造新的 consta‑g‑circulant 矩阵，给出了其可逆性及 MDS 性质的必要条件，并在二、三、四阶情况下完成了完全表征。

Abstract: A matrix $M$ over the finite field $ \mathbb{F}_q $ is called \emph{maximum distance separable} (MDS) if all of its square submatrices are non-singular. These MDS matrices are very important in cryptography and coding theory because they provide strong data protection and help spread information efficiently. In this paper, we introduce a new type of matrix called a \emph{consta-$g$-circulant matrix}, which extends the idea of $g$-circulant matrices. These matrices come from a linear transformation defined by the polynomial
  $
  h(x) = x^m - λ+ \sum_{i=0}^{m-1} h_i x^i
  $
  over $ \mathbb{F}_q $. We find the upper bound of such matrices exist and give conditions to check when they are invertible. This helps us know when they are MDS matrices. If the polynomial $ x^m - λ$ factors as
  $
  x^m - λ= \prod_{i=1}^{t} f_i(x)^{e_i},
  $
  where each \( f_i(x) \) is irreducible, then the number of invertible consta-$g$-circulant matrices is
  $
  N \cdot \prod_{i=1}^{t} \left( q^{°f_i} - 1 \right),
  $
  where $r$ is the multiplicative order of $λ$, and \( N \) is the number of integers \( k \) such that
  $
  0 \leq k < \left\lfloor \frac{m - 1}{r} \right\rfloor + 1 \quad \text{and} \quad \gcd(1 + rk, m) = 1.
  $
  This formula help us to reduce the number of cases to check whether such matrices is MDS. Moreover, we give complete characterization of $g$-circulant MDS matrices of order 3 and 4. Additionally, inspired by skew polynomial rings, we construct a new variant of $g$-circulant matrix. In the last, we provide some examples related to our findings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Enhanced Graph Transformer with Serialized Graph Tokens](https://arxiv.org/abs/2602.09065)
*Ruixiang Wang,Yuyang Hong,Shiming Xiang,Chunhong Pan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers have demonstrated success in graph learning, particularly for node-level tasks. However, existing methods encounter an information bottleneck when generating graph-level representations. The prevalent single token paradigm fails to fully leverage the inherent strength of self-attention in encoding token sequences, and degenerates into a weighted sum of node signals. To address this issue, we design a novel serialized token paradigm to encapsulate global signals more effectively. Specifically, a graph serialization method is proposed to aggregate node signals into serialized graph tokens, with positional encoding being automatically involved. Then, stacked self-attention layers are applied to encode this token sequence and capture its internal dependencies. Our method can yield more expressive graph representations by modeling complex interactions among multiple graph tokens. Experimental results show that our method achieves state-of-the-art results on several graph-level benchmarks. Ablation studies verify the effectiveness of the proposed modules.

</details>


### [41] [Spectral Disentanglement and Enhancement: A Dual-domain Contrastive Framework for Representation Learning](https://arxiv.org/abs/2602.09066)
*Jinjin Guo,Yexin Li,Zhichao Huang,Jun Fang,Zhiyuan Liu,Chao Liu,Pengzhang Liu,Qixia Jiang*

Main category: cs.LG

TL;DR: SDE tackles spectral imbalance in multimodal contrastive learning via singular‑value‑based disentanglement, selective enhancement, and a dual‑domain loss, yielding superior robustness and performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large‑scale multimodal contrastive learning suffers from spectral imbalance: embeddings collapse into narrow cones, leaving most dimensions noisy and less useful, which hampers generalization.

Method: SDE performs singular value decomposition on high‑dimensional embeddings to partition dimensions into strong signals, weak signals, and noise, applies a curriculum‑based spectral enhancement to amplify informative components, and introduces a dual‑domain contrastive loss that jointly aligns features in both feature and spectral spaces.

Result: Experiments on large multimodal benchmarks show that SDE consistently outperforms state‑of‑the‑art methods, offering stronger robustness and generalization while integrating seamlessly with existing pipelines.

Conclusion: Spectral Disentanglement and Enhancement (SDE) improves multimodal representation learning by capturing task‑relevant semantics and reducing noise through spectral analysis, leading to more robust and generalizable models.

Abstract: Large-scale multimodal contrastive learning has recently achieved impressive success in learning rich and transferable representations, yet it remains fundamentally limited by the uniform treatment of feature dimensions and the neglect of the intrinsic spectral structure of the learned features. Empirical evidence indicates that high-dimensional embeddings tend to collapse into narrow cones, concentrating task-relevant semantics in a small subspace, while the majority of dimensions remain occupied by noise and spurious correlations. Such spectral imbalance and entanglement undermine model generalization. We propose Spectral Disentanglement and Enhancement (SDE), a novel framework that bridges the gap between the geometry of the embedded spaces and their spectral properties. Our approach leverages singular value decomposition to adaptively partition feature dimensions into strong signals that capture task-critical semantics, weak signals that reflect ancillary correlations, and noise representing irrelevant perturbations. A curriculum-based spectral enhancement strategy is then applied, selectively amplifying informative components with theoretical guarantees on training stability. Building upon the enhanced features, we further introduce a dual-domain contrastive loss that jointly optimizes alignment in both the feature and spectral spaces, effectively integrating spectral regularization into the training process and encouraging richer, more robust representations. Extensive experiments on large-scale multimodal benchmarks demonstrate that SDE consistently improves representation robustness and generalization, outperforming state-of-the-art methods. SDE integrates seamlessly with existing contrastive pipelines, offering an effective solution for multimodal representation learning.

</details>


### [42] [Learning to Remember, Learn, and Forget in Attention-Based Models](https://arxiv.org/abs/2602.09075)
*Djohan Bonnet,Jamie Lohoff,Jan Finkbeiner,Elidona Skhikerujah,Emre Neftci*

Main category: cs.LG

TL;DR: Palimpsa通过贝叶斯元可塑性提升变压器的ICL记忆容量，克服遗忘与干扰，在MQAR与常识推理任务中大幅优于现有门控线性注意力模型。


<details>
  <summary>Details</summary>
Motivation: 利用transformers中的ICL作为自注意力的在线关联记忆，可解释其在复杂序列处理中的高性能。然而，门控线性注意力模型的记忆容量固定，易受干扰，尤其在长序列中，因此需要更高效的记忆学习机制。

Method: 提出Palimpsa模型，将ICL视为连续学习问题，使用贝叶斯元可塑性调控每个注意状态的可塑性，使其依赖于由先验分布捕捉的累积知识的重要性状态；将多种门控线性注意力模型映射为特定架构及后验近似，并证明Mamba2为遗忘占优的特殊情况。

Result: 在Multi-Query Associative Recall (MQAR) 基准及Commonsense Reasoning 任务上，Palimpsa 在所有对比基准模型中表现均优，显著提升记忆容量与性能。

Conclusion: Palimpsa 将非元可塑模型转为元可塑模型，突破记忆容量瓶颈，并在多任务下实现稳健的可塑性与稳定性平衡。

Abstract: In-Context Learning (ICL) in transformers acts as an online associative memory and is believed to underpin their high performance on complex sequence processing tasks. However, in gated linear attention models, this memory has a fixed capacity and is prone to interference, especially for long sequences. We propose Palimpsa, a self-attention model that views ICL as a continual learning problem that must address a stability-plasticity dilemma. Palimpsa uses Bayesian metaplasticity, where the plasticity of each attention state is tied to an importance state grounded by a prior distribution that captures accumulated knowledge. We demonstrate that various gated linear attention models emerge as specific architecture choices and posterior approximations, and that Mamba2 is a special case of Palimpsa where forgetting dominates. This theoretical link enables the transformation of any non-metaplastic model into a metaplastic one, significantly expanding its memory capacity. Our experiments show that Palimpsa consistently outperforms baselines on the Multi-Query Associative Recall (MQAR) benchmark and on Commonsense Reasoning tasks.

</details>


### [43] [Patient foundation model for risk stratification in low-risk overweight patients](https://arxiv.org/abs/2602.09079)
*Zachary N. Flamholz,Dillon Tracy,Ripple Khera,Jordan Wolinsky,Nicholas Lee,Nathaniel Tann,Xiao Yin Zhu,Harry Phillips,Jeffrey Sherman*

Main category: cs.LG

TL;DR: PatientTPP通过时序事件学习提升肥胖患者风险评估，优于BMI；适用于临床决策与成本优化。


<details>
  <summary>Details</summary>
Motivation: 在超重或肥胖患者中进行精准风险分层对指导预防护理及高成本治疗方案的分配至关重要。

Method: 利用超过50万条真实世界临床轨迹，PatientTPP 在诊断、实验室和药物事件序列中训练神经时序点过程模型，扩展了现有方法加入静态与数值特征，并融入临床知识对事件进行编码。

Result: PatientTPP 在心血管相关医疗费用分层中优于BMI，且能够预测低风险个体的肥胖相关结局，包括未在训练期间显式建模的事件。

Conclusion: PatientTPP是一种基于神经时序点过程的通用患者风险模型，在肥胖相关护理与成本分配中表现优于BMI，能够有效且可解释地预测未来心血管相关医疗费用。

Abstract: Accurate risk stratification in patients with overweight or obesity is critical for guiding preventive care and allocating high-cost therapies such as GLP-1 receptor agonists. We present PatientTPP, a neural temporal point process (TPP) model trained on over 500,000 real-world clinical trajectories to learn patient representations from sequences of diagnoses, labs, and medications. We extend existing TPP modeling approaches to include static and numeric features and incorporate clinical knowledge for event encoding. PatientTPP representations support downstream prediction tasks, including classification of obesity-associated outcomes in low-risk individuals, even for events not explicitly modeled during training. In health economic evaluation, PatientTPP outperformed body mass index in stratifying patients by future cardiovascular-related healthcare costs, identifying higher-risk patients more efficiently. By modeling both the type and timing of clinical events, PatientTPP offers an interpretable, general-purpose foundation for patient risk modeling with direct applications to obesity-related care and cost targeting.

</details>


### [44] [Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models](https://arxiv.org/abs/2602.09080)
*Ruihan Xu,Yuting Gao,Lan Wang,Jianing Li,Weihao Chen,Qingpei Guo,Ming Yang,Shiliang Zhang*

Main category: cs.LG

TL;DR: RecursiveVLM通过可调递归层和单调递归损失，实现可按需求调优的多模态Transformer，获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 通过递归循环复用模型参数，提升多模态表示能力，而无需增加模型体积。

Method: 提出RecursiveVLM结构，包含Recursive Connector（对齐不同层隐藏状态并进行模态特定投影）和Monotonic Recursion Loss（对每一步监督，保证递归深度性能单调提升）。

Result: 相较标准Transformer提升约3%，相较无循环基线提升约7%，并在资源受限设备上仍能得到强劲表现。

Conclusion: 递归循环是实现高效、可部署自适应LMM的有效路径。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in vision-language tasks, yet their vast parameter counts are often underutilized during both training and inference. In this work, we embrace the idea of looping back to move forward: reusing model parameters through recursive refinement to extract stronger multimodal representations without increasing model size. We propose RecursiveVLM, a recursive Transformer architecture tailored for LMMs. Two key innovations enable effective looping: (i) a Recursive Connector that aligns features across recursion steps by fusing intermediate-layer hidden states and applying modality-specific projections, respecting the distinct statistical structures of vision and language tokens; (ii) a Monotonic Recursion Loss that supervises every step and guarantees performance improves monotonically with recursion depth. This design transforms recursion into an on-demand refinement mechanism: delivering strong results with few loops on resource-constrained devices and progressively improving outputs when more computation resources are available. Experiments show consistent gains of +3% over standard Transformers and +7% over vanilla recursive baselines, demonstrating that strategic looping is a powerful path toward efficient, deployment-adaptive LMMs.

</details>


### [45] [From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics](https://arxiv.org/abs/2602.09101)
*Carlos Heredia*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we derive an accelerated continuous-time formulation of Adam by modeling it as a second-order integro-differential dynamical system. We relate this inertial nonlocal model to an existing first-order nonlocal Adam flow through an $α$-refinement limit, and we provide Lyapunov-based stability and convergence analyses. We also introduce an Adam-inspired nonlocal Lagrangian formulation, offering a variational viewpoint. Numerical simulations on Rosenbrock-type examples show agreement between the proposed dynamics and discrete Adam.

</details>


### [46] [Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide](https://arxiv.org/abs/2602.09109)
*Hossam Amer,Rezaul Karim,Ali Pourranjbar,Weiwei Zhang,Walid Ahmed,Boxing Chen*

Main category: cs.LG

TL;DR: 综述并系统分析大模型分布式训练与推理的并行技术，提出数学模型与自动搜索方案，给出经验性指南并指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的综述多为描述性，缺乏对分布式训练及推理方法的系统性效果与权衡分析，导致难以为大型语言模型的高效、可扩展系统设计提供有力理论与实践指导。

Method: 对集合操作和分布式并行策略进行全面回顾，并用数学模型加深理论把握；探讨混合并行设计，重点考虑不同部署阶段的通信与计算重叠；讨论基于成本模型的自动搜索优化混合并行策略；以主流架构为案例，给出经验性洞见。

Result: 提供了分布式大模型训练的系统性框架与定量评估，揭示了最佳混合并行选择的经验法则；同时指出了当前训练范式的主要挑战与局限，为后续研究指明了方向。

Conclusion: 本文构建了从理论到实践的完整分析体系，为研究者和工程师在设计分布式系统时选择并行策略提供坚实依据，并展望了下一代大模型发展趋势。

Abstract: With the rapid growth of large language models (LLMs), a wide range of methods have been developed to distribute computation and memory across hardware devices for efficient training and inference. While existing surveys provide descriptive overviews of these techniques, systematic analysis of their benefits and trade offs and how such insights can inform principled methodology for designing optimal distributed systems remain limited. This paper offers a comprehensive review of collective operations and distributed parallel strategies, complemented by mathematical formulations to deepen theoretical understanding. We further examine hybrid parallelization designs, emphasizing communication computation overlap across different stages of model deployment, including both training and inference. Recent advances in automated search for optimal hybrid parallelization strategies using cost models are also discussed. Moreover, we present case studies with mainstream architecture categories to reveal empirical insights to guide researchers and practitioners in parallelism strategy selection. Finally, we highlight open challenges and limitations of current LLM training paradigms and outline promising directions for the next generation of large scale model development.

</details>


### [47] [Epistemic Throughput: Fundamental Limits of Attention-Constrained Inference](https://arxiv.org/abs/2602.09127)
*Lei You*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent generative and tool-using AI systems can surface a large volume of candidates at low marginal cost, yet only a small fraction can be checked carefully. This creates a decoder-side bottleneck: downstream decision-makers must form reliable posteriors from many public records under scarce attention. We formalize this regime via Attention-Constrained Inference (ACI), in which a cheap screening stage processes $K$ records and an expensive verification stage can follow up on at most $B$ of them. Under Bayes log-loss, we study the maximum achievable reduction in posterior uncertainty per window, which we call \emph{epistemic throughput}. Our main result is a ``JaKoB'' scaling law showing that epistemic throughput has a baseline term that grows linearly with verification and prevalence, and an additional \emph{information-leverage} term that scales as $\sqrt{JKB}$, where $J$ summarizes screening quality. Thus, expanding cheap screening can nonlinearly amplify scarce verification, even when informative records are rare. We further show that this scaling is tight in a weak-screening limit, and that in the sparse-verification regime ($B \ll K$), substantial leverage requires heavy-tailed score distributions; for light-tailed scores the amplification is only logarithmic.

</details>


### [48] [Benchmarking the Energy Savings with Speculative Decoding Strategies](https://arxiv.org/abs/2602.09113)
*Rohit Dutta,Paramita Koley,Soham Poddar,Janardan Misra,Sanjay Podder,Naveen Balani,Saptarshi Ghosh,Niloy Ganguly*

Main category: cs.LG

TL;DR: 论文综述了投机式解码对能源消耗的影响，并给出了针对模型尺寸、策略与数据特点的优化建议。


<details>
  <summary>Details</summary>
Motivation: 本工作聚焦于针对大语言模型（LLM）推理中通过投机式解码（speculative decoding）降低延迟和推理成本所忽视的能源需求。

Method: 对不同模型尺寸、家族、投机解码策略以及数据集特征所导致的能源消耗进行系统综述并做细致分析。

Result: 提出并验证了多种因素对能源优化的影响规律，并为能源友好型解码方案提供指南。

Conclusion: 投机式解码不仅能提升性能，还需兼顾能源效率；本文为进一步研究与实践提供了可操作的参考。

Abstract: Speculative decoding has emerged as an effective method to reduce latency and inference cost of LLM inferences. However, there has been inadequate attention towards the energy requirements of these models. To address this gap, this paper presents a comprehensive survey of energy requirements of speculative decoding strategies, with detailed analysis on how various factors -- model size and family, speculative decoding strategies, and dataset characteristics -- influence the energy optimizations.

</details>


### [49] [SpinCastML an Open Decision-Making Application for Inverse Design of Electrospinning Manufacturing: A Machine Learning, Optimal Sampling and Inverse Monte Carlo Approach](https://arxiv.org/abs/2602.09120)
*Elisa Roldan,Tasneem Sabir*

Main category: cs.LG

TL;DR: SpinCastML是一个开源的、分布感知的化学知觉机器学习+IMC平台，可对电纺纤维直径分布进行全方位预测与逆向设计，显著提高实验效率、降低资源浪费，适用于生物医学、过滤和能源等应用。


<details>
  <summary>Details</summary>
Motivation: 现有框架缺乏可逆设计工具，无法同时纳入溶剂化学约束且难以预测完整直径分布，导致实验浪费和开发周期长；

Method: 采用包含68,480条纤维直径数据、16种聚合物的严谨数据集，结合三种结构化采样方法与11种高性能学习器，并嵌入化学约束，进一步使用Inverse Monte Carlo（IMC）对完整分布进行预测与反向设计；

Result: Cubist模型+聚合物平衡Sobol D最优采样最优，R²>0.92；IMC模型R²>0.90，预测与实验成功率误差<1%；提供逆向设计，给定目标可生成物理化学可行的溶剂组合及成功概率；

Conclusion: SpinCastML将电纺机理从试错式转为基于数据驱动的多分布反向设计平台，形成可复制、可持续的新标准；

Abstract: Electrospinning is a powerful technique for producing micro to nanoscale fibers with application specific architectures. Small variations in solution or operating conditions can shift the jet regime, generating non Gaussian fiber diameter distributions. Despite substantial progress, no existing framework enables inverse design toward desired fiber outcomes while integrating polymer solvent chemical constraints or predicting full distributions. SpinCastML is an open source, distribution aware, chemically informed machine learning and Inverse Monte Carlo (IMC) software for inverse electrospinning design. Built on a rigorously curated dataset of 68,480 fiber diameters from 1,778 datasets across 16 polymers, SpinCastML integrates three structured sampling methods, a suite of 11 high-performance learners, and chemistry aware constraints to predict not only mean diameter but the entire distribution. Cubist model with a polymer balanced Sobol D optimal sampling provides the highest global performance (R2 > 0.92). IMC accurately captures the fiber distributions, achieving R2 > 0.90 and <1% error between predicted and experimental success rates. The IMC engine supports both retrospective analysis and forward-looking inverse design, generating physically and chemically feasible polymer solvent parameter combinations with quantified success probabilities for user-defined targets. SpinCastML reframes electrospinning from trial and error to a reproducible, data driven design process. As an open source executable, it enables laboratories to analyze their own datasets and co create an expanding community software. SpinCastML reduces experimental waste, accelerates discovery, and democratizes access to advanced modeling, establishing distribution aware inverse design as a new standard for sustainable nanofiber manufacturing across biomedical, filtration, and energy applications.

</details>


### [50] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 将物理 LEI 模型与数据驱动校准结合，利用时间对比学习实现局部隐私校准，提升电网规划精度并揭示多 LEI 交互对恢复行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心快速增长，电力系统中大规模电子负载（LEL）建模越发重要。传统负载模型无法捕捉LEL的快速工作负载波动与保护驱动的断路/重接行为。

Method: 结合物理模型结构和数据驱动校准，构建可调参数的物理模型，并利用时间对比学习（TCL）进行时序与统计特征匹配，实现局部校准并共享仅校准参数以保障数据隐私。

Result: 在MIT Supercloud、ASU Sol、Blue Waters和ASHRAE等真实负载数据上校准后，将模型集成至ANDES平台，在IEEE 39、NPCC 140及WECC 179节点系统中测试，显示LEL相互作用会显著改变扰动后恢复行为，导致复合断路-重接动态与延迟稳定，传统未校准模型无法捕捉。

Conclusion: 提出的LEL模型通过物理结构+数据驱动校准与TCL同步，提高了在设施层面的电网规划准确性，并揭示多LEL交互对系统恢复的关键影响。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [51] [Counterfactual Maps: What They Are and How to Find Them](https://arxiv.org/abs/2602.09128)
*Awa Khouna,Julien Ferry,Thibaut Vidal*

Main category: cs.LG

TL;DR: 通过将反事实问题转化为最近邻区域查询，使用体积KD树实现高效、最优的树集合反事实生成，平均毫秒级响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用树集合的几何结构，导致求解效率低下或缺乏最优性保证，需寻找更高效且精确的反事实生成方案。

Method: 利用树集合的等价轴对齐超梯形划分，将反事实搜索转化为寻找最近不同标签矩形的最近邻区域，采用体积KD树实现带有最优性证书的分支限界搜索。

Result: 在多个高风险实际数据集上，该算法在一次预处理后平均查询时间为子线性，显著快于现有精确冷启动优化方法，并提供全局最优解。

Conclusion: 该方法实现了对树集合模型的全局最优反事实解释，并在交互式场景下实现毫秒级响应时间。

Abstract: Counterfactual explanations are a central tool in interpretable machine learning, yet computing them exactly for complex models remains challenging. For tree ensembles, predictions are piecewise constant over a large collection of axis-aligned hyperrectangles, implying that an optimal counterfactual for a point corresponds to its projection onto the nearest rectangle with an alternative label under a chosen metric. Existing methods largely overlook this geometric structure, relying either on heuristics with no optimality guarantees or on mixed-integer programming formulations that do not scale to interactive use.
  In this work, we revisit counterfactual generation through the lens of nearest-region search and introduce counterfactual maps, a global representation of recourse for tree ensembles. Leveraging the fact that any tree ensemble can be compressed into an equivalent partition of labeled hyperrectangles, we cast counterfactual search as the problem of identifying the generalized Voronoi cell associated with the nearest rectangle of an alternative label. This leads to an exact, amortized algorithm based on volumetric k-dimensional (KD) trees, which performs branch-and-bound nearest-region queries with explicit optimality certificates and sublinear average query time after a one-time preprocessing phase.
  Our experimental analyses on several real datasets drawn from high-stakes application domains show that this approach delivers globally optimal counterfactual explanations with millisecond-level latency, achieving query times that are orders of magnitude faster than existing exact, cold-start optimization methods.

</details>


### [52] [UniComp: A Unified Evaluation of Large Language Model Compression via Pruning, Quantization and Distillation](https://arxiv.org/abs/2602.09130)
*Jonathan von Rad,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: UniComp统筹评测剪枝、量化、蒸馏，覆盖性能、可靠性与效率；实验表明量化最佳兼顾性能与效率；压缩对知识任务友好，却削弱推理、多语能力；校准可显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型压缩评估方法有限，主要聚焦知识任务；缺乏统一评估标准来比较剪枝、量化和蒸馏；急需涵盖性能、可靠性和效率的综合评估。

Method: 构建具有性能、可靠性与效率三个维度的评估框架UniComp；收集多样化的能力与安全性基准；使用硬件感知效率分析；对六种压缩技术在40+数据集上进行广泛实验；并采用任务特定校准提升压缩模型推理表现。

Result: 发现压缩对知识任务保留较好，推理、多语和指令遵循能力显著退化；量化在性能与效率之间取得最佳权衡；蒸馏提供显著加速但计算成本高；任务特定校准可将剪枝模型推理能力提升约50%。

Conclusion: UniComp框架为大型语言模型压缩提供了统一的评估标准，揭示了不同压缩方法在性能、可靠性和效率上的优势与不足，尤其指出压缩过程中知识任务更易保留，而推理、多语言及指令遵循能力更易下降；量化在性能与效率兼顾方面表现最优，蒸馏虽能显著加速，但付出高计算成本。

Abstract: Model compression is increasingly essential for deploying large language models (LLMs), yet existing evaluations are limited in method coverage and focus primarily on knowledge-centric benchmarks. Thus, we introduce UniComp, a unified evaluation framework for comparing pruning, quantization, and knowledge distillation. UniComp evaluates compressed models along three dimensions: performance, reliability, and efficiency, using a diverse set of capability- and safety-oriented benchmarks together with a hardware-aware efficiency analysis. Through extensive evaluation of six compression techniques on modern LLMs across more than 40 datasets, we find that (i) compression exhibits a consistent knowledge bias, where knowledge-intensive tasks are relatively preserved while reasoning, multilingual, and instruction-following capabilities degrade substantially; (ii) quantization provides the best overall trade-off between retained performance and efficiency, whereas distillation yields strong runtime acceleration gains at high computational cost; and (iii) task-specific calibration can significantly improve the reasoning ability of pruned models by up to 50%.

</details>


### [53] [What do Geometric Hallucination Detection Metrics Actually Measure?](https://arxiv.org/abs/2602.09158)
*Eric Yeats,John Buckheit,Sarah Scullen,Brendan Kennedy,Loc Truong,Davis Brown,Bill Kay,Cliff Joslyn,Tegan Emerson,Michael J. Henry,John Emanuello,Henry Kvinge*

Main category: cs.LG

TL;DR: 研究发现大型语言模型内部几何特征可预测幻觉，且通过域归一化可显著改善多域检测效果。


<details>
  <summary>Details</summary>
Motivation: 在无外部真值信息的高风险应用场景，现有幻觉检测依赖外部知识，需寻找内部状态的几何信号以预测幻觉。

Method: 构建多属性合成数据集（正确性、置信度、关联性、连贯性、完整性），并评估现有几何检测方法对任务域的敏感性，随后提出域归一化技巧来缓解域漂移。

Result: 发现不同几何统计量分别对应不同幻觉类型；域归一化后，多域AUROC提升了34个百分点。

Conclusion: 本文展示了几何统计量能有效捕捉大型语言模型在不同偏差下产生的幻觉类型，并证明通过简单归一化可以显著提升多域检测性能。

Abstract: Hallucination remains a barrier to deploying generative models in high-consequence applications. This is especially true in cases where external ground truth is not readily available to validate model outputs. This situation has motivated the study of geometric signals in the internal state of an LLM that are predictive of hallucination and require limited external knowledge. Given that there are a range of factors that can lead model output to be called a hallucination (e.g., irrelevance vs incoherence), in this paper we ask what specific properties of a hallucination these geometric statistics actually capture. To assess this, we generate a synthetic dataset which varies distinct properties of output associated with hallucination. This includes output correctness, confidence, relevance, coherence, and completeness. We find that different geometric statistics capture different types of hallucinations. Along the way we show that many existing geometric detection methods have substantial sensitivity to shifts in task domain (e.g., math questions vs. history questions). Motivated by this, we introduce a simple normalization method to mitigate the effect of domain shift on geometric statistics, leading to AUROC gains of +34 points in multi-domain settings.

</details>


### [54] [Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning](https://arxiv.org/abs/2602.10044)
*Akshay Mete,Shahid Aamir Sheikh,Tzu-Hsiang Lin,Dileep Kalathil,P. R. Kumar*

Main category: cs.LG

TL;DR: 通过在世界模型中加入乐观动力学损失，OWM算法提高了探索效果，验证了在DreamerV3和STORM上的样本效率与收益提升。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励的强化学习环境中，探索效率仍然是核心难题。本研究旨在通过将经典的奖励偏置最大似然估计（RBMLE）应用于深度强化学习，以系统且可扩展的方式实现乐观探索。

Method: 提出“Optimistic World Models (OWMs)”框架，在模型学习阶段加入乐观性，通过增益乐观动力学损失（optimistic dynamics loss）将假想转移偏向高奖励结果；改动仅限于损失函数，无需不确定性估计或约束优化，易于集成到现有世界模型中。

Result: 在两个顶尖的世界模型架构（DreamerV3与STORM）中实现乐观版本，显著提升了采样效率与累积回报，优于基线模型。

Conclusion: OWM框架通过直接在模型学习中注入乐观性，既保持了可扩展性，又不需要额外的概率估计，展示了在稀疏奖励环境中改进探索的有效方案。

Abstract: Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.

</details>


### [55] [Faster Rates For Federated Variational Inequalities](https://arxiv.org/abs/2602.09164)
*Guanghui Wang,Satyen Kale*

Main category: cs.LG

TL;DR: 改进局部额外SGD分析，提出LIPPAX解决漂移问题，在多种场景下提升收敛速度，并推广到联邦复合VI。


<details>
  <summary>Details</summary>
Motivation: 现有联邦变分不等式收敛率与凸优化存在显著差距，需要改进并克服局部额外SGD的漂移缺陷。

Method: 基于梯度下降的局部额外SGD新分析，提出局部不精确近端点算法与额外步(LIPPAX)，并在多种情境下验证其收敛性能。

Result: 在光滑单调、Hessian有界、算子有界、低方差等多种情况下，通过LIPPAX实现更快的收敛速率，并对联邦复合变分不等式给出了改进保证。

Conclusion: 本文通过对局部额外SGD算法的细化分析，提出了更紧的收敛速率，并指出其存在的客户端漂移问题；随后设计了LIPPAX算法，成功缓解漂移并在多种场景下实现更优收敛；最终将成果推广到联邦复合变分不等式并获得改进保证。

Abstract: In this paper, we study federated optimization for solving stochastic variational inequalities (VIs), a problem that has attracted growing attention in recent years. Despite substantial progress, a significant gap remains between existing convergence rates and the state-of-the-art bounds known for federated convex optimization. In this work, we address this limitation by establishing a series of improved convergence rates. First, we show that, for general smooth and monotone variational inequalities, the classical Local Extra SGD algorithm admits tighter guarantees under a refined analysis. Next, we identify an inherent limitation of Local Extra SGD, which can lead to excessive client drift. Motivated by this observation, we propose a new algorithm, the Local Inexact Proximal Point Algorithm with Extra Step (LIPPAX), and show that it mitigates client drift and achieves improved guarantees in several regimes, including bounded Hessian, bounded operator, and low-variance settings. Finally, we extend our results to federated composite variational inequalities and establish improved convergence guarantees.

</details>


### [56] [Train Less, Infer Faster: Efficient Model Finetuning and Compression via Structured Sparsity](https://arxiv.org/abs/2602.09169)
*Jonathan Svirsky,Yehonathan Refael,Ofir Lindenbaum*

Main category: cs.LG

TL;DR: 用极少参数的随机稀疏门控对大模型进行微调，可在不降低准确率的前提下压缩40%参数并提升推理速度，理论和实验均证实优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型微调中，显存、算力和过拟合成为主要瓶颈。

Method: 通过在模型的行列上引入稀疏门控，仅微调极少量参数，消除20–40%权重，且无显著精度损失。

Result: 实验显示该方法在效率和性能上均优于现有LoRA等微调基线；理论上证明了随机门控的收敛性，并表明优化地形更好。

Conclusion: 稀疏化为任务特定的语言模型适配提供了一种更高效、更轻量的机制。

Abstract: Fully finetuning foundation language models (LMs) with billions of parameters is often impractical due to high computational costs, memory requirements, and the risk of overfitting. Although methods like low-rank adapters help address these challenges by adding small trainable modules to the frozen LM, they also increase memory usage and do not reduce inference latency. We uncover an intriguing phenomenon: sparsifying specific model rows and columns enables efficient task adaptation without requiring weight tuning. We propose a scheme for effective finetuning via sparsification using training stochastic gates, which requires minimal trainable parameters, reduces inference time, and removes 20--40\% of model parameters without significant accuracy loss. Empirical results show it outperforms recent finetuning baselines in efficiency and performance. Additionally, we provide theoretical guarantees for the convergence of this stochastic gating process, and show that our method admits a simpler and better-conditioned optimization landscape compared to LoRA. Our results highlight sparsity as a compelling mechanism for task-specific adaptation in LMs.

</details>


### [57] [Weighted Wasserstein Barycenter of Gaussian Processes for exotic Bayesian Optimization tasks](https://arxiv.org/abs/2602.09181)
*Antonio Candelieri,Francesco Archetti*

Main category: cs.LG

TL;DR: 本文提出W2BGP框架，用单一加权Wasserstein重心统一多种贝叶斯优化任务，并提升计算效率


<details>
  <summary>Details</summary>
Motivation: 为统一不同的贝叶斯优化任务，利用高斯分布与高斯过程后验之间的类比，引入加权Wasserstein重心实现框架

Method: 构造加权Wasserstein重心高斯过程（W2BGP），配合相应权重设置，可同时处理协作/联邦、批式、以及多保真度贝叶斯优化，并重新解释常用采集函数

Result: 实验表明，仅通过合适的权重选择即可完成上述三类任务，且在计算Wasserstein重心时比现有机器学习方法更高效

Conclusion: W2BGP提供了一个通用而高效的贝叶斯优化框架，具备可扩展的研究前景

Abstract: Exploiting the analogy between Gaussian Distributions and Gaussian Processes' posterior, we present how the weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) can be used to unify, under a common framework, different exotic Bayesian Optimization (BO) tasks. Specifically, collaborative/federated BO, (synchronous) batch BO, and multi-fidelity BO are considered in this paper. Our empirical analysis proves that each one of these tasks requires just an appropriate weighting schema for the W2BGP, while the entire framework remains untouched. Moreover, we demonstrate that the most well-known BO acquisition functions can be easily re-interpreted under the proposed framework and also enable a more computationally efficient way to deal with the computation of the Wasserstein Barycenter, compared with state-of-the-art methods from the Machine Learning literature. Finally, research perspectives branching from the proposed approach are presented.

</details>


### [58] [ML-DCN: Masked Low-Rank Deep Crossing Network Towards Scalable Ads Click-through Rate Prediction at Pinterest](https://arxiv.org/abs/2602.09194)
*Jiacheng Li,Yixiong Meng,Yi wu,Yun Zhao,Sharare Zehtabian,Jiayin Jin,Degao Peng,Jinfeng Zhuang,Qifei Shen,Kungang Li*

Main category: cs.LG

TL;DR: Pinterest 推荐系统面临模型扩容与推理成本冲突，作者提出 ML-DCN（低秩交叉+实例掩码）实现了更高 AUC、良好计算扩展性并已上线，提升广告效果。


<details>
  <summary>Details</summary>
Motivation: 在广告排序中，既需要模型容量提升以提高预测和业务效果，又受限于服务器内存和推理时延；因此需在固定计算预算下寻找可扩展且高效的特征交互模块。

Method: 通过在低秩交叉层中加入实例条件掩码，实现对每个样本的交互方向进行选择和放大，兼顾 DCNv2 与 MaskNet 的优势，形成 ML-DCN。

Result: 在 Pinterest 广告数据集上，ML-DCN 在相同 FLOPs 下获得更高 AUC，随着计算资源增加，其 AUC-FLOPs 平衡更优；线上 A/B 测试显示 CTR 等关键指标显著提升。

Conclusion: ML-DCN 在保持相同 FLOPs 的前提下，实现了比 DCNv2、MaskNet 等模型更高的 AUC，并在计算资源增量时的 AUC-FLOPs 均衡上表现更佳，已成功部署到 Pinterest 广告系统并提升关键指标。

Abstract: Deep learning recommendation systems rely on feature interaction modules to model complex user-item relationships across sparse categorical and dense features. In large-scale ad ranking, increasing model capacity is a promising path to improving both predictive performance and business outcomes, yet production serving budgets impose strict constraints on latency and FLOPs. This creates a central tension: we want interaction modules that both scale effectively with additional compute and remain compute-efficient at serving time. In this work, we study how to scale feature interaction modules under a fixed serving budget. We find that naively scaling DCNv2 and MaskNet, despite their widespread adoption in industry, yields rapidly diminishing offline gains in the Pinterest ads ranking system. To overcome aforementioned limitations, we propose ML-DCN, an interaction module that integrates an instance-conditioned mask into a low-rank crossing layer, enabling per-example selection and amplification of salient interaction directions while maintaining efficient computation. This novel architecture combines the strengths of DCNv2 and MaskNet, scales efficiently with increased compute, and achieves state-of-the-art performance. Experiments on a large internal Pinterest ads dataset show that ML-DCN achieves higher AUC than DCNv2, MaskNet, and recent scaling-oriented alternatives at matched FLOPs, and it scales more favorably overall as compute increases, exhibiting a stronger AUC-FLOPs trade-off. Finally, online A/B tests demonstrate statistically significant improvements in key ads metrics (including CTR and click-quality measures) and ML-DCN has been deployed in the production system with neutral serving cost.

</details>


### [59] [Fair Feature Importance Scores via Feature Occlusion and Permutation](https://arxiv.org/abs/2602.09196)
*Camille Little,Madeline Navarro,Santiago Segarra,Genevera Allen*

Main category: cs.LG

TL;DR: 提出两种模型无关的公平性特征重要度评估方法，实验验证其简便、有效，可用于公平性与可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 本文关注机器学习模型对社会的影响及其不透明性，强调在公平性背景下理解个体特征对模型结果的影响对于建立可解释和公平模型的重要性。

Method: 提出两套无模型依赖的方法：①通过置换特征值并比较模型公平性，测量特征对公平性的贡献；②通过对训练时包含或不包含某特征的模型公平性进行评估，并利用 minipatch 学习实现计算简化。

Result: 在多种预测任务上实验结果表明，这两种指标均简单、有效、可扩展，并能直观量化特征对公平性的影响。

Conclusion: 本文提供了两种易于实现、扩展性强且可解释的工具，用于评估特征对公平性的影响，促进负责任的机器学习开发。

Abstract: As machine learning models increasingly impact society, their opaque nature poses challenges to trust and accountability, particularly in fairness contexts. Understanding how individual features influence model outcomes is crucial for building interpretable and equitable models. While feature importance metrics for accuracy are well-established, methods for assessing feature contributions to fairness remain underexplored. We propose two model-agnostic approaches to measure fair feature importance. First, we propose to compare model fairness before and after permuting feature values. This simple intervention-based approach decouples a feature and model predictions to measure its contribution to training. Second, we evaluate the fairness of models trained with and without a given feature. This occlusion-based score enjoys dramatic computational simplification via minipatch learning. Our empirical results reflect the simplicity and effectiveness of our proposed metrics for multiple predictive tasks. Both methods offer simple, scalable, and interpretable solutions to quantify the influence of features on fairness, providing new tools for responsible machine learning development.

</details>


### [60] [CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning](https://arxiv.org/abs/2602.09207)
*Xiaofeng Xiao,Xiao Hu,Yang Ye,Xubo Yue*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) has achieved remarkable success in a wide range of sequential decision-making problems. Recent diffusion-based policies further improve RL by modeling complex, high-dimensional action distributions. However, existing diffusion policies primarily rely on statistical associations and fail to explicitly account for causal relationships among states, actions, and rewards, limiting their ability to identify which action components truly cause high returns. In this paper, we propose Causality-guided Diffusion Policy (CausalGDP), a unified framework that integrates causal reasoning into diffusion-based RL. CausalGDP first learns a base diffusion policy and an initial causal dynamical model from offline data, capturing causal dependencies among states, actions, and rewards. During real-time interaction, the causal information is continuously updated and incorporated as a guidance signal to steer the diffusion process toward actions that causally influence future states and rewards. By explicitly considering causality beyond association, CausalGDP focuses policy optimization on action components that genuinely drive performance improvements. Experimental results demonstrate that CausalGDP consistently achieves competitive or superior performance over state-of-the-art diffusion-based and offline RL methods, especially in complex, high-dimensional control tasks.

</details>


### [61] [A Lightweight Multi-View Approach to Short-Term Load Forecasting](https://arxiv.org/abs/2602.09220)
*Julien Guité-Vinet,Alexandre Blondin Massé,Éric Beaudry*

Main category: cs.LG

TL;DR: 提出一种轻量多视角短期负荷预测模型：使用单值嵌入与缩放时间范围输入，并加入Embedding Dropout，达成与大模型相当的预测精度，参数更少，鲁棒性更强，并提供特征解释。


<details>
  <summary>Details</summary>
Motivation: 虽然大参数Transformer在时间序列预测上表现优异，但其复杂度导致过拟合和对旧数据影响不足，且参数量大不利于资源受限场景。

Method: 基于单值嵌入与缩放时间范围输入的多视角模型，结合Embedding Dropout机制以抑制过度依赖特征，并提升可解释性。

Result: 在多组数据集（含噪声和稀疏数据）上，该模型参数量显著减少，预测性能仍保持竞争力，且通过特征贡献分析实现了更强的可解释性。

Conclusion: 该方法通过轻量级多视角设计，在短期负荷预测中实现了与大型Transformer相当的性能，同时显著降低了参数量和过拟合风险，具有良好的鲁棒性和解释性。

Abstract: Time series forecasting is a critical task across domains such as energy, finance, and meteorology, where accurate predictions enable informed decision-making. While transformer-based and large-parameter models have recently achieved state-of-the-art results, their complexity can lead to overfitting and unstable forecasts, especially when older data points become less relevant. In this paper, we propose a lightweight multi-view approach to short-term load forecasting that leverages single-value embeddings and a scaled time-range input to capture temporally relevant features efficiently. We introduce an embedding dropout mechanism to prevent over-reliance on specific features and enhance interpretability. Our method achieves competitive performance with significantly fewer parameters, demonstrating robustness across multiple datasets, including scenarios with noisy or sparse data, and provides insights into the contributions of individual features to the forecast.

</details>


### [62] [Computationally Efficient Replicable Learning of Parities](https://arxiv.org/abs/2602.09499)
*Moshe Noivirt,Jessica Sorrell,Eliad Tsfadia*

Main category: cs.LG

TL;DR: 本文给出了一种新的可复制算法，可在任意分布下高效学习公认可辨的奇偶性函数，说明了可复制学习在计算上能超越SQ学习，更接近差分隐私学习的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨可复制性与稳定性概念（如差分隐私和SQ模型）之间的计算关系，尤其是聚焦可复制PAC学习在效率与能力方面的差距。

Method: 构造了一个高效可复制的子空间覆盖算法，利用它处理向量集合以产生覆盖大部分向量的子空间，并在此基础上实现对偶性函数的可复制学习。

Result: 提出了首个在任意分布下对线性可辨奇偶性函数的高效可复制学习算法，验证了可复制学习与SQ学习和差分隐私学习之间的复杂度分离。

Conclusion: 本文证明了存在高效可复制学习算法，可在任意分布下对线性可辨的奇偶性函数进行学习，显示了可复制学习在算法效率上优于SQ学习并更接近差分隐私学习的能力。

Abstract: We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.
  Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.

</details>


### [63] [LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection](https://arxiv.org/abs/2602.09634)
*Naveen Gill,Ajvad Haneef K,Madhu Kumar S D*

Main category: cs.LG

TL;DR: 本文验证了使用大型语言模型在侵害检测中进行零样本特征选择的可行性，并证明其在多种性能指标上与传统特征选择方法匹配且更具解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法关注统计或模型重要性，易忽视特征语义；探索LLM在零样本场景下利用语义指引进行特征选择。

Method: 使用多种大型语言模型（GPT‑5.0、GPT‑4.0、Gemini‑2.5等），仅基于特征名称和任务描述执行零样本特征选择，并将其与Extra Trees、Variance Threshold、Tree-based、Chi‑Squared、ANOVA、Random Selection、Sequential Attention等传统方法在随机森林、Extra Trees、MLP、KNN等分类器上进行对比。

Result: LLM指导的零样本特征选择在准确率、精确率、召回率、F1、AUC、MCC和运行时指标上与传统方法相当，并在可解释性、稳定性、对标签依赖度方面更具优势。

Conclusion: LLM驱动的零样本特征选择在恶意软件检测任务上与传统方法竞争，并在可解释性、稳定性和对标注数据的依赖性上具有优势；因此可视为一种有前景的替代策略。

Abstract: Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications

</details>


### [64] [Beyond the Unit Hypersphere: Embedding Magnitude in Contrastive Learning](https://arxiv.org/abs/2602.09229)
*Xincan Feng,Taro Watanabe*

Main category: cs.LG

TL;DR: 移除不必要的归一化约束，依据任务对称性自由选择余弦或点积即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究嵌入幅值所携带的信息、其何时有益以及如何有效利用，因现有工作仅笼统比较点积与余弦相似度，却未剖析幅值在不同任务中的作用。

Method: 对文本和视觉模型进行2×2消融实验，分别独立控制输入侧和输出侧正则化（归一化）。

Result: 1）在文本检索中，输出（文档）幅值与相关性高度相关（Cohen'd 最高1.80），在推理密集任务上提升最大；2）输入和输出幅值作用不对称，输出幅值直接缩放相似度分数，输入幅值调节训练动态；3）幅值学习对异质任务有益，却对同质任务有害，导致对称与非对称任务表现差异。

Conclusion: 本文提出任务对称原则：在输入与输出角色不同（非对称）任务中，应保留嵌入幅值以使用点积；而在输入与输出角色相同（对称）任务中，应使用余弦相似度去除幅值约束，可实现成本为零的性能提升。

Abstract: Cosine similarity is prevalent in contrastive learning, yet it makes an implicit assumption: embedding magnitude is noise. Prior work occasionally found dot product and cosine similarity comparable, but left unanswered WHAT information magnitude carries, WHEN it helps, and HOW to leverage it. We conduct a systematic study through a $2 \times 2$ ablation that independently controls input-side and output-side normalization across text and vision models. Our findings reveal three key insights. First, in text retrieval, output (document) magnitude strongly correlates with relevance (Cohen's $d$ up to 1.80), yielding the largest gains on reasoning-intensive tasks. Second, input and output magnitudes serve asymmetric roles: output magnitude directly scales similarity scores while input magnitude modulates training dynamics. Third, magnitude learning benefits asymmetric tasks (text retrieval, RAG) but harms symmetric tasks (STS, text-image alignment). These findings establish a task symmetry principle: the choice between cosine and dot product depends on whether the task has distinct input roles, enabling cost-free improvements by simply removing an unnecessary constraint.

</details>


### [65] [Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy](https://arxiv.org/abs/2602.10100)
*Júlio Oliveira,Rodrigo Ferreira,André Riker,Glaucio H. S. Carvalho,Eirini Eleni Tsilopoulou*

Main category: cs.LG

TL;DR: 本文提出FEXT-DP：联邦可解释树+差分隐私，兼顾隐私与解释，可在更少轮次下实现更好均方误差并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 在当今机器学习系统中，数据隐私和模型可解释性同等重要，传统深度学习模型往往兼顾不到两者，本文旨在同时满足这两项需求。

Method: 通过将可解释性强的决策树与联邦学习机制相结合，并在树结构上施加差分隐私保护，得到FEXT-DP模型。

Result: 实验表明，FEXT-DP在训练轮次、均方误差和解释性指标上均优于基准模型，证明其在速度、性能与可解释性方面的提升。

Conclusion: 本文提出了FEXT-DP——一种在联邦学习框架下结合差分隐私与可解释决策树的机器学习模型，展示了在保持数据隐私的同时仍能提升模型解释性的可行性。

Abstract: Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.

</details>


### [66] [RAPID: Risk of Attribute Prediction-Induced Disclosure in Synthetic Microdata](https://arxiv.org/abs/2602.09235)
*Matthias Templ,Oscar Thees,Roman Müller*

Main category: cs.LG

TL;DR: RAPID衡量从合成微数据中推断敏感属性的易受攻击性，既可解释又稳健，适用于任意学习算法和属性类型。


<details>
  <summary>Details</summary>
Motivation: 传统身份泄露度量在全合成微数据上信息有限，研究需要直接测量攻击者基于发布数据推断敏感属性的能力。

Method: 提出RAPID风险度量：攻击者仅利用发布的全合成微数据训练预测模型，再对真实个体的准标识符作预测。对连续属性，根据指定相对误差阈值报告预测成功比例；对分类属性，给出标准化置信度，衡量攻击者对真类的额外置信度，并统计超过政策阈值的记录比例。

Result: 在仿真与真实数据上验证了阈值校准、误差不确定性量化，并比较了多种合成生成器的属性推断泄露风险。结果显示RAPID提供了可解释、稳健、与类不平衡无关的攻击者逼真上界风险评估。

Conclusion: RAPID成为实用的属性推断泄露风险上界指标，补充了现有实用性诊断与泄漏控制框架，具有可解释性、限界性和通用性。

Abstract: Statistical data anonymization increasingly relies on fully synthetic microdata, for which classical identity disclosure measures are less informative than an adversary's ability to infer sensitive attributes from released data. We introduce RAPID (Risk of Attribute Prediction--Induced Disclosure), a disclosure risk measure that directly quantifies inferential vulnerability under a realistic attack model. An adversary trains a predictive model solely on the released synthetic data and applies it to real individuals' quasi-identifiers. For continuous sensitive attributes, RAPID reports the proportion of records whose predicted values fall within a specified relative error tolerance. For categorical attributes, we propose a baseline-normalized confidence score that measures how much more confident the attacker is about the true class than would be expected from class prevalence alone, and we summarize risk as the fraction of records exceeding a policy-defined threshold. This construction yields an interpretable, bounded risk metric that is robust to class imbalance, independent of any specific synthesizer, and applicable with arbitrary learning algorithms. We illustrate threshold calibration, uncertainty quantification, and comparative evaluation of synthetic data generators using simulations and real data. Our results show that RAPID provides a practical, attacker-realistic upper bound on attribute-inference disclosure risk that complements existing utility diagnostics and disclosure control frameworks.

</details>


### [67] [Feature salience -- not task-informativeness -- drives machine learning model explanations](https://arxiv.org/abs/2602.09238)
*Benedict Clark,Marta Oliveira,Rick Wilming,Stefan Haufe*

Main category: cs.LG

TL;DR: 该研究发现XAI归因受图像显著性影响，需重新评估过去研究与工作流程。


<details>
  <summary>Details</summary>
Motivation: 澄清可解释性AI中重要性标记是否真正对应目标信息，还是受其他数据属性干扰。

Method: 在三种图像分类任务中训练深度模型，引入透明水印，使用五种流行的归因方法评估相对重要性。

Result: 水印区的相对重要性显著上升（R²≥0.45），水印的类别相关性影响有限（R²≤0.03），并与模型无关的边缘检测相似。

Conclusion: 重要性归因主要受图像结构的显著性驱动，而非统计关联。

Abstract: Explainable AI (XAI) promises to provide insight into machine learning models' decision processes, where one goal is to identify failures such as shortcut learning. This promise relies on the field's assumption that input features marked as important by an XAI must contain information about the target variable. However, it is unclear whether informativeness is indeed the main driver of importance attribution in practice, or if other data properties such as statistical suppression, novelty at test-time, or high feature salience substantially contribute. To clarify this, we trained deep learning models on three variants of a binary image classification task, in which translucent watermarks are either absent, act as class-dependent confounds, or represent class-independent noise. Results for five popular attribution methods show substantially elevated relative importance in watermarked areas (RIW) for all models regardless of the training setting ($R^2 \geq .45$). By contrast, whether the presence of watermarks is class-dependent or not only has a marginal effect on RIW ($R^2 \leq .03$), despite a clear impact impact on model performance and generalisation ability. XAI methods show similar behaviour to model-agnostic edge detection filters and attribute substantially less importance to watermarks when bright image intensities are encoded by smaller instead of larger feature values. These results indicate that importance attribution is most strongly driven by the salience of image structures at test time rather than statistical associations learned by machine learning models. Previous studies demonstrating successful XAI application should be reevaluated with respect to a possibly spurious concurrency of feature salience and informativeness, and workflows using feature attribution methods as building blocks should be scrutinised.

</details>


### [68] [Generalizing GNNs with Tokenized Mixture of Experts](https://arxiv.org/abs/2602.09258)
*Xiaoguang Guo,Zehong Wang,Jiazheng Li,Shawn Spitzel,Qi Yang,Kaize Ding,Jundong Li,Chuxu Zhang*

Main category: cs.LG

TL;DR: 静态 GNN 受到稳健性-泛化折衷限制，STEM‑GNN通过多专家、量化接口和Lipschitz正则实现更优鲁棒性与性能


<details>
  <summary>Details</summary>
Motivation: 部署后的 GNN 需要兼顾稳健性、泛化和稳定性，但静态推理导致不可逆的性能上限，需寻找突破方法

Method: 预训练-微调框架，利用多专家混合模型、向量量化令牌接口和输出激活限制

Result: 在九个节点/边/图任务中，STEM‑GNN 在保持干净数据性能的同时显著提高了对度/同质性偏移以及特征/边破坏的鲁棒性

Conclusion: STEM‑GNN通过多专家编码器、量化接口和Lipschitz正则化实现了更好的三方平衡，提升了在分布偏移和噪声攻击下的鲁棒性；

Abstract: Deployed graph neural networks (GNNs) are frozen at deployment yet must fit clean data, generalize under distribution shifts, and remain stable to perturbations. We show that static inference induces a fundamental tradeoff: improving stability requires reducing reliance on shift-sensitive features, leaving an irreducible worst-case generalization floor. Instance-conditional routing can break this ceiling, but is fragile because shifts can mislead routing and perturbations can make routing fluctuate. We capture these effects via two decompositions separating coverage vs selection, and base sensitivity vs fluctuation amplification. Based on these insights, we propose STEM-GNN, a pretrain-then-finetune framework with a mixture-of-experts encoder for diverse computation paths, a vector-quantized token interface to stabilize encoder-to-head signals, and a Lipschitz-regularized head to bound output amplification. Across nine node, link, and graph benchmarks, STEM-GNN achieves a stronger three-way balance, improving robustness to degree/homophily shifts and to feature/edge corruptions while remaining competitive on clean graphs.

</details>


### [69] [The effect of whitening on explanation performance](https://arxiv.org/abs/2602.09278)
*Benedict Clark,Stoyan Karastoyanov,Rick Wilming,Stefan Haufe*

Main category: cs.LG

TL;DR: 本文评估了白化预处理对解释方法的影响，发现其对部分方法有效，强调预处理在解释质量中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法往往错误给非信息变量（如抑制变量）重要性，目前缺乏有效策略降低此类误判。

Method: 在XAI-TRIS基准下，对16种常见特征归因方法与5种白化变换进行实验，并对Wilming等提出的二维线性分类问题进行理论分析。

Result: 实验显示，部分白化技术能显著提升归因准确度，但提升幅度不一；理论分析表明在Bayes优模型中白化可削弱抑制变量影响。

Conclusion: 本研究表明，白化预处理可对某些特征归因方法的解释性能产生积极影响，但效果因方法和模型结构而异；因此，特征解相关预处理对于提升可解释性至关重要。

Abstract: Explainable Artificial Intelligence (XAI) aims to provide transparent insights into machine learning models, yet the reliability of many feature attribution methods remains a critical challenge. Prior research (Haufe et al., 2014; Wilming et al., 2022, 2023) has demonstrated that these methods often erroneously assign significant importance to non-informative variables, such as suppressor variables, leading to fundamental misinterpretations. Since statistical suppression is induced by feature dependencies, this study investigates whether data whitening, a common preprocessing technique for decorrelation, can mitigate such errors. Using the established XAI-TRIS benchmark (Clark et al., 2024b), which offers synthetic ground-truth data and quantitative measures of explanation correctness, we empirically evaluate 16 popular feature attribution methods applied in combination with 5 distinct whitening transforms. Additionally, we analyze a minimal linear two-dimensional classification problem (Wilming et al., 2023) to theoretically assess whether whitening can remove the impact of suppressor features from Bayes-optimal models. Our results indicate that, while specific whitening techniques can improve explanation performance, the degree of improvement varies substantially across XAI methods and model architectures. These findings highlight the complex relationship between data non-linearities, preprocessing quality, and attribution fidelity, underscoring the vital role of pre-processing techniques in enhancing model interpretability.

</details>


### [70] [Measuring Privacy Risks and Tradeoffs in Financial Synthetic Data Generation](https://arxiv.org/abs/2602.09288)
*Michael Zuo,Inwon Kang,Stacy Patterson,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 针对金融表格数据的合成案例，研究提出隐私改进 GAN 与自编码器，实验表明在类别失衡环境下可提升合成质量与隐私兼顾。


<details>
  <summary>Details</summary>
Motivation: 金融数据高监管风险与极大类别不平衡的挑战，需要评估与改进合成数据在隐私与实用性之间的权衡。

Method: 对多种生成模型（自编码器、GAN、扩散模型、Copula）进行实验，并实现受限数据的隐私增强 GAN 与自编码器版本，随后在平衡与不平衡数据集上评估其数据质量、下游任务表现与隐私保护效果。

Result: 实验结果揭示了在缺失类别严重失衡的数据环境下，传统合成器难以同时满足高质量与隐私需求；提供的隐私增强实现提升了在不平衡集上的可用性。

Conclusion: 研究显示，在严重类别不平衡和混合属性的金融表格数据中，隐私保护的GAN和自编码器合成器能够在数据信息质量、下游用途及隐私泄露风险之间实现可接受的平衡；但不平衡样本仍显著降低合成质量与模型泛化。

Abstract: We explore the privacy-utility tradeoff of synthetic data generation schemes on tabular financial datasets, a domain characterized by high regulatory risk and severe class imbalance. We consider representative tabular data generators, including autoencoders, generative adversarial networks, diffusion, and copula synthesizers. To address the challenges of the financial domain, we provide novel privacy-preserving implementations of GAN and autoencoder synthesizers. We evaluate whether and how well the generators simultaneously achieve data quality, downstream utility, and privacy, with comparison across balanced and imbalanced input datasets. Our results offer insight into the distinct challenges of generating synthetic data from datasets that exhibit severe class imbalance and mixed-type attributes.

</details>


### [71] [Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation](https://arxiv.org/abs/2602.09295)
*Bret Nestor,Bohan Yao,Jasmine Moore,Jasper Kanes*

Main category: cs.LG

TL;DR: 收集30+年海听器音频数据，构建大规模杀人鲸及其他海洋哺乳动物数据集；利用弱监督+主动学习的Transformer模型，实现高精度检测和分类，结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 缺乏包含多种海洋哺乳动物的大规模公开音频数据，限制了对关键濒危物种栖息地使用和保护研究的进展

Method: 对30年以上公开桨式水听器数据实施弱监督正负标注主动学习，采用基于Transformer的模型实现检测与多类别分类

Result: Transformer检测器在DEEPAL、DCLDE-2026及两个新专家标注集上优于现有模型；检测特异度可达0–28.8%（95%灵敏度），多分类精度分别为42.1%（11个训练类、4个测试类）和43.0%（4个训练类、5个测试类）

Conclusion: 本研究提供了迄今为止最大规模的浸入型南方居住型杀人鲸音频数据集，并通过强化学习方法构建了高精度、高能效的多类海洋哺乳动物检测与分类模型

Abstract: This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals. The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity. Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset.
  We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined. The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.

</details>


### [72] [The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry](https://arxiv.org/abs/2602.09297)
*Yuchong Zhang,Vardan Papyan*

Main category: cs.LG

TL;DR: 研究在Transformer中用Laplacian机制替代传统注意力，改进token方差控制，提升多任务性能，并使token映射趋于更清晰的可分离几何结构


<details>
  <summary>Details</summary>
Motivation: 探究在Transformer中的注意力机制如何影响token表示的方差，从而提高模型的几何分离度

Method: 将注意力替换为Laplacian机制，并在多种机器学习基准上评估其改进；使用PCA、余弦相似度、方差分析和Neural Collapse评估token表示几何

Result: Laplacian机制在计算机视觉和语言任务上持续提升性能，并将token embedding重新塑造为更具可分离性的几何结构，类内聚集并展现Neural Collapse特征

Conclusion: 通过更直接地控制token方差，Laplacian注意力机制能实现理想的token几何形态，从而提升Transformer在多任务上的性能

Abstract: Transformers leverage attention, the residual connection, and layer normalization to control the variance of token representations. We propose to modify attention into a Laplacian mechanism that gives the model more direct control over token variance. We conjecture that this helps transformers achieve the ideal token geometry. To investigate our conjecture, we first show that incorporating the Laplacian mechanism into transformers induces consistent improvements across benchmarks in computer vision and language. Next, we study how the Laplacian mechanism impacts the geometry of token representations using various tools: 1) principal component analysis, 2) cosine similarity metric, 3) analysis of variance, and 4) Neural Collapse metrics. Our investigation shows that the Laplacian mechanism reshapes token embeddings toward a geometry of maximal separability: tokens collapse according to their classes, and the class means exhibit Neural Collapse.

</details>


### [73] [Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk](https://arxiv.org/abs/2602.09300)
*Sumedh Gupte,Shrey Rakeshkumar Patel,Soumen Pachal,Prashanth L. A.,Sanjay P. Bhat*

Main category: cs.LG

TL;DR: 本文提出了针对期望值、效用短缺风险和OCE的风险敏感政策梯度算法，并给出MSE和收敛理论，实验验证支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习侧重期望收益，缺乏对风险的显式建模；本文旨在为三类重要风险度量提供可行且理论可分析的PG方法，填补风险敏感RL的空白。

Method: 1) 在有限时域马尔可夫决策过程中，对每种风险度量推导政策梯度定理；2) 为每种风险度量提出政策梯度估计器，并证明其均方误差为O(1/m)；3) 在常见假设下证明风险敏感目标函数光滑，进而给出整体算法的驻点收敛率上界；4) 在常见强化学习基准上进行实验验证。

Result: 均方误差界O(1/m); 风险敏感目标光滑; 驻点收敛率上界; 实验与理论一致。

Conclusion: 提出了专门针对期望值、基于效用的短缺风险和优化置信等价风险的风险敏感强化学习政策梯度算法，并在理论上证明了其均方误差界和收敛速度；在标准假设下证明了目标函数光滑性，从而得到稳定收敛率，并通过数值实验证实了理论结果。

Abstract: We propose risk-sensitive reinforcement learning algorithms catering to three families of risk measures, namely expectiles, utility-based shortfall risk and optimized certainty equivalent risk. For each risk measure, in the context of a finite horizon Markov decision process, we first derive a policy gradient theorem. Second, we propose estimators of the risk-sensitive policy gradient for each of the aforementioned risk measures, and establish $\mathcal{O}\left(1/m\right)$ mean-squared error bounds for our estimators, where $m$ is the number of trajectories. Further, under standard assumptions for policy gradient-type algorithms, we establish smoothness of the risk-sensitive objective, in turn leading to stationary convergence rate bounds for the overall risk-sensitive policy gradient algorithm that we propose. Finally, we conduct numerical experiments to validate the theoretical findings on popular RL benchmarks.

</details>


### [74] [Stabilizing Physics-Informed Consistency Models via Structure-Preserving Training](https://arxiv.org/abs/2602.09303)
*Che-Chia Chang,Chen-Yang Dai,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: 两阶段训练 + 冻结解码器 + 两步残差目标，使 PDE 求解在少步推理中既稳定又高效，前向解可用投影零样本修补代替昂贵的迭代


<details>
  <summary>Details</summary>
Motivation: 解决物理约束一致性学习中PDE残差导致模型退化的问题

Method: 采用两阶段结构保持训练，并在细化阶段冻结系数解码器，引入两步残差目标保证物理一致性

Result: 实现了快速、少步的高保真推理，既可无条件生成也能求解前向问题；通过投影式零样本修补获得与扩散基线相同的精度，但计算成本大幅降低

Conclusion: 所提出的物理信息一致性建模框架在保持分布学习完整性的同时，有效地约束物理一致性，显著提升求解效率与精度

Abstract: We propose a physics-informed consistency modeling framework for solving partial differential equations (PDEs) via fast, few-step generative inference. We identify a key stability challenge in physics-constrained consistency training, where PDE residuals can drive the model toward trivial or degenerate solutions, degrading the learned data distribution. To address this, we introduce a structure-preserving two-stage training strategy that decouples distribution learning from physics enforcement by freezing the coefficient decoder during physics-informed fine-tuning. We further propose a two-step residual objective that enforces physical consistency on refined, structurally valid generative trajectories rather than noisy single-step predictions. The resulting framework enables stable, high-fidelity inference for both unconditional generation and forward problems. We demonstrate that forward solutions can be obtained via a projection-based zero-shot inpainting procedure, achieving consistent accuracy of diffusion baselines with orders of magnitude reduction in computational cost.

</details>


### [75] [Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation](https://arxiv.org/abs/2602.09305)
*Pei-Chi Pan,Yingbin Liang,Sen Lin*

Main category: cs.LG

TL;DR: 奖励设计决定 LLM 推理质量，RARL 框架系统化奖励模式、分析奖励攻击，并评估基准缺陷，指明更可靠评估方法。


<details>
  <summary>Details</summary>
Motivation: 在 RL 微调中奖励设计决定模型学习和泛化能力，然而其与核心推理挑战（偏差、幻觉、分布偏移、学习效率）的关联尚不明确，亟需系统化研究。

Method: 构建 Reasoning‑Aligned Reinforcement Learning（RARL）框架，设计奖励机制分类法，分析奖励攻击，联合讨论推理扩展性、幻觉抑制等问题，并批判性评估当前基准。

Result: 提出了奖励机制目录与统一框架，揭示奖励攻击是普遍的失败模式，展示奖励信号如何统一推理相关挑战，并指出基准评估的主要脆弱点。

Conclusion: 奖励建模是影响大型语言模型推理表现的核心因素，本文提出 RARL 框架以系统化奖励机制，阐明奖励与推理一致性的内在联系并指出奖励攻击等常见失误；同时评估现有基准数据的脆弱性，并为更稳健的评价提出路径。

Abstract: Large Language Models (LLMs) demonstrate transformative potential, yet their reasoning remains inconsistent and unreliable. Reinforcement learning (RL)-based fine-tuning is a key mechanism for improvement, but its effectiveness is fundamentally governed by reward design. Despite its importance, the relationship between reward modeling and core LLM challenges--such as evaluation bias, hallucination, distribution shift, and efficient learning--remains poorly understood. This work argues that reward modeling is not merely an implementation detail but a central architect of reasoning alignment, shaping what models learn, how they generalize, and whether their outputs can be trusted. We introduce Reasoning-Aligned Reinforcement Learning (RARL), a unifying framework that systematizes diverse reward paradigms for multi-step reasoning. Within this framework, we present a taxonomy of reward mechanisms, analyze reward hacking as a pervasive failure mode, and examine how reward signals unify challenges ranging from inference-time scaling to hallucination mitigation. We further critically evaluate existing benchmarks, highlighting vulnerabilities such as data contamination and reward misalignment, and outline directions for more robust evaluation. By integrating fragmented research threads and clarifying the interplay between reward design and fundamental reasoning capabilities, this work provides a foundational roadmap for building reasoning models that are robust, verifiable, and trustworthy.

</details>


### [76] [Empowering Contrastive Federated Sequential Recommendation with LLMs](https://arxiv.org/abs/2602.09306)
*Thi Minh Chau Nguyen,Minh Hieu Nguyen,Duc Anh Nguyen,Xuan Huong Tran,Thanh Trung Huynh,Quoc Viet Hung Nguyen*

Main category: cs.LG

TL;DR: LUMOS通过本地LLM产生未来、重述、对立三种序列，实现三视角对比学习，提升联邦序列推荐性能与抗噪性。


<details>
  <summary>Details</summary>
Motivation: 在联邦序列推荐中，用户数据脱离中心化且存在碎片化、噪声大及同质交互日志，导致模型效果受限。

Method: 提出LUMOS框架，利用本地LLM生成三种语义增强序列：未来导向轨迹、语义等价重述和不一致的对立序列，并通过三视角对比学习联合编码。

Result: 在三大公开基准试验中，LUMOS在HR@20和NDCG@20上持续优于中心化与传统联邦基线，并在噪声与对抗环境中保持鲁棒性，无需额外服务器保护模块。

Conclusion: LLM驱动的语义生成为隐私保护联邦推荐提供了新的范式，显著提升模型质量与鲁棒性。

Abstract: Federated sequential recommendation (FedSeqRec) aims to perform next-item prediction while keeping user data decentralised, yet model quality is frequently constrained by fragmented, noisy, and homogeneous interaction logs stored on individual devices. Many existing approaches attempt to compensate through manual data augmentation or additional server-side constraints, but these strategies either introduce limited semantic diversity or increase system overhead. To overcome these challenges, we propose \textbf{LUMOS}, a parameter-isolated FedSeqRec architecture that integrates large language models (LLMs) as \emph{local semantic generators}. Instead of sharing gradients or auxiliary parameters, LUMOS privately invokes an on-device LLM to construct three complementary sequence variants from each user history: (i) \emph{future-oriented} trajectories that infer plausible behavioural continuations, (ii) \emph{semantically equivalent rephrasings} that retain user intent while diversifying interaction patterns, and (iii) \emph{preference-inconsistent counterfactuals} that serve as informative negatives. These synthesized sequences are jointly encoded within the federated backbone through a tri-view contrastive optimisation scheme, enabling richer representation learning without exposing sensitive information. Experimental results across three public benchmarks show that LUMOS achieves consistent gains over competitive centralised and federated baselines on HR@20 and NDCG@20. In addition, the use of semantically grounded positive signals and counterfactual negatives improves robustness under noisy and adversarial environments, even without dedicated server-side protection modules. Overall, this work demonstrates the potential of LLM-driven semantic generation as a new paradigm for advancing privacy-preserving federated recommendation.

</details>


### [77] [Clarifying Shampoo: Adapting Spectral Descent to Stochasticity and the Parameter Trajectory](https://arxiv.org/abs/2602.09314)
*Runa Eschenhagen,Anna Cai,Tsung-Hsien Lee,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: Shampoo在权重矩阵层面比Muon更高效，优势归因于矩阵结构利用，质疑了对参数形状不敏感的解释。


<details>
  <summary>Details</summary>
Motivation: 探究利用矩阵结构的优化器相较于逐元素算法在数据效率上的差异，并澄清它们技术关系及对参数形状的依赖。

Method: 对比实验检验Shampoo、Muon、Adam、Signum的标记效率，并分解Shampoo更新为改造后的Muon更新，进一步分析权重矩阵上的影响。

Result: 实验表明Shampoo显著高于Muon；其改进可以被解释为单纯在权重矩阵上使用海绵式更新，支持矩阵特化的权证而非形状无关或方差白化的叙述。

Conclusion: Shampoo在权重矩阵层面数据效率高于Muon，模仿Adam优于Signum的效果；其优势源于对矩阵更新的特化，而非仅靠形状无关的解释。

Abstract: Optimizers leveraging the matrix structure in neural networks, such as Shampoo and Muon, are more data-efficient than element-wise algorithms like Adam and Signum. While in specific settings, Shampoo and Muon reduce to spectral descent analogous to how Adam and Signum reduce to sign descent, their general relationship and relative data efficiency under controlled settings remain unclear. Through extensive experiments on language models, we demonstrate that Shampoo achieves higher token efficiency than Muon, mirroring Adam's advantage over Signum. We show that Shampoo's update applied to weight matrices can be decomposed into an adapted Muon update. Consistent with this, Shampoo's benefits can be exclusively attributed to its application to weight matrices, challenging interpretations agnostic to parameter shapes. This admits a new perspective that also avoids shortcomings of related interpretations based on variance adaptation and whitening: rather than enforcing semi-orthogonality as in spectral descent, Shampoo's updates are time-averaged semi-orthogonal in expectation.

</details>


### [78] [Effective MoE-based LLM Compression by Exploiting Heterogeneous Inter-Group Experts Routing Frequency and Information Density](https://arxiv.org/abs/2602.09316)
*Zhendong Mi,Yixiao Chen,Pu Zhao,Xiaodong Yu,Hao Wang,Yanzhi Wang,Shaoyi Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixture-of-Experts (MoE) based Large Language Models (LLMs) have achieved superior performance, yet the massive memory overhead caused by storing multiple expert networks severely hinders their practical deployment. Singular Value Decomposition (SVD)-based compression has emerged as a promising post-training technique; however, most existing methods apply uniform rank allocation or rely solely on static weight properties. This overlooks the substantial heterogeneity in expert utilization observed in MoE models, where frequent routing patterns and intrinsic information density vary significantly across experts. In this work, we propose RFID-MoE, an effective framework for MoE compression by exploiting heterogeneous Routing Frequency and Information Density. We first introduce a fused metric that combines expert activation frequency with effective rank to measure expert importance, adaptively allocating higher ranks to critical expert groups under a fixed budget. Moreover, instead of discarding compression residuals, we reconstruct them via a parameter-efficient sparse projection mechanism to recover lost information with minimal parameter overhead. Extensive experiments on representative MoE LLMs (e.g., Qwen3, DeepSeekMoE) across multiple compression ratios demonstrate that RFID-MoE consistently outperforms state-of-the-art methods like MoBE and D2-MoE. Notably, RFID-MoE achieves a perplexity of 16.92 on PTB with the Qwen3-30B model at a 60% compression ratio, reducing perplexity by over 8.0 compared to baselines, and improves zero-shot accuracy on HellaSwag by approximately 8%.

</details>


### [79] [In-Hospital Stroke Prediction from PPG-Derived Hemodynamic Features](https://arxiv.org/abs/2602.09328)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.LG

TL;DR: 本文首次在真实临床数据中证明PPG能在发作前数小时预测中风，提出一种基于LLM的数据挖掘+ResNet-1D模型的早期预警框架。


<details>
  <summary>Details</summary>
Motivation: 缺乏术前生理数据限制了中风的早期预测。

Method: 利用LLM辅助在MIMIC-III和MC-MED中提取准确的住院中风发作时间戳，获取无偏的病前PPG数据；提取血流动力学特征，使用ResNet-1D模型预测不同预警时段的中风。

Result: 在MIMIC-III上，4、5、6小时前的F1分数分别为0.7956、0.8759、0.9406；在MC-MED上同一时段的F1分数分别为0.9256、0.9595、0.9888。

Conclusion: PPG信号在发作前数小时内就包含可预测的中风特征，表明可利用被动获取的生理信号进行可靠的早期预警，可推动从事件后识别转向主动基于生理监测的监护，以改善临床结果。

Abstract: The absence of pre-hospital physiological data in standard clinical datasets fundamentally constrains the early prediction of stroke, as patients typically present only after stroke has occurred, leaving the predictive value of continuous monitoring signals such as photoplethysmography (PPG) unvalidated. In this work, we overcome this limitation by focusing on a rare but clinically critical cohort - patients who suffered stroke during hospitalization while already under continuous monitoring - thereby enabling the first large-scale analysis of pre-stroke PPG waveforms aligned to verified onset times. Using MIMIC-III and MC-MED, we develop an LLM-assisted data mining pipeline to extract precise in-hospital stroke onset timestamps from unstructured clinical notes, followed by physician validation, identifying 176 patients (MIMIC) and 158 patients (MC-MED) with high-quality synchronized pre-onset PPG data, respectively. We then extract hemodynamic features from PPG and employ a ResNet-1D model to predict impending stroke across multiple early-warning horizons. The model achieves F1-scores of 0.7956, 0.8759, and 0.9406 at 4, 5, and 6 hours prior to onset on MIMIC-III, and, without re-tuning, reaches 0.9256, 0.9595, and 0.9888 on MC-MED for the same horizons. These results provide the first empirical evidence from real-world clinical data that PPG contains predictive signatures of stroke several hours before onset, demonstrating that passively acquired physiological signals can support reliable early warning, supporting a shift from post-event stroke recognition to proactive, physiology-based surveillance that may materially improve patient outcomes in routine clinical care.

</details>


### [80] [MacrOData: New Benchmarks of Thousands of Datasets for Tabular Outlier Detection](https://arxiv.org/abs/2602.09329)
*Xueying Ding,Simon Klüttermann,Haomin Wen,Yilong Chen,Leman Akoglu*

Main category: cs.LG

TL;DR: MacrOData：包含超过2400个表格异常检测数据集的公开基准，提供统一划分、元数据和在线排行榜，支持深度学习与传统方法的系统评估与对比。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如AdBench）仅包含57个数据集，缺乏多样性与统计显著性，限制了异常检测方法的评估与比较。

Method: 通过三部分构造：OddBench（790个真实语义异常数据集）、OvrBench（856个真实统计异常数据集）以及SynBench（800个合成多先验、多异常类型数据集），并为每个数据集提供标准化训练/测试划分、公开/私有评测子集与语义元数据。随后在整个套件上对传统、深度、基础模型等多种异常检测方法进行大规模实验。

Result: 在2446个数据集上完成全面实验，报告了方法表现差异、调参建议及实践指南；所有数据集开放源，配套在线排行榜。

Conclusion: 本研究提出了MacrOData——一个包含2446个数据集、覆盖异常检测实际场景与合成数据的大规模基准套件，为表格数据异常检测方法的公平、完善评估提供了必要工具。

Abstract: Quality benchmarks are essential for fairly and accurately tracking scientific progress and enabling practitioners to make informed methodological choices. Outlier detection (OD) on tabular data underpins numerous real-world applications, yet existing OD benchmarks remain limited. The prominent OD benchmark AdBench is the de facto standard in the literature, yet comprises only 57 datasets. In addition to other shortcomings discussed in this work, its small scale severely restricts diversity and statistical power. We introduce MacrOData, a large-scale benchmark suite for tabular OD comprising three carefully curated components: OddBench, with 790 datasets containing real-world semantic anomalies; OvrBench, with 856 datasets featuring real-world statistical outliers; and SynBench, with 800 synthetically generated datasets spanning diverse data priors and outlier archetypes. Owing to its scale and diversity, MacrOData enables comprehensive and statistically robust evaluation of tabular OD methods. Our benchmarks further satisfy several key desiderata: We provide standardized train/test splits for all datasets, public/private benchmark partitions with held-out test labels for the latter reserved toward an online leaderboard, and annotate our datasets with semantic metadata. We conduct extensive experiments across all benchmarks, evaluating a broad range of OD methods comprising classical, deep, and foundation models, over diverse hyperparameter configurations. We report detailed empirical findings, practical guidelines, as well as individual performances as references for future research. All benchmarks containing 2,446 datasets combined are open-sourced, along with a publicly accessible leaderboard hosted at https://huggingface.co/MacrOData-CMU.

</details>


### [81] [Large Language Models for Designing Participatory Budgeting Rules](https://arxiv.org/abs/2602.09349)
*Nguyen Thach,Xingchen Sha,Hau Chan*

Main category: cs.LG

TL;DR: 采用LLM与进化搜索结合的LLMRule框架，能自动生成超越人工规则、同时公平度不下降的参与式预算方案。


<details>
  <summary>Details</summary>
Motivation: 传统规则在满足效用与公平双重目标时面临域知识需求高、两者存在权衡等难题，需寻找更灵活的自动化方法。

Method: 引入LLMRule框架，将大型语言模型融入进化搜索过程，实现参与式预算规则的自动化设计。

Result: 在来自美国、加拿大、波兰和荷兰的600多项真实PB实例上，LLM生成的规则在效用方面明显优于手工规则，公平性保持相近。

Conclusion: LLM生成的参与式预算规则在整体效用上普遍优于现有手工规则，同时保持了相似的公平度。

Abstract: Participatory budgeting (PB) is a democratic paradigm for deciding the funding of public projects given the residents' preferences, which has been adopted in numerous cities across the world. The main focus of PB is designing rules, functions that return feasible budget allocations for a set of projects subject to some budget constraint. Designing PB rules that optimize both utility and fairness objectives based on agent preferences had been challenging due to the extensive domain knowledge required and the proven trade-off between the two notions. Recently, large language models (LLMs) have been increasingly employed for automated algorithmic design. Given the resemblance of PB rules to algorithms for classical knapsack problems, in this paper, we introduce a novel framework, named LLMRule, that addresses the limitations of existing works by incorporating LLMs into an evolutionary search procedure for automating the design of PB rules. Our experimental results, evaluated on more than 600 real-world PB instances obtained from the U.S., Canada, Poland, and the Netherlands with different representations of agent preferences, demonstrate that the LLM-generated rules generally outperform existing handcrafted rules in terms of overall utility while still maintaining a similar degree of fairness.

</details>


### [82] [Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning](https://arxiv.org/abs/2602.09395)
*Yifei Cheng,Xianglin Yang,Guoxia Wang,Chao Huang,Fei Ma,Dianhai Yu,Xiaochun Cao,Li Shen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers. Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity. We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \#1 rank on LLM fine-tuning. Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\%, 22\% and 21\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\%), verifying the efficiency of our proposed algorithm.

</details>


### [83] [Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning](https://arxiv.org/abs/2602.09396)
*Nilaksh,Antoine Clavaud,Mathieu Reymond,François Rivest,Sarath Chandar*

Main category: cs.LG

TL;DR: 提出SPR+正交梯度更新解决流式RL样本低效，实验表明在多套基准上超越基线，学习表示更丰富，计算负担低。


<details>
  <summary>Details</summary>
Motivation: 在流式强化学习中，过渡仅在一次更新后即被丢弃，导致样本效率低下。本工作旨在最大化每帧数据的利用价值。

Method: 将自预测表示(SPR)引入流式管道，并采用相对于动量目标的正交梯度更新，以缓解相关样本导致的梯度冲突，并处理流式优化器的特殊梯度冲突。

Result: 在Atari、MinAtar和Octax数据集上，所提方法持续优于现有流式基线；t-SNE可视化和有效秩测量表明其学习了更丰富的表示，并在仅使用少量CPU核心的情况下保持高效。

Conclusion: 通过正交梯度更新优化SPR，本研究成功弥合了无经验回放缓冲区造成的性能差距，实现了高效、样本友好的流式RL。

Abstract: In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.

</details>


### [84] [Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models](https://arxiv.org/abs/2602.09402)
*Alireza F. Pour,Farnam Mansouri,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文提供了多答案在线学习的误差、回报和样本复杂度完全解读，在三类反馈模型下分别给出最优界限，并通过组合维度进行统一描述。


<details>
  <summary>Details</summary>
Motivation: 在自然语言生成等任务中，同一提示可能接受多种有效完成，但并非所有完成都是合格的。为此需研究在每轮只能输出一个有效标签的在线学习框架。

Method: 在可实现设定下引入合适的组合维度，分别对三种反馈模型求解最优误差界限；在非可实现设定下构造回报三角结构并给出下界；随后利用这些维度推导批量学习的样本复杂度上界。

Result: 1) 给出三种反馈模型的最优误差界限；2) 在非可实现设定下证明三种模型间的回报三角结构；3) 以相应组合维度为参数得到批量学习的样本复杂度上界。

Conclusion: 本文系统阐述了多答案在线学习问题的理论边界，首次给出三种反馈模型的最优误差界限与回报三角结构，并推导了对应的批量学习样本复杂度，展现了组合维度在多答案设置中的决定性作用。

Abstract: We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models. For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.

</details>


### [85] [Diffusion-Guided Pretraining for Brain Graph Foundation Models](https://arxiv.org/abs/2602.09437)
*Xinxu Wei,Rong Zhou,Lifang He,Yu Zhang*

Main category: cs.LG

TL;DR: 用扩散技术设计的结构感知掩码与全局读取/重建，解决脑图预训练中语义破坏与信息缺失问题，取得显著实验提升


<details>
  <summary>Details</summary>
Motivation: 传统在脑图中使用的随机掩码能破坏语义连通性，且现有读取方案缺乏全局结构信息，限制了表示学习质量

Method: 基于扩散的预训练，采用结构感知的删除/掩码和拓扑感知的图级读取与节点级全局重建

Result: 在超过25,000名受试者、60,000扫描的多组学数据集上，实验显示性能持续提升

Conclusion: 该框架通过扩散引导的结构感知增强和全局读取/重建显著提升了脑信号图模型的泛化性能

Abstract: With the growing interest in foundation models for brain signals, graph-based pretraining has emerged as a promising paradigm for learning transferable representations from connectome data. However, existing contrastive and masked autoencoder methods typically rely on naive random dropping or masking for augmentation, which is ill-suited for brain graphs and hypergraphs as it disrupts semantically meaningful connectivity patterns. Moreover, commonly used graph-level readout and reconstruction schemes fail to capture global structural information, limiting the robustness of learned representations. In this work, we propose a unified diffusion-based pretraining framework that addresses both limitations. First, diffusion is designed to guide structure-aware dropping and masking strategies, preserving brain graph semantics while maintaining effective pretraining diversity. Second, diffusion enables topology-aware graph-level readout and node-level global reconstruction by allowing graph embeddings and masked nodes to aggregate information from globally related regions. Extensive experiments across multiple neuroimaging datasets with over 25,000 subjects and 60,000 scans involving various mental disorders and brain atlases demonstrate consistent performance improvements.

</details>


### [86] [Taming the Monster Every Context: Complexity Measure and Unified Framework for Offline-Oracle Efficient Contextual Bandits](https://arxiv.org/abs/2602.09456)
*Hao Qin,Chicheng Zhang*

Main category: cs.LG

TL;DR: OE2D算法将大动作空间情景bandit学习转化为离线回归；仅少量离线回归调用即可获得近似最优回报，提出DOEC衡量复杂度并桥接DEC。


<details>
  <summary>Details</summary>
Motivation: 在大动作空间的情景bandit问题中，实现低调用量的离线回归的同时保持近似最优的召回回归，解决在线与离线oracle高效性设计的统一问题。

Method: OE2D通过构造“exploitative F-design”动作分布，利用离线回归oracle估计奖励，联合覆盖与探索机制，并将算法设计扩展泛化自Falcon及其线性奖励版本。

Result: 在O(log(T))次离线回归调用下获得近似最优回报，若预先已知T可进一步降至O(loglog(T))。推导出DOEC，证明其与Eluder维度及DEC之间的关系，为离线/在线oracle高效算法提供理论依据。

Conclusion: OE2D框架将具有通用奖励函数逼近的情景bandit学习转化为离线回归，并在大动作空间中实现近似最优弱化，需仅O(log(T))次离线回归调用（若已知T可降至O(loglog(T))）。其核心是利用“exploitative F-design”行动分布，兼顾覆盖与低调度；并提出了InDecision-Offline Estimation Coefficient（DOEC）这一新复杂度度量，其在Eluder维数约束下有界，并与Decision Estimation Coefficient（DEC）建立了桥接，首次将离线与在线oracle高效性设计原理统一起来。

Abstract: We propose an algorithmic framework, Offline Estimation to Decisions (OE2D), that reduces contextual bandit learning with general reward function approximation to offline regression. The framework allows near-optimal regret for contextual bandits with large action spaces with $O(log(T))$ calls to an offline regression oracle over $T$ rounds, and makes $O(loglog(T))$ calls when $T$ is known. The design of OE2D algorithm generalizes Falcon~\citep{simchi2022bypassing} and its linear reward version~\citep[][Section 4]{xu2020upper} in that it chooses an action distribution that we term ``exploitative F-design'' that simultaneously guarantees low regret and good coverage that trades off exploration and exploitation. Central to our regret analysis is a new complexity measure, the Decision-Offline Estimation Coefficient (DOEC), which we show is bounded in bounded Eluder dimension per-context and smoothed regret settings. We also establish a relationship between DOEC and Decision Estimation Coefficient (DEC)~\citep{foster2021statistical}, bridging the design principles of offline- and online-oracle efficient contextual bandit algorithms for the first time.

</details>


### [87] [Scalable and Reliable State-Aware Inference of High-Impact N-k Contingencies](https://arxiv.org/abs/2602.09461)
*Lihao Mai,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Increasing penetration of inverter-based resources, flexible loads, and rapidly changing operating conditions make higher-order $N\!-\!k$ contingency assessment increasingly important but computationally prohibitive. Exhaustive evaluation of all outage combinations using AC power-flow or ACOPF is infeasible in routine operation. This fact forces operators to rely on heuristic screening methods whose ability to consistently retain all critical contingencies is not formally established. This paper proposes a scalable, state-aware contingency inference framework designed to directly generate high-impact $N\!-\!k$ outage scenarios without enumerating the combinatorial contingency space. The framework employs a conditional diffusion model to produce candidate contingencies tailored to the current operating state, while a topology-aware graph neural network trained only on base and $N\!-\!1$ cases efficiently constructs high-risk training samples offline. Finally, the framework is developed to provide controllable coverage guarantees for severe contingencies, allowing operators to explicitly manage the risk of missing critical events under limited AC power-flow evaluation budgets. Experiments on IEEE benchmark systems show that, for a given evaluation budget, the proposed approach consistently evaluates higher-severity contingencies than uniform sampling. This allows critical outages to be identified more reliably with reduced computational effort.

</details>


### [88] [Online Learning in MDPs with Partially Adversarial Transitions and Losses](https://arxiv.org/abs/2602.09474)
*Ofir Schlisselberg,Tal Lancewicki,Yishay Mansour*

Main category: cs.LG

TL;DR: 引入条件占用量，解决有限对抗步骤的 MDP，给出两类算法和对应 regret 上界；同时在完全对抗情形下完成上下界匹配。


<details>
  <summary>Details</summary>
Motivation: 现实环境往往稳定，只有极少点会出现不确定或对抗性变化，该模型可更贴近实际并降低学者需要适应的复杂度。

Method: 定义条件占用量，利用其在对抗步骤下的稳定性，构造两种求解算法：全局对抗与连续对抗；随后通过 regret 缩减消除对对抗步骤位置的先验知识；最后分析全对抗情形下的全信息与 bandit 反馈。

Result: 得到两条 regret 上界：全对抗勒为 $\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})$，连续对抗为 $\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})$；并提供 $K^{2/3}$-regret 缩减；在完全对抗（$Λ=H-1$）环境下给出几乎最佳匹配的上下界。

Conclusion: 本文证明了在有限次数的对抗性转移下可实现可观的 regret 上界，并通过条件占用量实现不受对抗性影响的稳定性；在完全对抗的设置下给出近乎匹配的上下界。

Abstract: We study reinforcement learning in MDPs whose transition function is stochastic at most steps but may behave adversarially at a fixed subset of $Λ$ steps per episode. This model captures environments that are stable except at a few vulnerable points. We introduce \emph{conditioned occupancy measures}, which remain stable across episodes even with adversarial transitions, and use them to design two algorithms. The first handles arbitrary adversarial steps and achieves regret $\tilde{O}(H S^Λ\sqrt{K S A^{Λ+1}})$, where $K$ is the number of episodes, $S$ is the number of state, $A$ is the number of actions and $H$ is the episode's horizon. The second, assuming the adversarial steps are consecutive, improves the dependence on $S$ to $\tilde{O}(H\sqrt{K S^{3} A^{Λ+1}})$. We further give a $K^{2/3}$-regret reduction that removes the need to know which steps are the $Λ$ adversarial steps. We also characterize the regret of adversarial MDPs in the \emph{fully adversarial} setting ($Λ=H-1$) both for full-information and bandit feedback, and provide almost matching upper and lower bounds (slightly strengthen existing lower bounds, and clarify how different feedback structures affect the hardness of learning).

</details>


### [89] [Towards Uniformity and Alignment for Multimodal Representation Learning](https://arxiv.org/abs/2602.09507)
*Wenzhe Yin,Pan Zhou,Zehao Xiao,Jie Liu,Shujian Yu,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.LG

TL;DR: 论文指出 InfoNCE 目标在多模态学习中产生的对齐与均匀性冲突，对此提出对齐/均匀性分离的损失，并理论证明可近似 Hölder 散度；在检索与生成实验中均显著提升。


<details>
  <summary>Details</summary>
Motivation: InfoNCE 的统一目标引入对齐-均匀性冲突和多模态内对齐冲突，导致在更多模态时出现分布差距；需要一种无任务专用模块的冲突自由学习方案。

Method: 在InfoNCE目标的基础上，提出将对齐与均匀性分离的损失，理论上等价于对多模态分布做全局 Hölder 散度近似，从而消除交叉模态的冲突。

Result: 实验表明，在检索任务和 UnCLIP 风格生成任务上，该方法均实现了稀释的分布差距并取得了一致的性能提升。

Conclusion: 该论文提供了一种基于对齐与均匀性的分离方案，显著减小多模态分布间的差距，并在检索与生成任务上均取得提升。

Abstract: Multimodal representation learning aims to construct a shared embedding space in which heterogeneous modalities are semantically aligned. Despite strong empirical results, InfoNCE-based objectives introduce inherent conflicts that yield distribution gaps across modalities. In this work, we identify two conflicts in the multimodal regime, both exacerbated as the number of modalities increases: (i) an alignment-uniformity conflict, whereby the repulsion of uniformity undermines pairwise alignment, and (ii) an intra-alignment conflict, where aligning multiple modalities induces competing alignment directions. To address these issues, we propose a principled decoupling of alignment and uniformity for multimodal representations, providing a conflict-free recipe for multimodal learning that simultaneously supports discriminative and generative use cases without task-specific modules. We then provide a theoretical guarantee that our method acts as an efficient proxy for a global Hölder divergence over multiple modality distributions, and thus reduces the distribution gap among modalities. Extensive experiments on retrieval and UnCLIP-style generation demonstrate consistent gains.

</details>


### [90] [Rashomon Sets and Model Multiplicity in Federated Learning](https://arxiv.org/abs/2602.09520)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato*

Main category: cs.LG

TL;DR: 本文在联邦学习背景下首次正式定义Rashomon集合并提出三种视角，演示如何在隐私约束下估计多重度，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有Rashomon和多重度度量仅适用于集中式学习，无法迁移到联邦学习环境，难以解决客户端异构和隐私限制下模型多样性与公平性的问题。

Method: 首先将Rashomon集合划分为全球集合、t-一致集合和个体集合；其次在保持隐私的协议下估计多重度指标；最后设计了基于多重度的FL流程，并在标准基准数据集上进行实验验证。

Result: 实验表明三种联邦Rashomon定义均能提供有价值的认识，帮助客户端部署更符合本地数据与公平需求的模型。

Conclusion: 我们提出了在联邦学习中设定Rashomon集合的三种形式，并证明了它们能够帮助客户端选择更适合本地分布、提升公平性与实用性的模型。

Abstract: The Rashomon set captures the collection of models that achieve near-identical empirical performance yet may differ substantially in their decision boundaries. Understanding the differences among these models, i.e., their multiplicity, is recognized as a crucial step toward model transparency, fairness, and robustness, as it reveals decision boundaries instabilities that standard metrics obscure. However, the existing definitions of Rashomon set and multiplicity metrics assume centralized learning and do not extend naturally to decentralized, multi-party settings like Federated Learning (FL). In FL, multiple clients collaboratively train models under a central server's coordination without sharing raw data, which preserves privacy but introduces challenges from heterogeneous client data distribution and communication constraints. In this setting, the choice of a single best model may homogenize predictive behavior across diverse clients, amplify biases, or undermine fairness guarantees. In this work, we provide the first formalization of Rashomon sets in FL.First, we adapt the Rashomon set definition to FL, distinguishing among three perspectives: (I) a global Rashomon set defined over aggregated statistics across all clients, (II) a t-agreement Rashomon set representing the intersection of local Rashomon sets across a fraction t of clients, and (III) individual Rashomon sets specific to each client's local distribution.Second, we show how standard multiplicity metrics can be estimated under FL's privacy constraints. Finally, we introduce a multiplicity-aware FL pipeline and conduct an empirical study on standard FL benchmark datasets. Our results demonstrate that all three proposed federated Rashomon set definitions offer valuable insights, enabling clients to deploy models that better align with their local data, fairness considerations, and practical requirements.

</details>


### [91] [Learning to Discover Iterative Spectral Algorithms](https://arxiv.org/abs/2602.09530)
*Zihang Liu,Oleg Balabanov,Yaoqing Yang,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce AutoSpec, a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task. The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set. We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines. We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials.

</details>


### [92] [Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.09578)
*Zhida Jiang,Zhaolong Xing,Jiawei Lu,Yipei Niu,Qingyuan Sang,Liangxu Zhang,Wenquan Dai,Junhua Shu,Jiaxing Wang,Qiangyu Pei,Qiong Chen,Xinyu Liu,Fangming Liu,Ai Han,Zhen Chen,Ke Zhang*

Main category: cs.LG

TL;DR: FlexMARL通过微批次异步流水线、并行采样+层次负载均衡、按需硬件分配，实现大规模MARL训练7.3×加速和5.6×硬件利用提升。


<details>
  <summary>Details</summary>
Motivation: 现有框架缺乏针对大规模MARL的系统级优化，导致同步阻塞、负载不均、资源闲置等瓶颈。

Method: 提出FlexMARL：包含联合协调器、基于经验库的微批次异步流水线、并行采样与层次负载均衡的Rollout引擎、按需硬件绑定的训练引擎；通过统一、位置无关的通信实现状态交换。

Result: 在大型生产集群实验中，与传统框架相比，FlexMARL实现了最高7.3倍加速，硬件利用率提高至5.6倍。

Conclusion: FlexMARL实现了全流程的端到端优化，有效提升大规模LLM基MARL训练效率及资源利用率。

Abstract: Despite algorithm-level innovations for multi-agent reinforcement learning (MARL), the underlying networked infrastructure for large-scale MARL training remains underexplored. Existing training frameworks primarily optimize for single-agent scenarios and fail to address the unique system-level challenges of MARL, including rollout-training synchronization barriers, rollout load imbalance, and training resource underutilization. To bridge this gap, we propose FlexMARL, the first end-to-end training framework that holistically optimizes rollout, training, and their orchestration for large-scale LLM-based MARL. Specifically, FlexMARL introduces the joint orchestrator to manage data flow under the rollout-training disaggregated architecture. Building upon the experience store, a novel micro-batch driven asynchronous pipeline eliminates the synchronization barriers while providing strong consistency guarantees. Rollout engine adopts a parallel sampling scheme combined with hierarchical load balancing, which adapts to skewed inter/intra-agent request patterns. Training engine achieves on-demand hardware binding through agent-centric resource allocation. The training states of different agents are swapped via unified and location-agnostic communication. Empirical results on a large-scale production cluster demonstrate that FlexMARL achieves up to 7.3x speedup and improves hardware utilization by up to 5.6x compared to existing frameworks.

</details>


### [93] [Mitigating the Likelihood Paradox in Flow-based OOD Detection via Entropy Manipulation](https://arxiv.org/abs/2602.09581)
*Donghwan Kim,Hyunsoo Yoon*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep generative models that can tractably compute input likelihoods, including normalizing flows, often assign unexpectedly high likelihoods to out-of-distribution (OOD) inputs. We mitigate this likelihood paradox by manipulating input entropy based on semantic similarity, applying stronger perturbations to inputs that are less similar to an in-distribution memory bank. We provide a theoretical analysis showing that entropy control increases the expected log-likelihood gap between in-distribution and OOD samples in favor of the in-distribution, and we explain why the procedure works without any additional training of the density model. We then evaluate our method against likelihood-based OOD detectors on standard benchmarks and find consistent AUROC improvements over baselines, supporting our explanation.

</details>


### [94] [Why the Counterintuitive Phenomenon of Likelihood Rarely Appears in Tabular Anomaly Detection with Deep Generative Models?](https://arxiv.org/abs/2602.09593)
*Donghwan Kim,Junghun Phee,Hyunsoo Yoon*

Main category: cs.LG

TL;DR: 作者验证表格域异常检测中，归一化流的似然度评分更可靠；在多数据集实验中，反直觉高似然异常现象罕见，归一化流优于其他13种基线模型；


<details>
  <summary>Details</summary>
Motivation: 探究为何深度生成模型在图像域常出现高似然率异常，调查该现象在表格数据中的普遍性，为表格异常检测提供可靠方法；

Method: 提出无域特定的度量框架，利用ABLBench在47个表格和10个CV/NLP嵌入数据集上，结合13个基线模型评估反直觉现象并进行理论与经验分析，重点关注维度与特征相关性；

Result: 实验表明，该反直觉行为在表格数据中明显稀少，同时维度和特征相关性影响该现象，而归一化流在表格域异常检测表现优良；

Conclusion: 在表格数据中，基于归一化流的似然度检测模型相较图像域更少出现高似然率异常的反直觉现象，且代以高可靠的异常检测方式；

Abstract: Deep generative models with tractable and analytically computable likelihoods, exemplified by normalizing flows, offer an effective basis for anomaly detection through likelihood-based scoring. We demonstrate that, unlike in the image domain where deep generative models frequently assign higher likelihoods to anomalous data, such counterintuitive behavior occurs far less often in tabular settings. We first introduce a domain-agnostic formulation that enables consistent detection and evaluation of the counterintuitive phenomenon, addressing the absence of precise definition. Through extensive experiments on 47 tabular datasets and 10 CV/NLP embedding datasets in ADBench, benchmarked against 13 baseline models, we demonstrate that the phenomenon, as defined, is consistently rare in general tabular data. We further investigate this phenomenon from both theoretical and empirical perspectives, focusing on the roles of data dimensionality and difference in feature correlation. Our results suggest that likelihood-only detection with normalizing flows offers a practical and reliable approach for anomaly detection in tabular domains.

</details>


### [95] [Blind denoising diffusion models and the blessings of dimensionality](https://arxiv.org/abs/2602.09639)
*Zahra Kadkhodaie,Aram-Alexandre Pooladian,Sinho Chewi,Eero Simoncelli*

Main category: cs.LG

TL;DR: 盲去噪扩散模型无需显式噪声调度，能自适应并提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 探究在训练和采样过程中不提供噪声幅度的盲去噪扩散模型表现，旨在理解为何无调度模型能获得更高样本质量。

Method: 理论推导结合经验验证：对内在维度低的数据分布进行分析，证明盲去噪模型能自适应噪声调度，并通过合成数据和图像实验验证理论。

Result: 盲去噪模型能在多项式步数内准确复制数据分布，实验显示其能从噪声图像准确估计噪声方差，且生成样本质量高于传统模型。

Conclusion: 我们证明了盲去噪扩散模型能够自动跟踪隐式噪声调度，并在与内在维数相关的多项式步数内准确采样，同时实验发现无调度的盲去噪模型产生的样本质量优于有调度模型。

Abstract: We analyze, theoretically and empirically, the performance of generative diffusion models based on \emph{blind denoisers}, in which the denoiser is not given the noise amplitude in either the training or sampling processes. Assuming that the data distribution has low intrinsic dimensionality, we prove that blind denoising diffusion models (BDDMs), despite not having access to the noise amplitude, \emph{automatically} track a particular \emph{implicit} noise schedule along the reverse process. Our analysis shows that BDDMs can accurately sample from the data distribution in polynomially many steps as a function of the intrinsic dimension. Empirical results corroborate these mathematical findings on both synthetic and image data, demonstrating that the noise variance is accurately estimated from the noisy image. Remarkably, we observe that schedule-free BDDMs produce samples of higher quality compared to their non-blind counterparts. We provide evidence that this performance gain arises because BDDMs correct the mismatch between the true residual noise (of the image) and the noise assumed by the schedule used in non-blind diffusion models.

</details>


### [96] [Model soups need only one ingredient](https://arxiv.org/abs/2602.09689)
*Alireza Abdollahpoorrostam,Nikolaos Dimitriadis,Adam Hazimeh,Pascal Frossard*

Main category: cs.LG

TL;DR: MonoSoup 基于 SVD 的单点加权方案，利用有效秩自动调节高低能量方向，实现高效、鲁棒的 ID–OOD 兼顾。


<details>
  <summary>Details</summary>
Motivation: 传统微调提升 ID 准确率，却削弱 OOD 鲁棒；权重空间集成能缓解但计算成本高昂，需多模型训练与存储。

Method: 对每层梯度更新做奇异值分解，分离高能量（任务特定适配）和低能量（噪声/残余信号）方向；利用熵基有效秩自动给出层级权重，对不同方向按谱结构和几何特征重新加权。

Result: 在 CLIP 微调 ImageNet 并评估自然分布偏移，以及 Qwen 语言模型在数学推理和多项选择基准上的实验表明，MonoSoup 能与多检查点方法媲美，却显著降低计算开销。

Conclusion: MonoSoup通过单一检查点即可实现强大的 ID–OOD 平衡，兼具高准确率与鲁棒性，且不需要多模型训练和存储。

Abstract: Fine-tuning large pre-trained models on a target distribution often improves in-distribution (ID) accuracy, but at the cost of out-of-distribution (OOD) robustness as representations specialize to the fine-tuning data. Weight-space ensembling methods, such as Model Soups, mitigate this effect by averaging multiple checkpoints, but they are computationally prohibitive, requiring the training and storage of dozens of fine-tuned models. In this paper, we introduce MonoSoup, a simple, data-free, hyperparameter-free, post-hoc method that achieves a strong ID-OOD balance using only a single checkpoint. Our method applies Singular Value Decomposition (SVD) to each layer's update and decomposes it into high-energy directions that capture task-specific adaptation and low-energy directions that introduce noise but may still encode residual signals useful for robustness. MonoSoup then uses entropy-based effective rank to automatically re-weigh these components with layer-wise coefficients that account for the spectral and geometric structure of the model. Experiments on CLIP models fine-tuned on ImageNet and evaluated under natural distribution shifts, as well as on Qwen language models tested on mathematical reasoning and multiple-choice benchmarks, show that this plug-and-play approach is a practical and effective alternative to multi-checkpoint methods, retaining much of their benefits without their computational overhead.

</details>


### [97] [Physics-informed diffusion models in spectral space](https://arxiv.org/abs/2602.09708)
*Davide Gallon,Philippe von Wurstemberger,Patrick Cheridito,Arnulf Jentzen*

Main category: cs.LG

TL;DR: 结合潜在扩散与物理约束的新方法，在频谱空间高效生成 PDE 解，性能领先传统扩散求解器。


<details>
  <summary>Details</summary>
Motivation: 解决参数化偏微分方程在有限观测条件下的前向与逆问题，克服传统基于网格的扩散模型所面临的高维度与物理不一致的问题。

Method: 利用生成潜在扩散模型在二维缩放谱表示的潜在空间学习 PDE 参数与解的联合分布，随后在后验采样中通过基于 Adam 的更新强化物理约束和测量条件。

Result: 在 Poisson、Helmholtz 与不可压 Navier–Stokes 方程实验中，所提方法在准确性和计算效率上均优于现有的基于扩散的 PDE 求解器，尤其是在稀疏观测情形下。

Conclusion: 该方法通过在频谱投影空间中进行扩散，显著降低维度并保持物理一致性，使生成的PDE解在条件观测下更加准确与高效；在 Poisson、Helmholtz 与 Navier–Stokes 方程上取得了相较现有扩散式求解器更优的性能。

Abstract: We propose a methodology that combines generative latent diffusion models with physics-informed machine learning to generate solutions of parametric partial differential equations (PDEs) conditioned on partial observations, which includes, in particular, forward and inverse PDE problems. We learn the joint distribution of PDE parameters and solutions via a diffusion process in a latent space of scaled spectral representations, where Gaussian noise corresponds to functions with controlled regularity. This spectral formulation enables significant dimensionality reduction compared to grid-based diffusion models and ensures that the induced process in function space remains within a class of functions for which the PDE operators are well defined. Building on diffusion posterior sampling, we enforce physics-informed constraints and measurement conditions during inference, applying Adam-based updates at each diffusion step. We evaluate the proposed approach on Poisson, Helmholtz, and incompressible Navier--Stokes equations, demonstrating improved accuracy and computational efficiency compared with existing diffusion-based PDE solvers, which are state of the art for sparse observations. Code is available at https://github.com/deeplearningmethods/PISD.

</details>


### [98] [ExO-PPO: an Extended Off-policy Proximal Policy Optimization Algorithm](https://arxiv.org/abs/2602.09726)
*Hanyong Wang,Menglong Yang*

Main category: cs.LG

TL;DR: ExO-PPO融合PPO的保守更新与离线重放，提升样本效率，实验结果显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 PPO 在保持稳定更新的同时样本效率有限；离线方法虽提高样本利用率但导致方差和偏差增大。论文旨在同时兼顾稳定性与效率。

Method: 通过从泛化策略改进下界的期望形式推导出离线改进策略；采用分段指数函数扩展裁剪机制构建鲁棒的 surrogate；将过去M个策略生成的轨迹存入 replay buffer 进行离线训练，形成新的 ExO-PPO 训练流程。

Result: 在实验中，ExO-PPO 在样本效率与稳定性能均取得了显著提升，超过了 PPO 及其他状态-of-the-art 变体。

Conclusion: 该论文提出了ExO-PPO算法，该算法兼顾了PPO的稳定性和离线学习的样本效率，实验验证其在多种任务上均优于标准PPO及其他变体。

Abstract: Deep reinforcement learning has been able to solve various tasks successfully, however, due to the construction of policy gradient and training dynamics, tuning deep reinforcement learning models remains challenging. As one of the most successful deep reinforcement-learning algorithm, the Proximal Policy Optimization algorithm (PPO) clips the policy gradient within a conservative on-policy updates, which ensures reliable and stable policy improvement. However, this training pattern may sacrifice sample efficiency. On the other hand, off-policy methods make more adequate use of data through sample reuse, though at the cost of increased the estimation variance and bias. To leverage the advantages of both, in this paper, we propose a new PPO variant based on the stability guarantee from conservative on-policy iteration with a more efficient off-policy data utilization. Specifically, we first derive an extended off-policy improvement from an expectation form of generalized policy improvement lower bound. Then, we extend the clipping mechanism with segmented exponential functions for a suitable surrogate objective function. Third, the trajectories generated by the past $M$ policies are organized in the replay buffer for off-policy training. We refer to this method as Extended Off-policy Proximal Policy Optimization (ExO-PPO). Compared with PPO and some other state-of-the-art variants, we demonstrate an improved performance of ExO-PPO with balanced sample efficiency and stability on varied tasks in the empirical experiments.

</details>


### [99] [Explainability in Generative Medical Diffusion Models: A Faithfulness-Based Analysis on MRI Synthesis](https://arxiv.org/abs/2602.09781)
*Surjo Dey,Pallabi Saikia*

Main category: cs.LG

TL;DR: 通过归一化原型方法和忠实度分析，扩散模型的医学图像生成实现可解释和透明化，提升安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在医疗影像生成表现优异，但内部决策过程不透明，需要可解释方案提升可信度。

Method: 采用ProtoPNet、Enhanced ProtoPNet和ProtoPool等基于原型的可解释方法，结合扩散模型的去噪轨迹进行忠实度分析。

Result: Enhanced ProtoPNet 在忠实度指标上最高（0.1534），提供更可靠的生成过程解释。

Conclusion: 该研究表明，通过基于忠实度的可解释框架，可使扩散模型的生成过程更透明可靠，为医疗影像生成提供可信解释。

Abstract: This study investigates the explainability of generative diffusion models in the context of medical imaging, focusing on Magnetic resonance imaging (MRI) synthesis. Although diffusion models have shown strong performance in generating realistic medical images, their internal decision making process remains largely opaque. We present a faithfulness-based explainability framework that analyzes how prototype-based explainability methods like ProtoPNet (PPNet), Enhanced ProtoPNet (EPPNet), and ProtoPool can link the relationship between generated and training features. Our study focuses on understanding the reasoning behind image formation through denoising trajectory of diffusion model and subsequently prototype explainability with faithfulness analysis. Experimental analysis shows that EPPNet achieves the highest faithfulness (with score 0.1534), offering more reliable insights, and explainability into the generative process. The results highlight that diffusion models can be made more transparent and trustworthy through faithfulness-based explanations, contributing to safer and more interpretable applications of generative AI in healthcare.

</details>


### [100] [Flexible Entropy Control in RLVR with Gradient-Preserving Perspective](https://arxiv.org/abs/2602.09782)
*Kun Chen,Peng Shi,Fanfan Liu,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao*

Main category: cs.LG

TL;DR: 通过动态梯度裁剪阈值实现精确熵调控，解决RL中熵崩溃问题，在多项任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统RL在持续训练时容易出现熵崩溃，导致过早收敛、输出多样性降低；现有裁剪策略静态且缺乏精确熵控制的框架。

Method: 理论+实证分析重要采样比对熵的影响，基于此设计动态裁剪阈值调节机制，并实现多种熵调节策略（升-降、降-升-降、振荡衰减）。

Result: 实验表明动态阈值策略显著减缓熵崩溃，提升多种评测基准的性能。

Conclusion: 本文提出通过动态梯度保留裁剪阈值来实现熵控制，有效缓解了RL中熵崩溃问题，并在多项基准上获得更佳表现。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.

</details>


### [101] [Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints](https://arxiv.org/abs/2602.09783)
*Andres Saurez,Yousung Lee,Dongsoo Har*

Main category: cs.LG

TL;DR: 论文证明Transformer的线性接口导致语义特征位于上下文不变的线性子空间，从而解释了线性探测器和稀疏自编码器的有效性，并通过理论与实验相结合提供了零样本的语义结构识别方法。


<details>
  <summary>Details</summary>
Motivation: 解释为什么简单线性方法在深度非线性Transformer中能有效工作，并统一线性探测器与稀疏自编码器的应用。

Method: 提出“Invariant Subspace Necessity”定理，推导“Self-Reference Property”，在不需要标记数据或学习探测器的情况下，通过零射测识别语义结构，并在八种分类任务与四种模型族上进行验证。

Result: 实验验证了类别标记与语义相关实例的对齐，支持理论假设。

Conclusion: 线性探测器和稀疏自编码器之所以能从Transformer表示中提取有意义结构，是因为Transformer的架构要求信息传递必须通过线性接口，任何语义特征必须位于上下文不变的线性子空间中。

Abstract: Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace. We formalize this as the \emph{Invariant Subspace Necessity} theorem and derive the \emph{Self-Reference Property}: tokens directly provide the geometric direction for their associated features, enabling zero-shot identification of semantic structure without labeled data or learned probes. Empirical validation in eight classification tasks and four model families confirms the alignment between class tokens and semantically related instances. Our framework provides \textbf{a principled architectural explanation} for why linear interpretability methods work, unifying linear probes and sparse autoencoders.

</details>


### [102] [Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path](https://arxiv.org/abs/2602.09784)
*Andres Saurez,Neha Sengar,Dongsoo Har*

Main category: cs.LG

TL;DR: 作者发现答案 token 的方向编码可用作电路识别与控制的几何指引，构建无梯度发现方法，并验证在多模型、多任务上具有可比性能，进一步证明 transformer 电路是几何结构。


<details>
  <summary>Details</summary>
Motivation: 尽管电路发现与激活方向控制是不同的研究线索，但它们共享同一表征空间，研究者想知道它们是否属于同一基本结构。

Method: 提出 Circuit Fingerprint 假设：依赖于答案标记的方向表征该标记的产生方向；通过几何对齐而非梯度或因果干预来识别电路；随后利用相同方向进行受控引导。

Result: 在 IOI、SVA、MCQA 等标准基准上，对四类模型验证，电路发现效果与梯度方法相当；同一方向用于控制提升情感分类精度至 69.8%（对比 53.1% 的指令提示），且保持事实准确性。

Conclusion: 本文表明 transformer 里的电路本质上是几何结构，解释性与可控性只是同一事物的两面；电路探测和激活方向控制可以通过同一几何原则实现。

Abstract: Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods. The same directions that identify circuit components also enable controlled steering -- achieving 69.8\% emotion classification accuracy versus 53.1\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.

</details>


### [103] [When Less is More: The LLM Scaling Paradox in Context Compression](https://arxiv.org/abs/2602.09789)
*Ruishan Guo,Yibing Liu,Guoxin Ma,Yan Wang,Yueyang Zhang,Long Xia,Kecheng Chen,Zhiyuan Sun,Daiting Shi*

Main category: cs.LG

TL;DR: 更大的语言模型在压缩-解码设置下可能更少忠实重构上下文，主要由知识覆盖和语义漂移造成，说明模型规模扩张时生成不确定性提升。


<details>
  <summary>Details</summary>
Motivation: 探究在压缩-解码框架下，模型规模增大如何影响上下文重构的忠实度，验证“大小-忠实度悖论”。

Method: 对参数规模从0.6B到90B的多个模型进行大规模实验，使用压缩器-解码器架构评估重构上下文的准确性，分析知识覆盖和语义漂移的影响。

Result: 发现随着压缩器规模增大，训练损失下降但重构上下文的忠实度降低；主要原因是知识覆盖（模型替换原始事实）和语义漂移（模型改写内容）。更高的张量嵌入秩和更大的预测熵导致先验知识侵入和重新表述。

Conclusion: 问题不在于参数计数本身，而是规模增加后语义容量和生成不确定性的放大。此现象揭示了在开放式生成中关于忠实保存的缩放定律存在分解。

Abstract: Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.

</details>


### [104] [CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization](https://arxiv.org/abs/2602.09851)
*Beicheng Xu,Keyao Ding,Wei Liu,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: CoFEH利用LLM+贝叶斯HPO联合优化，解决特征工程和超参数搜索的耦合问题，实验显示性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 传统的自动机器学习（AutoML）中，特征工程（FE）被视作黑盒搜索，受限于固定搜索空间，缺乏领域意识；现有使用大型语言模型（LLM）的方案只能生成单一子任务特征，未能构建自由形式的 FE 流程，也未能与模型的超参数优化（HPO）协同，导致“FE-后HPO”流程缺失 FE-HPO 交互。

Method: 提出 CoFEH 框架，结合 LLM-driven FE 与贝叶斯 HPO，采用 Tree of Thought (ToT) 生成灵活的 FE 管道；通过动态优化器选择器实现 FE 与 HPO 的交替优化；引入互相条件机制，共享 LLM 与 BO 的上下文，增强两者决策的互补性。

Result: 实验表明 CoFEH 在传统与 LLM 基准上均超越表现，并在联合优化下获得更优的端到端性能。

Conclusion: CoFEH 通过协同 LLM 生成 FE 和贝叶斯 HPO，解决了传统 FE 的瓶颈和 FE-HPO 交互不足的问题，显著提升 AutoML 整体效能。

Abstract: Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy "FE-then-HPO" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.

</details>


### [105] [Statistical benchmarking of transformer models in low signal-to-noise time-series forecasting](https://arxiv.org/abs/2602.09869)
*Cyril Garcia,Guillaume Remy*

Main category: cs.LG

TL;DR: 研究发现，两路注意力transformer加动态稀疏化在低数据量、多变量时间序列预测场景中表现优于常规方法，学习到的注意力模式给出可解释性，并与经典正则化关联。


<details>
  <summary>Details</summary>
Motivation: 在仅有数年的每日观测记录的低数据量环境下，需要高效的多变量时间序列预测方法，传统方法受限；探究transformer及其稀疏化技术的有效性。

Method: 利用合成时间序列（已知时序和交叉依赖结构）进行自助采样实验，通过对比真实最佳预测器的外部样本相关性来评估模型；比较两路注意力transformer、Lasso、提升树、全连接多层感知器，并在训练期间对注意力矩阵实施动态稀疏化。

Result: 两路注意力transformer在低信噪比场景下相较于传统基线表现更优；动态稀疏化在噪声环境下显著提升相关性，可将目标与最佳预测器的相关性提升数个百分点。

Conclusion: 两路注意力transformer在低数据量、多变量时间序列预测任务中能在多种设置下超越传统基线，并且其动态稀疏化能在噪声严重的环境中进一步提升性能，学习到的注意力模式可解释并与经典稀疏正则化相联系。

Abstract: We study the performance of transformer architectures for multivariate time-series forecasting in low-data regimes consisting of only a few years of daily observations. Using synthetically generated processes with known temporal and cross-sectional dependency structures and varying signal-to-noise ratios, we conduct bootstrapped experiments that enable direct evaluation via out-of-sample correlations with the optimal ground-truth predictor. We show that two-way attention transformers, which alternate between temporal and cross-sectional self-attention, can outperform standard baselines-Lasso, boosting methods, and fully connected multilayer perceptrons-across a wide range of settings, including low signal-to-noise regimes. We further introduce a dynamic sparsification procedure for attention matrices applied during training, and demonstrate that it becomes significantly effective in noisy environments, where the correlation between the target variable and the optimal predictor is on the order of a few percent. Analysis of the learned attention patterns reveals interpretable structure and suggests connections to sparsity-inducing regularization in classical regression, providing insight into why these models generalize effectively under noise.

</details>


### [106] [Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education](https://arxiv.org/abs/2602.09904)
*Anna Bodonhelyi,Mengdi Wang,Efe Bozkir,Babette Bühler,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 开发一种基于视频、在设备侧训练、云端不共享数据的联邦学习模型，能实时识别在线学习者的注意力缺失，为教育提供隐私保護的实时支持。


<details>
  <summary>Details</summary>
Motivation: 为实现实时学习者支持，自动检测在线学习中的注意力缺失和心游，而传统机器学习需要分享敏感数据，导致隐私问题。

Method: 跨设备联邦学习框架，利用面部表达和注视特征的基于视频的认知脱离检测模型，实验在五个数据集上，并对多种联邦学习算法进行基准测试。

Result: 实验表明该方法在保护隐私的同时，实现了对行为脱离、心游和无聊的有效检测，加入眼镜特征后模型性能有所提升。

Conclusion: 跨设备联邦学习可以在保持隐私的前提下，实时检测远程学习者的行为与认知脱离，具有良好的教育技术应用前景。

Abstract: Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.

</details>


### [107] [Causal Identification in Multi-Task Demand Learning with Confounding](https://arxiv.org/abs/2602.09969)
*Varun Gupta,Vijay Kamble*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study a canonical multi-task demand learning problem motivated by retail pricing, in which a firm seeks to estimate heterogeneous linear price-response functions across a large collection of decision contexts. Each context is characterized by rich observable covariates yet typically exhibits only limited historical price variation, motivating the use of multi-task learning to borrow strength across tasks. A central challenge in this setting is endogeneity: historical prices are chosen by managers or algorithms and may be arbitrarily correlated with unobserved, task-level demand determinants. Under such confounding by latent fundamentals, commonly used approaches, such as pooled regression and meta-learning, fail to identify causal price effects.
  We propose a new estimation framework that achieves causal identification despite arbitrary dependence between prices and latent task structure. Our approach, Decision-Conditioned Masked-Outcome Meta-Learning (DCMOML), involves carefully designing the information set of a meta-learner to leverage cross-task heterogeneity while accounting for endogenous decision histories. Under a mild restriction on price adaptivity in each task, we establish that this method identifies the conditional mean of the task-specific causal parameters given the designed information set. Our results provide guarantees for large-scale demand estimation with endogenous prices and small per-task samples, offering a principled foundation for deploying causal, data-driven pricing models in operational environments.

</details>


### [108] [Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings](https://arxiv.org/abs/2602.09985)
*Alexander Fertig,Karthikeyan Chandra Sekaran,Lakshman Balasubramanian,Michael Botsch*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.

</details>


### [109] [Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions](https://arxiv.org/abs/2602.09987)
*J Rosser,Robert Kirk,Edward Grefenstette,Jakob Foerster,Laura Ruis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models. In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike. We provide the code here: https://github.com/jrosseruk/infusion.

</details>


### [110] [Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning](https://arxiv.org/abs/2602.10006)
*Shijie Zhang,Xiang Guo,Rujun Guo,Shaoyu Liu,Xiaozhao Wang,Guanjun Jiang,Kevin Zhang*

Main category: cs.LG

TL;DR: 提出AFRL范式，结合SFT+RL与模式平衡训练，克服RL模式崩塌，构建既低延迟又高性能的检索相关性模型，并通过知识蒸馏成功压缩至0.6B模型。


<details>
  <summary>Details</summary>
Motivation: 在搜索行业中，既需要毫秒级响应，又需保持大型语言模型可解释的推理轨迹，传统方法存在低延迟与高性能、可解释性之间的矛盾。

Method: 设计AFRL框架，首词输出确切相关性分数，随后提供结构化解释；采用SFT+RL管道，加入模式平衡损失（SFT辅助损失）以对抗RL模式崩塌；使用Stepwise‑GRPO训练、多阶段课程以及自动指令演化系统提升数据质量；最终通过知识蒸馏实现从教师到学生模型的迁移。

Result: 32B教师模型在检索相关性任务中实现了SOTA表现；0.6B学生模型在保持推理深度和可解释性的同时，达到了可部署的低延迟。

Conclusion: 通过Answer‑First, Reason‑Later(AFRL)范式和模式平衡优化，本文构建了一种既能实现毫秒级低延迟又保持LLM可解释推理轨迹的检索相关性模型，并成功将32B教师模型的专家级推理逻辑迁移到0.6B小模型，兼顾性能与部署效率。

Abstract: Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a "Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to "reward hacking." On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.

</details>


### [111] [A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula](https://arxiv.org/abs/2602.10014)
*Chenruo Liu,Yijun Dong,Yiqiu Shen,Qi Lei*

Main category: cs.LG

TL;DR: 本研究为自我提升的迭代过程提供了有限样本理论保证，揭示正反馈机制并验证递增难度课程优越性。


<details>
  <summary>Details</summary>
Motivation: 为了解释自主迭代提升（self‑improvement）在有限样本下的理论基础。

Method: 将每轮自我提升模型化为在奖励过滤后的分布上做最大似然微调，并推导期望奖励的有限样本上界。随后在多难度推理任务上，通过对初始化、任务难度与样本预算的量化条件，比较优先难度递增课程与固定混合任务的保障。

Result: 得到期望奖励的上界，揭示更好模型收纳更多数据的正反馈循环，并说明为何自我提升最终趋于饱和。实验与Monte‑Carlo模拟确认理论结论在基于图的推理任务上的有效性。

Conclusion: 在有限样本量下，迭代自我提升可在理论上得到保证，其效果受模型初始化、任务难度与样本预算的影响；递增难度的课程在有条件下优于固定任务混合训练。

Abstract: Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.

</details>


### [112] [ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning](https://arxiv.org/abs/2602.10019)
*Qingnan Ren,Shiting Huang,Zhen Fang,Zehui Chen,Lin Chen,Lijun Li,Feng Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.

</details>


### [113] [Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization](https://arxiv.org/abs/2602.10048)
*Xinchen Han,Hossam Afifi,Michel Marot,Xilu Wang,Lu Yin*

Main category: cs.LG

TL;DR: LLMs的冗长 Chain-of-Thought 给系统带来额外开销；FGO 通过 RL 权重分配进行细粒度压缩，实验验证其效果优于 GRPO。


<details>
  <summary>Details</summary>
Motivation: LLMs在Chain-of-Thought (CoT) 生成中往往产生冗长的推理步骤，导致计算成本和延迟增加，但性能提升不成比例。

Method: 本研究提出Fine-grained Group policy Optimization (FGO)，利用强化学习对群组响应进行细分，并根据长度与熵值分配权重，实现有效的CoT压缩；同时改进GRPO，解决了数据利用低效和熵坍塌两大限制。

Result: 在多个推理LLM和基准（MATH500、AIME24、AMC23、Minerva）上实验表明，FGO可在不降低性能的前提下实现CoT压缩，并成功解决了GRPO的主要不足。

Conclusion: FGO提供了一种高效、无性能损失的CoT压缩方案，并通过改进GRPO克服了其原有限制。

Abstract: Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \textbf{F}ine-grained \textbf{G}roup policy \textbf{O}ptimization (\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.

</details>


### [114] [Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability](https://arxiv.org/abs/2602.10067)
*Aaditya Vikram Prasad,Connor Watts,Jack Merullo,Dhruvil Gala,Owen Lewis,Thomas McGrath,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 通过将模型的内部特征当作奖励，RLFR训练出可在测试时实时纠正幻觉的政策，显著减少幻觉且不影响整体表现。


<details>
  <summary>Details</summary>
Motivation: 传统上语言模型特征用作监控或引导，难以在开放式任务中实现可扩展监督。论文的出发点是把这些特征转化为可直接优化的奖励，降低生成文本的幻觉。

Method: 构建RLFR（Reinforcement Learning from Feature Rewards）流程：先用新的探测框架识别潜在幻觉声明；接着训练RL策略以对不确定的生成结果进行干预并纠正；最后在推理时利用同类特征奖励进行计算调节。

Result: 在Gemma-3-12B-IT模型上，RLFR策略相较原模型幻觉生成率下降58%，同时在标准基准测试上保持性能。

Conclusion: 本文提出利用语言模型内部特征作为奖励函数，通过强化学习实现对幻觉生成的可控干预，并在Gemma-3-12B-IT上取得58%幻觉率下降、保持传统评测性能的成果。

Abstract: Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

</details>


### [115] [Step-resolved data attribution for looped transformers](https://arxiv.org/abs/2602.10097)
*Georgios Kaissis,David Mildenberger,Juan Felipe Gomez,Martin J. Menten,Eleni Triantafillou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.

</details>


### [116] [Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders](https://arxiv.org/abs/2602.10099)
*Amandeep Kumar,Vishal M. Patel*

Main category: cs.LG

TL;DR: RJF 在保持标准 DiT-B 规模不变的情况下，通过 Riemannian 流匹配改进生成性能，解决了传统扩散变换器在表征空间上的收敛问题。


<details>
  <summary>Details</summary>
Motivation: 利用表征编码器进行生成建模，动力在于实现高效高保真合成，但传统扩散变换器在直接应用这些表示时无法收敛。

Method: 提出Riemannian Flow Matching with Jacobi Regularization（RJF），通过将生成过程限制在特征空间的圆球面流形 geodesics 上，并修正曲率导致的误差传播，实现对流形几何的兼容。

Result: 在标准 DiT-B 架构（1.31 亿参数）下，RJF 能够实现收敛，FID 下降到 3.37，显著优于此前无法收敛的方法。

Conclusion: RJF 通过纠正几何干扰，实现了对表征编码器特性空间的有效生成，突破了宽度扩展难题。

Abstract: Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF

</details>


### [117] [Biases in the Blind Spot: Detecting What LLMs Fail to Mention](https://arxiv.org/abs/2602.10117)
*Iván Arcuschin,David Chanin,Adrià Garriga-Alonso,Oana-Maria Camburu*

Main category: cs.LG

TL;DR: 构建黑箱自动检测LLM未显式偏见的流程，验证并发现多种偏见。


<details>
  <summary>Details</summary>
Motivation: CoT常掩盖内部偏见，传统评估需人工预设类别与数据。需要可自动、任务特定且无人工干预的检测方法。

Method: 利用LLM自评者生成候选偏见概念，逐步扩大输入样本、生成正负变体，采用多重检验与早停统计技术判定偏见。

Result: 在六款LLM与三项决策任务上，自动发现诸如西班牙语流利度、英语水平、写作正式度等未知偏见，并验证已有性别、种族、宗教等偏见。

Conclusion: 该研究提出了一个全自动黑箱流程，能够探测大型语言模型中对特定任务的未显式偏见，并在多模型、多任务中发现新旧已知偏见。

Abstract: Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [118] [E2CAR: An Efficient 2D-CNN Framework for Real-Time EEG Artifact Removal on Edge Devices](https://arxiv.org/abs/2602.09035)
*Haoliang Liu,Chengkun Cai,Xu Zhao,Lei Li*

Main category: eess.SP

TL;DR: E2CAR—2‑D CNNs on an Edge TPU—cuts EEG artifact‑removal latency by 90 % and power use by ~19 % with no noticeable loss in accuracy, enabling efficient real‑time processing on edge devices.


<details>
  <summary>Details</summary>
Motivation: EEG signals are routinely corrupted by artifacts, and existing artifact‑removal CNNs are too compute‑heavy for real‑time, low‑power edge deployments, limiting practical applications in portable or embedded settings.

Method: The approach substitutes conventional 1‑D convolutional layers with 2‑D convolutions, allowing the model to exploit the parallelism of the Edge TPU accelerator; the E2CAR framework orchestrates this architecture and runs inference directly on the device.

Result: On the Edge TPU, the E2CAR model achieves a 90 % reduction in inference time and an 18.98 % drop in power consumption, while its artifact‑removal performance remains comparable to state‑of‑the‑art methods.

Conclusion: By replacing 1‑D CNNs with 2‑D CNNs and deploying the resulting E2CAR model on an Edge TPU, the authors achieve a drastic drop in latency and power consumption while sustaining artifact‑removal accuracy, demonstrating a viable path for real‑time EEG analysis on edge hardware.

Abstract: Electroencephalography (EEG) signals are frequently contaminated by artifacts, affecting the accuracy of subsequent analysis. Traditional artifact removal methods are often computationally expensive and inefficient for real-time applications in edge devices. This paper presents a method to reduce the computational cost of most existing convolutional neural networks (CNN) by replacing one-dimensional (1-D) CNNs with two-dimensional (2-D) CNNs and deploys them on Edge Tensor Processing Unit (TPU), which is an open-resource hardware accelerator widely used in edge devices for low-latency, low-power operation. A new Efficient 2D-CNN Artifact Removal (E2CAR) framework is also represented using the method above, and it achieves a 90\% reduction in inference time on the TPU and decreases power consumption by 18.98\%, while maintaining comparable artifact removal performance to existing methods. This approach facilitates efficient EEG signal processing on edge devices.

</details>


### [119] [WiLoc: Massive Measured Dataset of Wi-Fi Channel State Information with Application to Machine-Learning Based Localization](https://arxiv.org/abs/2602.09115)
*Yuning Zhang,Lei Chu,Omer Gokalp Serbetci,Jorge Gomez-Ponce,Andreas F. Molisch*

Main category: eess.SP

TL;DR: WiLoc是全球最大规模的Wi-Fi CSI定位数据集（>12M UE位置、>3K AP），可公开获取，显著提升ML定位算法的精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习定位算法依赖大量、覆盖广泛的数据才能实现高精度和鲁棒性，但现有数据集规模有限，限制了算法发展；构建巨大、多样的数据集满足该需求。

Method: 收集>12百万用户设备（UE）位置、>3,000 个接入点（AP）位置的数据，覆盖16栋室内建筑及30条街道的室外场景，采用三个月的高精度测量实验，随后对数据结构、测量环境、协议和验证步骤进行系统描述并做案例研究。

Result: WiLoc数据集包含12M+ UE位置、3000+ AP，涉及16栋室内楼宇和30条街道；通过标准与迁移学习的案例研究验证了数据规模对定位性能的提升。

Conclusion: 该研究通过发布庞大的WiLoc CSI数据集，为基于机器学习的定位技术提供了重要的训练与验证资源，展示了大规模数据在提高定位准确性与鲁棒性方面的优势。

Abstract: Localization is a key component of the wireless ecosystem. Machine learning (ML)-based localization using channel state information (CSI) is one of the most popular methods for achieving high-accuracy localization with low cost. However, to be accurate and robust, ML-based algorithms need to be trained and tested with large amounts of data, covering not only many user equipment (UE)/target locations, but also many different access points (APs) locations to which the UEs connect, in a variety of different environment types. This paper presents a massive-sized CSI dataset, WiLoc (Wi-Fi Localization), and makes it publicly available. WiLoc is obtained by a series of precision measurement campaigns that span three months, and it is massive in all the above-mentioned three dimensions: > 12 million UE locations, > 3,000 APs, covering 16 buildings for indoor localization, and > 30 streets for outdoor use. The paper describes the dataset structure, measurement environments, measurement protocols, and the dataset validations. Comprehensive case studies validate the advantages of large datasets in ML-driven localization strategies for both "standard" and transfer learning. We envision this dataset, which is by far the largest of its kind, to become a standard resource for researchers in the field of ML-based localization.

</details>


### [120] [Digital-Twin-Aided Dynamic Spectrum Sharing and Resource Management in Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2602.09191)
*Hung Nguyen-Kha,Vu Nguyen Ha,Ti Nguyen,Eva Lagunas,Joel Grotz,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 基于数字孪生的 ISTN 时窗式动态频谱共享框架，采用压缩感知与序列凸逼近实现两阶段优化，仿真表现出显著的拥塞减少与良好的适应性。


<details>
  <summary>Details</summary>
Motivation: LEO 卫星与动态频谱共享为 ISTN 扩展覆盖、提升频谱效率和降低部署成本提供新途径，但多样化地面环境、用户及卫星移动、以及长距离传播等因素导致资源管理复杂；数字孪生可提供系统的虚拟表征，预测资源需求，协助决策。

Method: 本文提出时窗式数字孪生辅助动态频谱共享框架：先用数字孪生预测资源需求，构建长时决策优化模型；随后引入实时信息进行短时细化，形成两阶段优化。为求解随后运用压缩感知技术预先压缩问题规模，随后采用序列凸逼近求解非凸子问题。

Result: 利用真实交通数据和伦敦 3D 地图进行仿真，所提出算法在拥塞最小化指标上比基准方案性能更优，并证明了适应性强和实际可行性。

Conclusion: 通过建立基于数字孪生的 ISTN 动态频谱共享框架，并针对长短期资源决策设计了两套优化方案，结合压缩感知与序列凸逼近算法实现高效求解，实验表明在可行性和拥塞抑制方面优于传统方法。

Abstract: The explosive growth in wireless service demand has prompted the evolution of integrated satellite-terrestrial networks (ISTNs) to overcome the limitations of traditional terrestrial networks (TNs) in terms of coverage, spectrum efficiency, and deployment cost. Particularly, leveraging LEO satellites and dynamic spectrum sharing (DSS), ISTNs offer promising solutions but face significant challenges due to diverse terrestrial environments, user and satellite mobility, and long propagation LEO-to-ground distance. To address these challenges, digitial-twin (DT) has emerged as a promising technology to offer virtual replicas of real-world systems, facilitating prediction for resource management. In this work, we study a time-window-based DT-aided DSS framework for ISTNs, enabling joint long-term and short-term resource decisions to reduce system congestion. Based on that, two optimization problems are formulated, which aim to optimize resource management using DT information and to refine obtained solutions with actual real-time information, respectively. To efficiently solve these problems, we proposed algorithms using compressed-sensing-based and successive convex approximation techniques. Simulation results using actual traffic data and the London 3D map demonstrate the superiority in terms of congestion minimization of our proposed algorithms compared to benchmarks. Additionally, it shows the adaptation ability and practical feasibility of our proposed solutions.

</details>


### [121] [Orthogonal Circular Polarized Transmitter and Receiver Antennas for Mitigation of Mutual Coupling in Monostatic Radars](https://arxiv.org/abs/2602.09450)
*Shobha Sundar Ram,Akanksha Sneh*

Main category: eess.SP

TL;DR: 通过对正交圆极化四腔螺旋天线的实验验证，证明其在穿墙雷达中能够有效抑制直耦合并保持目标反射，提高探测性能


<details>
  <summary>Details</summary>
Motivation: 考察穿墙雷达在墙壁衰减下的信号传输与台射耦合问题

Method: 采用正交圆极化的四腔螺旋天线(QHA)，比较其与Vivaldi、喇叭天线的互耦性能

Result: QHA在保持首反射的同时显著降低了发射机与接收机之间的直接耦合，提升了雷达检测效果

Conclusion: 正交圆极化QHA天线是适用于穿墙雷达的高效、紧凑、宽带芯天线方案

Abstract: Through-wall radar systems require compact, wideband and high gain antennas for detecting targets. Building walls introduce considerable attenuation on the radar signals. When the transmitted power is raised to compensate the through-wall attenuation, the direct coupling between the transmitter and receiver can saturate the receiver because of which weaker reflections off the target may remain undetected. In this paper, we propose using transmitter and receiver antennas of orthogonal circular polarization to reduce the direct coupling between the transmitter and receiver while retaining the first bounce off the target. In our paper, we demonstrate that the quadrafilar helical antenna (QHA) is a good candidate for this operation since it is characterized by a small size, wide frequency band of operation, high gain and low axial ratio over a wide field of view. We compare the reduced mutual coupling between the transmitter and receiver elements for the oppositely polarized QHA antennas with other commonly used through-wall radar antennas such as the Vivaldi and horn antennas. The system is tested in through-wall conditions.

</details>


### [122] [Performance Analysis of Millimeter Wave Radar Waveforms for Integrated Sensing and Communication](https://arxiv.org/abs/2602.09451)
*Akanksha Sneh,Aakanksha Tewari,Shobha Sundar Ram,Sumit J Darak*

Main category: eess.SP

TL;DR: 本文比较 PMCW、FMCW 与 IEEE 802.11ad 雷达在 ISAC 中对目标检测性能，发现 IEEE 802.11ad 最佳；并在 Zynq SoC 上实现硬件‑软件协同并评估计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 下一代智能交通系统需在同一频谱、同一硬件平台上实现感知与通信，传统分别使用雷达与通信设备导致频谱拥挤与硬件冗余。

Method: 比较传统雷达与通信候选波形（PMCW、FMCW 与 IEEE 802.11ad）在 ISAC 中对单点及扩展目标的检测性能，并在 Zynq SoC 上实现硬件‑软件协同的雷达信号处理算法，进行定点分析评估计算复杂度。

Result: FMCW 在移动目标检测响应较差；PMCW 与 IEEE 802.11ad 在目标检测上优于 FMCW，其中 IEEE 802.11ad 雷达性能最佳。实现层面，基于 Zynq SoC 的硬件‑软件协同与固定点分析验证了可行的低复杂度实现。

Conclusion: IEEE 802.11ad 基于雷达的方案在检测单点与扩展目标时优于传统 PMCW 与 FMCW 雷达，且硬件实现可通过硬件-软件协同设计在 Zynq SoC 上实现低复杂度。

Abstract: Next-generation intelligent transportation systems require both sensing and communication between road users. However, deploying separate radars and communication devices involves the allocation of individual frequency bands and hardware platforms. Integrated sensing and communication (ISAC) offers a robust solution to the challenges of spectral congestion by utilizing a shared waveform, hardware, and spectrum for both localization of mobile users and communication. Various waveforms, including phase-modulated continuous waves (PMCW) and frequency-modulated continuous waves (FMCW), have been explored for target localization using traditional radar. On the other hand, new protocols such as the IEEE 802.11ad have been proposed to support wideband communication between vehicles. This paper compares both traditional radar and communication candidate waveforms for ISAC to detect single-point and extended targets. We show that the response of FMCW to mobile targets is poorer than that of PMCW. However, the IEEE 802.11ad radar outperforms PMCW radar and FMCW radar. Additionally, the radar signal processing algorithms are implemented on Zynq system-on-chip through hardware-software co-design and fixed-point analysis to evaluate their computational complexity in real-world implementations.

</details>


### [123] [Motion Compensation for Multiple-Input-Multiple-Output Inverse Synthetic Aperture Imaging of Automotive Targets](https://arxiv.org/abs/2602.09452)
*Devansh Mathur,Akanksha Sneh,Debojyoti Sarkar,Shobha Sundar Ram*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inverse synthetic aperture radar (ISAR) images generated from single-channel automotive radar data provide critical information about the shape and size of automotive targets. However, the quality of ISAR images degrades due to road clutter and when translational and higher order rotational motions of the targets are not suitably compensated. One method to enhance the signal-to-clutter-and-noise ratio (SCNR) of the systems is to leverage the advantages of the multiple-input-multiple-output (MIMO) framework available in commercial automotive radars to generate MIMO-ISAR images. While substantial research has been devoted to motion compensation of single-channel ISAR images, the effectiveness of these methods for MIMO-ISAR has not been studied extensively. This paper analyzes the performance of three popular motion compensation techniques - entropy minimization, cross-correlation, and phase gradient autofocus - on MIMO-ISAR. The algorithms are evaluated on the measurement data collected using Texas Instruments millimeter-wave MIMO radar. The results indicate that the cross-correlation MOCOMP performs better than the other two MOCOMP algorithms in the MIMO configuration, with an overall improvement of 36%.

</details>


### [124] [Geometric Analysis of Blind User Identification for Massive MIMO Networks](https://arxiv.org/abs/2602.09910)
*Levi Bohnacker,Ralf R. Müller*

Main category: eess.SP

TL;DR: 该研究提出一种仅依赖未知符号的训练序列的盲用户识别方案，利用 replica 方法与 Monte Carlo 验证，显示出在大规模 MIMO 中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在大规模 MIMO 通信系统中，实现盲用户识别的需求日益突出。

Method: 提出使用 Nearest Convex Hull Classification (NCHC) 的无盲方法，基站仅需包含未知符号的训练序列，不需已知信道/调制/编码或噪声功率信息，并采用 replica 方法和自由 Fourier 变换辅助分析。

Result: 通过 Monte Carlo 验证 Operator Valued Free Fourier Transform 的存在，在大系统规模下通过 saddle-point 集成估计分类器准确率，使用高斯矩匹配进行近似。

Conclusion: 该方法在未获得任何频道或模型信息的情况下，能实现高效盲用户识别，并通过理论与仿真验证其可行性。

Abstract: Applying Nearest Convex Hull Classification (NCHC) to blind user identification in a massive Multiple Input Multiple Output (MIMO) communications system is proposed. The method is blind in the way that the Base Station (BS) only requires a training sequence containing unknown data symbols obtained from the user without further knowledge on the channel, modulation, coding or even noise power. We evaluate the algorithm under the assumption of gaussian transmit signals using the non-rigorous replica method. To facilitate the computations the existence of an Operator Valued Free Fourier Transform is postulated, which is verified by Monte Carlo simulation. The replica computations are conducted in the large but finite system by applying saddle-point integration with inverse temperature $β$ as the large parameter. The classifier accuracy is estimated by gaussian approximation through moment-matching.

</details>


### [125] [Collaborative Spectrum Sensing in Cognitive and Intelligent Wireless Networks: An Artificial Intelligence Perspective](https://arxiv.org/abs/2602.09615)
*Peng Yi,Ying-Chang Liang*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial intelligence (AI) has become a key enabler for next-generation wireless communication systems, offering powerful tools to cope with the increasing complexity, dynamics, and heterogeneity of modern wireless environments. To illustrate the role and impact of AI in wireless communications, this paper takes collaborative spectrum sensing (CSS) in cognitive and intelligent wireless networks as a representative application and surveys recent advances from an AI perspective. We first introduce the fundamentals of CSS, including the general framework, classical detector design, and fusion strategies. Then, we present an overview of the state-of-the-art research on AI-driven CSS, classified into three categories: discriminative deep learning (DL) models, generative DL models, and deep reinforcement learning (DRL). Furthermore, we explore semantic communication (SemCom) as a promising solution for CSS, in which task-oriented representations are exchanged to reduce reporting overhead while preserving decision-critical information. Finally, we discuss limitations, open challenges, and future research directions at the intersection of AI and wireless communication.

</details>


### [126] [Generalizable and Robust Beam Prediction for 6G Networks: An Deep-Learning Framework with Positioning Feature Fusion](https://arxiv.org/abs/2602.09685)
*Yanliang Jin,Yunfan Li,Jiang Jun,Yuan Gao,Shengli Liu,Jianbo Du,Zhaohui Yang,Shugong Xu*

Main category: eess.SP

TL;DR: 该研究提出一种利用位置与时域特征融合的深度学习波束预测框架，比传统方法更精确且训练更轻量。


<details>
  <summary>Details</summary>
Motivation: 在大规模MIMO系统中，完整波束训练开销大，需寻找高效、准确的波束预测方法。

Method: 采用Position Extraction Branch对空间坐标进行监督，结合Beam-Domain Features，使用双分支RegNet架构，并引入Adaptive Fusion与Adversarial Fusion两种特征融合策略。

Result: 在DeepMIMO生成的4个城市场景下，3.5 GHz、3GPP规范的实验表明所提方法在分布内和分布外均优于传统基线，并实现更准确、更鲁棒的波束预测。

Conclusion: 通过深度学习框架结合位置感知特征，显著提升了大规模MIMO系统的波束预测精度，降低了训练开销。

Abstract: Beamforming (BF) is essential for enhancing system capacity in fifth generation (5G) and beyond wireless networks, yet exhaustive beam training in ultra-massive multiple-input multiple-output (MIMO) systems incurs substantial overhead. To address this challenge, we propose a deep learning based framework that leverages position-aware features to improve beam prediction accuracy while reducing training costs. The proposed approach uses spatial coordinate labels to supervise a position extraction branch and integrates the resulting representations with beam-domain features through a feature fusion module. A dual-branch RegNet architecture is adopted to jointly learn location related and communication features for beam prediction. Two fusion strategies, namely adaptive fusion and adversarial fusion, are introduced to enable efficient feature integration. The proposed framework is evaluated on datasets generated by the DeepMIMO simulator across four urban scenarios at 3.5 GHz following 3GPP specifications, where both reference signal received power and user equipment location information are available. Simulation results under both in-distribution and out-of-distribution settings demonstrate that the proposed approach consistently outperforms traditional baselines and achieves more accurate and robust beam prediction by effectively incorporating positioning information.

</details>


### [127] [A Dual Belief-Driven Bayesian-Stackelberg Framework for Low-Complexity and Secure Near-Field ISAC Systems](https://arxiv.org/abs/2602.09754)
*Mehzabien Iqbal,Ahmad Y Javaid*

Main category: eess.SP

TL;DR: 引入 Bayesian-Stackelberg ISAC 安全框架，结合自适应节点角色切换和置信度驱动感知/波束成形，实现了保密速率提升35% 与98%成功率，计算复杂度保持线性，适合低成本部署。


<details>
  <summary>Details</summary>
Motivation: 在毫米波及太赫兹频段的近场ISAC系统中，动态信道、多窃听者威胁以及实时优化的高计算负荷对安全性能提出了严峻挑战，需要一种既能提升安全性能又能降低计算复杂度的解决方案。

Method: 采用 Bayesian-Stackelberg 框架，设计了双算法：一是Adaptive Hybrid Node Role Switching，实现安全传输与协同干扰的自适应切换；二是Belief-Driven Sensing 与 Beamforming，基于置信度进行资源分配。

Result: 仿真表明，在28~410 GHz 电池情况下，该方法相较传统通信系统提升了最大35% 的保密速率，攻击成功率超过98%，且运行时开销极小。

Conclusion: 该统一框架通过对感知、波束成形和通信的联合优化，显著提升了近场ISAC系统在面对动态信道、多重窃听者以及实时优化计算密集度高等挑战下的安全鲁棒性，同时保持线性计算复杂度，体现了其可扩展性与低成本部署潜力。

Abstract: Ensuring robust security in near-field Integrated Sensing and Communication (ISAC) systems remains a critical challenge due to dynamic channel conditions, multi-eavesdropper threats, and the high computational burden of real-time optimization at mmWave and THz frequencies. To address these challenges, this paper introduces a novel Bayesian-Stackelberg framework that jointly optimizes sensing, beamforming, and communication. The dual-algorithm design integrates (i) Adaptive Hybrid Node Role Switching between secure transmission and cooperative jamming (ii) Belief-Driven Sensing and Beamforming for confidence based resource allocation. The proposed unified framework significantly improves robustness against attacks while preserving linear computational complexity. Simulation results across carrier frequencies ranging from 28 to 410 GHz demonstrate that the method achieves up to a 35% increase in secrecy rates and a success rate exceeding 98%, outperforming conventional communication systems with minimal runtime overhead. These findings underscore the scalability of belief-driven ISAC security solutions for low-complexity deployment in next generation communications.

</details>


### [128] [An Unsupervised Normalizing Flow-Based Neyman-Pearson Detector for Covert Communications in the Presence of Disco Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2602.09763)
*Luyao Sun,Sitian Li,Huan Huang,Hongliang Zhang,Weidong Mei,Dongdong Zou,Jun Li,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 本文在DRIS干扰的隐蔽通信场景下，设计了无监督MAF‑NP检测框架，理论与仿真显示其检测性能与监督方法相当，并解析了通信SNR表现。


<details>
  <summary>Details</summary>
Motivation: 当伪装躲避者Willie部署可重配置智能表面(DRIS)时，传统NP检验因PDF不可解析且缺乏标签数据而失效，需要开发无需标签且能应对DRIS干扰的检测方法。

Method: 提出基于无监督掩码自回归流 (MAF) 的NP检测框架，利用隐蔽通信的先验信息。通过理论推导获得信号噪声比(SJNR)公式，并在DRIS环境下验证其性能。

Result: 仿真结果表明无监督MAF‑NP检测器的误报率(FAR)、漏检率(MDR)接近监督方法，且在DRIS环境下对Alice-Bob通信的SJNR分析揭示了独特的性能特性。

Conclusion: 展示了在DRIS干扰下，利用无监督MAF构造的Neyman–Pearson检测器能够实现与监督模型相近的检测性能，从而证明了在无标签环境下保持隐蔽通信安全的可行性。

Abstract: Covert communications, also known as low probability of detection (LPD) communications, offer a higher level of privacy protection compared to cryptography and physical-layer security (PLS) by hiding the transmission within ambient environments. Here, we investigate covert communications in the presence of a disco reconfigurable intelligent surface (DRIS) deployed by the warden Willie, which simultaneously reduces his detection error probabilities and degrades the communication performance between Alice and Bob, without relying on either channel state information (CSI) or additional jamming power. However, the introduction of the DRIS renders it intractable for Willie to construct a Neyman-Pearson (NP) detector, since the probability density function (PDF) of the test statistic is analytically intractable under the Alice-Bob transmission hypothesis. Moreover, given the adversarial relationship between Willie and Alice/Bob, it is unrealistic to assume that Willie has access to a labeled training dataset. To address these challenges, we propose an unsupervised masked autoregressive flow (MAF)-based NP detection framework that exploits prior knowledge inherent in covert communications. We further define the false alarm rate (FAR) and the missed detection rate (MDR) as monitoring performance metrics for Willie, and the signal-to-jamming-plus-noise ratio (SJNR) as a communication performance metric for Alice-Bob transmissions. Furthermore, we derive theoretical expressions for SJNR and uncover unique properties of covert communications in the presence of a DRIS. Simulations validate the theory and show that the proposed unsupervised MAF-based NP detector achieves performance comparable to its supervised counterpart.

</details>


### [129] [Robust Processing and Learning: Principles, Methods, and Wireless Applications](https://arxiv.org/abs/2602.09848)
*Shixiong Wang,Wei Dai,Li-Chun Wang,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 论文总结了鲁棒统计、优化和ML在无线感知通信中的核心方法与实际应用，强调鲁棒技术在提升系统抵抗模型误差与对抗攻击方面的价值，并指出其带来的性能与计算成本权衡。


<details>
  <summary>Details</summary>
Motivation: WSC系统面临模型不匹配、数据稀缺、对抗扰动及分布偏移等不确定性，传统方法难以满足性能与可靠性双重需求。因此，需要将鲁棒理论贯穿于系统的各个环节，实现对不确定性的主动控制与补偿。

Method: 采用教程式阐述方式，先从概念与数学定义出发，剖析鲁棒统计、鲁棒优化与机器学习之间的对应关系，然后系统评述各类鲁棒技术（估计、检验、分布鲁棒优化、正则化与对抗训练）及其在WSC中的实际应用。

Result: 提出了完整的鲁棒性方法论，并通过案例（稳健定位、多模态感知、信道估计、波形设计、联邦学习等）展示了鲁棒技术在提升WSC系统鲁棒性与性能上的有效性。

Conclusion: 本文综述了鲁棒性理论在无线感知与通信（WSC）领域的理论基础与实践方法，为信号处理社区提供了从经典到前沿的鲁棒技术框架，强调鲁棒性在系统设计中的双刃剑性质。

Abstract: This tutorial-style overview article examines the fundamental principles and methods of robustness, using wireless sensing and communication (WSC) as the narrative and exemplifying framework. First, we formalize the conceptual and mathematical foundations of robustness, highlighting the interpretations and relations across robust statistics, optimization, and machine learning. Key techniques, such as robust estimation and testing, distributionally robust optimization, and regularized and adversary training, are investigated. Together, the costs of robustness in system design, for example, the compromised nominal performances and the extra computational burdens, are discussed. Second, we review recent robust signal processing solutions for WSC that address model mismatch, data scarcity, adversarial perturbation, and distributional shift. Specific applications include robust ranging-based localization, modality sensing, channel estimation, receive combining, waveform design, and federated learning. Through this effort, we aim to introduce the classical developments and recent advances in robustness theory to the general signal processing community, exemplifying how robust statistical, optimization, and machine learning approaches can address the uncertainties inherent in WSC systems.

</details>


### [130] [Doppler Effect: Analyses and Applications in Wireless Sensing and Communications](https://arxiv.org/abs/2602.09955)
*Lie-Liang Yang*

Main category: eess.SP

TL;DR: 提供了一套全面的多普勒效应理论框架，涵盖从基础运动学到相对论、气象及介质因素，适用于通信、物联网、雷达等多领域。


<details>
  <summary>Details</summary>
Motivation: 该章节旨在为现代通信、物联网、机载雷达、导航、探测及集成感知与通信等广泛应用场景中的电磁和声学信号多变多样的多普勒效应提供系统、严谨且全面的理论分析。

Method: 通过研究从匀速运动、恒定加速度到更复杂的通用运动等各种运动学剖面，综合考虑经典运动学、特殊与广义相对论、气象动力学以及传播媒介特性，构建多维度、多因素的多普勒位移模型与解析方法。

Result: 对多普勒位移机理进行了系统梳理，提出了一套统一的理论框架，并通过数值示例和案例验证了模型的准确性与可推广性，为后续应用与算法设计奠定了坚实基础。

Conclusion: 本文奠定了面向爱好者与专业研究者的多普勒效应理论基础，提升了对现代无线感知与通信系统中频率偏移复杂性的掌握。

Abstract: This chapter is motivated by the need for a rigorous and comprehensive analysis of the Doppler effects encountered by electromagnetic and acoustic signals across a diverse spectrum of modern applications. These include land mobile communications, various Internet of Things (IoT) networks, machine-type communications (MTC), and various radar and satellite-based systems for navigation and sensing, as well as the emerging regime of integrated sensing and communications (ISAC). A wide array of kinematic profiles is investigated, ranging from uniform motion and constant acceleration to more complex general motion. Consequently, the multi-faceted factors influencing the Doppler shift are addressed in detail, encompassing classical kinematics, special and general relativity, atmospheric dynamics, and the properties of the propagation medium. This work is intended to establish a definitive theoretical foundation for both the general enthusiast and the specialized researcher seeking to master the complexities of signal frequency shifts in modern wireless sensing and communications systems.

</details>


### [131] [RIS-Assisted Rank Enhancement With Commodity WiFi Transceivers: Real-World Experiments](https://arxiv.org/abs/2602.10025)
*Aymen Khaleel,Aydin Sezgin*

Main category: eess.SP

TL;DR: RISs raised MIMO rank by up to 112 % (≈+1.5) in rank‑deficient channels and 61 % (≈+1) in medium‑rank channels via passive beam‑focusing on WiFi hardware—showing practical rank enhancement.


<details>
  <summary>Details</summary>
Motivation: RISs can reshape wireless channel; to enhance MIMO effective rank and spatial multiplexing.

Method: Passive beam‑focusing algorithm on commodity WiFi transceivers; manipulate channel per transmit‑receive pair.

Result: Rank‑deficient case: +112 % rank, +1.5 increment; medium rank: +61 % rank, +1 increment; demonstrates RIS-driven rank manipulation with off‑the‑shelf hardware.

Conclusion: First experimental evidence that RISs can improve MIMO spatial multiplexing by increasing channel rank; practical insights for 6G RIS deployment.

Abstract: Reconfigurable intelligent surfaces (RISs) are a promising enabling technology for the sixth-generation ($6$G) of wireless communications. RISs, thanks to their intelligent design, can reshape the wireless channel to provide favorable propagation conditions for information transfer. In this work, we experimentally investigate the potential of RISs to enhance the effective rank of multiple-input multiple-output (MIMO) channels, thereby improving spatial multiplexing capabilities. In our experiment, commodity WiFi transceivers are used, representing a practical MIMO system. In this context, we propose a passive beam-focusing technique to manipulate the propagation channel between each transmit-receive antenna pair and achieve a favorable propagation condition for rank improvement. The proposed algorithm is tested in two different channel scenarios: low and medium ranks. Experimental results show that, when the channel is rank-deficient, the RIS can significantly increase the rank by $112\%$ from its default value without the RIS, providing a rank increment of $1.5$. When the rank has a medium value, a maximum of $61\%$ enhancement can be achieved, corresponding to a rank increment of $1$. These results provide the first experimental evidence of RIS-driven rank manipulation with off-the-shelf WiFi hardware, offering practical insights into RIS deployment for spatial multiplexing gains.

</details>
