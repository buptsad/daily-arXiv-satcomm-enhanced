<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data](https://arxiv.org/abs/2511.21600)
*Yizhou Zhao,Xiang Li,Peter Song,Qi Long,Weijie Su*

Main category: cs.CR

TL;DR: 提出 TAB-DRW，与现有水印方案相比在时效性、混合型数据支持和鲁棒性方面有显著提升。通过在频域嵌入水印信号，结合 Yeo-Johnson 变换和标准化、DFT 及自适应伪随机比特位的调整实现高效鲁棒的后编辑水印。引入基于秩的伪随机比特生成方法实现行级检索，无额外存储开销。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在医疗、金融等领域生成高保真表格数据，对数据来源追踪与滥用防护的需求日益增强。现有水印方法常常计算成本高、难以处理混合离散-连续特征、对后修改鲁棒性不足，因此需要更高效且鲁棒的水印方案。

Method: 将表格数据通过 Yeo-Johnson 变换与标准化进行归一化；对归一后的数据应用离散傅里叶变换（DFT）；根据预计算的伪随机比特，对选定条目的虚部进行调整以嵌入水印。并提出一种新颖的基于秩的伪随机比特生成方法，支持按行检索水印且无需额外存储。

Result: 在五个基准表格数据集上的实验显示，TAB-DRW 具有较强的可检测性和对常见后处理攻击的鲁棒性，同时保持高数据保真度，且完全支持混合类型特征。

Conclusion: TAB-DRW 提供一种高效且鲁棒的后编辑水印方案，解决了现有方法在计算成本、混合数据处理和鲁棒性方面的不足，具有实际应用潜力。

Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [2] [SIR Analysis for Affine Filter Bank Modulation](https://arxiv.org/abs/2511.21615)
*Henrique L. Senger,Gustavo P. Gonçalves,Bruno S. Chang,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: 在AFBM波形下，MMSE均衡下分析两域（仿射域与滤波时域TD）的SIR，发现滤波TD中存在一个有趣且反直觉的抵消现象：信道干扰与正交性近似误差的无效叠加被抵消，仿射域中则不存在此效应，导致BER显著提升的解释来自于该抵消机制。


<details>
  <summary>Details</summary>
Motivation: 揭示AFBM在MMSE处理下的性能差异根源，解释为何经过滤波的TD检测比其在仿射域的等效实现具有更高的BER性能，并阐明离散仿射傅里叶变换（DAFT）和去扩频/映射在干扰抵消中的作用。

Method: 在仿射域与滤波TD两域对AFBM进行SIR分析；引入DAFT及去扩频/映射；推导两域中的干扰项及其相互作用，揭示仅在滤波TD中出现的干扰抵消机制；通过BER验证来对分析进行直接验证。

Result: 在滤波TD中观察到的信道诱导干扰与正交性近似误差之间的叠加被抵消，导致SIR改善并带来BER性能的显著提升；而在仿射域中未出现此抵消现象。

Conclusion: 滤波TD检测结合DAFT/去扩频提供了显著的BER提升，解释了为何其性能优于仿射域实现；该抵消机制为设计AFBM系统提供了关键直觉与理论依据。

Abstract: The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent

</details>


### [3] [Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading](https://arxiv.org/abs/2511.21649)
*Mohamed El Jbari,Fernando D. A. García,Hugerles S. Silva,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 在Rician衰落下，提出了对温噪声通信（TNC）的精确最大似然检测分析框架，通过χ平方统计推导最优检测阈值，并用Gauss-Laguerre求积得到比特错误概率（BEP）的解析表达式；对有限样本的误差进行无近似处理，数值仿真验证显著优于基于高斯近似的检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有TNC分析普遍采用高斯近似且忽略衰落导致的误差问题，建立一个在Rician衰落下的精准、可用于设计的检测框架。

Method: 将TNC的热噪声方差调制视为统计问题，使用χ平方统计量推导出最优最大似然检测阈值；通过Gauss-Laguerre求积给出BEP表达式；并结合有限样本分析和Monte Carlo仿真验证。

Result: 给出无近似的阈值和BEP解析表达式，证明在有限样本下仍准确；仿真显示相较于基于高斯近似的检测，性能有显著提升；系统参数（样本大小、阻抗比、K因子）对性能影响被量化。

Conclusion: 为未来物联网与B5G/6G大规模场景中的TNC接收机设计提供一种稳健且精确的分析工具，克服高斯近似的局限并充分考虑Rician衰落。

Abstract: Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过对 Transformer 的潜在状态几何进行降维分析，论文揭示了层间几何模式、注意力与 MLP 分量输出分离、初始序列位置的高范数、以及位置嵌入的螺旋结构等，可用于促进可重复的解释性研究。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型内部机制，提供可重复的分析框架，促进 Transformer 内部表示的理解。

Method: 对 GPT-2、LLaMa 等模型在 Transformer 块内多点提取层级激活，采用 PCA 与 UMAP 进行降维并可视化，系统分析注意力与 MLP 的输出、向量范数演变、以及位置嵌入等几何特征。

Result: 在中间层观察到注意力与 MLP 输出之间的明显几何分离；初始序列位置的潜在状态范数较高；层级演化可视化；GPT-2 的位置嵌入呈现螺旋结构；LLaMa 展现序列级几何模式；包含重复 token 序列的实验；并提供可复现的代码。

Conclusion: 该方法为 Transformer 内部表示的系统分析提供了可重复的工具与框架，促进解释性研究的发展，并揭示了注意力与 MLP 在几何空间中的潜在分工等现象。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [5] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 通过对2012-2023年的关键算法创新进行消融和标度实验，作者发现 AI 训练 FLOP 效率提升的主要来源来自于规模相关的算法效率改进，而非单一小型创新；最终估计的总体提升远低于最初的估计，且对小模型的算法进展被低估。


<details>
  <summary>Details</summary>
Motivation: 澄清 AI 训练效率提升的来源，量化算法创新与规模效应对总体效率的贡献，并评估小模型阶段的算法进展是否被高估。

Method: 对2012-2023年的关键创新进行小规模消融实验，比较 LSTM 与 Transformer 的标度规律，进行跨文献综述并进行实验外推来估算长期效率增益。

Result: 消融贡献 <10x；其他创新 <10x，总体仍 <100x；规模相关的效率提升在 LSTM-Transformer 转换中占主导，导致对总体提升的解释偏向规模效应；通过外推与文献估计，总体效率提升可达约6930x，且小模型的算法进展远慢于以往假设。

Conclusion: 算法进步的效能与可比尺度密切相关，规模变化是主要驱动；小模型阶段的算法效率提升被低估，且效 pace取决于参照尺度。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [6] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 本工作提出三阶段的 Cliff-Plateau-Climb 规律，揭示深层 Vision Transformer 在 ImageNet 上的表示随深度演化的非单调性，并引入信息混合的量化指标 Information Scrambling Index，用于诊断模型信息扩散与任务性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 挑战当前对深层大模型的普遍假设：深度越大越好。通过系统比较 ViT-S/B/L，在不同深度的表现波动及信息流动的规律，揭示深度的最优区间及设计要点。

Method: 系统地在 ImageNet 上对 ViT-S、ViT-B、ViT-L 的不同深度进行训练和评估，提出并量化三阶段模式；定义并计算 Information Scrambling Index；对比 CLS-token 与 patch tokens 的信息贡献，分析信息扩散与任务性能的关系。

Result: 观察到三阶段的 Cliff-Plateau-Climb 模式，CLS 的全球聚合作用逐步边缘化，patch token 之间的分布式共识增强；Information Scrambling Index 能解释信息混合程度与任务需求之间的关系；在 ViT-L 中，信息-任务权衡的显现要比 ViT-B 晚约 10 层，并且额外的层数更多地推动信息扩散而非提升任务性能。

Conclusion: 深层 Vision Transformer 的性能并非简单的参数增量所致，需通过精心设计的深度阶段转换来实现有效的表示演化；Information Scrambling Index 为现有模型提供诊断工具，并为未来体系结构的设计指向一个潜在目标。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [7] [Escaping the Verifier: Learning to Reason via Demonstrations](https://arxiv.org/abs/2511.21667)
*Locke Cai,Ivan Provilkov*

Main category: cs.LG

TL;DR: RARO通过逆强化学习，从仅含专家演示的数据中学习强推理能力；在没有任务特定验证器的情况下，通过策略（生成器）与相对判别器之间的对抗学习实现鲁棒推理。


<details>
  <summary>Details</summary>
Motivation: 现实中的推理任务往往缺乏可用的任务特定验证器，但存在大量的专家演示数据未被充分用于推理训练，迫切需要在没有验证器的前提下有效利用专家示例来提升推理能力。

Method: 建立一个生成器（策略）和一个相对判别器（critic）的对抗交互框架：策略模仿专家答案，判别器通过比较策略输出与专家答案来学习区分两者；二者通过强化学习进行联合且持续训练，并引入使训练稳定化的关键技巧。

Result: 在 Countdown、DeepMath、Poetry Writing 等任务上，RARO显著优于无验证器基线；并展示出与有可验证任务相似的稳健扩展性趋势，证实仅依赖专家演示即可获得较强的推理能力。

Conclusion: 方法证明在没有任务特定验证器的情况下，仍能从专家演示中高效学习强推理能力，提升推理鲁棒性並缓解对验证器的依赖。

Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

</details>


### [8] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 提出基于样本重要性的框架，通过样本级梯度分析识别影响力与冗余，优先选择关键数据以降低计算和数据需求，同时保持准确性，在三组真实电信数据集上验证，支持在电信领域的可持续AI。


<details>
  <summary>Details</summary>
Motivation: 电信领域生成了极大且嘈杂的高维数据，传统训练通常对所有样本等权处理，导致高存储、计算与标注成本。随着RAN优化、用户体验管理等应用的兴起，亟需高效、可持续的AI模型，同时降低数据与能源消耗。

Method: 在训练过程中对样本进行逐样本级梯度分析，跨训练轮次识别样本的影响力与冗余性，基于此提出一个样本重要性框架，按重要性 electively(选择性地) prioritizes有影响力的数据，削减被用于训练的数据量以降低计算开销，同时尽量不损失模型准确性。

Result: 在三个真实电信数据集上的实验表明，该方法在减少数据需求与计算资源的同时，保持了模型性能，推动了电信领域可持续AI的发展。

Conclusion: 通过引入样本层面的重要性评估，可以实现更高效且可持续的电信AI训练流程，证明数据级别的样本优先化在实际应用中的有效性与可行性。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>
