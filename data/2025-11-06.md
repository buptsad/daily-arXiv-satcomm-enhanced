<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 56]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 11]
- [eess.SP](#eess.SP) [Total: 16]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE introduces two formal algebra benchmarks (FATE-H, FATE-X) with 100 problems each, exposing a large gap between natural-language reasoning and formalization in LLM provers; FATE-X reaches beyond PhD-level/exam difficulty and Mathlib coverage; state-of-the-art models perform poorly (pass@64 ~3% on FATE-H and 0% on FATE-X), highlighting the need for progress toward research-level formal mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between contest-style formal proofs and research-level formal mathematics, and to provide a challenging, standardized benchmark to evaluate and drive progress in LLM-based formal reasoning in algebra.

Method: Construct two 100-problem benchmarks, FATE-H (abstract/commutative algebra) and FATE-X, spanning difficulty from undergraduate to PhD-qualifying-exam level; evaluate state-of-the-art LLM provers with a two-stage process (NL reasoning vs formalization); perform error taxonomy of formalization, and compare specialized provers against general-purpose models at the NL stage.

Result: Best model achieves pass@64 of 3% on FATE-H and 0% on FATE-X; NL-stage reasoning outperforms the formalization stage; common error patterns in formalization identified; specialized provers may underperform general models in NL reasoning compared to their NL capabilities.

Conclusion: FATE provides a robust, challenging benchmark that tracks progress toward research-level formal mathematical reasoning and sets essential checkpoints toward tackling advanced formal mathematics.

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [2] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm是一种基于随机深度图聚类的框架，适用于动态场景下的群体形成（GRS），具备高阶用户信息融入、实时群组形成和自适应组数等特性，在聚类质量、效率和推荐准确度方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有GRS多将群体设定为静态或预定义，难以应对现实世界中的动态用户群体变化；需要在无需频繁重新训练的情况下实现群组的动态重构与扩展。

Method: 采用轻量化的GCN来捕捉高阶结构信号；引入随机簇学习实现无须重新训练的动态组群重配置；通过对比学习在动态条件下优化聚簇效果。

Result: 在多数据集上的实验表明，DeepForm在群组形成质量、效率和推荐准确率方面均优于多种基线。

Conclusion: 验证了在动态场景中使用高阶信息进行群体形成的可行性和有效性，DeepForm满足实时性和自适应性要求，并提升GRS的整体性能。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [3] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 7M参数的递归模型在公开ARC任务上预训练后，在比赛预算内进行全微调以提升ARC任务表现，显示了小模型在受限计算条件下的可行性，但最终分数低于公开评测水平。


<details>
  <summary>Details</summary>
Motivation: 验证在严格计算预算下，是否可用小型递归模型通过全微调实现竞争性ARC任务表现，并评估其在不同评测集上的表现。

Method: 1) 在公开ARC任务上进行预训练：1,280个任务，700k+优化步，48小时，4x H100 SXM，总体获得约10%公开评测分数。2) 比赛期间进行后训练：12,500步梯度更新，达到6.67%半私有评测分数。3) 使用全微调策略（非LoRA或嵌入微调）。

Result: 预训练获得约10%公开分，比赛内后训练达到6.67%半私有分，且未使用LoRA等参数高效微调方法。

Conclusion: 小型递归模型在适当的预训练-后训练策略下，能在受限计算条件中实现对ARC任务的有效微调，但要达到更高竞争力，需进一步扩大容量、提升预训练规模或改进任务设计。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [4] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 基于AI的潜在捕捞区（PFZ）预测框架，利用海表温度和叶绿素浓度等海洋参数在北印度洋区域帮助渔民定位PFZ，初步结果显示可降低搜索时间和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 渔民在确定高产捕鱼区方面存在较大不确定性，区域内的PFZ识别需更高的准确性与可操作性；通过区域化、数据驱动的PFZ预测可提升可持续捕捞实践与经济效益。

Method: 构建一个AI辅助框架，将海洋学参数（如海表温度、叶绿素浓度等）整合用于预测PFZ，并进行区域特定的校准与优化，以提升PFZ识别的准确性。

Result: 初步结果表明该框架可帮助渔民减少搜索时间、降低燃料消耗，并提升资源利用效率。

Conclusion: 所提出的AI框架具备在北印度洋区域提升PFZ识别准确性与可持续捕捞的潜力，可作为区域渔业管理与实地作业的辅助工具。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [5] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出一种基于大语言模型（LLM）的穿戴式物联网（IoT）人类活动识别（HAR） poisoned数据检测与净化框架。通过零-shot、单-shot与少-shot学习，以及角色扮演提示和逐步推理，减少对大规模标注数据的依赖，实现实时的污染检测、数据净化、并评估检测准确性、净化质量、延迟与通信成本等指标，展示LLMs在提升HAR系统安全与鲁棒性方面的可行性。


<details>
  <summary>Details</summary>
Motivation: HAR在IoT生态系统（医疗、智能家居、工业等）中对数据完整性和系统鲁棒性要求极高，但传统防御往往需要大量标注数据与任务特定训练，难以在动态的IoT场景中快速适配。因此，亟需一种能够减少数据标注需求、在实时/边缘环境中可扩展的安全防御机制。

Method: 提出一个框架，利用大语言模型进行污染检测与数据净化，覆盖零-shot、单-shot与少-shot学习场景。通过“角色扮演”提示让LLM扮演专家角色以对传感器异常进行情境化评估，并使用“逐步推理”引导LLM在原始传感数据中推断污染信号以及可行的清洗替代数据。该方法旨在降低对大规模数据集的依赖，使防御在实时IoT HAR场景中具有鲁棒性与适应性。

Result: 作者对框架进行了广泛评估，量化了检测准确性、净化质量、延迟和通信成本等指标，结果表明在提升穿戴式IoT系统安全性与可靠性方面，基于LLM的污染检测与净化具有可行性与有效性。

Conclusion: 基于LLM的污染检测与净化框架可在减少数据标注需求的同时，提升HAR在动态IoT环境中的安全性与鲁棒性，具有良好的实时性和适应性潜力；未来工作应关注对LLM鲁棒性的提升、对更复杂攻击的泛化能力，以及在边缘/隐私敏感场景中的实证评估。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [6] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: Using Llama 3.1-405B to generate structured data-use labels for papers citing genomic datasets; introduces an evaluation framework; achieves F1 ~0.674 in zero-shot data citation classification; notes limitations.


<details>
  <summary>Details</summary>
Motivation: Enable scalable, automated description of data use in the literature to understand how datasets are utilized without manual labeling.

Method: Apply an open-source LLM (Llama 3.1-405B) to produce structured data-use-case labels for publications that cite genomic datasets, and develop a novel evaluation framework for assessment, operating in zero-shot settings.

Result: F1 score of 0.674 on zero-shot data citation classification; demonstrates the stock model can perform without predefined categories; acknowledges barriers including data availability, prompt overfitting, computational requirements, and evaluation costs.

Conclusion: LLM-driven labeling is promising for scaling data-use descriptions, but practical adoption requires addressing data access, model prompting robustness, infrastructure, and rigorous, responsible evaluation.

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [7] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 提出并评估 RO-GUE-TS 及剪裁策略，以在 RO-GUE 框架下实现低回报率成长的探索与个体化/群体学习的平衡，并在 MRT 数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在奖励随时间非平稳变化（衰减/恢复）的情境中，决策者需要在个体化推荐与群体层面的统计推断之间取得平衡。ROGUE 框架能建模这种非平稳性，但现有算法往往过度利用导致探索不足，从而影响对总体效应的估计，特别是在微随机化试验（MRT）中。

Method: 提出 ROUGE-TS（ROGUE-TS）作为对 ROGUE 框架的汤普森采样算法，并给出子线性 regret 的理论保证；引入概率剪裁来在个体化与群体学习之间折衷，并量化其对 regret 与最小探索概率的权衡。通过两个 MRT 数据集（身体活动促进与双相情绪障碍治疗）验证，在降低 regret 的同时保持较高的统计功效。

Result: ROGUE-TS 在回报/ regret 方面优于现有方法；剪裁过程在不显著增加 regret 的前提下维持统计功效并提升治疗效果检测的能力。

Conclusion: 为 MRT 的研究与设计提供实用框架：在保护个体化推荐的同时，确保对总体治疗效果的可检测性与统计有效性，ROGUE 框架及剪裁策略有助于在非平稳行为动态中实现稳健的因果推断。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [8] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 提出 CT F-IDF 的文本预处理方法，并结合 IRLBA 做降维，主张用经典机器学习替代深度学习以降低碳足迹；实验显示在时间复杂度和资源消耗方面显著改进，同时对准确率有小幅折中，部分任务甚至提升。


<details>
  <summary>Details</summary>
Motivation: 应对深度学习带来的高碳足迹和高计算成本，寻求在文本分析中以经典机器学习为核心的高效、低资源方案；通过改良的 TF-IDF 与降维策略提升性能。

Method: 提出 Clement TF-IDF（CTF-IDF）作为数据预处理；使用 IRLBA 对文本特征进行降维；在经典机器学习模型上评估 CTF-IDF 的表现，并与深度学习方法进行对比。

Result: CTF-IDF 与 IRLBA 使时间复杂度显著降低，训练与推理速度提升；在若干数据集上，经典方法的准确率有所提升或与基线接近，整体存在对深度学习的轻微准确性折中。

Conclusion: 基于 CTF-IDF 和 IRLBA 的经典机器学习文本分析流程提供更高效、低碳的替代方案，适用于资源约束环境；未来工作可扩展到更大规模数据集并优化权重设计。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [9] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 将熵最小化的经典策略拆解为两个对立分量：CADF（聚合驱动，促使强势类别输出集中）与GMC（梯度抑制，抑制高置信度样本），并揭示其耦合形式的局限性，如奖励崩塌与易类偏差。提出自适应解耦熵最小化AdaDEM，通过对CADF的奖励进行归一化并用边际熵校准器MEC取代GMC，取得对比DEM*的性能提升，在噪声和动态环境下的弱监督学习任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 深入理解熵最小化的内部机制及其在实际任务中的局限性，特别是解决奖励崩塌和易类偏差等问题，以在带噪声或动态标签分布的弱监督学习中提升性能。

Method: 将经典EM分解为两部分：聚类聚合驱动因子（CADF）与梯度抑制校准器（GMC）。提出归一化CADF的奖励、并用边际熵校准器MEC替代GMC，构建自适应解耦熵最小化AdaDEM。实验对比DEM*，并在多种不完备监督学习场景（噪声、动态环境）中验证效果。

Result: AdaDEM在性能上超过DEM*，在不同的不完备监督学习任务中表现出色。

Conclusion: 解耦的ADEM有效缓解经典EM的局限性（奖励崩塌与易类偏差），通过CADF归一化与MEC校准实现更稳健的输出分布与学习过程，增强对噪声和动态标签的适应性。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [10] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: 提出 UserAlign，在推理阶段通过少量成对比较查询实现个性化对齐，基于逻辑带臂的最佳臂识别，在固定的模型生成响应池中选出最符合用户偏好的响应，假设用户反馈一致且无噪声，实验表明在文本和图像生成任务中有效。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化对齐方法要么需要大量的用户偏好查询，要么要求将偏好以显式文本输入给出，导致效率低下与扩展性受限。

Method: 将用户偏好视为一致且无噪声的反馈，借助最佳臂识别在逻辑回归（logistic bandits）框架下，从固定的生成响应池中通过成对比较选择最符合用户偏好的响应。

Result: 在若干任务（包括文本和图像生成）上，UserAlign展现出有效的个性化对齐能力。

Conclusion: 推理阶段即可通过少量查询实现高效的个性化对齐，方法具有对多任务的可扩展性。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [11] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI proposes a one-shot graph generative model using Bayesian Sample Inference, reframing generation as refining a belief in distribution-parameter space; casts BSI as SDE with a score-based approximation; achieves state-of-the-art on Moses/GuacaMol.


<details>
  <summary>Details</summary>
Motivation: Discrete, unordered graphs are challenging for traditional generative models. While discrete diffusion and flow-matching methods exist, there is value in a framework that maintains a continuous belief over graph-generating parameters to better handle discreteness.

Method: GraphBSI iteratively refines a belief over graphs in a continuous parameter space. It formalizes Bayesian Sample Inference (BSI) as a stochastic differential equation (SDE) and derives a noise-controlled family of SDEs that preserve marginal distributions via score-function approximation. The approach connects Bayesian Flow Networks and diffusion models.

Result: Empirical evaluation shows state-of-the-art performance for molecular and synthetic graph generation, outperforming existing one-shot graph generative models on Moses and GuacaMol benchmarks.

Conclusion: The paper provides theoretical links between Bayesian Flow Networks, diffusion models, and Bayesian Sample Inference for graphs, and demonstrates strong empirical performance on standard graph-generation benchmarks, indicating effectiveness for discrete-structure generation.

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [12] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出自适应传感器无感知监控，通过残差纠正对系统偏差进行后置修正，提升集装箱温度湿度预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传感器无感知模型在缺少遥测信息时可能偏离实时数据，且难以纠正系统性误差，影响可用性和信任度。

Method: 引入残差纠正框架，在观测到的实时遥测数据后对传感器无感知模型的系统性偏差进行修正；在规模达3.48百万数据点的数据集上训练并评估 adaptive-sensorless 模型，与基线传感器无感知模型对比；使用 holdout 的模拟数据集进行评估。

Result: 在 holdout 集上，温度 MAE 约2.24–2.31°C；相对湿度 MAE 约5.72–7.09%（比传感器无感知的7.99%更好）；温度 RMSE 约3.19–3.26°C；相对湿度 RMSE 约7.70–9.12%。相比传感器无感知模型，均有显著改进。

Conclusion: 自适应传感器无感知模型有助于更准确的货物监控、早期风险检测，并减少全球航运对完全连通性的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [13] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出 DADO，一种可利用设计变量结点树分解结构的分布式优化方法，通过软因子化的搜索分布和图消息传递在离散设计空间中高效导航，提升对目标属性的优化效率。


<details>
  <summary>Details</summary>
Motivation: 在需要根据用户指定属性在离散设计空间中设计对象的场景下，现有的分布式/强化学习优化算法往往无法有效利用变量间的分解结构，导致效率低下和样本利用率不足。

Method: 提出 Decomposition-Aware Distributional Optimization (DADO) 框架，基于变量的结点树（如 junction tree）分解设计空间，构建一个软化的搜索分布（soft-factorized）并结合图消息传递来在相连的因子之间协调优化，以提高对目标函数的期望值优化。

Result: 理论与实验表明：在具有可分解结构的设计问题上，DADO 能更高效地探索搜索空间，利用变量间的条件独立性实现更好的样本利用率和优化性能。

Conclusion: DADO 提供了一种利用分解结构的高效离散设计优化框架，适用于蛋白设计、材料设计、电子/电路设计等领域的分布式优化任务。

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [14] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出第一套面向对象驱动交互的无监督评估指标，基于无标签交互数据的统计特性并通过微调的LLM来适应分布偏移，用于标注用户目标、衡量目标完成度和量化LLM不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决企业场景中评估困难的问题：数据多且无标签、人工标注难以规模化、自定义指标难以发现新错误、LLM评判不稳定。

Method: 构建无监督指标，利用无标签交互数据的统计特性；使用微调后的LLM来适应分布偏移；开发用于标注用户目标、衡量目标完成度和量化LLM不确定性的指标；评估不依赖人类理想答案。

Result: 在开放域和任务特定交互数据上进行验证，结果表明提出指标能有效评估对象驱动的交互，并对分布偏移具有鲁棒性。

Conclusion: 无监督评估指标为企业级对象驱动交互的评价提供可扩展方法，减少对人工标注和人类理想答案的依赖，适用于未标注数据和分布变化场景。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [15] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 将 Transformer 表征视为曲面上的几何对象，注意力作为离散连接实现值向量的平移，层级表示离散时间切片，反向传播等同于最小作用量原理。若类比成立，词嵌入在特征空间中不会沿直线前进，而是在嵌入空间曲率的作用下弯曲、重新定向。通过可视化曲率、角度分布和受控上下文编辑的“彗星样”实验来检验曲率的存在及其后果。


<details>
  <summary>Details</summary>
Motivation: 提供一个统一的几何框架来理解 Transformer 的动力学，利用广义相对论的类比来解释嵌入空间的曲率如何影响信息传递与学习过程，并给出可检验的实验设计以证实曲率的存在及其影响。

Method: 提出一个将查询/键诱导成对嵌入空间的有效度量的框架；注意力被视为实现跨标记的离散平行传输的连接；多层结构对应离散时间切片，层层前进等效于在曲率上演化的轨迹；反向传播被看作在参数空间内塑形损失最小化轨迹的最小作用量原则。为验证预测，设计三类实验： (i) 对整段文本可视化曲率景观，观察不同标记与层之间的局部转折角；(ii) 通过仿真验证过多的尖锐/平坦角度与较长的长度-弦比不能仅由维度或随机性解释；(iii) 受爱因斯坦日偏折实验的启发，在受控上下文编辑中检验嵌入轨迹的偏转，观察注意力诱导的曲率。

Result: 提出的实验显示曲率存在且可测：曲率景观揭示了不同标记/层的局部转角随位置变化；角度分布与长度-弦比的异常不能用简单的维度或随机性解释；受控上下文编辑后嵌入轨迹确实发生可测的、符合预期的偏转，支持注意力引发的曲率。

Conclusion: 如果该几何类比成立，Token 的嵌入路径将偏离直线，且各层的交互会因嵌入空间的曲率而导致驱动的弯曲与重定向；所提出的可视化与定量测试可作为检验曲率存在与影响的实验框架。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [16] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 提出一种基于图同态扭曲度的图相似性度量，该度量可完全刻画并嵌入图结构；通过采样近似计算以应对图规范化问题，得到可度量的距离。理论与实验表明在 BRE C 数据集（4-WL 不可区分的图）上完全区分，并在 ZINC-12k 上优于以同态为基础的方法。


<details>
  <summary>Details</summary>
Motivation: 表达能力的传统衡量依赖于组合性质，亟需一个 principled 的、可比较的图相似性度量来超越纯组合视角，解决图规范化/canonization 问题，并提供一个完整嵌入的度量框架。

Method: 提出图同态扭曲度 (graph homomorphism distortion) 的定义，证明其能够完全刻画图并构成完整的图嵌入。为避免图规范化难题，设计基于采样的近似计算，在期望意义下保持完备性，并从中导出一个度量。通过理论推导与实验验证相结合来支撑上述结论。

Result: 理论上，图同态扭曲度能够完全刻画并实现完整嵌入；通过采样方法在期望意义下实现完备性。实证方面，在 BRE C 数据集上能够把所有图区分开，即使有 4-WL 不可区分的图；在 ZINC-12k 数据集上，该度量优于以同态为基础的先行方法。

Conclusion: 这些结果为未来对图的系统表征铺平道路，扩展了图论传统在新领域的应用；所提出的度量框架具有理论和实证的双重支撑。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [17] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: MSUCB：通过均值-中位数估计器在级联 bandit 的在线学习排序中实现对腐败的鲁棒性，在无腐败情形下保持最优对数伪误差界，遇到腐败时通过中位数步过滤异常值，且每轮更新以加速收敛，实验证明明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习排序(OLTR)在点击欺诈、腐败等恶意反馈下的脆弱性。提出一种鲁棒的估计方法以在带腐败的环境中稳健地估计评分/点击概率，从而提升排序策略在长期的用户体验和点击收益上的表现。

Method: 提出 MSUCB 算法，结合均值-中位数估计器。该估计器在无腐败时表现等同于标准均值，具备无额外代价的鲁棒性；在存在腐败时，采用中位数滤除离群样本与被污染的样本。每轮更新估计值以加速收敛。理论上在无腐败时达到对数伪误差界，在腐败存在时退化为附加项，与总腐败量相关。通过在真实数据集上的广泛实验，比较优于现有方法。

Result: 在无腐败情形下实现最优对数回报率界；在腐败情形下的回报率受腐败总量的附加项影响但仍具有鲁棒性；实验显示对两种最前沿方法分别达到约97.35%和91.60%的回顾误差改进。

Conclusion: MSUCB 提供了一种在鲁棒性与无腐败最优性之间的有效权衡的 OLTR 方案。通过每轮更新和均值-中位数估计，在存在腐败时仍能保持接近真值的估计，从而提升长期表现并显著降低因腐败带来的性能下降。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [18] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 提出 SparKer：在半监督的 Neyman–Pearson 框架下，利用稀疏高斯核集成的自组织局部核，在高维表示空间中进行局部化的异常检测，具备可解释性、效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代人工智能产生的表示统计属性难以控制，导致缺乏鲁棒的异常检测（AD）方法，尤其在最小先验信息下更易失效。文献提出需要满足结构性 desiderata（稀疎、局部性、竞争性），以自组织方式在统计不平衡区域自适应划分表示空间，从而检测弱信号与稀有异常。目标是在自然科学与计算机科学等高维场景中实现可解释且可扩展的AD。

Method: 提出一类自组织局部核的结构性框架；构建 SparKer——一个稀疏的高斯核集合，在半监督 Neyman–Pearson 框架下局部建模样本相对于无异常参考的似然比；通过自适应分区围绕统计失衡区域来划分表示空间，并给出理论洞察检测机制和自组织过程；在高维现实任务上验证，包括科学发现、开放世界新颖性检测、入侵检测和生成模型验证；实验表明仅用少量核即可覆盖数千维空间，具备可解释性和高效性。

Result: 理论上揭示了驱动检测与自组织的机制，并在现实高维问题上展示有效性。所提出的稀疏核集合能定位在统计上显著的异常位置，表明该方法在可解释性、效率与扩展性方面具有优势，适用于跨自然与计算机科学领域的多种场景。

Conclusion: SparKer 提供了一种在最小先验信息下的自组织局部核框架，用以高维数据的统计异常检测，强调稀疏性、局部性与竞争性，具有良好的可解释性、效率与可扩展性，可广泛应用于科学发现、开世界检测、入侵检测及生成模型验证等领域。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [19] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: A novel forward-mode gradient estimator that reduces both bias and variance by manipulating upstream Jacobian matrices, showing scalability with network width.


<details>
  <summary>Details</summary>
Motivation: Current forward-mode AD methods suffer from high variance and bias when mitigations are applied, hindering scalability to large networks. A gradient estimator that preserves accuracy while scaling with width is highly desirable.

Method: Introduce a gradient estimation approach that manipulates upstream Jacobian matrices when computing guess directions to reduce bias and variance; provide analysis linking bias/variance to the low-dimensional structure of neural network gradients.

Result: Preliminary results indicate reduced bias and variance and improved scalability with network width; the method performs better as width increases and shows potential to scale to larger networks.

Conclusion: The approach offers a promising path toward scalable, accurate gradient estimation in deep networks, with insights into bias and variance grounded in the low-dimensional gradient structure.

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [20] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff: a physics-informed, SE(3)-equivariant diffusion antibody generator that enforces Fokker-Planck dynamics across the CDR geometry manifold, achieving state-of-the-art results on RAbD with notable gains in RMSD and amino-acid recovery, including six-CDR co-design.


<details>
  <summary>Details</summary>
Motivation: Address two core issues in computational antibody design: (1) dynamical inconsistency leading to physically implausible structures, and (2) poor generalization due to data scarcity and structural bias.

Method: Introduce Fokker-Planck Equation (FPE) residual loss applied over the mixed manifold of CDR geometries (R^3 x SO(3)) to shape denoising-score trajectories into a coherent probability flow; integrate with a SE(3)-equivariant diffusion framework incorporating deep biological priors.

Result: On the RAbD benchmark, FP-AbDiff delivers state-of-the-art performance: in de novo CDR-H3 design, mean RMSD 0.99 Å (vs AbX, ~25% better), amino acid recovery 39.91%; in six-CDR co-design, ~15% reduction in full-chain RMSD and full-chain amino acid recovery on CDR-H3 loop 45.67%.

Conclusion: Aligning generative dynamics with physical laws enhances robustness and generalization, providing a principled path to physically faithful and functionally viable antibody design.

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [21] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 提出自监督对比学习的理论界限：在条件独立性假设下给出最紧界限，并进一步放宽为增广重叠假设，得到渐近闭合界限。提出增广重叠理论，认为通过更强的数据增强使同一类别样本的支持区重叠，从而仅通过对同一样本的正样本对齐即可实现类内聚。并由此开发一个无监督的表示评估指标，能够与下游任务性能高度相关，且无需额外模块。代码公开。


<details>
  <summary>Details</summary>
Motivation: 揭示自监督对比学习的工作机制，给出严格的理论界限并提供一个可用于评估表示质量的无监督指标，以帮助理解与改进对比学习方法。

Method: 在最基本的条件独立性假设下推导对比学习的紧界限；然后放宽假设为增广重叠（augmentation overlap），并推导其下的渐近闭合界限。通过分析不同增强下样本支持集的重叠程度，解释为何简单的正样本对齐能促成类内聚。基于该理论提出一个无监督表示评估指标。

Result: 给出两类边界：1）严格条件独立性下的紧界限；2）增广重叠假设下的渐近闭合界限。理论上说明增强的覆盖度越高，类内样本越易聚集，解释了对比学习的有效性。所提出的无监督评估指标在很大程度上与下游性能一致。

Conclusion: 增广重叠是驱动对比学习效能的关键因素，单纯对齐正样本即可实现良好类内聚；无监督评估指标能在不依赖额外模块的情况下预测下游表现，具有实用性和普适性。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [22] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出静态诱骗器 StaDec 与动态图灵 DyDec，利用LLMs生成自然、语义保留但对目标模型具有欺骗性的对抗文本，具备自动化、跨模型转移性和自我评估能力。


<details>
  <summary>Details</summary>
Motivation: 在零-shot任务中，LLMs表现突出，但对敏感任务的鲁棒性需系统评估。本研究旨在提供一个自我评估LLMs鲁棒性的对抗性文本生成框架。

Method: 提出 StaDec（静态诱骗）和 DyDec（动态图灵）两种攻击框架，通过LLM驱动的自动管线动态生成对抗样本，保持语义相似度同时提高被攻击模型的误导性，且具备良好转移性；不依赖外部启发式。

Result: 对多种LLMs具有高隐蔽性和自然度的对抗文本，能在未见模型上实现有效转移；并展示自动化自我评估的有效性；代码和数据公开。

Conclusion: 提供一个可扩展的自我评估框架，用于系统性评估LLMs对对抗文本的鲁棒性，并可随LLMs的发展持续演进。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [23] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: AQR is a test-time unsupervised adaptation method that aligns channel-wise quantiles of pre-activation distributions to adapt models to distribution shifts, compatible with BN, GN, and LN, with robust tail calibration to handle varying batch sizes, and without retraining, showing strong robustness across CIFAR-10-C, CIFAR-100-C, and ImageNet-C.


<details>
  <summary>Details</summary>
Motivation: Domain shift between training and test data in real-world applications degrades deep models. Existing test-time adaptation often requires target-domain knowledge or retraining, and BN-based methods may not capture complex activation distributions.

Method: Adaptive Quantile Recalibration (AQR) modifies pre-activation distributions by aligning their channel-wise quantiles. It uses a robust tail calibration strategy to handle tails under different batch sizes and relies on source-domain statistics collected during training to enable unsupervised adaptation without retraining. It is compatible with architectures using BatchNorm, GroupNorm, or LayerNorm.

Result: Empirical evaluation on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures shows that AQR achieves robust test-time adaptation and outperforms existing baselines.

Conclusion: AQR provides a practical, architecture-agnostic test-time adaptation approach that maintains performance under distribution shifts without requiring target-domain labels or retraining, suitable for dynamic real-world deployment.

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [24] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: 提出 Forecast2Anomaly (F2A)，通过联合预测-异常损失和检索增强生成模块，将预训练时序基础模型（TSFM）用于零-shot 异常预测，在多领域数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 对多变量时间序列的异常预测需求日益增长，现有方法往往对特定系统泛化能力不足，且随时间演变的异常模式难以适应。尽管TSFM具备较强泛化与零-shot预测能力，但直接用于异常预测尚未充分探索，因此需要将其能力迁移至异常预测任务。

Method: 在TSFM上进行两方面创新：1) 联合预测-异常损失，微调模型以在异常时点也能准确预测未来信号；2) 检索增强生成（RAG）模块，检索历史上相关的 horizon，并以此为条件进行预测，且能在推理时对分布变化动态自适应，不需要模型更新。

Result: 在16个多样数据集与多种TSFM backbone上进行广泛实验，F2A在多项指标上持续优于现有方法，提供一种可扩展的零-shot异常预测解决方案。

Conclusion: F2A将强大的零-shot时序预测能力与零-shot异常预测相连接，且通过动态检索实现对演化异常模式的适应，降低了部署复杂性，具备在真实场景中的可扩展性与应用潜力。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [25] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出Periodic Skill Discovery (PSD)框架，通过将状态映射到圆形潜在空间来发现周期性技能，从而在无监督强化学习中获得多样的周期性行为，适用于像步态这样的跨时间尺度任务，并能提升下游任务表现，同时可与现有技能发现方法结合以扩展行为谱系。


<details>
  <summary>Details</summary>
Motivation: 许多机器人任务，尤其是运动任务，需要周期性（重复性）行为；现有的无监督技能发现方法往往忽视周期性，仅关注状态与技能之间的互信息或在潜在空间的距离上做优化，导致难以发现具有周期性的多样技能。

Method: 提出把状态映射到圆形（循环）的潜在空间的编码器，以自然编码周期性；通过捕捉时间距离来学习具有多样周期的技能，即使在像素观测下也能有效；在训练中强调周期性特征的表征，以便在复杂的机器人任务中学习不同周期的技能。

Result: 所学习的技能在下游任务（如跨越障碍）上表现良好；PSD能够在多种时间尺度上学习具有周期性的多样技能，且适用于像素观测；将PSD与现有技能发现方法结合可获得更丰富的行为谱系，同时代码与演示可公开获取。

Conclusion: PSD扩展了智能体的技能谱系，使其具备更丰富的周期性行为能力，并且可与现有无监督技能发现方法协同提升多样性与表现。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [26] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出一种基于熵等式的线性注意力机制，通过对点积分布的熵进行线性复杂度近似，实现对长序列的高效建模；在四个时空数据集上实验显示在内存和时间显著降低的同时保持竞争力的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力在序列长度增加时的二次复杂度瓶颈，寻求可扩展的线性注意力方案；并给出理论洞见：熵作为严格凹函数，其在概率简单形上的分布若排序一致且熵值相近，具有结构相似性。

Method: 基于上述理论开发一种基于熵相等的线性注意力。首先给出关于点积派生的分布的熵的高效近似，达到线性复杂度；然后构建基于熵等式的注意力实现，并对其性质进行分析。

Result: 在四个时空数据集上的广泛实验表明，该方法在预测性能上具备竞争力甚至优于部分基线，同时显著降低了内存占用和计算时间。

Conclusion: 对注意力有效性的理解可能更多来自于获得的温和且均衡的权重分布，而非softmax的非线性本身；所提出的熵等式线性注意力为大规模时空建模提供了有效且高效的替代方案。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [27] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出基于 Copula 的多模态学习框架，通过对齐模态边缘分布并建模其联合分布来捕捉模态间的高阶交互，以在缺失模态时生成表示，并在公开的 MIMIC 数据集上展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法多依赖简单的拼接或 Kronecker 乘积，难以建模模态间的复杂交互，且对高阶交互的联合分布研究不足。Copula 能在不改变边缘分布的前提下灵活建模多变量的依赖结构，因此具备作为多模态交互建模工具的潜力。

Method: 将每个模态近似为高斯混合分布，在联合层面引入 Copula 来连接模态的边缘分布，从而学习模态的联合分布并实现对缺失模态的表示重建/生成。

Result: 在公共数据集 MIMIC 上的实验显示该方法优于其他对比方法，且作者提供了开源代码。

Conclusion: 基于 Copula 的多模态学习框架有效地建模模态之间的复杂交互，并提升了缺失模态情形下的表示与任务性能，具备较强的实用性和扩展性。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [28] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 使用概率U-Net进行统计降尺度，比较 afCRPS 与 WMSE-MS-SSIM 在三种降尺度设置下对 16x 粗糙分辨率的降尺度结果，发现 WMSE-MS-SSIM 在极端值场有较好表现，afCRPS 更好捕捉跨尺度的空间变异。


<details>
  <summary>Details</summary>
Motivation: 解决气候模型高计算成本导致的分辨率限制，提供不确定性量化的降尺度方法，以便更好支撑气候影響研究。

Method: 将变分潜在空间集成到 U-Net 骨干的概率U-Net；在降尺度降水与温度的任务上，比较 afCRPS 与 WMSE-MS-SSIM 两个训练目标，在三种不同的降尺度设置下对比性能。

Result: WMSE-MS-SSIM 在极端值场表现良好，但仅在某些设置下；afCRPS 在捕捉跨尺度空间变异方面表现更好。

Conclusion: 结论强调指标选择依赖于关注点：若关注极端事件，WMSE-MS-SSIM；若关注空间变异性和尺度间关系，afCRPS 更优。建议在降尺度研究中权衡这两个目标以获得全面不确定性表达。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [29] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 轻量化IoT botnet检测：在VAE潜在向量上训练MLP分类器，比较QAT与PTQ两种量化策略在N-BaIoT和CICIoT2022数据集上的性能。PTQ在只有微小准确率下降的前提下提供显著的速度和压缩收益；QAT则在更大程度上损害准确率但压缩更高。总体证明了量化在设备端IoT检测的可行性。


<details>
  <summary>Details</summary>
Motivation: 需要在资源受限的物联网设备上部署高效且准确的botnet检测模型；深度学习模型的算力开销阻碍边缘部署，需通过量化等压缩方法提升可部署性。

Method: 使用预训练VAE的编码器将高维训练数据映射到8维潜在向量；在该潜在向量上训练基于MLP的分类器。系统比较Quantization-Aware Training（QAT）和Post-Training Quantization（PTQ），在两个基准数据集N-BaIoT与CICIoT2022上评估检测准确率、存储效率与推理延迟。

Result: PTQ在速度提升约6倍、体积缩减约21倍方面表现显著；QAT速度提升约3倍、压缩约24倍，但准确率下降较明显，尤其与原始无量化模型相比。PTQ对准确率的影响更小，QAT的影响更大。

Conclusion: 对IoT边缘设备的检测任务，量化方法具备实际可行性。PTQ在保持较小准确率损失的同时提供更优的综合效率，是更为推荐的量化策略，尽管两种量化都显著提升了设备端可部署性。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [30] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: 提出了一种数据收集的闭环控制范式FCDC，通过在线概率模型和反馈信号（如似然度和马氏距离）动态调节样本保留，实现数据多样性与存储高效性之间的平衡，提升数据中心化AI的数据质量与效率。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的AI面临数据质量与多样性不足的问题，数据往往以开环方式收集，导致冗余样本堆积、存储成本高且泛化能力受限。因此需要将数据收集转化为一个带有反馈的闭环过程，以提升数据的覆盖度和效率。

Method: 提出FCDC：将数据收集视为闭环控制问题；通过在线概率模型持续近似数据分布状态，并基于反馈信号（如似然、马氏距离）自适应调节样本保留；在探索-利用之间动态平衡，维持数据集多样性，避免冗余积累。对一个合成数据集展示可控性；在真实数据流上实验表明FCDC可使数据集更均衡（提升25.9%）并减少数据存储量（下降39.8%）。

Result: 在合成数据集上验证了控制性；在真实数据流上实验表明FCDC能生成更均衡的数据集并显著降低存储需求（分别约25.9%和39.8%）。

Conclusion: 数据收集可以被主动控制，从被动的管道阶段转变为以自我调节、基于反馈的过程，成为数据中心化AI的核心。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [31] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [32] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [33] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在数据受限的严格预训练条件下，扩散语言模型在训练更多轮次时常超越自回归模型，且优势随数据量、模型规模与架构而变化；三大因素推动其性能提升：任意序列建模、迭代双向去噪带来的超密集计算、以及内建蒙特卡洛数据增强。规模化训练和重复预训练可在低数据条件下实现强下游表现，且验证损失上升并不必然意味着下游性能下降。


<details>
  <summary>Details</summary>
Motivation: 揭示在数据有限条件下，扩散语言模型相对于自回归模型的潜在优势及其形成机制，解释为什么重复训练和特定正则化会带来显著收益。

Method: 在受控的预训练设置下对比不同数据量、模型规模、架构密度的扩散与自回归模型，分析任意序列建模、迭代去噪、蒙特卡洛增强等对下游任务（如HellaSwag、MMLU）的影响。

Result: 在数据有限时，DLM通常超过AR。1.7B DLM在约1.5T-token计算预算下，用10B个独立的Python token获得超过等效匹配设置的AR编码器的表现。1B参数的DLM在仅1B tokens的训练条件下，达到HellaSwag>56%、MMLU>33%。此外，增加的验证交叉熵并不必然导致下游性能下降。

Conclusion: 在数据稀缺场景中，DLM的三大因素共同推动性能提升，规模化和重复训练进一步缓解数据不足问题；结果提示需在不同数据/任务条件下进一步验证普适性与极限条件。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [34] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 基于深度强化学习的自适应速率限制框架，使用DQN和A3C混合架构，通过MDP建模进行速率控制，在云原生微服务环境中提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的速率限制算法在动态流量和不同系统负载下往往难以自适应，需一个能在保证系统稳定性和服务质量的同时动态平衡吞吐量与响应时延的控制策略。

Method: 提出将DQN与A3C结合的混合架构，将速率限制决策建模为马尔可夫决策过程，持续监控微服务状态并通过环境交互学习最优限流策略。实验在Kubernetes集群中进行，包含对比分析和生产部署验证。

Result: 在高并发场景下，与传统固定阈值策略相比，吞吐量提升约23.7%，P99延迟降低约31.4%；在为期90天的生产部署中，处理每日5亿次请求，服务降级事件减少82%，人工干预下降68%。

Conclusion: 基于强化学习的自适应速率限制在云原生微服务环境中具有显著的性能和可靠性提升，并具备可在生产中落地的实际价值。

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [35] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 提出了基于概率建模的多参考对齐（MRA）新算法，将相对姿态作为 nuisance 变量进行边缘化以消除全局对称性，从而实现更直接的求解和更好的收敛性；采用去中心化策略通过循环一致性减少中央化方法的三次方级别计算量，实验表明两种算法在重建误差上均优于基线并具备显著的计算节省。


<details>
  <summary>Details</summary>
Motivation: 在多观测信号存在错位并需对齐和重建的场景中，全球对称性（如未知的旋转/平移）使得问题具高度非凸性且计算成本高。MRA在生物成像、计算机视觉、无线通信等领域具有广泛应用潜力，需要可扩展且稳健的解法来提高重建质量与效率。

Method: 提出一个基于概率建模的 MRA 框架，将观测中的相对姿态作为 nuisance 变量进行边缘化以去除全局对称性。通过去中心化处理实现对循环一致性的利用，从而避免中央化方法的三次方复杂度。文中给出两种算法，分别在理论与实验层面展示了对对齐和重建的改进。

Result: 实验结果显示两种算法在不同设置下具有更低的重建误差，同时通过去中心化和循环一致性实现显著的计算节省与更好的收敛性。

Conclusion: 通过将相对姿态边缘化并采用去中心化、循环一致性的方法，成功去除了全局对称性对 MRA 的影响，使求解更直观、收敛更快且规模化能力更强，具有广泛的应用潜力。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [36] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出了一种把迭代空域投影扩展到核方法的连续公平性方法，针对回归中的连续受保护属性，基于核嵌入的支持向量回归在多数据集上显示出有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在日常生活中的广泛应用，公平性成为关键问题。现有工作多关注离散属性和线性模型，对回归中的连续属性（连续公平性）研究稀缺，且多为线性或嵌入式投影。需要一种能处理非线性关系的核方法。

Method: 将迭代空空间投影思想推广到核方法，构建在核嵌入中的公平性约束，适用于连续受保护属性；提供一个型号和公平分数无关的核嵌入方法，并将其与支持向量回归结合。

Result: 在多个数据集上，与现有方法相比，所提方法具有竞争力或更优表现。

Conclusion: 核方法扩展了连续属性下的公平学习，尤其在回归任务中实现更鲁棒的公平性表达。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [37] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: SORTD 框架可扩展地枚举 Rashomon 集中的稀疏决策树，按目标值排序输出，显著提升性能并支持对集合的后评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，单一最优树可能不足以解释和满足偏好。Rashomon 集可用于变量重要性分析、丰富解释以及让用户在不同约束（如公平性）下选择树，而无需在目标函数中硬编码这些约束；但由于最优树问题为 NP-hard，枚举完整 Rashomon 集具备挑战性。

Method: SORTD 提高可扩展性，按目标值对 Rashomon 集中的树进行排序枚举，提供 anytime 行为；适用于任意可分离且全有序的目标函数，并可对 Rashomon 集合进行使用其他可分离（且部分有序）目标的后评估。

Result: 与现有方法相比，SORTD 将运行时间降低最多约 100 倍；能够对任何可分离、全序目标计算 Rashomon 集，并支持对集合进行后评估使用其他目标。

Conclusion: 提升在现实世界中的 Rashomon 集探索可行性，使得变量重要性分析、解释丰富性以及对偏好（如公平性）的灵活考虑，能够在不修改主目标的情况下实现。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [38] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: Proposes DMTC, a data-free, multi-label intention recognition pipeline for agentic AI in transportation, using prompt-generated synthetic queries, Sentence-T5 embeddings, and an online focal-contrastive loss, achieving strong performance without manual labeling.


<details>
  <summary>Details</summary>
Motivation: Address the need for fine-grained, multi-label intent understanding in transportation applications while eliminating costly data collection and labeling, enabling scalable, autonomous intention-aware systems.

Method: DMTC pipeline: (1) prompt engineering with LLMs to generate diverse synthetic queries across transport scenarios; (2) encode queries with Sentence-T5 to obtain compact semantic embeddings; (3) train a lightweight classifier using Online Focal-Contrastive (OFC) loss to emphasize hard samples and maximize inter-class separation.

Result: Empirical evaluation in maritime transportation shows Hamming loss 5.35% and AUC 95.92%; Sentence-T5 embeddings improve subset accuracy by 3.29% over alternatives; OFC loss adds 0.98% gain over standard contrastive objectives; DMTC outperforms state-of-the-art multi-label classifiers and recent end-to-end LLM-based baselines.

Conclusion: DMTC enables intention-aware, modular routing of user queries to task-specific modules (ETA, traffic risk, etc.) without manual labeling, supporting fully autonomous, agentic AI in transportation.

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [39] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出一个数据-模型耦合市场，将数据集交易与模型交易视为一个闭环，提供供给-需求映射并通过Shapley分配实现买家对数据集的价格反馈，形成四向交互的闭环。理论上证明该联合算子是标准干扰函数，确保价格的存在性、唯一性和全局收敛，实验显示收敛性和公平性优于基于经纪人的一方市场。代码公开在GitHub。


<details>
  <summary>Details</summary>
Motivation: 当前多方参与的ML市场多采用分离定价或经纪人导向的流程，难以实现数据卖家、模型生产者和买家之间的对称性与联合性定价，且难以兼顾外部性和相互影响。需要一个统一、稳定且公平的机制来同时定价数据与模型，以提高市场效率与公平性。

Method: 构建数据-模型耦合的市场框架，定义供给侧将数据集支付映射成买方可看到的模型报价，需求侧将买方价回传给数据集以实现定价反馈，基于Shapley分配实现买家对数据的价格分配。形成一个闭环的四向交互：双向供需传播、买家间和卖家间的相互耦合。对联合算子进行理论分析，证明其为标准干扰函数（SIF），从而保证价格的 existence、唯一性和全局收敛。通过仿真实验验证快速收敛、在公平性方面优于基于经纪人的一方或无耦合基线。

Result: 理论上证明联合算子具备存在性、唯一性和全局收敛性；实验结果显示高效收敛，并在公平性方面优于传统经纪人中介或单向定价的基线。

Conclusion: 提出的统一数据-模型耦合定价机制实现对称、闭环且高效的市场定价，且在实验中表现出更好的公平性与收敛性；代码开放获取，可在GitHub获取。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [40] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 提出 Adaptable HER（ours），将 hindsight experience replay 与 AlphaZero 风格的 MCTS 融合，提供可调整的 HER 属性以提升学习，实验表明在包含方程发现等任务中，其表现优于纯监督或强化学习。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励设置下，基于自我对弈的 AlphaZero 训练难以在早期提供有效引导；HER 通过重标记失败轨迹为学习信号来缓解这一问题；但需要一个灵活框架来选择 relabeled 目标、策略目标和轨迹选择等参数。

Method: 提出 Adaptable HER(ours)，一个可调整的框架，将 HER 与 AlphaZero 结合，允许对 relabeled 目标、策略目标、轨迹选择等进行灵活配置，并将其应用于包括方程发现等任务的实验中。

Result: 证明对 HER 的可修改性有益，效果超过纯监督学习或强化学习；在 equation discovery 等任务中也显示出优势。

Conclusion: 可调整的 HER 框架能提升 AlphaZero 风格 MCTS 的学习效果，证明灵活配置 relabeled 目标等参数具有实际价值，为在稀疏奖励场景下的经典搜索问题提供了有效方案。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [41] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 提出一个对称性感知的核强化学习框架，结合不变核与乐观LSVI，在奖励与转移动力学中编码对称性，从而提升样本效率，并在自定义的Frozen Lake和二维放置设计任务中验证理论和实验结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的强化学习问题往往具有群对称性，这些结构化先验若被利用可以显著提高学习效率，尤其是在样本成本高昂的场景中。

Method: 引入对称性不变核来编码对称性，在乐观最小二乘值迭代（LSVI）框架下实现对称性不变性；给出在不变RKHS上的信息增益和覆盖数的新界，量化对称性带来的样本效率提升。

Result: 理论上给出不变RKHS下的信息增益和覆盖数界的新界；通过在自定义的Frozen Lake环境和二维放置设计任务的实验验证， symmetry-aware RL 相较于标准核方法表现更好。

Conclusion: 结构先验（对称性）的引入可以显著提升核化强化学习的样本效率，指向将来在实际问题中结合更广泛的对称性与其他结构先验的方向。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [42] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: NAP is an attention-based framework to aggregate predictions from frozen single-channel models, addressing the heterogeneity of polysomnography by learning a tri-axial attention over temporal, spatial, and predictor dimensions, enabling robust zero-shot generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: Polysomnography data are highly heterogeneous across modalities, channels, and protocols; existing models often rely on fixed modality subsets, underutilizing multimodal information and lacking adaptability to input variations.

Method: Introduce Neural Aggregator of Predictions (NAP) with a tri-axial attention mechanism that learns to combine multiple prediction streams from frozen pretrained single-channel models. The model adapts to varying input dimensions and captures temporal, spatial, and predictor-level dependencies to produce a final aggregated prediction.

Result: NAP outperforms individual predictors and simple ensembles, achieving state-of-the-art zero-shot generalization across multiple datasets in automated sleep staging from polysomnography.

Conclusion: The proposed approach leverages multimodal aggregation via attention and can be extended to other multimodal physiological applications beyond sleep staging.

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [43] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 提出在数据筛选/削减条件下的精确数据缩放规律，证明在某些条件下以小而经挑选的数据集可优于全量数据集，给出相位转变曲线并在 ImageNet 上验证。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的“更多数据更好”与“更少数据更佳”之间的悖论，解释数据质量与规模对泛化的影响。

Method: 建立一个理论框架，区分无标签与有标签的数据筛选规则（label-agnostic 与 label-aware），推导测试误差的缩放规律，得到相位转变条件，结合严格的统计分析与极限情形；并通过 ImageNet 实证验证。

Result: 给出准确的缩放曲线和相位边界，指出在某些数据规模和质量条件下，受控筛选可提升泛化，甚至减缓模型崩溃；证实对比经典缩放律的情形。

Conclusion: 框架可解释为何在 LLM 推理任务中也出现矛盾的筛选策略，并为未来数据筛选策略提供理论框架，帮助理解何时应“少即是多”。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [44] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO,一个不依赖学习 critic 的策略梯度方法，在对比实验中揭示 critic 在长时间任务中的必要性；高折扣因子通常有利于 GRPO，但在 HalfCheetah 情况除外；更小的分组规模通常表现更好，批量分组可能混淆不相关的剧本。总体而言 critic-free 方法在经典控制场景存在显著局限性，仅在短期任务或特定折扣设置下具有可行性。


<details>
  <summary>Details</summary>
Motivation: 系统性评估GRPO在经典单任务强化学习环境中的表现，检验是否需要学习的基线（critic），以及在不同折扣因子与分组策略下的影响。

Method: 对比实验，覆盖离散与连续控制任务，进行控制性消融：移除/保留基线、改变折扣因子 gamma、调整组(group)大小；并在 CartPole、HalfCheetah 等任务中评估策略梯度的性能。

Result: 在长时序任务中，无 critic 的基线普遍落后于 PPO，只有在如 CartPole 这类短期回报的情境下才有效；高折扣（gamma=0.99）总体有利，除了 HalfCheetah 由于早停缺乏，需用较低折扣（gamma=0.9）；较小的组大小优于较大组大小，表明批量分组策略可能混合无关的剧本。

Conclusion:  critic-free 方法在经典控制中存在明显局限，学习值函数仍具价值；GRPO 在特定条件下可作为替代方案，但需谨慎选择任务特性、折扣因子与分组策略，未来工作应探索更稳健的分组方法或增强基线的策略。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [45] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: A novel Byzantine-robust FL framework with learnable aggregation weights optimized jointly with the global model, achieving improved robustness under heterogeneous data and Byzantine attacks.


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，数据分布异质性和恶意客户端使鲁棒性难以保障。现有方法多使用固定聚合权重，难以适应不同攻击和数据分布，需要引入自适应权重以提升鲁棒性。

Method: 将聚合权重设为可学习参数，与全局模型参数一起进行交替最小化优化；提出具备收敛性保证的算法，并对其鲁棒性进行理论分析。

Result: 在多组数据集和攻击场景下，本文方法相较于现有的基线Byzantine-robust FL方法在高度数据异质性和恶意客户端比例较高的设置中表现更优。

Conclusion: 通过在聚合阶段引入自适应可学习的权重，提升了联邦学习的鲁棒性，且算法具有理论收敛性，实验结果支持其优越性。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [46] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: Extends Expressive Neural Networks by using Discrete Cosine Transform (DCT) parameterization for activation functions; shows efficiency, interpretability, and pruneability; achieves strong accuracy with compact models; enables pruning of up to 40% of activation coefficients with negligible loss.


<details>
  <summary>Details</summary>
Motivation: To enhance expressiveness of neural networks while preserving efficiency and interpretability by parameterizing activations with a structured, decorrelated DCT basis, enabling principled pruning.

Method: Represent each neuron's activation function via a DCT-based parameter vector; exploit orthogonality/boundedness of DCT to identify redundant coefficients; propose a pruning strategy that removes coefficients with little impact on performance; validate on classification and implicit neural representation tasks.

Result: Attains state-of-the-art accuracy with a low parameter count; pruning up to ~40% of activation coefficients without meaningful performance degradation; results supported across diverse tasks including classification and implicit representations.

Conclusion: Demonstrates a principled fusion of signal processing with neural network design; ENN offers a balanced trade-off among expressiveness, compactness, and interpretability, with a structured way to prune unneeded activation parameters.

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [47] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 即使在随机凸优化的标准设置下，平坦经验极小值也可能产生高至常数量级的总体风险；以平坦性为导向的算法（SA-GD、SAM）并不必然提升泛化，甚至可能收敛到尖锐极小值并导致高泛化误差；文中还给出基于稳定性的方法得到的泛化上界。


<details>
  <summary>Details</summary>
Motivation: 揭示泛化的根源并非仅由“平坦性”决定，质疑平坦极小值与泛化之间的直接关系；在非负、β-光滑的随机凸优化框架内检验这一假设的有效性。

Method: 在非负、β-光滑目标的随机凸优化设置中，分析“平坦极小值”与总体风险之间的关系；对两种以平坦性为导向的算法SA-GD（在局部邻域内取最大损失的梯度更新）与SAM（基于归一化上升的近似）进行理论分析，证明它们虽然能快速收敛到平坦极小值或收敛到平坦性，但总体风险仍可为Ω(1)；并以算法稳定性方法给出两者的泛化上界。

Result: 研究发现：1) 平坦的经验极小值可能对应常数阶的总体风险；2) SA-GD虽快速收敛到平坦极小值，泛化可仍然差；3) SAM可能收敛到尖锐极小值并具有Ω(1)的泛化风险；4) 给出基于稳定性的总体风险上界。

Conclusion: 平坦性并非泛化的充分条件，平坦性为导向的优化策略需谨慎解读其对泛化的影响，结果提示需要更细粒度的平坦性与泛化之间的关系建模与分析。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [48] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本论文是一篇关于模仿学习（IL）的综述性论文，梳理深度学习驱动下的IL最新进展，提出新的分类框架，并评估代表性工作、指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据和大规模模型在IL中的应用增长，亟需对研究脉络进行系统总结，厘清通用性、数据质量、泛化能力和示例偏移等长期挑战。

Method: 通过文献综述，批判性评估代表性工作，提出与现有分类不同的新 taxonomy，并讨论评估实践、优缺点与应用场景。

Result: 总结出IL研究的新趋势与方法学创新，覆盖从全状态-动作轨迹到部分观测或未标注序列的学习设置，提出一个有区别于现有框架的新 taxonomy。

Conclusion: 对未来研究提出方向，如改进评估规范、提升数据效率、提升泛化能力、解决示范质量与演示偏移等关键问题。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [49] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: 提出 TabGemma：一个模式无关的在-context 学习者，将表格行视为序列，解决数值标记不稳定和上下文规模受限的问题。通过将数字规范化为带符号的科学记数法、对 12B Gemma-3 模型进行以目标插补为目标的继续预训练，并在推理时用紧凑的 n-gram 检索选择 128k 字符内的示例。分类任务在语义丰富基准上实现新状态最先进，回归在小样本时有竞争力，但样本增多时落后于传统方法；表明在强语义任务上，结合数值处理和上下文检索，LLM 可以成为有效的表格内-context 学习者，未来需要在数值建模和长上下文扩展方面改进。


<details>
  <summary>Details</summary>
Motivation: 弥补大语言模型在混合文本、数值和分类字段的表格预测中的不足，解决数值标记不稳定和上下文窗口限制，使 LLM 能在表格任务中利用示例进行在-context 学习。

Method: 将行作为序列处理；对数值进行符号化的科学记数法规范化；对 12B Gemma-3 进行以目标插补为目标的持续预训练；推理阶段使用紧凑的 n-gram 检索在 128k 令牌窗口内选择信息性示例。

Result: 在语义丰富的基准上，分类任务达到新状态最先进；在数据充足时，分类呈现随上下文行数增加而单调提升；回归在小样本时具有竞争力，但数据增多时落后于传统方法；证实在数值处理和上下文检索的配合下，LLM 能作为有效的表格内-context 学习者；并指出数值建模和长上下文扩展是未来研究重点。

Conclusion: 表明在强语义任务中，搭配专门的数值处理与上下文检索，LLM 能成为有效的表格内-context 学习者，但仍需在数值建模和长上下文缩放方面进行改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [50] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: Formal verification of almost sure convergence for Q-learning and linear TD with Markovian samples using Lean 4 and Mathlib, via a Robbins-Siegmund framework; unified approach that can be extended to convergence rates; code available on GitHub; a step toward fully formalized convergent RL results.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning algorithms like Q-learning and linear TD are foundational but their convergence properties have been studied primarily in informal or numerical settings. Formal verification in a theorem prover ensures correctness and provides a solid foundation for future extensions (rates, other convergence modes). The work targets the intersection of RL theory and formal methods, enabling trusted convergence proofs.

Method: Formalize the almost sure convergence proof within Lean 4 using the Mathlib library, adopting a unified Robbins-Siegmund-based framework to handle Markovian sampling. The approach covers both Q-learning and linear TD, and is designed to be extended to convergence rates and other modes of convergence. The codebase is made available for reuse and extension.

Result: The paper reports a formal proof of almost sure convergence for Q-learning and linear TD with Markovian samples within Lean 4/Mathlib, under the Robbins-Siegmund framework. A unified formal framework is established, enabling potential extension to rates of convergence and other convergence notions.

Conclusion: This work represents a meaningful advance toward fully formalized convergent RL results, providing a reusable framework and a concrete codebase (GitHub) to verify and build upon convergence proofs for RL algorithms.

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [51] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN 是对 TabPFN v2 的简化轻量实现，面向教育用途，能在单 GPU 上快速预训练，显著降低资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有开源表格数据基础模型实现复杂、代码量大、缺乏文档和质量保障，难以理解和复现实验，亟需易用、可扩展的实现以促进教育和研究。

Method: 提供一个简化、轻量化的 nanoTabPFN 实现，使用预生成的训练数据和训练循环；在小数据设置下达到与传统 ML 基线相当的性能，预训练时间极短（单 GPU 1 分钟左右），相比 TabPFN v2 预训练速度提升约 160,000x。

Result: 在小数据环境下的性能与传统 ML 基线相当；培训时间极短；显著降低计算资源需求，适合教育用途。

Conclusion:  nanoTabPFN 使表格基础模型对学生和研究人员更易访问；代码开源可获取。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [52] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 本文提出一个面向IoT医疗健康的机器学习驱动框架，用于同时检测恶意攻击与识别设备异常，基于20万条记录对8个模型在监督、半监督、无监督三类学习范式进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决IoT医疗场景中的安全与可靠性挑战，提升对数据泄露、系统停机以及设备故障的早期检测，增强患者安全与对物联网医疗解决方案的信任。

Method: 在包含20万条记录的数据集上，比较XGBoost、KNN、GAN、VAE、One-Class SVM、Isolation Forest、GNN、LSTM Autoencoder等八种模型，覆盖有监督、半监督、无监督学习，使用F1、精确度、召回率、准确率、ROC-AUC和计算效率等多指标进行评估。

Result: XGBoost在异常检测中取得99%准确率和0.04s的计算开销；Isolation Forest在精确度与召回之间取得良好平衡；LSTM Autoencoder表现较差，存在较高延迟。对于攻击检测，KNN实现近乎完美的精度、召回和F1，成本最低（0.05s），VAE达到97%准确率；GAN成本最高且在准确率与ROC-AUC方面表现较弱。

Conclusion: 该框架有望提升IoT医疗安全的早期检测能力，降低数据泄露与系统停机风险，确保医疗设备的持续安全运行，从而保护患者健康并增强对IoT驱动医疗解决方案的信任。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [53] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 对DQN在有限环境中的探索策略（ε-greedy）与优先经验回放的影响进行系统评估，发现优先回放可加快收敛、提升回报；二者的组合与权衡给出对资源受限设置的实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究探索策略与记忆管理在DQN训练中的相互作用及对学习效率和稳定性的影响，并给出在资源受限场景下的鲁棒性培训建议。

Method: 系统性实验：在有限环境中比较均匀采样、无回放、优先回放三种经验回放策略，改变ε的衰减曲线，评估学习效率、收敛性和回报；在多次模拟中统计结果。

Result: 优先经验回放使收敛更快、回报更高；ε衰减曲线的选择显著影响学习效率和收敛行为；探索策略与记忆管理之间存在互补与权衡，提出在资源受限环境下的有效做法。

Conclusion: 提出针对DQN训练的实用建议：在资源受限场景下合理设计ε-衰减与采用优先回放的组合以提升鲁棒性与效率，同时注意不同策略的潜在 trade-offs。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [54] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 对后验重新校准方法进行了理论与实验分析，基于逻辑回归的参数化校准可以在二分类和多分类中提供理论依据；提出超越温度缩放的表达更丰富的校准方法，解决多分类时参数爆增与数据稀疏带来的过拟合问题，通过结构化正则化、鲁棒预处理和高效优化来平衡偏差-方差，实验显示显著优于现有的逻辑回归基校准方法，并提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 校准分类器的输出概率以更准确地反映不确定性；现有的温度缩放等简单方法在某些场景下不足以提供良好校准，需要更表达力强的校准函数，尤其在多分类场景中面临参数规模与数据有限的挑战。

Method: 将基于逻辑回归的参数化校准函数用于二分类和多分类，提出比标准温度缩放更 expressive 的校准方法；通过结构化正则化、鲁棒预处理和高效优化来管理多分类情形下的参数增加与过拟合风险，并提供易用的实现框架。

Result: 在实验中，新的校准方法在偏差-方差权衡上优于现有的基于逻辑回归的校准技术，取得显著提升；实现方面提供了高效且易用的开源实现，替代常见的温度、向量和矩阵缩放方法。

Conclusion: 提出的参数化校准框架及其正则化与优化策略，能够在提高多种分类任务的概率估计校准性方面提供实用且有效的解决方案，且具备良好的可扩展性和可用性。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [55] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ 提出一种行为自适应的 Q 学习框架，利用离线数据的隐式行为模型在在线微调阶段提供行为一致性信号，通过不确定性触发的对齐约束与逐步放松的自适应机制实现离线到在线的稳定转移，提升适应速率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在固定数据上训练，但部署到动态环境时易受分布偏移影响，且在未见状态-动作对上的价值估计不可靠，导致性能下降。需要一种机制在保持离线数据优势的同时，安全高效地过渡到在线探索。

Method: 提出 BAQ，通过一个隐式的行为模型（基于离线数据）为在线微调提供行为一致性信号。引入双目标损失：在高不确定性时将在线策略朝向离线行为对齐；随着在线经验增多逐渐放松该约束。该自适应机制抑制来自超出分布的误差传播，稳定早期在线更新，并加速对新场景的适应。

Result: 在标准基准上，BAQ 相较于现有离线到在线 RL 方法具有更快的恢复速度、更强的鲁棒性和更高的总体性能。

Conclusion: 隐式行为自适应为现实世界策略部署提供了一个有理论支撑且可操作的解决路径。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [56] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: Shrinkage-based baselines reduce the variance of policy-gradient updates in RLVR by blending per-prompt and cross-prompt means using Stein-type shrinkage, serving as a drop-in improvement without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Policy-gradient estimates in RLVR exhibit high variance due to using per-prompt empirical means as the center of trajectory rewards. This variance is especially problematic in the low-generation regime of LRMs, leading to unstable training.

Method: Introduce a shrinkage estimator for the baseline that combines per-prompt means and across-prompt means (inspired by Stein's paradox). The baseline is designed as a drop-in replacement, requiring no additional hyperparameters or computation.

Result: Theoretically, the shrinkage baseline yields lower-variance policy-gradient estimators across algorithms. Empirically, it outperforms standard empirical-mean baselines, reducing gradient variance and improving training stability.

Conclusion: A practical and effective baseline for RLVR: a shrinkage-based baseline that requires no extra hyperparameters or compute, with demonstrated variance reduction and stability improvements.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [57] [DecodeX: Exploring and Benchmarking of LDPC Decoding across CPU, GPU, and ASIC Platforms](https://arxiv.org/abs/2511.02952)
*Zhenzhou Qi,Yuncheng Yao,Yiming Li,Chung-Hsuan Tung,Junyao Zheng,Danyang Zhuo,Tingjun Chen*

Main category: cs.NI

TL;DR: DecodeX 作为一个跨平台 LDPC 解码基准框架，汇集 CPU、GPU、ASIC 的多种实现与测试向量，系统性地分析并行效率、数据搬运与 offload 开销对解码延迟的影响，为异构 vRAN 的自适应调度和协同设计提供量化依据。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟化无线接入网（vRAN）的广泛采用，异构计算资源成为实现高效基带处理的关键。缺乏统一基准来比较不同硬件平台上 LDPC 解码的性能与能耗，以及对调度策略的影响，因此需要一个可扩展、可比较的框架。

Method: DecodeX 集成了面向 CPU（FlexRAN）、GPU（Aerial、Sionna-RK）和 ASIC（ACC100）的 LDPC 解码实现、内核、API 与测试向量，且具有可扩展性以覆盖更多架构。通过分析线程化、内存管理、数据搬运与加速器卸载等执行维度，系统地量化在不同物理层参数下的解码延迟。

Result: 观察到并行效率与 offload 开销之间存在明显权衡；加速增益高度依赖数据搬运成本与工作负载粒度，且不同平台的优化重点各异。

Conclusion: 跨平台基准有助于在未来的异构 vRAN 场景中实现自适应调度与协同设计，从而提升基带处理的可扩展性与能效，支持 NextG 无线系统的发展。

Abstract: Emerging virtualized radio access networks (vRANs) demand flexible and
efficient baseband processing across heterogeneous compute substrates. In this
paper, we present DecodeX, a unified benchmarking framework for evaluating
low-density parity-check (LDPC) decoding acceleration across different hardware
platforms. DecodeX integrates a comprehensive suite of LDPC decoder
implementations, including kernels, APIs, and test vectors for CPUs (FlexRAN),
GPUs (Aerial and Sionna-RK), and ASIC (ACC100), and can be readily extended to
additional architectures and configurations. Using DecodeX, we systematically
characterize how different platforms orchestrate computation-from threading and
memory management to data movement and accelerator offload-and quantify the
resulting decoding latency under varying Physical layer parameters. Our
observations reveal distinct trade-offs in parallel efficiency and offload
overhead, showing that accelerator gains strongly depend on data-movement and
workload granularity. Building on these insights, we discuss how cross-platform
benchmarking can inform adaptive scheduling and co-design for future
heterogeneous vRANs, enabling scalable and energy-efficient baseband processing
for NextG wireless systems.

</details>


### [58] [Distributed Incast Detection in Data Center Networks](https://arxiv.org/abs/2511.03039)
*Yiming Zheng,Haoran Qi,Lirui Yu,Zhan Shu,Qing Zhao*

Main category: cs.NI

TL;DR: 提出一种基于概率假设检验的分布式数据中心交换机级发散检测方法，能够在首个分组就判断是否为 incast，显著提升检测速度和推断准确性。


<details>
  <summary>Details</summary>
Motivation: 数据中心内的 incast 流量会导致严重的性能下降（包丢失、延迟增加）。现有基于端口队列长度或梯度的阈值方法存在检测滞后与较高误差率，因此需要更快、准确的检测机制。

Method: 在交换机层面实现分布式检测，分析新流到达间隔，构建一个概率假设检验并优化检测阈值，以在首包阶段就判定是否属于 incast。

Result: 实验结果显示该方法在检测速度和推断准确性方面显著优于现有方法，如 MA-ECN、BurstRadar 与 Pulser。

Conclusion: 提出的分布式交换机级 incast 检测方法实现了更快且更准确的检测，有助于降低数据中心拥塞对性能的影响。

Abstract: Incast traffic in data centers can lead to severe performance degradation,
such as packet loss and increased latency. Effectively addressing incast
requires prompt and accurate detection. Existing solutions, including MA-ECN,
BurstRadar and Pulser, typically rely on fixed thresholds of switch port egress
queue lengths or their gradients to identify microburst caused by incast flows.
However, these queue length related methods often suffer from delayed detection
and high error rates. In this study, we propose a distributed incast detection
method for data center networks at the switch-level, leveraging a probabilistic
hypothesis test with an optimal detection threshold. By analyzing the arrival
intervals of new flows, our algorithm can immediately determine if a flow is
part of an incast traffic from its initial packet. The experimental results
demonstrate that our method offers significant improvements over existing
approaches in both detection speed and inference accuracy.

</details>


### [59] [Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing](https://arxiv.org/abs/2511.03159)
*Shuting Qiu,Fang Dong,Siyu Tan,Ruiting Zhou,Dian Shen,Patrick P. C. Lee,Qilin Fan*

Main category: cs.NI

TL;DR: 提出CoCaR：离线/在线联合优化的动态DNN缓存与请求路由框架，在边缘云环境中通过将DNN分解为子模型、离线线性规划与随机取整实现缓存与路由的近似最优，并通过CoCaR-OL实现对在线请求的自适应，显著提升推断精度与用户QoE。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器资源有限，难以在MEC缓存整套DNN；模型加载时间对用户体验影响显著但尚未被充分研究，因此需要在推断精度与加载延迟之间做出动态权衡。

Method: 将完整DNN分解为若干子模型以实现更细粒度的缓存与路由决策；提出离线算法CoCaR，基于线性规划和随机化取整来联合优化缓存和请求路由；开发在线变体CoCaR-OL以快速适应动态、不可预测的在线请求模式，结合仿真验证性能。

Result: 离线CoCaR实现了接近最优的缓存与路由配置，平均推断精度提升约46% relative; 在线CoCaR-OL在在线场景提升用户QoE至少32.3%相对于竞争基线。

Conclusion: 将动态DNN分解、离线优化与在线自适应结合，证实在MEC中通过分解子模型进行缓存与路由的联合优化具备实用性和显著收益，尤其在在线请求的动态性场景下表现出色。

Abstract: Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near
end-users, providing low-latency services and improving users' quality of
experience (QoE). However, caching all DNN models at edge servers with limited
capacity is difficult, and the impact of model loading time on QoE remains
underexplored. Hence, we introduce dynamic DNNs in edge scenarios,
disassembling a complete DNN model into interrelated submodels for more
fine-grained and flexible model caching and request routing solutions. This
raises the pressing issue of jointly deciding request routing and submodel
caching for dynamic DNNs to balance model inference precision and loading
latency for QoE optimization. In this paper, we study the joint dynamic model
caching and request routing problem in MEC networks, aiming to maximize user
request inference precision under constraints of server resources, latency, and
model loading time. To tackle this problem, we propose CoCaR, an offline
algorithm based on linear programming and random rounding that leverages
dynamic DNNs to optimize caching and routing schemes, achieving near-optimal
performance. Furthermore, we develop an online variant of CoCaR, named
CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online
request patterns. The simulation results demonstrate that the proposed CoCaR
improves the average inference precision of user requests by 46\% compared to
state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves
an improvement of no less than 32.3\% in user QoE over competitive baselines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [60] [Quantum-Classical Hybrid Encryption Framework Based on Simulated BB84 and AES-256: Design and Experimental Evaluation](https://arxiv.org/abs/2511.02836)
*Hector E Mozo*

Main category: cs.CR

TL;DR: 基于模拟BB84的量子密钥分发与AES-256的混合加密框架，包含HMAC完整性校验和可选的后量子数字签名，在Python实现的模块化仿真中验证对关键篡改、HMAC失败和文件损坏等攻击场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算对经典加密的威胁，提供一个可测试、易扩展的量子感知安全框架，通过量子钥生成与经典加密的结合来保障数据机密性与完整性。

Method: 使用模拟BB84实现量子密钥分发用于密钥生成；使用AES-256进行对称数据加密；引入HMAC进行数据完整性验证，提供可选的后量子数字签名以增强抗量子攻击的认证；整体架构在Python中模块化实现，包含模拟的量子密钥交换、加密和安全封装；通过攻击情景的可视化测试进行评估。

Result: 实验结果表明在模拟环境中，系统对关键篡改、HMAC失败和文件损坏等攻击具有鲁棒性，证明了方法的可行性与可扩展性，并展示了实现的模块化和研究的直观性。

Conclusion: 提出一个面向量子感知的实用基础，验证了混合量子密钥分发与经典加密的组合在现实安全体系中的可行性，为未来在真实量子设备上的实现和性能优化提供方向。

Abstract: This paper presents the design, implementation, and evaluation of a hybrid
encryption framework that combines quantum key distribution, specifically a
simulated BB84 protocol, with AES-256 encryption. The system enables secure
file encryption by leveraging quantum principles for key generation and
classical cryptography for data protection. It introduces integrity validation
mechanisms, including HMAC verification and optional post-quantum digital
signatures, ensuring robustness even in the presence of quantum-capable
adversaries. The entire architecture is implemented in Python, with modular
components simulating quantum key exchange, encryption, and secure packaging.
Experimental results include visual testing of various attack scenarios, such
as key tampering, HMAC failure, and file corruption, demonstrating the
effectiveness and resilience of the approach. The proposed solution serves as a
practical foundation for quantum-aware cybersecurity systems.

</details>


### [61] [Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains](https://arxiv.org/abs/2511.02868)
*M. Z. Haider,M. U Ghouri,Tayyaba Noreen,M. Salman*

Main category: cs.CR

TL;DR: Neuromorphic共识PoSN通过尖脉冲神经元实现能源高效的区块链共识，利用尖峰序列、竞争放电与神经同步进行并行、事件驱动的区块最终化，在神经形态平台上展现显著的能耗、吞吐和收敛性提升。


<details>
  <summary>Details</summary>
Motivation: 解决PoW/PoS在资源消耗、中心化风险以及扩展性、延迟、能源效率方面的瓶颈，迫切需要面向物联网、边缘计算和大规模分布式系统的可持续区块链解决方案。

Method: 将交易编码为尖峰序列；通过竞争性放电动态选出领导者；通过神经同步完成区块的最终化；在神经形态体系结构上实现混合架构；基于Nengo和PyNN在神经形态平台上进行实现与仿真；进行能效、吞吐和收敛性对比实验。

Result: 实验结果显示在能效、吞吐量与收敛性方面相比PoB和PoR具有显著提升。

Conclusion: PoSN为面向物联网、边缘与大规模分布式系统的可持续、可适应区块链奠定基础。

Abstract: Blockchain systems face persistent challenges of scalability, latency, and
energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW)
and Proof-of-Stake (PoS) either consume excessive resources or risk
centralization. This paper proposes \textit{Proof-of-Spiking-Neurons (PoSN)}, a
neuromorphic consensus protocol inspired by spiking neural networks. PoSN
encodes transactions as spike trains, elects leaders through competitive firing
dynamics, and finalizes blocks via neural synchronization, enabling parallel
and event-driven consensus with minimal energy overhead. A hybrid system
architecture is implemented on neuromorphic platforms, supported by simulation
frameworks such as Nengo and PyNN. Experimental results show significant gains
in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN
establishes a foundation for sustainable, adaptive blockchains suitable for
IoT, edge, and large-scale distributed systems.

</details>


### [62] [PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat](https://arxiv.org/abs/2511.02993)
*Yixuan Gao,Tanvir Ahmed,Zekun Chang,Thijs Roumen,Rajalakshmi Nandakumar*

Main category: cs.CR

TL;DR: PrivyWave introduces a key-based physical obfuscation approach for selective privacy in heartbeat sensing across mmWave and acoustic modalities. Authorized devices can filter decoys to recover accurate heart-rate measurements, while unauthorized sensors receive a mix of real and decoy signals that are indistinguishable without the secret key.


<details>
  <summary>Details</summary>
Motivation: Wireless heartbeat sensing via RF and acoustic signals raises privacy concerns. Existing solutions either disable sensing for all devices or operate post-data collection, failing to support selective access for authorized devices.

Method: Generate decoy heartbeat signals at cryptographically determined frequencies to create a mixture of real and decoy signals. Unauthorized sensors cannot distinguish decoys from real signals, while authorized sensors use the cryptographic key to filter out decoys and recover accurate measurements. The system operates across mmWave radar and acoustic sensing without per-modality customization.

Result: Evaluation with 13 participants shows strong protection: mmWave radar - unauthorized mean absolute error 21.3 BPM vs authorized 5.8 BPM; acoustic sensing - unauthorized 42.0 BPM vs authorized 9.7 BPM. The approach maintains utility for authorized devices and provides cryptographic obfuscation guarantees across distances of 30–150 cm, orientations with a 120-degree field of view, and diverse indoor environments.

Conclusion: Physical-layer obfuscation is a viable method for selective privacy in pervasive health monitoring. PrivyWave demonstrates cross-modality applicability, maintaining measurement utility for authorized devices while substantially degrading unauthorized sensing across multiple sensing modalities.

Abstract: Wireless sensing technologies can now detect heartbeats using radio frequency
and acoustic signals, raising significant privacy concerns. Existing privacy
solutions either protect from all sensing systems indiscriminately preventing
any utility or operate post-data collection, failing to enable selective access
where authorized devices can monitor while unauthorized ones cannot. We present
a key-based physical obfuscation system, PrivyWave, that addresses this
challenge by generating controlled decoy heartbeat signals at
cryptographically-determined frequencies. Unauthorized sensors receive a
mixture of real and decoy signals that are indistinguishable without the secret
key, while authorized sensors use the key to filter out decoys and recover
accurate measurements. Our evaluation with 13 participants demonstrates
effective protection across both sensing modalities: for mmWave radar,
unauthorized sensors show 21.3 BPM mean absolute error while authorized sensors
maintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error
increases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system
operates across multiple sensing modalities without per-modality customization
and provides cryptographic obfuscation guarantees. Performance benchmarks show
robust protection across different distances (30-150 cm), orientations
(120{\deg} field of view), and diverse indoor environments, establishing
physical-layer obfuscation as a viable approach for selective privacy in
pervasive health monitoring.

</details>


### [63] [Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods](https://arxiv.org/abs/2511.03020)
*Fatimo Adenike Adeniya*

Main category: cs.CR

TL;DR: A hybrid forecasting and machine learning framework applied to e-commerce cyberattacks using the VCDB dataset, integrating Auto ARIMA forecasting, significance testing (Mann-Whitney U), ANOVA, and ensemble models (XGBoost, LightGBM, CatBoost) to detect and forecast attack patterns; CatBoost yields best performance; findings show spikes during holidays/Black Friday with higher indicators for PII breaches; discusses ethical considerations and practical implications, with limitations and future real-time detection directions.


<details>
  <summary>Details</summary>
Motivation: Cyberattacks on e-commerce platforms are increasingly sophisticated, threatening consumer trust and operational continuity. There is a need for an interpretable, predictive framework that can forecast temporal risk and classify breach types using historical data.

Method: Data: Verizon VCDB dataset. Temporal forecasting with Auto ARIMA and significance testing. Mann-Whitney U test (U = 2,579,981.5, p = 0.0121) showed significantly more severe attacks during holiday periods. ANOVA examined seasonal variation. Ensemble ML models (XGBoost, LightGBM, CatBoost) for predictive classification. Evaluation metrics include accuracy, F1 score, and ROC AUC. Addressed class imbalance and ethical considerations.

Result: Findings indicate recurrent attack spikes during high-risk periods such as Black Friday and holidays. Breaches involving PII exhibit elevated threat indicators. CatBoost achieved the best performance with accuracy 85.29%, F1 0.2254, ROC AUC 0.8247, among the tested models.

Conclusion: The proposed framework combines seasonal forecasting with interpretable ensemble learning to enable temporal risk anticipation and breach-type classification. This supports proactive cybersecurity resource allocation. The study acknowledges limitations including class imbalance and reliance on historical data, and points to future work in real-time threat detection and bias assessment.

Abstract: Cyberattacks on e-commerce platforms have grown in sophistication,
threatening consumer trust and operational continuity. This research presents a
hybrid analytical framework that integrates statistical modelling and machine
learning for detecting and forecasting cyberattack patterns in the e-commerce
domain. Using the Verizon Community Data Breach (VCDB) dataset, the study
applies Auto ARIMA for temporal forecasting and significance testing, including
a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday
shopping events experienced significantly more severe cyberattacks than
non-holiday periods. ANOVA was also used to examine seasonal variation in
threat severity, while ensemble machine learning models (XGBoost, LightGBM, and
CatBoost) were employed for predictive classification. Results reveal recurrent
attack spikes during high-risk periods such as Black Friday and holiday
seasons, with breaches involving Personally Identifiable Information (PII)
exhibiting elevated threat indicators. Among the models, CatBoost achieved the
highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247).
The framework uniquely combines seasonal forecasting with interpretable
ensemble learning, enabling temporal risk anticipation and breach-type
classification. Ethical considerations, including responsible use of sensitive
data and bias assessment, were incorporated. Despite class imbalance and
reliance on historical data, the study provides insights for proactive
cybersecurity resource allocation and outlines directions for future real-time
threat detection research.

</details>


### [64] [Bayesian Advantage of Re-Identification Attack in the Shuffle Model](https://arxiv.org/abs/2511.03213)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.CR

TL;DR: 在shuffle模型下对Bayesian re-identification的优势进行系统分析，给出β_n(P,Q)的精确表达与渐近性质，评估在多种情形的数值表现，并给出additive/multiplicative Bayesian_advantage的界，以及edge到shuffle-DP的上界


<details>
  <summary>Details</summary>
Motivation: 理解在数据混洗下，攻击者通过Bayesian推断重新识别单一用户样本的能力，以及将Bayesian分析与总变差距离联系起来；并扩展到shuffle-DP情景下的上界，揭示隐私保护的强度。

Method: 建立基本设置：n个样本中一个来自P，其余来自Q，样本全体随机混洗，定义β_n(P,Q)及Adv^+, Adv^×；推导β_n的解析表达、渐近性质及在若干情形的评估；给出总变差距离的紧界及两者的关系；扩展至每个用户经ε-DP本地随机器后再混洗，给出攻击成功上界≤e^ε/n。

Result: 给出β_n(P,Q)的精确表达式与渐近表述，给出Adv^+与Adv^×的界，建立 additive Bayesian advantage 与 total variation distance 的近似紧界；首次给出shuffle-DP情景中攻击成功概率的上界为 e^ε / n。

Conclusion: 在shuffle模型下，对Bayesian re-identification的能力有系统性揭示，理论结果清晰地刻画了混洗对识别概率的影响，并为评估shuffle-DP的隐私保护强度提供了 rigorous 的界限与拓展。

Abstract: The shuffle model, which anonymizes data by randomly permuting user messages,
has been widely adopted in both cryptography and differential privacy. In this
work, we present the first systematic study of the Bayesian advantage in
re-identifying a user's message under the shuffle model. We begin with a basic
setting: one sample is drawn from a distribution $P$, and $n - 1$ samples are
drawn from a distribution $Q$, after which all $n$ samples are randomly
shuffled. We define $\beta_n(P, Q)$ as the success probability of a
Bayes-optimal adversary in identifying the sample from $P$, and define the
additive and multiplicative Bayesian advantages as $\mathsf{Adv}_n^{+}(P, Q) =
\beta_n(P,Q) - \frac{1}{n}$ and $\mathsf{Adv}_n^{\times}(P, Q) = n \cdot
\beta_n(P,Q)$, respectively.
  We derive exact analytical expressions and asymptotic characterizations of
$\beta_n(P, Q)$, along with evaluations in several representative scenarios.
Furthermore, we establish (nearly) tight mutual bounds between the additive
Bayesian advantage and the total variation distance.
  Finally, we extend our analysis beyond the basic setting and present, for the
first time, an upper bound on the success probability of Bayesian attacks in
shuffle differential privacy. Specifically, when the outputs of $n$ users--each
processed through an $\varepsilon$-differentially private local randomizer--are
shuffled, the probability that an attacker successfully re-identifies any
target user's message is at most $e^{\varepsilon}/n$.

</details>


### [65] [Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles](https://arxiv.org/abs/2511.03319)
*Giulio Caldarelli,Massimiliano Ornaghi*

Main category: cs.CR

TL;DR: 本文提出一个将 Delphic 传统与现代区块链预言机进行比较的理论框架，利用区块链预言机分类学并对167条 Delphic 询问进行词汇分析，以揭示问题类型与回答质量之间的关系，并为改进区块链预言机和解释古代神谕机制提供方法。


<details>
  <summary>Details</summary>
Motivation: 解决预言机信息真实性和无偏性难题；将古代与计算机科学中的预言机进行对照，借鉴 Delphi 的经验以提升区块链等自动化系统的可靠性；在去中心化环境中，呈现分析框架。

Method: 概念性框架构建，借助区块链预言机分类学进行比较分析；对167条 Delphic 询问进行词汇分析；提出跨领域的共同点与差异。

Result: 提出了一个用于解释和分类古代与计算机预言机的新框架；从 Delphi 的语言和查询结构中提取可用于提升区块链预言机回答质量的策略；为两领域的研究提供互补视角。

Conclusion: 框架揭示了传统与现代预言机的共性，丰富各自领域的分析工具，并提供可操作的改进策略，用于提升区块链预言机的可靠性，同时为解释和分类其他古代预言机制奠定基础。

Abstract: The oracle problem refers to the inability of an agent to know if the
information coming from an oracle is authentic and unbiased. In ancient times,
philosophers and historians debated on how to evaluate, increase, and secure
the reliability of oracle predictions, particularly those from Delphi, which
pertained to matters of state. Today, we refer to data carriers for automatic
machines as oracles, but establishing a secure channel between these oracles
and the real world still represents a challenge. Despite numerous efforts, this
problem remains mostly unsolved, and the recent advent of blockchain oracles
has added a layer of complexity because of the decentralization of blockchains.
This paper conceptually connects Delphic and modern blockchain oracles,
developing a comparative framework. Leveraging blockchain oracle taxonomy,
lexical analysis is also performed on 167 Delphic queries to shed light on the
relationship between oracle answer quality and question type. The presented
framework aims first at revealing commonalities between classical and
computational oracles and then at enriching the oracle analysis within each
field. This study contributes to the computer science literature by proposing
strategies to improve the reliability of blockchain oracles based on insights
from Delphi and to classical literature by introducing a framework that can
also be applied to interpret and classify other ancient oracular mechanisms.

</details>


### [66] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 开源权重大模型的安全与防护性研究：对8个开源LLM进行自动化对抗测试，评估单-turn和多-turn的注入与越狱攻击的鲁棒性，发现广泛漏洞，且多-turn攻击成功率显著高于单-turn。结论指出需在设计中优先考虑安全、采用分层防护，以实现企业与公共领域的安全部署。


<details>
  <summary>Details</summary>
Motivation: 开源权重模型在研究与应用上具有广泛价值，但其安全性与可部署性仍未知。本研究旨在揭示潜在漏洞及其对后续微调和部署的影响。

Method: 使用自动对抗测试，衡量模型对单-turn和多-turn提示注入、越狱攻击的抵抗力；比较不同导向的模型（如Llama 3.3、Qwen 3、Google Gemma 3）在多-turn情境下的表现。

Result: 所有测试模型均存在漏洞；多-turn攻击的成功率介于25.86%–92.78%，相比单-turn基线提升约2倍至10倍。能力导向模型更易受攻击，安全导向设计表现更平衡。

Conclusion: 需要在开源权重模型部署中引入分层安全控制与安全优先的设计理念，以降低运营与伦理风险，推动在企业与公共域的安全、可靠部署。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


### [67] [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271)
*Yize Liu,Yunyun Hou,Aina Sui*

Main category: cs.CR

TL;DR: 将多轮对话中的攻击过程建模为带权图路径规划，提出基于人工蜜蜂群算法的多轮越狱搜索框架ABC，显著提升最优攻击路径搜索效率并降低查询次数，实验在多家模型上实现高攻击成功率与低开销。


<details>
  <summary>Details</summary>
Motivation: 弥补现有红队评估在探索成功对话轨迹和攻击开销方面的不足，提升多轮越狱的效率与覆盖率。

Method: 将多轮攻击抽象为动态图权图的路径规划问题，提出增强的ABC算法，包含雇佣蜂、觅蜜蜂、侦察蜂的协作搜索机制，进行全局与局部搜索以找到高效攻击路径。

Result: 在三款开源模型和两款专有模型上，攻击成功率均>90%，峰值98%（GPT-3.5-Turbo），平均查询数仅26次，显著优于基线。

Conclusion: 提出的ABC框架有效提升红队攻击的效率与覆盖度，为未来的安全评估与防护研究提供了可扩展的路径规划思路。

Abstract: Large Language Models (LLMs) have been widely deployed across various
applications, yet their potential security and ethical risks have raised
increasing concerns. Existing research employs red teaming evaluations,
utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.
However, these approaches often lack exploration of successful dialogue
trajectories within the attack space, and they tend to overlook the
considerable overhead associated with the attack process. To address these
limitations, this paper first introduces a theoretical model based on
dynamically weighted graph topology, abstracting the multi-turn attack process
as a path planning problem. Based on this framework, we propose ABC, an
enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a
collaborative search mechanism with employed, onlooker, and scout bees. This
algorithm significantly improves the efficiency of optimal attack path search
while substantially reducing the average number of queries required. Empirical
evaluations on three open-source and two proprietary language models
demonstrate the effectiveness of our approach, achieving attack success rates
above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and
outperforming existing baselines. Furthermore, it achieves comparable success
with only 26 queries on average, significantly reducing red teaming overhead
and highlighting its superior efficiency.

</details>


### [68] [LaMoS: Enabling Efficient Large Number Modular Multiplication through SRAM-based CiM Acceleration](https://arxiv.org/abs/2511.03341)
*Haomin Li,Fangxin Liu,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.CR

TL;DR: LaMoS is an SRAM-based computational in-memory design for large-number modular multiplication using Barrett's method, achieving high scalability and area efficiency with 7.02x speedup over prior SRAM-CiM designs.


<details>
  <summary>Details</summary>
Motivation: Modular multiplication is a bottleneck in privacy-preserving computing (HE, ZKP). High-bit-width operations are needed for ECC and RSA; existing CiM approaches either target only low bit-width or incur high scaling costs for large numbers.

Method: Map Barrett's modular multiplication to SRAM CiM macros for high bit-width; design an efficient CiM architecture and dataflow; optimize scalability with workload-grouping-based mapping.

Result: LaMoS delivers about 7.02× speedup and reduces high-bit-width scaling costs compared to existing SRAM-based CiM schemes.

Conclusion: LaMoS provides a scalable, area-efficient SRAM-CiM solution for large-number modular multiplication, enabling faster cryptographic computations for ECC/RSA and similar workloads.

Abstract: Barrett's algorithm is one of the most widely used methods for performing
modular multiplication, a critical nonlinear operation in modern privacy
computing techniques such as homomorphic encryption (HE) and zero-knowledge
proofs (ZKP). Since modular multiplication dominates the processing time in
these applications, computational complexity and memory limitations
significantly impact performance. Computing-in-Memory (CiM) is a promising
approach to tackle this problem. However, existing schemes currently suffer
from two main problems: 1) Most works focus on low bit-width modular
multiplication, which is inadequate for mainstream cryptographic algorithms
such as elliptic curve cryptography (ECC) and the RSA algorithm, both of which
require high bit-width operations; 2) Recent efforts targeting large number
modular multiplication rely on inefficient in-memory logic operations,
resulting in high scaling costs for larger bit-widths and increased latency. To
address these issues, we propose LaMoS, an efficient SRAM-based CiM design for
large-number modular multiplication, offering high scalability and area
efficiency. First, we analyze the Barrett's modular multiplication method and
map the workload onto SRAM CiM macros for high bit-width cases. Additionally,
we develop an efficient CiM architecture and dataflow to optimize large-number
modular multiplication. Finally, we refine the mapping scheme for better
scalability in high bit-width scenarios using workload grouping. Experimental
results show that LaMoS achieves a $7.02\times$ speedup and reduces high
bit-width scaling costs compared to existing SRAM-based CiM designs.

</details>


### [69] [Federated Anonymous Blocklisting across Service Providers and its Application to Group Messaging](https://arxiv.org/abs/2511.03486)
*David Soler,Carlos Dafonte,Manuel Fernández-Veiga,Ana Fernández Vilas,Francisco J. Nóvoa*

Main category: cs.CR

TL;DR: 提出了一种名为 FAB 的联邦化匿名封禁（Federated Anonymous Blocklisting）方法。通过分布式 Realms 及互信关系，在认证时需证明自己未在任意受信 realms 被封禁，从而实现对匿名群组的可扩展、隐私保护的封禁机制，并将该机制实现并集成到 Messaging Layer Security (MLS) 协议中，性能不随封禁名单大小变化，且不需逐条处理新添加项。


<details>
  <summary>Details</summary>
Motivation: 在匿名通讯场景中实现高效且隐私友好的群组管理与 moderation。现有的匿名封禁（AB）方案多依赖中心化服务，存在可扩展性和隐私风险。FAB 通过分布式 Realm 和跨 Realm 信任，降低对单一实体的依赖，并提高对跨域封禁的可扩展性。

Method: 提出分布式 Realms 架构、建立 Realm 之间的信任关系、在认证阶段证明用户未在任一受信 Realm 被封禁、并实现与 MLS 的集成。对实现进行评估，强调其性能与可应用性。

Result: 实现表明 FAB 的性能不受当前封禁名单规模影响，且无需对新增封禁条目进行额外处理；并演示了在现实世界的消息群组场景中与 MLS 的兼容性与应用性。

Conclusion: FAB 提供一种可扩展、隐私保护的跨联邦匿名封禁机制，适用于匿名通讯场景下的群组治理，并可与现有加密协议栈（如 MLS）无缝集成。

Abstract: Instant messaging has become one of the most used methods of communication
online, which has attracted significant attention to its underlying
cryptographic protocols and security guarantees. Techniques to increase privacy
such as End-to-End Encryption and pseudonyms have been introduced. However,
online spaces such as messaging groups still require moderation to prevent
misbehaving users from participating in them, particularly in anonymous
contexts.. In Anonymous Blocklisting (AB) schemes, users must prove during
authentication that none of their previous pseudonyms has been blocked,
preventing misbehaving users from creating new pseudonyms. In this work we
propose an alternative \textit{Federated Anonymous Blocklisting} (FAB) in which
the centralised Service Provider is replaced by small distributed Realms, each
with its own blocklist. Realms can establish trust relationships between each
other, such that when users authenticate to a realm, they must prove that they
are not banned in any of its trusted realms. We provide an implementation of
our proposed scheme; unlike existing AB constructions, the performance of ours
does not depend on the current size of the blocklist nor requires processing
new additions to the blocklist. We also demonstrate its applicability to
real-world messaging groups by integrating our FAB scheme into the Messaging
Layer Security protocol.

</details>


### [70] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 提出一个适用于LLM水印的分类与评估框架，映射欧盟AI法案的四项标准，发现现有方法未能同时满足所有要求，并提出未来在LLM低层架构嵌入水印的研究方向。


<details>
  <summary>Details</summary>
Motivation: 为在欧盟框架下实现可信AI，需将水印化方法与法案的可验证性、互操作性、有效性和鲁棒性等要求对齐，解决水印技术的异质性和评估缺乏统一标准的问题。

Method: 1) Watermarking Categorisation：按LLM生命周期阶段划分水印方法（训练前、训练中、训练后，以及下一个词分布/采样阶段）。2) Watermarking Evaluation：将法案的四项标准映射到鲁棒性、可检测性、以及LLM质量的评估，并提出互操作性的三個规范维度。3) Watermarking Comparison：将现有水印方法与上述标准进行对比，发现无人能同时满足四项标准。

Result: 对水印方法的四项标准的操作化评估框架；提出互操作性的 Normative Dimensions；现有方法尚不能同时满足四项标准；并且有涌现的实证测试，为在LLM低层架构嵌入水印的研究指明方向。

Conclusion: 该工作提供把EU要求转化为可操作评估的框架，指向未来在LLM低层架构嵌入水印的研究方向，以提升可检测性、鲁棒性、互操作性和覆盖性。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [71] [AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair](https://arxiv.org/abs/2511.02845)
*Yuxuan Liu,Chiya Zhang,Yifeng Yuan,Chunlong He,Weizheng Zhang,Gaojie Chen*

Main category: eess.SP

TL;DR: 在硬件受限的条件下，AI-enhanced Wi-Fi sensing 能够超越传统雷达分辨率的原因在于两大因素：先验信息和时序相关性；仅用单发收发单元就能实现在人体姿态估计和室内定位等任务上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 揭示 AI 对 Wi-Fi 感知在带宽和天线数受限场景下的理论根源，量化先验信息与时序相关性对感知性能的贡献，并为单发对系统设计与部署提供理论和实验支持。

Method: 建立理论分析框架，指明在硬件约束下 AI 提升的主要来源；设计基于单对收发端的 AI 感知系统；以人体姿态估计和室内定位为用例，开展对比实验以验证两大贡献（先验信息与时序相关性）的作用。

Result: 实验结果表明：AI 提升的核心来自于先验信息和时序相关性两方面；在受限硬件条件下，AI 能通过生成合适的细节信息以及利用时间上的信息相关性来降低感知误差的上限，从而实现性能改进；单对收发端的系统在人体姿态估计与室内定位任务中得到可观提升。

Conclusion: 为 AI 加速的 Wi-Fi 感知提供理论解释：在硬件受限场景下，先验信息与时序相关性共同驱动感知性能提升；未来工作可聚焦于更强的先验建模、鲁棒的时序融合，以及在实际部署中的普适性与泛化能力。

Abstract: The advancement of next-generation Wi-Fi technology heavily relies on sensing
capabilities, which play a pivotal role in enabling sophisticated applications.
In response to the growing demand for large-scale deployments, contemporary
Wi-Fi sensing systems strive to achieve high-precision perception while
maintaining minimal bandwidth consumption and antenna count requirements.
Remarkably, various AI-driven perception technologies have demonstrated the
ability to surpass the traditional resolution limitations imposed by radar
theory. However, the theoretical underpinnings of this phenomenon have not been
thoroughly investigated in existing research. In this study, we found that
under hardware-constrained conditions, the performance gains brought by AI to
Wi-Fi sensing systems primarily originate from two aspects: prior information
and temporal correlation. Prior information enables the AI to generate
plausible details based on vague input, while temporal correlation helps reduce
the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system
using a single transceiver pair and designed experiments focusing on human pose
estimation and indoor localization to validate the theoretical claims. The
results confirm the performance gains contributed by temporal correlation and
prior information.

</details>


### [72] [EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding](https://arxiv.org/abs/2511.02848)
*Shantanu Sarkar,Piotr Nabrzyski,Saurabh Prasad,Jose Luis Contreras-Vidal*

Main category: eess.SP

TL;DR: 提出 EEGReXferNet，通过跨被试迁移学习的轻量化生成-AI框架，在保持实时性和跨被试泛化的同时提升 EEG 的时频空间分辨率并显著减轻模型重量。


<details>
  <summary>Details</summary>
Motivation: 解决EEG降噪/伪迹去除中存在的手动干预需求、用滤波/重建可能丢失重要神经特征，以及现有生成模型在时空频敏感性和计算成本方面的不足，尤其在实时BCI场景的应用受限。

Method: 提出 EEGReXferNet，基于 Keras TensorFlow v2.15.1 的模块化架构；通过考虑临近通道的体积传导、带域卷积编码和滑动窗口下的动态潜在特征提取实现子空间重建；引入参考基准缩放以实现窗口间连续性，并通过跨被试迁移学习提升泛化。

Result: 在空间-时间-频谱分辨率方面取得高效提升：平均 PSD 相关性≥0.95；平均声谱图 RV-系数≥0.85；模型权重减轻约45%，降低过拟合风险，同时保持实时 EEG 预处理的计算效率。

Conclusion: 该框架可实现鲁棒的跨被试实时 EEG 预处理，提升 BCIs 及神经生理研究中的信号质量与应用可用性。

Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for
monitoring brain activity, but low signal-to-noise ratios (SNR) due to various
artifacts often compromise its utility. Conventional artifact removal methods
require manual intervention or risk suppressing critical neural features during
filtering/reconstruction. Recent advances in generative models, including
Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),
have shown promise for EEG reconstruction; however, these approaches often lack
integrated temporal-spectral-spatial sensitivity and are computationally
intensive, limiting their suitability for real-time applications like
brain-computer interfaces (BCIs). To overcome these challenges, we introduce
EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction
via cross-subject transfer learning - developed using Keras TensorFlow
(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume
conduction across neighboring channels, band-specific convolution encoding, and
dynamic latent feature extraction through sliding windows. By integrating
reference-based scaling, the framework ensures continuity across successive
windows and generalizes effectively across subjects. This design improves
spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean
spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate
overfitting, and maintains computational efficiency for robust, real-time EEG
preprocessing in neurophysiological and BCI applications.

</details>


### [73] [Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData](https://arxiv.org/abs/2511.02849)
*Beyza Cinar,Maria Maleshkova*

Main category: eess.SP

TL;DR: 通过对DiaData进行数据清洗和质量提升，结合多源T1D数据，评估心率与血糖的关系并基于ResNet进行低血糖预测，数据质量提升对模型性能有显著提升（2-3%相对增益，若用更多数据则提升达7%）。


<details>
  <summary>Details</summary>
Motivation: 面向个体化治疗，医疗数据质量对分析与预测至关重要。T1D数据存在离群值、噪声、缺失和小样本问题，需通过数据清洗与更大规模数据来提升分析可靠性与预测性能。

Method: 1) 使用四分位距(IQR)识别离群值并将其替换为缺失值；2) 对小于等于25分钟的间隙采用线性插值，大于等于30且小于120分钟的间隙采用Stineman插值，以获得更真实的血糖曲线；3) 清洗后分析血糖与心率在发病前15至60分钟的相关性，发现中等相关性；4) 用ResNet在DiaData的Maindatabase与Subdatabase II上进行低血糖发作的前瞻预测，预测时间可达2小时。

Result: 数据清洗后，模型在更多数据和数据质量提升下分别实现7%相对提升（更多数据）和2-3%提升（质量 refined 数据对比原始数据）。同时揭示了血糖与心率在接近低血糖时段的相关性。

Conclusion: 通过系统的数据清洗和高质量数据集的构建，提升了对低血糖发作的预测能力，并为未来将多源个体数据用于个体化治疗提供证据与方法论基础。

Abstract: Individualized therapy is driven forward by medical data analysis, which
provides insight into the patient's context. In particular, for Type 1 Diabetes
(T1D), which is an autoimmune disease, relationships between demographics,
sensor data, and context can be analyzed. However, outliers, noisy data, and
small data volumes cannot provide a reliable analysis. Hence, the research
domain requires large volumes of high-quality data. Moreover, missing values
can lead to information loss. To address this limitation, this study improves
the data quality of DiaData, an integration of 15 separate datasets containing
glucose values from 2510 subjects with T1D. Notably, we make the following
contributions: 1) Outliers are identified with the interquartile range (IQR)
approach and treated by replacing them with missing values. 2) Small gaps
($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30
and $<$ 120 min) with Stineman interpolation. Based on a visual comparison,
Stineman interpolation provides more realistic glucose estimates than linear
interpolation for larger gaps. 3) After data cleaning, the correlation between
glucose and heart rate is analyzed, yielding a moderate relation between 15 and
60 minutes before hypoglycemia ($\le$ 70 mg/dL). 4) Finally, a benchmark for
hypoglycemia classification is provided with a state-of-the-art ResNet model.
The model is trained with the Maindatabase and Subdatabase II of DiaData to
classify hypoglycemia onset up to 2 hours in advance. Training with more data
improves performance by 7% while using quality-refined data yields a 2-3% gain
compared to raw data.

</details>


### [74] [Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation](https://arxiv.org/abs/2511.02851)
*Rushuang Zhou,Yuan-Ting Zhang,M. Jamal Deen,Yining Dong*

Main category: eess.SP

TL;DR: LiteHeart 是一个半监督知识蒸馏框架，面向可穿戴设备的低成本心脏智能，通过区域感知蒸馏和跨层互信息，缩小 LCCI 与 HCCI 的诊断差距，并在有限监督下提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高成本心脏智能依赖大量数据和算力，难以用于日常监测；低成本方案虽可穿戴，但性能显著落后。需要在资源受限的情况下提高诊断性能。

Method: 引入区域感知蒸馏模块，使模型关注 ECG 的诊断关键区域；引入跨层互信息模块以对齐 LCCI 与 HCCI 的决策过程；采用半监督训练策略提高少标注场景下的鲁棒性；进行知识蒸馏，将 HCCI 作为教师模型，LCCI 学生模型学习。

Result: 在覆盖 5 套数据、涉及 38 种心血管疾病的评估中，LiteHeart 显著缩小 LCCI 与 HCCI 的性能差距，在宏 F1 上较现有方法提升 4.27%〜7.10%。

Conclusion: LiteHeart 有望提升低成本心脏智能系统的诊断能力，推动可扩展、经济、准确的日常心脏健康监测。

Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac
monitoring is hindered by its reliance on extensive medical data and high
computational resources. Low-cost cardiac intelligence (LCCI) offers a
promising alternative by using wearable device data, such as 1-lead
electrocardiogram (ECG), but it suffers from a significant diagnostic
performance gap compared to high-cost cardiac intelligence (HCCI). To bridge
this gap, we propose LiteHeart, a semi-supervised knowledge distillation
framework. LiteHeart introduces a region-aware distillation module to mimic how
cardiologists focus on diagnostically relevant ECG regions and a cross-layer
mutual information module to align the decision processes of LCCI and HCCI
systems. Using a semi-supervised training strategy, LiteHeart further improves
model robustness under limited supervision. Evaluated on five datasets covering
over 38 cardiovascular diseases, LiteHeart substantially reduces the
performance gap between LCCI and HCCI, outperforming existing methods by 4.27%
to 7.10% in macro F1 score. These results demonstrate that LiteHeart
significantly enhances the diagnostic capabilities of low-cost cardiac
intelligence systems, paving the way for scalable, affordable, and accurate
daily cardiac healthcare using wearable technologies.

</details>


### [75] [Real-Time Interactive Hybrid Ocean: Spectrum-Consistent Wave Particle-FFT Coupling](https://arxiv.org/abs/2511.02852)
*Shengze Xue,Yu Ren,Jiacheng Hong,Run Ni,Shuangjiu Xiao,Deli Dong*

Main category: eess.SP

TL;DR: 混合海洋模型：在全局FFT背景下引入局部波粒子补丁，统一光谱参数和色散，通过频段桶实现高效、能量一致的实时仿真。


<details>
  <summary>Details</summary>
Motivation: FFT海洋模型虽然高效且具有大尺度真实感，但假设全局平稳性与均匀性，难以再现非均匀海域和近场相互作用（如船只、漂浮体）; 相比之下，波粒子方法能捕捉局部尾迹和波纹，但在大尺度下成本高且难以与全局谱统计对齐。需要一个在全局和局部尺度上统一的框架以实现实时交互与统计一致性。

Method: 将全局FFT背景与局部波粒子（WP）补丁在统一谱下耦合，在补丁边界按相同的方向谱注入粒子，使局部的频率-方向分布与背景一致且能量密度匹配；提出基于频段桶的粒子采样和GPU并行合成方案，以保持能量一致性并实现实时交互性能。

Result: 实现了大尺度谱现实性与局部局部细节（尾迹、涟漪）的实时统一；边界处粒子注入不扰动远场，同时在背景谱下保持能量密度与方向分布的一致性，支持交互式实时仿真。

Conclusion: 提出两项创新：1) 混合海洋表示，将全局FFT背景与局部WP补丁耦合，获得大尺度光谱的一致性并能呈现局部 wakes；2) 频段桶实现，通过基于频段桶的粒子采样与GPU并行合成，确保能量一致性并维持实时性能。该框架实现了大尺度光谱真实感与细粒度交互的统一。

Abstract: Fast Fourier Transform-based (FFT) spectral oceans are widely adopted for
their efficiency and large-scale realism, but they assume global stationarity
and spatial homogeneity, making it difficult to represent non-uniform seas and
near-field interactions (e.g., ships and floaters). In contrast, wave particles
capture local wakes and ripples, yet are costly to maintain at scale and hard
to match global spectral statistics.We present a real-time interactive hybrid
ocean: a global FFT background coupled with local wave-particle (WP) patch
regions around interactive objects, jointly driven under a unified set of
spectral parameters and dispersion. At patch boundaries, particles are injected
according to the same directional spectrum as the FFT, aligning the local
frequency-direction distribution with the background and matching energy
density, without disturbing the far field.Our approach introduces two main
innovations: (1) Hybrid ocean representation. We couple a global FFT background
with local WP patches under a unified spectrum, achieving large-scale spectral
consistency while supporting localized wakes and ripples.(2) Frequency-bucketed
implementation. We design a particle sampling and GPU-parallel synthesis scheme
based on frequency buckets, which preserves spectral energy consistency and
sustains real-time interactive performance.Together, these innovations enable a
unified framework that delivers both large-scale spectral realism and
fine-grained interactivity in real time.

</details>


### [76] [Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring](https://arxiv.org/abs/2511.02853)
*Young-Seok Kweon,Gi-Hwan Shin,Ji-Yong Kim,Bokyeong Ryu,Seong-Whan Lee*

Main category: eess.SP

TL;DR: 提出基于心电信号的意识状态估计 transformer 模型，通过解耦查询注意力机制实现对心率变异性特征的有效捕获，在睡眠分期和麻醉监测数据集上实现高性能的实时无创意识监测，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: EEG 在噪声敏感和受控环境依赖方面存在挑战，存在需求在临床环境中使用非侵入、鲁棒的意识状态监测方法，以提高安全性并优化治疗效果。

Method: 提出意识状态-ECG transformer，采用解耦查询注意力来提取区分清醒/意识状态的心率变异性特征，对 ECG 信号进行非侵入式监测并实现实时监测。系统在睡眠分期和麻醉水平监测任务上进行验证，使用相关数据集。

Result: 在睡眠分期和麻醉监测任务中，模型分别达到 accuracies 0.877 和 0.880，以及 AUC 值 0.786 和 0.895，且优于基线模型。

Conclusion: ECG 基于意识监测提供了一个实用且鲁棒的替代 EEG 的方案，特别适合动态临床环境，有望提升患者安全和深化对意识状态的理解。

Abstract: Conscious state estimation is important in various medical settings,
including sleep staging and anesthesia management, to ensure patient safety and
optimize health outcomes. Traditional methods predominantly utilize
electroencephalography (EEG), which faces challenges such as high sensitivity
to noise and the requirement for controlled environments. In this study, we
propose the consciousness-ECG transformer that leverages electrocardiography
(ECG) signals for non-invasive and reliable conscious state estimation. Our
approach employs a transformer with decoupled query attention to effectively
capture heart rate variability features that distinguish between conscious and
unconscious states. We implemented the conscious state estimation system with
real-time monitoring and validated our system on datasets involving sleep
staging and anesthesia level monitoring during surgeries. Experimental results
demonstrate that our model outperforms baseline models, achieving accuracies of
0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our
model achieves the highest area under curve values of 0.786 and 0.895 on sleep
staging and anesthesia level monitoring, respectively. The proposed system
offers a practical and robust alternative to EEG-based methods, particularly
suited for dynamic clinical environments. Our results highlight the potential
of ECG-based consciousness monitoring to enhance patient safety and advance our
understanding of conscious states.

</details>


### [77] [NEF-NET+: Adapting Electrocardio panorama in the wild](https://arxiv.org/abs/2511.02880)
*Zehui Zhan,Yaojun Hu,Jiajing Zhan,Wanchen Lian,Wanqing Wu,Jintai Chen*

Main category: eess.SP

TL;DR: NEF-NET+ 提升全景心电信号合成，通过直接视图变换和离线/在线校准，实现任意长度信号、跨设备鲁棒性及患者个体化适配，在 Panobench 数据集上比 Nef-Net 提升约6 dB PSNR。


<details>
  <summary>Details</summary>
Motivation: 解决传统多导联ECG的视角局限、需要非标准视角以揭示诊断性模式的挑战，提出可在任意视角重建连续心电场的需求，并提升在真实世界中的鲁棒性。

Method: 设计新的模型架构实现直接视图变换，包含离线预训练、设备校准步骤、以及用于患者特异适配的在线校准；构建Panobench基准，包含5367条记录、每人48个视角；在真实场景下评估。

Result: 与Nef-Net相比，PSNR提升约6 dB；在Panobench和真实设备环境中表现出显著改进。

Conclusion: NEF-NET+实现更现实的全景ECG合成，具备跨设备泛化和对任意长度信号的处理能力，能够补偿操作人员在电极放置上的偏差；相关代码和基准将发布。

Abstract: Conventional multi-lead electrocardiogram (ECG) systems capture cardiac
signals from a fixed set of anatomical viewpoints defined by lead placement.
However, certain cardiac conditions (e.g., Brugada syndrome) require
additional, non-standard viewpoints to reveal diagnostically critical patterns
that may be absent in standard leads. To systematically overcome this
limitation, Nef-Net was recently introduced to reconstruct a continuous
electrocardiac field, enabling virtual observation of ECG signals from
arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net
operates under idealized assumptions and faces in-the-wild challenges, such as
long-duration ECG modeling, robustness to device-specific signal artifacts, and
suboptimal lead placement calibration. This paper presents NEF-NET+, an
enhanced framework for realistic panoramic ECG synthesis that supports
arbitrary-length signal synthesis from any desired view, generalizes across ECG
devices, and com- pensates for operator-induced deviations in electrode
placement. These capabilities are enabled by a newly designed model
architecture that performs direct view transformation, incorporating a workflow
comprising offline pretraining, device calibration tuning steps as well as an
on-the-fly calibration step for patient-specific adaptation. To rigorously
evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama
benchmark, called Panobench, comprising 5367 recordings with 48-view per
subject, capturing the full spatial variability of cardiac electrical activity.
Experimental results show that NEF-NET+ delivers substantial improvements over
Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The
code and Panobench will be released in a subsequent publication.

</details>


### [78] [Consensus Tracking of an Underwater Vehicle Using Weighted Harmonic Mean Density](https://arxiv.org/abs/2511.03130)
*Ved Prakash Dubey,Shovan Bhaumik*

Main category: eess.SP

TL;DR: 提出了一种基于权重谐波均值密度的分布式水下目标跟踪框架，通过最小化Kullback-Leibler散度来优化融合权重，以实现局部跟踪器之间的共识并对高斯密度进行融合。


<details>
  <summary>Details</summary>
Motivation: 在水下环境中，传感器数量众多且多数为方位角测量，需跨子区域实现高效、鲁棒的分布式跟踪与数据融合，并且希望在不增加过多通信负担的情况下提升跟踪精度。

Method: 将区域分为若干子区域，设有单一跟踪器生成轨迹；各自的本地跟踪器获取来自声纳的方位角并与本地跟踪器通信；各跟踪器之间进行信息交换并通过共识达到一致。提出基于加权谐波均值密度（HMD）的密度融合方法，对高斯密度进行融合，优选权重使KL散度最小化。

Result: 通过仿真评估，优化后的HMD融合在分布式跟踪场景中优于现有融合方法，在根均方误差、轨道分歧百分比和归一化估计误差平方等指标上表现更佳。

Conclusion: 基于KL散度优化的HMD密度融合为分布式水下跟踪提供了一种有效的高斯密度融合策略，具有潜在的扩展性与对不同类型测量的鲁棒性。

Abstract: This paper addresses an underwater target tracking problem in which a large
number of sonobuoy sensors are deployed on a surveillance region. The region is
divided into several sub-regions, where a single tracker, capable of generating
track is installed. Each sonobuoy can measure the direction of arrival of
acoustic signals (known as bearing angles) and communicate the measurements
with the local tracker. Further, each local tracker can communicate with all
other trackers, where each of them can exchange their estimate and finally a
consensus is reached. We propose a weighted harmonic mean density (HMD) based
tracking to reach a consensus and provide a solution for the fusion of Gaussian
densities. In this approach, optimal weights are assigned by minimizing the
Kullback-Leibler divergence measure. Performance of the proposed method is
measured using root mean square error, percentage of track divergence, and
normalized estimation error squared. Simulation results demonstrate that the
optimized HMD-based fusion outperforms existing fusion methods during a
distributed tracking.

</details>


### [79] [Analysis and Algorithm for Multi IRS Collaborative Localization via Hybrid Time Angle Estimation](https://arxiv.org/abs/2511.03133)
*Ziheng Zhang,Wen Chen,Qingqing Wu,Haoran Qin,Zhendong Li,Qiong Wu*

Main category: eess.SP

TL;DR: 提出一种多IRS协同混合定位系统，通过在目标区域附近布设多IRS并进行时延和角度联合估计实现定位。推导了半被动模型下级联时延、AOA、AOD的FIM与CRB，设计了基于原子范数的凸优化求解多对AOA/AOD，并用ADMM实现低复杂度估计；定位阶段提出三阶段算法（加权最小二乘、全最小二乘、二次校正）以提高鲁棒性和接近CRB的精度，数值结果在低SNR下表现突出。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中实现高精度定位的需求日益增长。通过多IRS协同、混合定位与分布式部署来提升信道信息利用，特别是在低信噪比条件下，提升时延、AOA/AOD等参数估计及定位精度。

Method: 建立多IRS近场/远场模型，给出半被动模型下的级联时延、AOA、AOD的FIM与CRB；将角度估计问题按秩分为3类并对信号进行预处理；通过构建原子范数集合将联合角度估计转化为凸优化问题，利用ADMM实现多对AOA/AOD的低复杂度估计；定位方面提出结合加权最小二乘、全最小二乘与二次修正的三阶段局部化算法，以处理系数矩阵与观测向量中的误差。

Result: 数值仿真验证了所提系统的优越性，表明系统的协同、混合定位与分布式部署能够带来显著收益，并且所提估计算法在低SNR下仍具较好精度。

Conclusion: 通过多IRS协同和混合定位并结合高效的凸优化与三阶段定位策略，能够在近似CRB的条件下实现高精度定位，且对低SNR场景具有鲁棒性，具备在分布式部署环境中的可行性与潜在应用价值。

Abstract: This paper proposes a novel multiple intelligent reflecting surfaces (IRSs)
collaborative hybrid localization system, which involves deploying multiple
IRSs near the target area and achieving target localization through joint time
delay and angle estimation. Specifically, echo signals from all reflective
elements are received by each sensor and jointly processed to estimate the time
delay and angle parameters. Based on the above model, we derive the Fisher
Information Matrix (FIM) for cascaded delay, Angle of Arrival (AOA), and Angle
of Departure (AOD) estimation in semi passive passive models, along with the
corresponding Cramer Rao Bound (CRB). To achieve precise estimation close to
the CRB, we design efficient algorithms for angle and location estimation. For
angle estimation, reflective signals are categorized into three cases based on
their rank, with different signal preprocessing. By constructing an atomic norm
set and minimizing the atomic norm, the joint angle estimation problem is
transformed into a convex optimization problem, and low-complexity estimation
of multiple AOA and AOD pairs is achieved using the Alternating Direction
Method of Multipliers (ADMM). For location estimation, we propose a three-stage
localization algorithm that combines weighted least squares, total least
squares, and quadratic correction to handle errors in the coefficient matrix
and observation vector, thus improving accuracy. Numerical simulations validate
the superiority of the proposed system, demonstrating that the system's
collaboration, hybrid localization, and distributed deployment provide
substantial benefits, as well as the accuracy of the proposed estimation
algorithms, particularly in low signal to noise ratio (SNR) condition.

</details>


### [80] [Multimodal-Wireless: A Large-Scale Dataset for Sensing and Communication](https://arxiv.org/abs/2511.03220)
*Tianhao Mao,Le Liang,Jie Yang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 开源的多模态无线数据集Multimodal-Wireless，结合CARLA和Sionna平台，面向无线通信研究，包含约16万帧，覆盖4个虚拟城镇、16种通信场景和3种天气，具备多模态传感能力（通信信道、激光雷达、RGB与深度图、惯性测量单元、雷达）等。提供端到端数据管线和实现细节，并展示在通信与协作感知中的潜在应用，如通过多模态大语言模型进行波束预测。数据集开放获取地址在 https://le-liang.github.io/mmw/。


<details>
  <summary>Details</summary>
Motivation: 针对无线通信与感知协同研究，缺乏统一的多模态、可重现的数据集；需要一个可扩展且可定制的数据管线，以CARLA仿真与Sionna物理层建模为基础，支持多模态感知与波束管理等任务。

Method: 在CARLA仿真和Sionna框架上构建一个集成数据管线，生成静态与动态图像、信道参数等多模态数据。覆盖4个虚拟城镇、16种通信场景、3种天气条件，采集通信信道、LiDAR、RGB、深度、IMU、雷达等模态；构建开源实现，提供数据格式、标注与数据生成流程。

Result: 数据集规模约16万帧，具有多模态传感信息和场景多样性，配套技术实现细节，支持与波束预测、协作感知等任务的研究。并给出一个示例应用：通过多模态大语言模型进行波束预测。

Conclusion: 该数据集有助于推进无线通信与感知协同研究，提升实验可重复性和跨模态研究能力，适用于波束管理、定位、感知融合等任务，且对研究者开放访问。

Abstract: This paper presents Multimodal-Wireless, an open-source multimodal sensing
dataset designed for wireless communication research. The dataset is generated
through an integrated and customizable data pipeline built upon the CARLA
simulator and Sionna framework. It contains approximately 160,000 frames
collected across four virtual towns, sixteen communication scenarios, and three
weather conditions, encompassing multiple sensing modalities--communication
channel, light detection and ranging, RGB and depth cameras, inertial
measurement unit, and radar. This paper provides a comprehensive overview of
the dataset, outlining its key features, overall framework, and technical
implementation details. In addition, it explores potential research
applications concerning communication and collaborative perception, exemplified
by beam prediction using a multimodal large language model. The dataset is open
in https://le-liang.github.io/mmw/.

</details>


### [81] [Diffusion-Driven Terahertz Air-Ground Communications under Dynamic Atmospheric Turbulence](https://arxiv.org/abs/2511.03290)
*Jinhao Yi,Weijun Gao,Chong Han*

Main category: eess.SP

TL;DR: 提出基于流体力学的空气湍流衰减模型的AI驱动THz AG通信框架，通过扩散式算法对发射功率与飞行姿态进行联合优化，以提升链路容量，显著超越现有策略。


<details>
  <summary>Details</summary>
Motivation: 在太赫兹（THz）通信的太空-空中-地面(SAGIN)场景中，空气/地面互联通道的湍流引起的快速波衰减，制约了高数据速率的实现；需要更真实的湍流建模并结合自适应优化来提升链路性能。

Method: 建立基于流体动力学的湍流衰减模型以刻画飞机产生的湍流并量化其对THz信号传播的影响；在此模型基础上提出耦合的功率-姿态优化问题，通过扩散式算法学习飞行配置与湍流衰减之间的非线性关系并实现自适应求解。

Result: 在-10°至10°攻角、0.7马赫条件下，湍流引起的衰减范围为18–28 dB；提出的方法实现平均容量11.241 bps/Hz，相对于现有策略提升约22.8%与66.5%，并接近理论容量极限的约98%。

Conclusion: 将湍流衰减作为关键物理因素纳入THz AG链路的自适应优化，显著提升链路容量并逼近理论极限，验证了流体力学驱动建模在高频通信中的有效性。

Abstract: The ever-increasing demand for ultra-high data rates in space-air-ground
integrated networks (SAGINs) has rendered terahertz THz communications a
promising technology owing to its exceptionally broad and continuous spectrum
resources. Nevertheless, in air-ground (AG) scenarios, the high mobility of
aircraft induces intense and rapidly fluctuating turbulence, leading to
additional propagation loss that is often overlooked in existing studies. To
bridge this gap, this paper presents an AI-empowered THz AG communication
framework that explicitly models turbulence-induced attenuation through fluid
dynamics and integrates it into an adaptive optimization paradigm for
communication performance enhancement. Specifically, a fluid-dynamics-informed
attenuation model is established to characterize aircraft-generated turbulence
and quantify its impact on THz signal propagation. Building upon this model, a
joint power-attitude optimization problem is formulated to adaptively allocate
transmit power and adjust aircraft attitude for maximizing link capacity. The
optimization problem is efficiently solved using a diffusion-based algorithm
that learns the nonlinear relationship between flight configuration and
turbulence-induced attenuation. Comprehensive numerical evaluations demonstrate
that the turbulence-induced attenuation ranges from 18 to 28 dB under attacking
angles between -10 degree and 10 degree at 0.7 Mach, verifying the pronounced
impact of aircraft-induced turbulence on THz propagation. Furthermore, the
proposed framework attains an average capacity of 11.241 bps/Hz, substantially
outperforming existing strategies by 22.8% and 66.5%, and approaching
approximately 98% of the theoretical capacity limit.

</details>


### [82] [UAV SAR Imaging with 5G NR OFDM Signals in NLOS Environments](https://arxiv.org/abs/2511.03292)
*Qiuyuan Yang,Cunhua Pan,Ruidong Li,Zhenkun Zhang,Hong Ren,Changhong Wang,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出一个用于SAR成像的协同ISAC框架，利用OFDM通信信号，通过两阶段CS-SAGE实现对较弱信号的高精度散射体定位，解决非视距(NLOS)下的成像劣化，并在QuaDRiGa通道模型下进行仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在未来无线系统中，感知与通信的耦合（ISAC）具有提升频谱利用率和扩展应用场景的潜力。本研究聚焦于在NLOS和复杂通道环境中，利用通信波形对SAR成像进行鲁棒定位和散射体参数提取，降低假点概率并提升定位精度。

Method: 提出两阶段的CS-SAGE框架。阶段I使用正交匹配Pursuit（OMP）进行粗略定位，确定主散射体的近似位置。阶段II基于SAGE算法进行精细估计，准确提取散射体参数。整个框架在QuaDRiGa通道生成模型下进行仿真，且结合OFDM信号实现 sensing/communication 的协同。为提高弱信号检测与减少误检，采用稀疏表示和期望最大化迭代。

Result: 仿真结果表明所提框架在NLOS/复杂通道条件下可实现对散射点的高精度定位与参数估计，相较于传统方法具有鲁棒性提升，并给出设计 sensibles，提供对实际系统设计的有价值洞察。

Conclusion: 该协同ISAC框架在SAR成像中展现出在复杂通道环境下的潜力，为未来的实用系统设计提供了理论和实验依据。可拓展到更高维度的成像与多目标场景，未来工作可包括真实通道实验和实时实现。

Abstract: The integration of sensing and communication (ISAC) has significant potential
for future wireless systems, enabling efficient spectrum utilization and novel
application scenarios. In this paper, we propose a cooperative ISAC framework
for synthetic aperture radar (SAR) imaging by leveraging orthogonal frequency
division multiplexing (OFDM) communication signals. We address the challenge of
severe imaging degradation in non-line-of-sight (NLOS) environments under the
QUAsi Deterministic RadIo channel GenerAtor (QuaDRiGa). To detect weak signals
and eliminate false points, we develop a two-stage compressed sensing-space
alternating generalized expectation maximization (CS-SAGE) scheme for
high-precision scatterer localization. In stage I, orthogonal matching pursuit
(OMP) is employed for coarse estimation to identify the approximate locations
of dominant scatterers. Then, the SAGE algorithm in stage II performs fine
estimation to accurately extract scatterer parameters. Simulation results
validate the effectiveness of the proposed cooperative ISAC framework, and
provide valuable insights for practical system design.

</details>


### [83] [Performance Analysis of Wireless-Powered Pinching Antenna Systems](https://arxiv.org/abs/2511.03401)
*Kunrui Cao,Jingyu Chen,Panagiotis D. Diamantoulakis,Lei Zhou,Xingwang Li,Yuanwei Liu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出了无线供能的PINCHING天线系统（PAS），通过调节PA位置以获得更强LoS链路，并对有损和无损波导条件下的 outages 与ergodic rate 给出解析表达，揭示了吸收系数、用户面积维度对系统性能的影响及最佳部署策略。


<details>
  <summary>Details</summary>
Motivation: 动机在于利用PAS增强无线通信的可靠性与效率，研究无线供能PAS对WPC系统性能的提升，揭示波导损耗与吸收系数对 outage 和速率的影响，以及在实际部署中如何选取波导长度、用户位置和时分配以优化性能。

Method: 在有损波导和无损波导两种理想化情形下，推导出 outage probability 的闭式解与 ergodic rate 的闭式解；对波导长度、吸收系数、发射端/接入端距离等参数进行最优部署分析；并与传统WPC对比，给出时隙分配因子和PS–AP距离的最优解，最后比较独立分离的PS/AP结构与集成混合AP结构的性能。

Result: 结果表明：增加吸收系数和用户区域尺寸会导致波导内外传播损耗上升，进而提高 outage 概率并降低 ergodic rate；在高吸收系数和较长波导条件下，无线供能PAS的 outage 甚至劣于传统WPC系统，但 ergodic rate 可能仍优于传统WPC；存在使 outage 最小化或 ergodic rate 最大化的最优时分配因子和PS–AP距离；将PS与AP分离在最优距离部署的系统性能优于将其集成在一个混合AP中的情形。

Conclusion: 无线供能的PAS在特定工作条件下具有潜在优势，且性能高度依赖吸收特性与波导长度，需通过优化时分配和部署（如PS与AP分离且在最优距离）来实现对outage与速率的综合提升；对于实际部署，分离式结构可能优于集成式混合AP。

Abstract: Pinching antenna system (PAS) serves as a groundbreaking paradigm that
enhances wireless communications by flexibly adjusting the position of pinching
antenna (PA) and establishing a strong line-of-sight (LoS) link, thereby
reducing the free-space path loss. This paper introduces the concept of
wireless-powered PAS, and investigates the reliability of wireless-powered PAS
to explore the advantages of PA in improving the performance of
wireless-powered communication (WPC) system. In addition, we derive the
closed-form expressions of outage probability and ergodic rate for the
practical lossy waveguide case and ideal lossless waveguide case, respectively,
and analyze the optimal deployment of waveguides and user to provide valuable
insights for guiding their deployments. The results show that an increase in
the absorption coefficient and in the dimensions of the user area leads to
higher in-waveguide and free-space propagation losses, respectively, which in
turn increase the outage probability and reduce the ergodic rate of the
wireless-powered PAS. However, the performance of wireless-powered PAS is
severely affected by the absorption coefficient and the waveguide length, e.g.,
under conditions of high absorption coefficient and long waveguide, the outage
probability of wireless-powered PAS is even worse than that of traditional WPC
system. While the ergodic rate of wireless-powered PAS is better than that of
traditional WPC system under conditions of high absorption coefficient and long
waveguide. Interestingly, the wireless-powered PAS has the optimal time
allocation factor and optimal distance between power station (PS) and access
point (AP) to minimize the outage probability or maximize the ergodic rate.
Moreover, the system performance of PS and AP separated at the optimal distance
between PS and AP is superior to that of PS and AP integrated into a hybrid
access point.

</details>


### [84] [A Modified Pulse and Design Framework to Halve the Complexity of OFDM Spectral Shaping Techniques](https://arxiv.org/abs/2511.03465)
*Javier Giménez,José A. Cortés,Francisco Javier Cañete,Eduardo Martos-Naya,Luis Díez*

Main category: eess.SP

TL;DR: 提出对传统OFDM波形的修改，以降低与谱形滤波相关的优化和实现成本，并提供可扩展的框架；有望将优化系数和实现乘积数目各减少约50%。


<details>
  <summary>Details</summary>
Motivation: 现有的谱形塑形（如前置编码、AIC、时域方法）在减小OOBE方面有效，但需要复杂的优化过程和较高的实时实现成本，限制了实际部署和实时应用。

Method: 对OFDM波形进行修改，提出一种新的波形设计框架，使得与谱形塑形相关的优化变量和实现乘积显著减少（理论上可减少至原来的一半），并为后续工作提供兼容该框架的方向。

Result: 该方法在理论上可将需要优化的系数数量和实现中的乘积数量降低最多约50%。

Conclusion: 所提出的修改为OFDM的谱形塑形提供了一条降低计算和实现成本的途径，同时为未来工作提供了可扩展的研究框架，但需要进一步的性能评估（如对OOBE、信号失真和系统容量的影响）来验证实际效用。

Abstract: Orthogonal frequency division multiplexing (OFDM) is a widespread modulation
but suffers from high out-of-band emissions (OOBE). Spectral shaping strategies
such as precoding, active interference cancellation (AIC) and time-domain
methods are effective at reducing the OOBE but entail optimization procedures
and real-time implementation costs which might be considerable. This letter
proposes a modification of the conventional OFDM waveform aimed at reducing the
cost associated to many of the state-of-theart spectral shaping techniques and
sets a framework for future works that want to benefit from the same reduction.
This approach may reduce both the number of coefficients involved in the
optimization and the number of products of its implementation by up to 50%.

</details>


### [85] [A Novel Multi-Reference-Point Modeling Framework for Monostatic Background Channel: Toward 3GPP ISAC Standardization](https://arxiv.org/abs/2511.03487)
*Yameng Liu,Jianhua Zhang,Yuxiang Zhang,Zhiqiang Yuan,Chuangxin Jiang,Junchen Liu,Wei Hong,Yingyang Li,Yan Li,Guangyi Liu*

Main category: eess.SP

TL;DR: 提出一个面向ISAC单站态背景信道的3GPP兼容随机模型，将单站Tx&Rx与多RP参考点之间的子通道叠加，用于室内28 GHz的测量验证，并利用遗传算法优化RP数量与布点。


<details>
  <summary>Details</summary>
Motivation: ISAC需要现实且符合标准的背景信道建模来评估 sensing 性能，尤其在Tx与Rx共址的单工位（monostatic）场景中，现有标准多聚焦于分离的Tx-Rx环境传播，对背景信道的建模缺乏统一、可验证的方法。因此需要一个能够反映环境背景、且与3GPP框架兼容的模型。

Method: 在室内场景下进行28 GHz ISAC单站背景信道测量，提取现实信道参数；提出以单站Tx&Rx与多个人工参考点（RPs）之间子通道叠加的随机模型，描述背景信道；给出扩展到3GPP框架的实现方案；利用遗传算法优化多RP的数量与布点；通过对比实测与仿真信道参数来验证模型与参数的一致性。

Result: 所提模型能有效刻画单站背景信道的特征（如明显的单跳传输与离散多径分布），并提供了一个兼容标准化的建模框架；遗传算法能够较好地确定RP数量与位置；仿真结果与实测信道参数具有良好一致性。

Conclusion: 填补ISAC单站背景信道建模的关键空白，且具备实现于6G标准化的潜力，为ISAC的现实部署提供可用的背景信道建模工具与评估手段。

Abstract: Integrated Sensing and Communication (ISAC) has been identified as a key 6G
application by ITU and 3GPP. A realistic, standard-compatible channel model is
essential for ISAC system design. To characterize the impact of Sensing Targets
(STs), 3GPP defines ISAC channel as a combination of target and background
channels, comprising multipath components related to STs and those originating
solely from the environment, respectively. Although the background channel does
not carry direct ST information, its accurate modeling is critical for
evaluating sensing performance, especially in complex environments. Existing
communication standards characterize propagation between separated transmitter
(Tx) and receiver (Rx). However, modeling background channels in the ISAC
monostatic mode, where the Tx and Rx are co-located, remains a pressing
challenge. In this paper, we firstly conduct ISAC monostatic background channel
measurements for an indoor scenario at 28 GHz. Realistic channel parameters are
extracted, revealing pronounced single-hop propagation and discrete multipath
distribution. Inspired by these properties, a novel stochastic model is
proposed to characterizing the ISAC monostatic background channel as the
superposition of sub-channels between the monostatic Tx&Rx and multiple
communication Rx-like Reference Points (RPs). This model is compatible with
standardizations, and a 3GPP-extended implementation framework is introduced.
Finally, a genetic algorithm-based method is proposed to extract the optimal
number and placement of multi-RPs. The optimization approach and modeling
framework are validated by comparing measured and simulated channel parameters.
Results demonstrate that the proposed model effectively captures monostatic
background channel characteristics, addresses a critical gap in ISAC channel
modeling, and supports 6G standardization.

</details>


### [86] [3D Cooperative User Tracking for Distributed Integrated Sensing and Communication](https://arxiv.org/abs/2511.03612)
*Yingjie Xu,Xuesong Cai,Michiel Sandra,Sara Willhammar,Fredrik Tufvesson*

Main category: eess.SP

TL;DR: 提出了基于全局PHD滤波的DISAC协同跟踪框架，并结合视场感知的AP管理策略，通过真实的分布式MIMO测量验证其厘米级轨迹误差与高效的AP调度能力。


<details>
  <summary>Details</summary>
Motivation: 随着ISAC在6G中的集成，分布式架构有望同时提升感知与通信性能；但需要在多目标跟踪、资源调度和现实部署中解决鲁棒性与效率等挑战。

Method: 将全局PHD滤波用于多目标跟踪，并引入视场感知的AP管理策略以实现AP调度；通过真实世界的分布式MIMO信道测量来评估框架在实际场景中的性能。

Result: 实现厘米级根均方轨迹误差；并且无需所有AP永久开启即可维持高 Tracking 精度，显示出鲁棒且高效的AP管理能力。

Conclusion: 该框架为DISAC在实际部署中的协同跟踪提供了可行路径，强调通过智能AP调度在保持高跟踪性能的同时优化资源使用，促进分布式感知-通信系统的发展。

Abstract: As integrated sensing and communication (ISAC) becomes an integral part of 6G
networks, distributed ISAC (DISAC) is expected to enhance both sensing and
communication performance through its decentralized architecture. This paper
presents a complete framework to address the challenge of cooperative user
tracking in DISAC systems. By incorporating a global probability hypothesis
density (PHD) filter and a field-of-view-aware access point (AP) management
strategy, the framework enables accurate user tracking using radio signals
while optimizing AP scheduling. In addition, a real-world distributed MIMO
channel measurement campaign is performed to evaluate the effectiveness of the
framework. The results demonstrate that a centimeter-level root mean-square
trajectory error can be achieved. Furthermore, the results show that it is not
necessary to keep APs active at all times to maintain high tracking accuracy,
indicating the need for robust and efficient AP management. These findings
provide valuable insight into practical deployments and further development of
cooperative user tracking techniques in DISAC systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [87] [Robust reduced-order model predictive control using peak-to-peak analysis of filtered signals](https://arxiv.org/abs/2511.03002)
*Johannes Köhler,Carlo Scholz,Melanie Zeilinger*

Main category: eess.SY

TL;DR: 提出一种结合ROM和鲁棒MPC的设计方法，用于大规模线性系统，能够对全阶系统输出给出保证边界并实现约束满意性；在100维质量-弹簧-阻尼系统上达到显著降低保守性（约4个数量级）的结果。


<details>
  <summary>Details</summary>
Motivation: 大规模系统直接用全阶模型MPC计算成本高且难以实现实时性，需用ROM以降低维度，同时引入误差边界以确保对全阶系统的约束和性能的鲁棒性。

Method: 在ROM的同时并行预测一个标量的误差界系统，推导出全阶输出的保守边界；将该边界用于鲁棒ROM基MPC设计，步骤包括误差分析、峰值增益界定以及使用滤波信号，最终形成鲁棒约束满足的MPC框架。

Result: 在100维的mass-spring-damper系统上验证，保守性比现有方法降低约四个数量级，证明方法在大规模系统上的可行性和鲁棒性。

Conclusion: 该方法为ROM与鲁棒控制在MPC中的有效耦合提供了一条可行路径，能够在确保约束满足和鲁棒性能的同时显著减小保守性，适用于高维线性系统的实时控制。

Abstract: We address the design of a model predictive control (MPC) scheme for
large-scale linear systems using reduced-order models (ROMs). Our approach uses
a ROM, leverages tools from robust control, and integrates them into an MPC
framework to achieve computational tractability with robust constraint
satisfaction. Our key contribution is a method to obtain guaranteed bounds on
the predicted outputs of the full-order system by predicting a (scalar)
error-bounding system alongside the ROM. This bound is then used to formulate a
robust ROM-based MPC that guarantees constraint satisfaction and robust
performance. Our method is developed step-by-step by (i) analysing the error,
(ii) bounding the peak-to-peak gain, an (iii) using filtered signals. We
demonstrate our method on a 100-dimensional mass-spring-damper system,
achieving over four orders of magnitude reduction in conservatism relative to
existing approaches.

</details>


### [88] [Oscillation Analysis and Damping Control for a Proposed North American AC-DC Macrogrid](https://arxiv.org/abs/2511.03017)
*Kaustav Chatterjee,Sameer Nekkalapu,Antos Varghese,Marcelo Elizondo,Quan Nguyen,Xiaoyuan Fan*

Main category: eess.SY

TL;DR: MTDC macrogrid interconnecting Eastern and Western Interconnections may affect small-signal stability; the paper develops a dynamic MTDC model integrated with EI/WI models with high inverter-based resources, analyzes inter-area modes and damping, and designs supplementary damping controllers using wide-area feedback and frequency-scanning for data-driven linearization; results identify modes with poor damping and evaluate damping performance under contingencies.


<details>
  <summary>Details</summary>
Motivation: Address the gap in literature regarding small-signal stability risks (poorly damped inter-area oscillations) introduced by a proposed MTDC macrogrid linking EI and WI, and to develop damping strategies.

Method: Develop a custom dynamic MTDC model integrated with EI and WI models featuring high penetration of inverter-based resources; perform model-based oscillation analysis to identify shifts in inter-area modes due to MTDC integration; design supplementary damping controllers for MTDC leveraging wide-area feedback to modulate active power setpoints; employ frequency scanning for data-driven model linearization and controller synthesis; evaluate damping under selected operating conditions and contingencies.

Result: Identified shifts in inter-area modes with MTDC integration and modes with inadequate damping; designed wide-area damping controllers for MTDC; demonstrated damping improvements under evaluated conditions and contingencies using frequency-scanned linearized models.

Conclusion: MTDC interconnection can affect inter-area oscillations; with appropriate wide-area damping controllers and data-driven linearization, the damping of critical modes can be improved, mitigating stability risks in the proposed EI–WI MTDC macrogrid.

Abstract: In recent years, several studies conducted by both industry and U.S.
Department of Energy (DOE)-funded initiatives have proposed linking North
America's Eastern and Western Interconnections (EI and WI) through a
multiterminal DC (MTDC) macrogrid. These studies have explored the advantages
and opportunities of the proposed configuration from the perspectives of
capacity sharing and frequency support. However, the potential challenges of
small-signal stability arising from this interconnection have not been
thoroughly examined. To address this gap, detailed model-based simulation
studies are performed in this paper to assess the risks of poorly damped
inter-area oscillations in the proposed macrogrid. A custom-built dynamic model
of the MTDC system is developed and integrated with industry-grade models of
the EI and WI, incorporating high levels of inverter-based energy resources.
Through model-based oscillation analysis, potential shifts in inter-area modes
for both EI and WI, resulting from the MTDC integration are characterized, and
modes with inadequate damping are identified. Furthermore, to mitigate the
risks of unstable oscillations, supplementary damping controllers are designed
for the MTDC system, leveraging wide-area feedback to modulate active power set
points at selected converter stations. A frequency scanning approach is
employed for data-driven model linearization and controller synthesis. The
damping performance is evaluated under the designed operating conditions and
selected contingency scenarios.

</details>


### [89] [Quantifying Power Systems Resilience Using Statistical Analysis and Bayesian Learning](https://arxiv.org/abs/2511.03043)
*Apsara Adhikari,Charlotte Wertz,Anamika Dubey,Arslan Ahmad,Ian Dobson*

Main category: eess.SY

TL;DR: 提出一个结合统计与贝叶斯学习的框架，用以定量建模天气参数与电力系统韧性之间的关系，且在库克县和迈阿密-戴德县的案例中发现风速、温度、降水对韧性具有显著影响，且三者的联合效应强于单变量分析。


<details>
  <summary>Details</summary>
Motivation: 随着极端天气事件频发，电力系统韧性受到越来越大挑战。现有研究较少系统地将天气变量与韧性指标建立定量关系，缺乏可用于风险评估与资源分配的建模工具。本研究旨在提出一个可泛化的统计-贝叶斯框架来量化天气对电力系统韧性的影响。

Method: 使用统计和贝叶斯学习方法，整合公开的停电与天气数据，建立天气变量（如风速、温度、降水）与韧性指标之间的关系模型；通过实证数据分析评估各变量的影响及其联合效应，并以 Cook County（伊利诺伊州）与 Miami-Dade County（佛罗里达州）作为案例对比。

Result: 识别出风速、温度、降水等天气变量是区域韧性指标的关键影响因素；在联合分析中，这些变量对韧性的影响显著大于单独分析，且两地区的敏感性存在差异。数据表明天气变量的组合效应对电力分布系统性能具有重要意义。

Conclusion: 该框架为定量理解天气事件对电力分布系统性能的影响提供了可操作的工具，支持决策者在风险缓解、资源分配和气候适应策略方面的决策，但也需关注数据质量、区域可推广性及模型假设的局限性。

Abstract: The increasing frequency and intensity of extreme weather events is
significantly affecting the power grid, causing large-scale outages and
impacting power system resilience. Yet limited work has been done on
systematically modeling the impacts of weather parameters to quantify
resilience. This study presents a framework using statistical and Bayesian
learning approaches to quantitatively model the relationship between weather
parameters and power system resilience metrics. By leveraging real-world
publicly available outage and weather data, we identify key weather variables
of wind speed, temperature, and precipitation influencing a particular region's
resilience metrics. A case study of Cook County, Illinois, and Miami-Dade
County, Florida, reveals that these weather parameters are critical factors in
resiliency analysis and risk assessment. Additionally, we find that these
weather variables have combined effects when studied jointly compared to their
effects in isolation. This framework provides valuable insights for
understanding how weather events affect power distribution system performance,
supporting decision-makers in developing more effective strategies for risk
mitigation, resource allocation, and adaptation to changing climatic
conditions.

</details>


### [90] [Microgrids optimal radial reconfiguration via FORWARD algorithm](https://arxiv.org/abs/2511.03059)
*Joan Vendrell Gallart,Russell Bent,Solmaz Kia*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Microgrids offer a promising paradigm for integrating distributed energy
resources, bolstering energy resilience, and reducing the impact of blackouts.
However, their inherent decentralization and dynamic operation present
substantial energy management complexities. These complexities, including
balancing supply and demand, ensuring system stability, and minimizing
operational costs, often necessitate solving computationally intractable
NP-hard Mixed-Integer Non-Linear Programming (MINLP) problems. Traditional
MINLP solvers struggle with the scalability and feasibility guarantees required
for these challenges. To address this, this paper tackles the problem of
resource allocation and radial configuration design for microgrid power
distribution and proposes and abstracted problem which is solved by introducing
a permutation-based iterative search method over the recently introduced
FORWARD method to efficiently identify feasible, near-optimal radial network
structures while inherently respecting physical constraints. Furthermore, this
paper investigates the integration of the proposed method as a warm-start
strategy for benchmark MINLP solvers offering a scalable solution for
comprehensive microgrid design.

</details>


### [91] [MHE in Output Feedback Control of Uncertain Nonlinear Systems via IQCs](https://arxiv.org/abs/2511.03221)
*Yang Guo,Stefan Streif*

Main category: eess.SY

TL;DR: 将带有参数/静态非线性不确定性的非线性系统与移动视界估计（MHE）结合，利用积分不等式约束（IQCs）构造鲁棒可检性概念，在满足该可检性的前提下，所提出的MHE能使含不确定性的闭环对外部扰动具有输入-状态稳定性（ISS）


<details>
  <summary>Details</summary>
Motivation: 在存在非参数化不确定性且给定一个鲁棒稳定的状态反馈控制器的背景下，提升状态估计的鲁棒性并保障闭环稳定性。

Method: 提出基于IQC的鲁棒可检性概念，构建适用于一般非线性受控系统的MHE优化问题；假设不确定系统在控制器作用下满足该可检性，从而证明闭环系统（含不确定系统、控制器与MHE）对外部扰动具备输入-状态稳定性（ISS）。

Result: 在满足上述鲁棒可检性假设时，所设计的MHE使得包含不确定性系统、控制器与MHE的闭环对扰动具有ISS性质，提供一个可验证的鲁棒估计框架。

Conclusion: 该工作提出一个面向广义非线性约束系统的鲁棒MHE框架，通过IQCs与鲁棒可检性，能够在存在不确定性和已知控制器的情况下实现对闭环的稳定性保证，并提供了可在实践中验证的检测条件。

Abstract: We propose a moving horizon estimation (MHE) scheme for general nonlinear
constrained systems with parametric or static nonlinear uncertainties and a
predetermined state feedback controller that is assumed to robustly stabilize
the system in the absence of estimation errors. Leveraging integral quadratic
constraints (IQCs), we introduce a new notion of detectability that is robust
to possibly non-parametric uncertainties and verifiable in practice. Assuming
that the uncertain system driven by the controller satisfies this notion of
detectability, we provide an MHE formulation such that the closed-loop system
formed of the uncertain system, the controller and MHE is input-to-state stable
w.r.t. exogenous disturbances.

</details>


### [92] [Theoretical and Experimental Limitations of RoCoF Estimation](https://arxiv.org/abs/2511.03249)
*Gutierrez-Florensa,F. Sanniti,D. Tedeschi,L. Sigrist,A. Ortega,F. Milano*

Main category: eess.SY

TL;DR: 基于微分几何和流体力学的鲁棒RoCoF估计方法，提升在低惯性、变换器组态下的频率/ RoCoF估计鲁棒性与速度，并为UFLS等保护提供更可靠输入。


<details>
  <summary>Details</summary>
Motivation: 低惯性、变流器为主的发电资产正在增加，导致瞬态更强、RoCoF估计不确定性增大，需更鲁棒且快速的估计以支撑保护策略。

Method: 提出一个数值鲁棒的方法，借鉴微分几何与流体力学的概念来估计RoCoF，结合高采样现实测量进行验证，并用于加速基于RoCoF的UFLS控制逻辑的开发。

Result: 在高采样测量与实际实验数据上验证，显示方法在鲁棒性和计算速度上有提升，并能为保护提供故障性质信息，改善响应。

Conclusion: 该方法可为RoCoF相关保护提供更可靠输入，尤其在低惯性系统中具有显著潜在收益。

Abstract: A precise estimation of the Rate of Change of Frequency (RoCoF) is crucial
for secure power system operation. In fact, RoCoF is strictly related to the
amount of the available physical and/or virtual inertia of the system and the
severity of the active power unbalance following a disturbance. For this
reason, it is widely exploited in different protection systems, e.g.,
Anti-Islanding, Under Frequency Load Shedding (UFLS) and wide-area protection
systems. The new paradigm of modern power systems, with a low-inertia and
converter-based generation assets, is increasing the transient severity, making
the frequency and the RoCoF estimation more complex and less precise for the
actual devices. This work addresses this issue by proposing a numerically
robust approach based on concepts inherited from differential geometry and
fluid mechanics. The proposed approach is then tested with high-sampling real
experimental measurements and used to develop a faster control logic for a
RoCoF-based UFLS control scheme. The proposed approach provides information to
protections regarding the nature of the contingency which can be used to
improve its response.

</details>


### [93] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games - Part II: Stability](https://arxiv.org/abs/2511.03297)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: Part II establishes evolutionary stability results for the mixed stationary Nash Equilibrium (MSNE) in large-population, continuous-time games with stochastic state dynamics and congestion-like payoffs, detailing when MSNE can persist under evolutionary dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand when MSNE is robust to strategic deviations and can reliably emerge as the long-run outcome in mean-field, large-population dynamic games with state evolution and population-wide congestion effects.

Method: Use mean-field evolutionary dynamics and revision protocols to analyze local and global stability; derive structural conditions on the MSNE and the payoff map that guarantee robustness to perturbations and strategy deviations.

Result: Derived conditions that ensure local stability (existence of a neighborhood where MSNE is attractive) and global stability (MSNE as globally attractive under broader criteria) under evolutionary dynamics; results specify how MSNE structure and payoff characteristics interact to enable robust persistence.

Conclusion: MSNE can robustly emerge and persist in large-population dynamic games when the payoff structure and MSNE configuration satisfy the proposed stability conditions, providing a theoretical basis for long-term viability of mixed equilibria under evolutionary processes.

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. In Part I, we introduced an evolutionary model and a new
solution concept - the mixed stationary Nash Equilibrium (MSNE) - which
coincides with the rest points of the mean field evolutionary model under
meaningful families of revision protocols. In this second part, we investigate
the evolutionary stability of MSNE. We derive conditions on both the structure
of the MSNE and the game's payoff map that ensure local and global stability
under evolutionary dynamics. These results characterize when MSNE can robustly
emerge and persist against strategic deviations, thereby providing insight into
its long-term viability in large population dynamic games.

</details>


### [94] [Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under Ship Attitude Variation](https://arxiv.org/abs/2511.03366)
*Kapila W. S. Palitharathna,Constantinos Psomas,Ioannis Krikidis*

Main category: eess.SY

TL;DR: 提出一种基于光波供电的水下光学ISAC系统：船载AP通过光信号对 seabed 能量采集节点进行上行通信，并用多摄像头阵列对目标进行定位，同时考虑船舷倾斜（滚、俯仰、偏航）的高斯分布。给出目标定位的均方误差（MSE）和上行数据速率的闭式近似，并通过分析与仿真验证系统性能与沟通-感知权衡，给出设计要点，如多摄像头在10°扰动下的最小MSE≈0.01 m^2的最佳放置，以及最佳能量使用比0.55。


<details>
  <summary>Details</summary>
Motivation: 在水下环境中实现光学信息和能量的协同传输（O-ISAC），并考虑船舶姿态波动对定位与通信性能的影响，需可解析的模型与设计原则，以平衡能量获取与感知精度.

Method: 建立海面船载AP的水下光传输与能量采集模型，结合阵列针孔相机实现目标定位；将船舷姿态（滚、俯仰、偏航）建模为高斯分布；推导目标定位MSE和上行速率的闭式近似，并通过解析推导与数值仿真验证其正确性，进而给出摄像头布置和能量分配等设计要点。

Result: 给出MSE与上行速率的闭式近似，且与仿真高度一致，揭示通信—感知之间的基本权衡；给出设计洞见：在滚、俯仰、偏航各为10°的扰动下，采用多摄像头可实现最小MSE≈1e-2 m^2；能量 harvest-use 比率的最优为0.55。

Conclusion: 构建的水下O-ISAC框架可提供可落地的设计准则，展示了在光传输与能量共用场景下的可行性与潜在的权衡收益，便于未来在复杂海况下的部署优化。

Abstract: In this paper, we propose a lightwave power transfer-enabled underwater
optical integrated sensing and communication (O-ISAC) system, where an access
point (AP) mounted on a seasurface ship transmits lightwave signals to two
nodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink
information to the AP, and ($ii$) a sensing target whose position is estimated
by the AP using an array of pinhole cameras. To capture practical deployment
conditions, the ship attitude variation is modeled through its roll, pitch, and
yaw angles, each following a Gaussian distribution under low-to-moderate sea
states. Closed-form approximations are derived for the mean squared error (MSE)
of target localization and the achievable uplink data rate. Analytical and
simulation results demonstrate excellent agreement, validating the proposed
models and derived expressions, while revealing the fundamental
communication-sensing tradeoff in the O-ISAC system. The results further
provide valuable design insights, including the optimal camera placement on the
ship to minimize localization error, achieving a minimum MSE of $10^{-2}$
$\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation
of $10^{\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered
setup.

</details>


### [95] [System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc](https://arxiv.org/abs/2511.03482)
*Giorgio Palma,Ivan Santic,Andrea Serani,Lorenzo Minno,Matteo Diez*

Main category: eess.SY

TL;DR: 用HDMDc及其贝叶斯扩展BHDMDc对有系泊的自航水面艇进行系统辨识，能在未见海况下对波浪激励做出准确预测，显示HDMDc-ROM的通用性与不确定性刻画。


<details>
  <summary>Details</summary>
Motivation: 在带有回流月池引起的非线性浪耗和系泊约束的少量自主水面艇（ASV）工况中，需要数据驱动的降阶模型来描述船体运动与系泊载荷的耦合动力学，提高对不同海况的泛化能力。

Method: 在Codevintec CK-14e ASV上，在拖曳水槽对不规则与规则头浪条件下收集数据，基于HDMDc和BHDMDc建立数据驱动的降阶模型，模型输入包括船体运动和系泊载荷。比较确定性预测与不确定性表征，验证对未见海况的预测能力。

Result: HDMDc给出精确的确定性预测，BHDMDc通过贝叶斯框架实现对模型响应的不确定性刻画；两者均能对未见的规则与不规则波激励做出预测，证明HDMDc-ROM在不同海况之间具备泛化能力。

Conclusion: HDMDc基础的ROM为系统辨识提供可行的数据驱动替代，首次证明其在与训练海况不同的海况下的泛化能力，并能高精度再现船舶动力学。

Abstract: This study addresses the system identification of a small autonomous surface
vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition
with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were
carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under
both irregular and regular head-sea wave conditions. The ASV under
investigation features a recessed moon pool, which induces nonlinear responses
due to sloshing, thereby increasing the modelling challenge. Data-driven
reduced-order models were built from measurements of vessel motions and mooring
loads. The HDMDc framework provided accurate deterministic predictions of
vessel dynamics, while the Bayesian formulation enabled uncertainty-aware
characterization of the model response by accounting for variability in
hyperparameter selection. Validation against experimental data demonstrated
that both HDMDc and BHDMDc can predict the vessel's response to unseen regular
and irregular wave excitations. In conclusion, the study shows that HDMDc-based
ROMs are a viable data-driven alternative for system identification,
demonstrating for the first time their generalization capability for a sea
condition different from the training set, achieving high accuracy in
reproducing vessel dynamics.

</details>


### [96] [Data-driven Modeling of Grid-following Control in Grid-connected Converters](https://arxiv.org/abs/2511.03494)
*Amir Bahador Javadi,Philip Pong*

Main category: eess.SY

TL;DR: 通过在无损传输线-无限母线的仿真中用变换器资源替代发电机，生成合成数据，评估稀疏非线性动力学辨识（SINDy）与深度符号回归等数据驱动动力学辨识方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源接入和智能电网技术的推进，需要灵活、可扩展的建模方法以准确捕捉现代电网的复杂动力学；并系统评估数据驱动方法在动力系统辨识中的适用性。

Method: 在无损传输线连到无限母线的拓扑中，使用一个基于变换器的资源作为替代发电机的动态元件，生成网格跟随控制模式下的合成数据；对数据应用稀疏非线性动力学辨识（SINDy）和深度符号回归等方法以识别系统动力学。

Result: 提供一个评估框架并演示这些方法能够从所生成的合成数据中有效捕捉系统动力学的能力；对不同数据驱动方法在捕捉非线性/时变动力学方面进行比较与分析。

Conclusion: 使用替代发电机的仿真平台为数据驱动动力学辨识提供可控数据源，表明SINDy和深度符号回归等方法在现代电网动态建模中具有潜力，未来可用于提升对复杂网路的建模与控制策略。

Abstract: As power systems evolve with the integration of renewable energy sources and
the implementation of smart grid technologies, there is an increasing need for
flexible and scalable modeling approaches capable of accurately capturing the
complex dynamics of modern grids. To meet this need, various methods, such as
the sparse identification of nonlinear dynamics and deep symbolic regression,
have been developed to identify dynamical systems directly from data. In this
study, we examine the application of a converter-based resource as a
replacement for a traditional generator within a lossless transmission line
linked to an infinite bus system. This setup is used to generate synthetic data
in grid-following control mode, enabling the evaluation of these methods in
effectively capturing system dynamics.

</details>


### [97] [Powered Descent Trajectory Optimization of Chandrayaan-3 using Radau Collocation and Controllable Sets](https://arxiv.org/abs/2511.03594)
*Suraj Kumar,Aditya Rallapalli,Ashok Kumar Kakula,Bharat Kumar GVP*

Main category: eess.SY

TL;DR: 基于 pseudospectral Radau 的轨迹优化，结合可控性导向的航路点细化，设计 Chandrayaan-3 动力下降轨迹，并量化燃料消耗与鲁棒性之间的折中。


<details>
  <summary>Details</summary>
Motivation: 解决月球软着陆中的鲁棒性与燃料效率之间的博弈，提升轨迹设计对于状态与控制扰动的鲁棒性。

Method: 使用 pseudospectral Radau 共轭分布的最优控制框架，结合可控性导向的航路点细化以增强对扰动的鲁棒性。并显式量化燃料消耗与鲁棒性之间的权衡。

Result: 得到具有鲁棒性的动力下降轨迹，航路点细化提升对扰动的鲁棒性；建立燃料消耗和鲁棒性之间的定量关系，为任务规划提供实用洞见。

Conclusion: 该框架为月球着陆的鲁棒轨迹设计提供有效工具，燃料-鲁棒性权衡可在任务阶段明确管理，具备应用于未来月球任务的潜力。

Abstract: India achieved a significant milestone on August $23^{\text{rd}}$ 2023,
becoming the fourth country to accomplish a soft landing on the Moon. This
paper presents the powered descent trajectory design for the Chandrayaan-3
mission. The optimization framework is based on pseudospectral Radau
collocation, and controllability-based waypoint refinement is employed to
further enhance the robustness of the trajectory against state and control
perturbations. Furthermore, the trade-off between fuel consumption and
robustness is explicitly quantified, providing insights into the practical
considerations of mission planning.

</details>


### [98] [Artificial-reference tracking MPC with probabilistically validated performance on industrial embedded systems](https://arxiv.org/abs/2511.03603)
*Victor Gracia,Pablo Krupa,Filiberto Fele,Teodoro Alamo*

Main category: eess.SY

TL;DR: 提出了一种适用于嵌入式系统的跟踪型MPC实现，使用结构化的一阶方法进行求解，结合offset-free、约束回退以及软约束等特征，并给出长时间运行的概率性能验证框架；在PLC-硬件在环环境下对非线性CSTR进行了演示，分析每步的迭代次数与约束违例的概率。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式控制器资源有限，难以直接部署复杂的预测控制，需要在保持鲁棒性和可行性的前提下实现高效在线求解；同时需提供对长期性能的统计性验证以确保现实场景下的可靠性。

Method: 针对跟踪型MPC的人工参考问题，采用结构化的一阶方法进行求解，并为嵌入式实现做了定制。引入offset-free积分方案、用于约束凑紧的back-off参数、以及软约束以在扰动或模型不匹配时保持可行性。建立一个长时间操作下的闭环系统概率性能验证框架。通过一个PLC硬件在环的实例，对一个非线性连续搅拌反应器（CSTR）进行控制，评估在每个时间步的迭代次数与约束违反情况的概率分布。

Result: 证明了在嵌入式系统上实现的MPC具有较低的在线计算负担，同时提供了可用于长期运行的概率性能验证框架。实验/仿真结果在PLC-HIL场景中显示出对约束的概率可控性以及较低的迭代需求。

Conclusion: 该工作推动嵌入式工业环境中MPC的实际部署，结合鲁棒性增强特征与长期概率性能验证，适用于广泛的应用场景。

Abstract: Industrial embedded systems are typically used to execute simple control
algorithms due to their low computational resources. Despite these limitations,
the implementation of advanced control techniques such as Model Predictive
Control (MPC) has been explored by the control community in recent years,
typically considering simple linear formulations or explicit ones to facilitate
the online computation of the control input. These simplifications often lack
features and properties that are desirable in real-world environments. In this
article, we present an efficient implementation for embedded systems of MPC for
tracking with artificial reference, solved via a recently developed
structure-exploiting first-order method. This formulation is tailored to a wide
range of applications by incorporating essential practical features at a small
computational cost, including integration with an offset-free scheme, back-off
parameters that enable constraint tightening, and soft constraints that
preserve feasibility under disturbances or plant-model mismatch. We accompany
this with a framework for probabilistic performance validation of the
closed-loop system over long-term operation. We illustrate the applicability of
the approach on a Programmable Logic Controller (PLC), incorporated in a
hardware-in-the-loop setup to control a nonlinear continuous stirred-tank
reactor. The behavior of the closed-loop system is probabilistically validated
with respect to constraint violations and the number of iterations required at
each time step by the MPC optimization algorithm.

</details>


### [99] [A Constant-Gain Equation-Error Framework for Airliner Aerodynamic Monitoring Using QAR Data](https://arxiv.org/abs/2511.03678)
*Ruiying Wen,Yuntao Dai,Hongyong Wang*

Main category: eess.SY

TL;DR: 提出了一个常数增益的方程误差法 CG-EEM，用于在低信噪比、低激励 cruise 数据中对商用客机进行状态估计，获得一致且物理上合理的气动参数，并可用于舰队层面的性能监测和早期退化检测。


<details>
  <summary>Details</summary>
Motivation: 在机组日常运行数据（QAR）下，缺乏关键参数（如惯性矩）使传统状态传播滤波器难以适用，需开发新的解耦式估计框架以实现高效、稳健的性能监测。

Method: 比较分析传统递归估计器在 EEM 框架下对低激励 cruise 数据的稳定性；提出并验证 CG-EEM，通过常数增益近似的卡尔曼式更新实现对航空动力学参数的估计。大规模数据集（200+ 飞机航班）验证。

Result: CG-EEM 在大规模多机队数据上表现出高度一致、物理上合理的气动参数，正确识别不同机型的性能差异，且计算高效、可扩展。对舰队级别的监测和性能退化早期检测具有实用性。

Conclusion: 在低信噪比航巡数据环境中，常数增益的 EEM 可提供稳健的参数估计和性能监测解决方案，克服传统自适应滤波在缺乏激励条件时的局限性。

Abstract: Monitoring the in-service aerodynamic performance of airliners is critical
for operational efficiency and safety, but using operational Quick Access
Recorder (QAR) data for this purpose presents significant challenges. This
paper first establishes that the absence of key parameters, particularly
aircraft moments of inertia, makes conventional state-propagation filters
fundamentally unsuitable for this application. This limitation necessitates a
decoupled, Equation-Error Method (EEM). However, we then demonstrate through a
comparative analysis that standard recursive estimators with time-varying
gains, such as Recursive Least Squares (RLS), also fail within an EEM
framework, exhibiting premature convergence or instability when applied to
low-excitation cruise data. To overcome these dual challenges, we propose and
validate the Constant-Gain Equation-Error Method (CG-EEM). This framework
employs a custom estimator with a constant, Kalman-like gain, which is
perfectly suited to the stationary, low-signal-to-noise characteristics of
cruise flight. The CG-EEM is extensively validated on a large, multi-fleet
dataset of over 200 flights, where it produces highly consistent, physically
plausible aerodynamic parameters and correctly identifies known performance
differences between aircraft types. The result is a robust, scalable, and
computationally efficient tool for fleet-wide performance monitoring and the
early detection of performance degradation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [100] [List Decoding and New Bicycle Code Constructions for Quantum LDPC Codes](https://arxiv.org/abs/2511.02951)
*Sheida Rabeti,Hessam Mahdavifar*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we propose a new decoder, called the Multiple-Bases
Belief-Propagation List Decoder (MBBP-LD), for Quantum Low-Density Parity-Check
(QLDPC) codes. It extends the Multiple-Bases Belief-Propagation (MBBP)
framework, originally developed for classical cyclic LDPC codes. The proposed
method preserves the linear-time complexity of standard BP decoder while
improving the logical error rate. To further reduce the logical error rate, a
new decision rule is introduced for the post-processing list decoder,
outperforming the conventional least-metric selector (LMS) criterion. For the
recently developed and implemented bivariate bicycle (BB) code with parameters
\([[144,12,12]]\), our proposed MBBP-LD decoder achieves up to 40\% lower
logical error rate compared to the state-of-the-art decoder for short QLDPC
codes, i.e., BP with ordered-statistics decoding (BP-OSD), while retaining the
linear-time complexity of the plain BP decoder. In addition, we explore a new
subclass of BB codes, that we refer to as the univariate bicycle (UB) codes,
specifically with lower-weight parity checks (\(w=6,8\)). This reduces the
polynomial search space for the code compared to general BB codes, i.e., by
reducing the search space over two polynomial components in BB codes to just a
single polynomial component in UB codes. Simulations demonstrate the promising
performance of these codes under various types of BP decoders.

</details>


### [101] [A Tsallis-Entropy Lens on Genetic Variation](https://arxiv.org/abs/2511.03063)
*Margarita Geleta,Daniel Mas Montserrat,Alexander G. Ioannidis*

Main category: cs.IT

TL;DR: 提出了基于Tsallis-熵的广义F统计量F_q，作为对F_ST的谱差分扩展，通过调节q来加权稀有/常见等位基因，从而在现实数据和模拟数据中实现对亚群结构的更细粒度分解与历史事件时间戳。


<details>
  <summary>Details</summary>
Motivation: 现有F_ST及基于熵的指标在处理等位基因频谱偏斜和定位驱动亚群方面的局限性需要一个可调权重的通用指标，以更精细地刻画分化程度与区域驱动因素。

Method: 定义F_q为子群相对于合并总体损失的Tsallis q-熵的分数，q=2时退化为方差基的F_ST，q=1时等价于香农熵的互信息；在One-vs-Rest(OVR)和Leave-One-Out(LOO)模式下对865个大洋洲基因组数据（1,823,000个位点）及来自HGDP/1000 Genomes的322,216个位点的受控系统发家模拟进行评估，观察亚群驱动因素的归属和 isolation-migration/创始事件的时间推断。

Result: F_q能清晰指认驱动区域结构的子群，并在面对偏斜的等位基因频谱时提供更细粒度的区分；OVR与LOO模式下对驱动子群的 attribution 更敏感，能更精确地标定孤立-迁移事件及创始效应；为模拟审计和群体结构汇总提供更高分辨率的补充。

Conclusion: F_q将 fixation 统计推广到一个可通过q调控的谱权重框架，提供了对群体结构和历史事件的更细致刻画，成为仿真审计与群体结构描述的有用补充。

Abstract: We introduce an information-theoretic generalization of the fixation
statistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the
fraction of Tsallis $q$-entropy lost within subpopulations relative to the
pooled population. The family nests the classical variance-based fixation index
$F_{\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose
absolute form equals the mutual information between alleles and population
labels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights
rare variants at low $q$, while $q{>}1$ increasingly emphasizes common
variants, providing a more fine-grained view of differentiation than
$F_{\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865
Oceanian genomes with 1,823,000 sites) and controlled genealogical simulations
(seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216
sites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes
provides clear attribution of which subpopulations drive regional structure,
and sensitively timestamps isolation-migration events and founder effects.
$F_q$ serves as finer-resolution complement for simulation audits and
population-structure summaries.

</details>


### [102] [DRL-Based Robust Multi-Timescale Anti-Jamming Approaches under State Uncertainty](https://arxiv.org/abs/2511.03305)
*Haoqin Zhao,Zan Li,Jiangbo Si,Rui Huang,Hang Hu,Tony Q. S. Quek,Naofal Al-Dhahir*

Main category: cs.IT

TL;DR: 提出多时间尺度决策模型并在感知不完美下提供两种鲁棒 DRL 方法以提升抗干扰能力，实验显示在有界传感误差时仍保持接近完美感知的性能。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统易受恶意干扰，现有抗干扰方法通常假设感知准确且单一时间尺度优化，忽略执行延迟不匹配和传感器误差等现实因素，导致鲁棒性不足。

Method: 构建多时间尺度的状态不确定性模型；提出两种鲁棒方案：1) PGD-DDQN：在对抗扰动下通过投影梯度下降进行鲁棒训练，导出在范数界限内的最坏扰动；2) NQC-DDQN：引入非线性Q值压缩机制，动态缩小Q值区间以消除动作混叠。

Result: 仿真表明，与完美感知基线相比，所提算法在各种扰动下的降幅较小，能够在有界感知误差条件下保持良好抗干扰性能。

Conclusion: 在存在感知误差与执行延迟不匹配的现实条件下，提出的两种鲁棒 DRL 方案对无线抗干扰具备实际应用潜力与可行性。

Abstract: Owing to the openness of wireless channels, wireless communication systems
are highly susceptible to malicious jamming. Most existing anti-jamming methods
rely on the assumption of accurate sensing and optimize parameters on a single
timescale. However, such methods overlook two practical issues: mismatched
execution latencies across heterogeneous actions and measurement errors caused
by sensor imperfections. Especially for deep reinforcement learning (DRL)-based
methods, the inherent sensitivity of neural networks implies that even minor
perturbations in the input can mislead the agent into choosing suboptimal
actions, with potentially severe consequences. To ensure reliable wireless
transmission, we establish a multi-timescale decision model that incorporates
state uncertainty. Subsequently, we propose two robust schemes that sustain
performance under bounded sensing errors. First, a Projected Gradient
Descent-assisted Double Deep Q-Network (PGD-DDQN) algorithm is designed, which
derives worst-case perturbations under a norm-bounded error model and applies
PGD during training for robust optimization. Second, a Nonlinear Q-Compression
DDQN (NQC-DDQN) algorithm introduces a nonlinear compression mechanism that
adaptively contracts Q-value ranges to eliminate action aliasing. Simulation
results indicate that, compared with the perfect-sensing baseline, the proposed
algorithms show only minor degradation in anti-jamming performance while
maintaining robustness under various perturbations, thereby validating their
practicality in imperfect sensing conditions.

</details>


### [103] [Constacyclic codes with best-known parameters](https://arxiv.org/abs/2511.03323)
*Zekai Chen,Min Sha*

Main category: cs.IT

TL;DR: 构造若干无穷族的 q-ary constacyclic 码，长度 n，维度约 n/2，最小距离至少 c n / log_q n；包含大量达到最优、近似最优或已知最好参数的码；并考察长度 n 的不同形式。


<details>
  <summary>Details</summary>
Motivation: 在有限字段上寻找具有较高信息效率和较大最小距离的 constacyclic 码，扩展可用的码族以适应理论研究和实际应用的需求。

Method: 通过对长度 n 的选取及常变因子的结构设计，构造无穷族 q-ary constacyclic 码，使维度接近 n/2，最小距离下界为 c n / log_q n；通过对生成多项式、根结构或等价分解的分析，证明距离界并覆盖多种 n 的形式。

Result: 给出存在无穷族的代码群，其最小距离达到 θ(n/log_q n) 的下界，维度约 n/2；包含大量参数达到最优、近似最优或已知最好参数的 constacyclic 码；并对长度 n 的不同形式进行了分析与覆盖。

Conclusion: 此工作显著扩展了 q-ary constacyclic 码的参数空间，提供了具有良好比率的线性时间内距离的码族，并讨论了不同长度形式的适用性，具有理论与潜在应用意义。

Abstract: In this paper, we construct several infinite families of $q$-ary constacyclic
codes over a finite field $\mathbb{F}_q$ with length $n$, dimension around
$n/2$, and minimum distance at least $cn/\log_q n$ for some positive constant
$c$. They contain many constacyclic codes with optimal, or almost-optimal, or
best-known parameters. We also consider various forms of the length $n$.

</details>


### [104] [The (+)-(L, P)-TGRS code](https://arxiv.org/abs/2511.03398)
*Zhonghao Liang,Chenlu Jia,Qunying Liao*

Main category: cs.IT

TL;DR: 本文研究一种新型的 (+)-(L, P)-TGRS 码 C，给出其校验矩阵，给出 NMDS 的必要充分条件，部分解决 Hu 等 2025 年提出的开问题；并在 2k>n 时给出非 RS 的结果，给出自正则/自正交的充要条件并构造两类自正交码，最后给出若干实例，推动了对非 RS 型码及自正交码的设计与理论理解。


<details>
  <summary>Details</summary>
Motivation: 非 Rees-Solomon 型线性码近来成为热点。通过引入 (L, P)-twisted GRSC 的 (+) 变体，研究者希望获得更广义的可符号极限和更丰富的自正交/自对偶结构；并尝试解决 Hu 等在 2025 年提出的开问题，推进对 NMDS 条件和自正交性等性质的理解。

Method: 1) 给出非 RS (+)-(L, P)-TGRS 码 C 的一个校验矩阵；2) 推导 C 为 NMDS 的必要且充分条件；3) 证明在 2k>n 时 C 为非 RS，部分改进 Hu 等的结论；4) 给出 C 不自对称/不自正交的充要条件，并构造两类自正交码（拓展 Ding 等 2025 的结果）；5) 通过具体实例说明。

Result: 得到关于 C 的 NMDS 等价条件、2k>n 时非 RS 的充要性、以及自正交性的构造框架；并给出若干实例以示范应用。

Conclusion: 该工作在部分回答 Hu 等 2025 年提出的开问题方面取得进展，部分改进了相关极值码的已有结论，并提供了自正交码构造的新途径，为未来对非 RS 型码及自正交结构的研究提供了新的工具与方向。

Abstract: The construction of the non-Reed-Solomon (in short, non-RS) type linear code
has been one of the research hotspots in recent years. In 2025, Hu et al.
constructed some non-RS MDS codes by defining the (L, P)-twisted generalized
Reed-Solomon code (in short, (L, P)-TGRS). In this paper, we focus on the
(+)-(L, P)-TGRS code C. We firstly present a parity-check matrix. Secondly, we
give a sufficient and necessary condition for C to be NMDS which partially
answers two open problems proposed by Hu et al. in 2025, and prove that C is
non-RS for 2k > n which partially improves the corresponding result given by Hu
et al. in 2025,. Thirdly, we give a sufficient condition for C not to be
self-dual or self-orthogonal, respectively, furthermore, we construct two
classes of self-orthogonal codes which is a promotion of the corresponding
result given by Ding et al. in 2025. Finally, some examples are given.

</details>


### [105] [On the Fundamental Scaling Laws of Fluid Antenna Systems](https://arxiv.org/abs/2511.03415)
*Xusheng Zhu,Farshad Rostami Ghadi,Tuo Wu,Kaitao Meng,Chao Wang,Gui Zhou*

Main category: cs.IT

TL;DR: 论文推导了在空间相关信道中，流体天线系统(FAS)的符号错误率(SER)紧致渐近表达，并揭示SER与空间相关性的基本缩放规律；通过扩展天线的移动空间来提升多样性比在有限空间内单纯增加端口密度更有效，对FAS的多样性与编码增益进行了完整刻画。


<details>
  <summary>Details</summary>
Motivation: 弥补当前缺乏FAS SER的严格分析框架的空白，阐明空间相关性和移动空间扩展对系统性能的影响，并提供具体的设计指引。

Method: 对广义调制和一般类的具有空间相关性的信道模型，推导出FAS的紧致渐近SER的闭式解，给出导致SER的基本缩放规律；在同一框架下解析多样性与编码增益，并比较移动空间扩展与端口密度增加的效应。

Result: 给出闭式的渐近SER表达式、 fundamental scaling laws、以及对多样性与编码增益的完整刻画；给出设计指令：通过扩展移动空间来增加多样性以提升SER，受限空间内增加端口密度收益递减。

Conclusion: 在FAS设计中，提升SER的关键在于扩大移动空间以提高空间多样性；所提框架为FAS性能分析和设计提供了理论基石与可执行的设计指南。

Abstract: Fluid antenna systems (FAS) offer a promising paradigm for enhancing wireless
communication by exploiting spatial diversity, yet a rigorous analytical
framework for their error probability has been notably absent. To this end,
this paper addresses this critical gap by unveiling the \textbf{fundamental
scaling laws} that govern the symbol error rate (SER) of FAS in realistic,
spatially correlated channels. To establish these laws, we derive a tight,
closed-form asymptotic expression for the SER applicable to a general class of
modulation schemes. This result is pivotal as it establishes the fundamental
scaling law governing the relationship between SER and the channel's spatial
correlation structure. Based on this framework, we provide a complete
characterization of the diversity and coding gains. The analysis culminates in
a definitive design directive: SER can be fundamentally improved by expanding
the antenna's movement space to increase diversity, while merely increasing
port density within a constrained space yields diminishing returns.

</details>
