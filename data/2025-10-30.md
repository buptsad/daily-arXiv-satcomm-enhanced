<div id=toc></div>

# Table of Contents

- [eess.SY](#eess.SY) [Total: 15]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CR](#cs.CR) [Total: 10]
- [eess.SP](#eess.SP) [Total: 17]
- [cs.IT](#cs.IT) [Total: 8]


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1] [Blockage-Aware Multi-RIS WSR Maximization via Per-RIS Indexed Synchronization Sequences and Closed-Form Riemannian Updates](https://arxiv.org/abs/2510.24723)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: 提出一个面向阻塞的多RIS加权和速率优化框架，通过对RIS面板阻塞进行能量检测识别，并在检测结果的约束下，使用闭式Riemannian相位对齐CRPA算法实现BS端和RIS端的联合优化，达到单模约束的无投影更新与单调上升性，获得显著的WSR与收敛提升。


<details>
  <summary>Details</summary>
Motivation: 毫米波多用户MIMO系统对阻塞极为敏感，虽然RIS可缓解，但RIS链路也可能被阻挡，因此需要一个端到端的阻塞感知优化框架来提升系统鲁棒性和吞吐量。

Method: BS发送按各RIS索引的短同步信号，利用能量检测使用户识别被阻塞的RIS面板；基于检测得到的可行子集，进行BS前端波束和RIS相位的联合优化，所提出的CRPA算法提供单元模保持的闭式更新，无需投影或线搜索，且保证单调上升。

Result: 仿真结果验证了阻塞检测的可靠性，并显示在WSR和收敛性方面相较于基线具有显著 gains。

Conclusion: 所提出的阻塞感知多RIS优化框架提高了系统鲁棒性和性能，CRPA提供了一种简单高效的实现途径。

Abstract: Millimeter-wave (mmWave) multi-user MIMO systems are highly vulnerable to
blockage, and reconfigurable intelligent surfaces (RIS) have been proposed as a
remedy. However, RIS links may themselves be blocked, while most prior works
assume ideal RIS availability. We propose an end-to-end blockage-aware
multi-RIS weighted sum-rate (WSR) optimization framework. The BS transmits
short per-RIS indexed synchronization signals, enabling each user to identify
blocked panels through a simple energy detection test. Based on the detected
feasible sets, we jointly optimize the BS precoder and RIS phases via a
Closed-form Riemannian Phase Alignment (CRPA) algorithm. CRPA provides
unit-modulus-preserving closed-form updates, requiring no projection or line
search, and ensures monotone ascent. Simulations validate reliable blockage
detection and notable WSR and convergence gains over existing baselines.

</details>


### [2] [Principal and Combination Parametric Resonances of an Electromagnetically Suspended Vehicle subject to Base Excitation](https://arxiv.org/abs/2510.24756)
*Jithu Paul,Karel N. van Dalen,Andrei B. Faragau,Rens J. van Leijden,Biagio Carboni,Andrei V. Metrikine*

Main category: eess.SY

TL;DR: 通过三自由度电磁悬浮车辆模型，分析在周期性外激励下的动态稳定性；利用PD控制的空气隙非线性磁力、时变系数线性化、Hills扩展法与 Floquet 理论，得到稳定边界为控制增益平面的椭圆，并揭示主共振与组合共振边界之比。


<details>
  <summary>Details</summary>
Motivation: 解决在 Hyperloop/Maglev 中，支撑表面粗糙或外部噪声导致的周期性激励下，窄间隙系统的动态稳定性问题，提供对控制增益的定量设计指引。

Method: 建立车辆由两组相同电磁执行器悬挂于刚性支撑的三自由度模型；基于力矩平衡和Kirchhoff定律推导非线性电磁力，并对空气隙实施PD控制；围绕支撑引起的稳态进行线性化得到时变系数方程，使用扩展 Hills 方法分析主共振与组合共振，并用 Floquet 理论进行数值验证。

Result: 稳定边界表现为控制增益平面的椭圆；主共振的两个椭圆大小之比为3:1，组合共振的两个椭圆大小之比为14:1；当三种椭圆同时存在时，组合共振相关的一个椭圆是最大的。

Conclusion: 结果揭示了不同参数与共振类型对稳定边界的影响格局，强调在控制器设计中需关注组合共振的显著影响，以及在包含多椭圆的情形下，某一组合共振可能主导稳定性边界，提供对系统参数调优的定量指南。

Abstract: This paper investigates the dynamic stability of an electromagnetically
suspended vehicle, encountered in Hyperloop and Maglev systems, subject to
periodic excitations caused by surface irregularities or vibration of the
support induced by external noise. The narrow clearance between the vehicle and
the support can make it highly sensitive to small oscillations, since the
admissible amplitudes of the vehicle oscillations can be comparable to external
excitation amplitude. The vehicle is modelled as a three-degree-of-freedom
model where the vehicle is suspended via two identical electromagnetic
actuators from a rigid support that oscillates. The governing equations are
derived using force and torque balances, incorporating nonlinear
electromagnetic forces, and Kirchhoffs law for the electromagnets with PD
control strategy on the airgap. The equations of motion are linearized around
the steady state induced by the surface oscillation, yielding a system with
time-periodic coefficients. We analytically explore both principal and
combination parametric resonances using an extended Hills method, and Floquet
theory is used for numerical validation. The stability boundaries are obtained
as ellipses in control gain parameter space, and the influence of system
parameters on these boundaries is characterized. For the principal parametric
resonance, the ratio of the sizes of the two obtained ellipses is three to one,
whereas for the combination parametric resonance, the ratio is fourteen to one.
When all ellipses are simultaneously present, one of the ellipses associated
with the combination parametric resonance is the largest.

</details>


### [3] [Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification](https://arxiv.org/abs/2510.24757)
*Ahmet Eren Sertbaş,Tufan Kumbasar*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate modeling of nonlinear systems is essential for reliable control, yet
conventional identification methods often struggle to capture latent dynamics
while maintaining stability. We propose a \textit{stable-by-design LPV neural
network-based state-space} (NN-SS) model that simultaneously learns latent
states and internal scheduling variables directly from data. The
state-transition matrix, generated by a neural network using the learned
scheduling variables, is guaranteed to be stable through a Schur-based
parameterization. The architecture combines an encoder for initial state
estimation with a state-space representer network that constructs the full set
of scheduling-dependent system matrices. For training the NN-SS, we develop a
framework that integrates multi-step prediction losses with a state-consistency
regularization term, ensuring robustness against drift and improving
long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark
nonlinear systems, and the results demonstrate that the model consistently
matches or surpasses classical subspace identification methods and recent
gradient-based approaches. These findings highlight the potential of
stability-constrained neural LPV identification as a scalable and reliable
framework for modeling complex nonlinear systems.

</details>


### [4] [Delay Tolerant Control for Autonomous Driving Using CDOB](https://arxiv.org/abs/2510.24898)
*Xincheng Cao,Haochong Chen,Levent Guvenc,Bilin Aksun-Guvenc*

Main category: eess.SY

TL;DR: 提出一种延迟容忍的通信干扰观测器(CDOB)框架，用于延迟系统中的路径跟踪控制，能在未知和变化的时延条件下保持轨迹跟踪精度。仿真结果表明该方法在单/双车道变换及弹性带(Elastic Band)生成的避碰路径等场景下优于传统方法，表现出更好的跟踪精度和对时延的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的发展，车辆需要在复杂交通场景下实现高效可靠的路径跟踪；但车联网中的通信与计算时延会显著降低传统控制器的性能，且 disturbance observer(DOB) 在未知时滞下性能下降明显。因此需要一种对时延具有鲁棒性的观测与控制方案。

Method: 提出延迟容忍的通信干扰观测器(CDOB)框架，将时延的影响纳入干扰观测与控制的设计中，以实现对轨迹的精准跟踪，并在不同时延条件下保持性能。通过仿真实验 comparing CDOB 与传统PID和DOB等控制策略，验证其对单车道变换、双车道变换和基于Elastic Band的避碰路径的跟踪鲁棒性。

Result: 仿真结果显示，CDOB 能在各种时延条件下维持与参考轨迹的高度对齐，且在跟踪精度和时延鲁棒性方面优于常规方法。

Conclusion: 所提出的CDOB 框架适用于自动驾驶路径跟踪任务，尤其在存在不可忽略且不确定的传输与计算时延时，能显著提升跟踪性能与鲁棒性。

Abstract: With the rapid growth of autonomous vehicle technologies, effective
path-tracking control has become a critical component in ensuring safety and
efficiency in complex traffic scenarios. When a high level decision making
agent generates a collision free path, a robust low level controller is
required to precisely follow this trajectory. However, connected autonomous
vehicles (CAV) are inherently affected by communication delays and computation
delays, which significantly degrade the performance of conventional controllers
such as PID or other more advanced controllers like disturbance observers
(DOB). While DOB-based designs have shown effectiveness in rejecting
disturbances under nominal conditions, their performance deteriorates
considerably in the presence of unknown time delays. To address this challenge,
this paper proposes a delay-tolerant communication disturbance observer (CDOB)
framework for path-tracking control in delayed systems. The proposed CDOB
compensates for the adverse effects of time delays, maintaining accurate
trajectory tracking even under uncertain and varying delay conditions. It is
shown through a simulation study that the proposed control architecture
maintains close alignment with the reference trajectory across various
scenarios, including single lane change, double-= lane change, and Elastic Band
generated collision avoidance paths under various time delays. Simulation
results further demonstrate that the proposed method outperforms conventional
approaches in both tracking accuracy and delay robustness, making it well
suited for autonomous driving applications.

</details>


### [5] [A Hamilton-Jacobi Reachability Framework with Soft Constraints for Safety-Critical Systems](https://arxiv.org/abs/2510.24933)
*Chams Eddine Mballo,Donggun Lee,Claire J. Tomlin*

Main category: eess.SY

TL;DR: 提出一种将软约束引入的可达性分析框架，扩展 Hamilton-Jacobi reachability，通过增广状态的预算变量和正则化近似来在最坏扰动下保证安全同时限制软约束违规量。


<details>
  <summary>Details</summary>
Motivation: 传统可达性分析严格将状态约束视为不可违背，导致在复杂场景下过于保守或不可行。本工作通过允许在安全边际内的软约束违规来提高可用性，解决现实系统中对软约束的需求与权衡。

Method: 引入增广状态模型，增加一个预算状态以跟踪软约束违规；对离散化的 Hamilton-Jacobi 值函数采用正则化近似以处理其不连续性，形成硬约束与软约束并存的 Reach-Avoid 框架；并给出用于计算 soft-constrained Reach-Avoid 集的算法和实现细节。

Result: 通过点质量模型着陆和在风扰动下的固定翼飞机紧急下降等数值示例，验证框架能在最坏扰动下确保到达目标集合的安全性，同时将软约束违规控制在用户设定的预算范围内。

Conclusion: 为带硬/软约束的安全性验证提供一种折中方法，扩展了传统哈密尔顿-贾庞可达性分析的适用性，特别适用于带扰动的安全关键系统。

Abstract: Traditional reachability methods provide formal guarantees of safety under
bounded disturbances. However, they strictly enforce state constraints as
inviolable, which can result in overly conservative or infeasible solutions in
complex operational scenarios. Many constraints encountered in practice, such
as bounds on battery state of charge in electric vehicles, recommended speed
envelopes, and comfort constraints in passenger-carrying vehicles, are
inherently soft. Soft constraints allow temporary violations within predefined
safety margins to accommodate uncertainty and competing operational demands,
albeit at a cost such as increased wear or higher operational expenses. This
paper introduces a novel soft-constrained reachability framework that extends
Hamilton-Jacobi reachability analysis for the formal verification of
safety-critical systems subject to both hard and soft constraints.
Specifically, the framework characterizes a subset of the state space, referred
to as the soft-constrained reach-avoid set, from which the system is guaranteed
to reach a desired set safely, under worst-case disturbances, while ensuring
that cumulative soft-constraint violations remain within a user-specified
budget. The framework comprises two principal components: (i) an
augmented-state model with an auxiliary budget state that tracks
soft-constraint violations, and (ii) a regularization-based approximation of
the discontinuous Hamilton-Jacobi value function associated with the
reach-avoid differential game studied herein. The effectiveness of the proposed
framework is demonstrated through numerical examples involving the landing of a
simple point-mass model and a fixed-wing aircraft executing an emergency
descent, both under wind disturbances. The simulation results validate the
framework's ability to simultaneously manage both hard and soft constraints in
safety-critical settings

</details>


### [6] [Control Synthesis with Reinforcement Learning: A Modeling Perspective](https://arxiv.org/abs/2510.25063)
*Nikki Xu,Hien Tran*

Main category: eess.SY

TL;DR: 基于强化学习的控制器对模型错配敏感；在精确模型下训练的控制器对扰动和小的模型偏差具鲁棒性，错误模型在仿真中表现良好却在物理系统中失败。


<details>
  <summary>Details</summary>
Motivation: 解释仿真-现实领域的模型不一致如何破坏强化学习控制器的鲁棒性，以及如何通过敏感性分析和经验区域估计来理解和可视化鲁棒性差异。

Method: 系统性地在不同模型精度下对控制器进行评估，比较用不准确模型与准确模型训练的控制器在扰动条件下的表现；利用敏感性分析与经验区域估计来可视化鲁棒性。

Result: 使用准确模型训练的控制器对扰动和小幅模型偏差表现鲁棒；而基于不准确模型的控制器在仿真中表现良好但在物理实验中失败；敏感性分析解释差异，经验区域估计帮助可视化鲁棒性。

Conclusion: 须优先使用尽可能准确的模型或引入鲁棒性增强的训练/仿真方法，单靠在不准确模型上训练得到的控制器难以直接迁移到真实系统。

Abstract: Controllers designed with reinforcement learning can be sensitive to model
mismatch. We demonstrate that designing such controllers in a virtual
simulation environment with an inaccurate model is not suitable for deployment
in a physical setup. Controllers designed using an accurate model is robust
against disturbance and small mismatch between the physical setup and the
mathematical model derived from first principles; while a poor model results in
a controller that performs well in simulation but fails in physical
experiments. Sensitivity analysis is used to justify these discrepancies and an
empirical region of attraction estimation help us visualize their robustness.

</details>


### [7] [Stochastic Long-Term Joint Decarbonization Planning for Power Systems and Data Centers: A Case Study in PJM](https://arxiv.org/abs/2510.25118)
*Zhentong Shao,Nanpeng Yu,Daniel Wong*

Main category: eess.SY

TL;DR: 动态联合规划框架：在15年内对数据中心和电力系统进行共同优化，纳入运维与 embodied 碳排放，以及成本与减排的综合收益；采用两阶段随机规划与改进的边界法求解，应用于 PJM。


<details>
  <summary>Details</summary>
Motivation: 随着AI和云服务快速增长，数据中心能源需求与碳排放上升，需碳感知的基础设施规划。现有研究多假设静态电力系统、仅考虑运营排放且缺乏协同优化，因此难以实现全面减排与成本优化。

Method: 提出一个动态的联合规划框架，在15年的时间尺度上对数据中心的选址、容量与类型，以及电力系统的发电扩容、储能部署与退役等进行联立优化；同时考虑运营排放和生命周期排放；将多尺度不确定性转化为大规模两阶段随机规划，并通过增强型 Benders 分解求解；以 PJM 为例，使用 GitHub 数据集；结果分析包括对比单一规划的优势。

Result: 框架可支持最高约55 GW峰值数据中心用量；最佳主机为弗吉尼亚州（DOM）与北伊利诺伊州（ComEd）。与非联合规划相比，投资成本降低约12.6%，运营成本降低约8.25%，排放降低约5.63%。若纳入生命周期排放，可使可再生能源部署增加约25.5%，凸显 embodied 碳在深度减碳中的作用。

Conclusion: 将生命周期排放纳入数据中心与电力系统的联合规划显著提升深度 decarbonization 的潜力，强调在长期规划中考虑 embodied carbon 对可再生部署与总排放的驱动作用。

Abstract: With the rapid growth of artificial intelligence (AI) and cloud services,
data centers have become critical infrastructures driving digital economies,
with increasing energy demand heightening concerns over electricity use and
carbon emissions, emphasizing the need for carbon-aware infrastructure
planning. Most studies assume static power systems, focus only on operational
emissions, and overlook co-optimization. This paper proposes a dynamic joint
planning framework that co-optimizes long-term data center and power system
development over 15 years. The model determines siting, capacity, and type of
data centers alongside power generation expansion, storage deployment, and
retirements, accounting for both operational and embodied emissions. To handle
multi-scale uncertainty, a large-scale two-stage stochastic program is
formulated and solved via an enhanced Benders decomposition. Applied to the PJM
Interconnection, with curated datasets released on GitHub, results show the
system can support up to 55 GW peak data center demand, with Virginia (DOM) and
Northern Illinois (ComEd) as optimal hosts. Compared to non-joint planning, the
framework cuts investment cost by 12.6%, operational cost by 8.25%, and
emissions by 5.63%. Including lifecycle emissions further raises renewable
deployment by 25.5%, highlighting embodied carbon's role in deeper
decarbonization.

</details>


### [8] [The Waterbed Effect on Quasiperiodic Disturbance Observer: Avoidance of Sensitivity Tradeoff with Time Delays](https://arxiv.org/abs/2510.25131)
*Hisayoshi Muramatsu*

Main category: eess.SY

TL;DR: 引入带时间延迟的准周期性干扰观测器可以打破经典的水床效应敏感性权衡，作者给出连续-和离散时间的类Bode敏感性积分来理论化地支持这一点，并阐明时间延迟如何规避敏感性权衡。


<details>
  <summary>Details</summary>
Motivation: 在线性时不变系统中，传递函数的敏感性受水床效应限制。对于准周期性干扰，时间延迟观测器提供广域带宽的准周期干扰抑制且不放大非周期性扰动或改变抑制频率。但时滞导致开环传递函数非有理形，难以直接应用传统的Bode敏感性积分。因此需要扩展到类Bode积分以涵盖时滞系统。

Method: 提出基于时间延迟的准周期干扰观测器；给出连续时间和离散时间表示下的类Bode敏感性积分；分析并解释为何时滞能避免传统的敏感性权衡，并给出对系统灵敏度的定量化关系。

Result: 建立了连续时间和离散时间的类Bode敏感性积分框架，理论上证明并阐明了时间延迟对敏感性权衡的规避作用，且观测器实现时可在广义上实现对准周期性分量的广带宽抑制而不显著放大非周期成分。

Conclusion: 时滞观测器使准周期干扰抑制在某些指标下突破了经典的敏感性权衡；将类Bode积分扩展到含时滞的系统中，为设计提供新的理论工具和直觉。

Abstract: In linear time-invariant systems, the sensitivity function to disturbances is
designed under a sensitivity tradeoff known as the waterbed effect. To
compensate for a quasiperiodic disturbance, a quasiperiodic disturbance
observer using time delays was proposed. Its sensitivity function avoids the
sensitivity tradeoff, achieving wideband harmonic suppression without
amplifying aperiodic disturbances or shifting harmonic suppression frequencies.
However, its open-loop transfer function is not rational and does not satisfy
the assumptions of existing Bode sensitivity integrals due to its time delays.
This paper provides Bode-like sensitivity integrals for the quasiperiodic
disturbance observer in both continuous-time and discrete-time representations
and clarifies the avoided sensitivity tradeoff with time delays.

</details>


### [9] [Silicon-based Josephson junction field-effect transistors enabling cryogenic logic and quantum technologies](https://arxiv.org/abs/2510.25208)
*Yusheng Xiong,Kaveh Delfanazari*

Main category: eess.SY

TL;DR: 综述从Josephson结到场效应晶体管的发展，评估JJFET在低温下的超导源/漏极及材料兼容性，强调Si/GaAs/InGaAs基板上的应用潜力，以及超导-硅-超导核心在低功耗、相干性信号处理中的作用。


<details>
  <summary>Details</summary>
Motivation: 在摩尔定律放缓的大背景下，寻求在低温环境下实现极低功耗与高速度的新型器件范式，JJFET作为连接半导体电子学与量子电路的桥梁，被视为实现 Cryogenic logic 与量子电子系统的重要候选。

Method: 作为综述论文，系统分析了从Josephson结到JJFET的发展历程，重点讨论结构与功能创新、在Si、GaAs、InGaAs等基板上的性能与材料兼容性、开关动态，以及以超导-硅-超导为核心的JJFET结构。

Result: 通过回顾四十余年的实验进展，指出JJFET具有成为低温逻辑与量子系统基础构建块的潜力，并对其在不同材料体系中的性能进行了综合评价。

Conclusion: JJFET有望在跨温度域实现能效高、相干性强的信号处理，促进传统半导体电子学与 cryogenic/量子电路的融合，指向未来的研究与集成路线。

Abstract: The continuous miniaturisation of metal-oxide-semiconductor field-effect
transistors (MOSFETs) from long- to short-channel architectures has advanced
beyond the predictions of Moore's Law. Continued advances in semiconductor
electronics, even near current scaling and performance boundaries under
cryogenic conditions, are driving the development of innovative device
paradigms that enable ultra-low-power and high-speed functionality. Among
emerging candidates, the Josephson Junction Field-Effect Transistor (JJFET or
JoFET) provides an alternative by integrating superconducting source and drain
electrodes for efficient, phase-coherent operation at ultra-low temperatures.
These hybrid devices have the potential to bridge conventional semiconductor
electronics with cryogenic logic and quantum circuits, enabling
energy-efficient and high-coherence signal processing across temperature
domains. This review traces the evolution from Josephson junctions to
field-effect transistors, emphasising the structural and functional innovations
that underpin modern device scalability. The performance and material
compatibility of JJFETs fabricated on Si, GaAs, and InGaAs substrates are
analysed, alongside an assessment of their switching dynamics and material
compatibility. Particular attention is given to
superconductor-silicon-superconductor Josephson junctions as the active core of
JJFET architectures. By unfolding more than four decades of experimental
progress, this work highlights the promise of JJFETs as foundational building
blocks for next-generation cryogenic logic and quantum electronic systems.

</details>


### [10] [Shared Control for Vehicle Lane-Changing with Uncertain Driver Behaviors](https://arxiv.org/abs/2510.25284)
*Jiamin Wu,Chenguang Zhao,Huan Yu*

Main category: eess.SY

TL;DR: 提出一种基于马尔可夫跳跃过程的人机共享变道控制框架，设计名义稳定控制器以在不完美模式估计下实现随机L2字符串稳定性，并引入最小干预控制器MIC以减少自动化负担；通过NGSIM/TGSIM数据集验证，显示在保持驾驶员主导权的同时提升稳定性、效率与舒适性，但存在稳定性与自动化程度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决人类驾驶行为的随机性导致的交通扰动和串联稳定性下降问题，寻求在保留驾驶员控制权的同时提供自动化协助以实现更稳健的变道性能。

Method: 将驾驶员行为建模为以任务难度驱动的马尔可夫跳跃过程；在此模型基础上设计一个名义稳定控制器，确保在模式估计不完全的情况下仍具备随机L2字符串稳定性；进一步提出最小干预控制器MIC，在保持可接受稳定性和性能的前提下降低自动化干预；通过对NGSIM数据集的变道数据进行仿真验证，并在带有SAE Level 2车辆的TGSIM数据上验证MIC的性能。

Result: 名义控制器在存在模式不确定时仍能降低速度扰动、缩短变道时间；MIC在减少自动化干预、提升舒适性的同时带来稳定性和效率的适度损失；在TGSIM中MIC允许比常规Level 2控制更早变道，同时保持驾驶员主导权，但伴随轻微的稳定性妥协。

Conclusion: 共享控制策略具有在稳定性、效率与驾驶员接受度之间实现平衡的潜力，为未来的混合主动控制在变道等场景中的应用提供了路径。

Abstract: Lane changes are common yet challenging driving maneuvers that require
continuous decision-making and dynamic interaction with surrounding vehicles.
Relying solely on human drivers for lane-changing can lead to traffic
disturbances due to the stochastic nature of human behavior and its variability
under different task demands. Such uncertainties may significantly degrade
traffic string stability, which is critical for suppressing disturbance
propagation and ensuring smooth merging of the lane-changing vehicles. This
paper presents a human-automation shared lane-changing control framework that
preserves driver authority while allowing automated assistance to achieve
stable maneuvers in the presence of driver's behavioral uncertainty. Human
driving behavior is modeled as a Markov jump process with transitions driven by
task difficulty, providing a tractable representation of stochastic state
switching. Based on this model, we first design a nominal stabilizing
controller that guarantees stochastic ${L}_2$ string stability under imperfect
mode estimation. To further balance performance and automated effort, we then
develop a Minimal Intervention Controller (MIC) that retains acceptable
stability while limiting automation. Simulations using lane-changing data from
the NGSIM dataset verify that the nominal controller reduces speed
perturbations and shorten lane-changing time, while the MIC further reduces
automated effort and enhances comfort but with moderate stability and
efficiency loss. Validations on the TGSIM dataset with SAE Level 2 vehicles
show that the MIC enables earlier lane changes than Level 2 control while
preserving driver authority with a slight stability compromise. These findings
highlight the potential of shared control strategies to balance stability,
efficiency, and driver acceptance.

</details>


### [11] [Data-Enabled Predictive Control and Guidance for Autonomous Underwater Vehicles](https://arxiv.org/abs/2510.25309)
*Sebastian Zieglmeier,Mathias Hudoba de Badyn,Narada D. Warakagoda,Thomas R. Krogstad,Paal Engelstad*

Main category: eess.SY

TL;DR: 基于数据驱动的DeePC框架用于AUV控制，在无需显式水动力建模的情况下实现更优跟踪，覆盖航向、深度与3D路径跟踪并在仿真中优于PI/PID。


<details>
  <summary>Details</summary>
Motivation: 降低对水动力模型的依赖、提升在海流扰动和非线性工况下的鲁棒性，同时显著降低建模工作量。

Method: 将DeePC用于航向控制、深度控制的级联DeePC并引入循环频率分离以处理不同输入输出的动态；将自适应线视线（ALOS）扩展为预测性形式并与DeePC结合实现3D路径跟踪；在REMUS 100 AUV的仿真中验证并与PI/PID对比。

Result: 仿真结果显示DeePC在跟踪性能和鲁棒性方面优于PI/PID，能有效应对海流扰动与非线性工况，同时显著降低建模工作量。

Conclusion: 数据驱动的DeePC在AUV多任务控制中表现出良好性能；级联设计和循环频率分离有助于处理不同动力模式，具备向实际应用推广的潜力。

Abstract: This paper presents a fully data-driven control framework for autonomous
underwater vehicles (AUVs) based on Data-Enabled Predictive Control (DeePC).
The approach eliminates the need for explicit hydrodynamic modeling by
exploiting measured input-output data to predict and optimize future system
behavior. Classic DeePC was employed in the heading control, while a cascaded
DeePC architecture is proposed for depth regulation, incorporating a
loop-frequency separation to handle the different dynamic modes of input and
output. For 3-D waypoint path following, the Adaptive Line-of-Sight algorithm
is extended to a predictive formulation and integrated with DeePC. All methods
are validated in extensive simulation on the REMUS 100 AUV and compared with
classical PI/PID control. The results demonstrate superior tracking performance
and robustness of DeePC under ocean-current disturbances and nonlinear
operating conditions, while significantly reducing modeling effort.

</details>


### [12] [Lightweight Federated Learning in Mobile Edge Computing with Statistical and Device Heterogeneity Awareness](https://arxiv.org/abs/2510.25342)
*Jinghong Tan,Zhichen Zhang,Kun Guo,Tsung-Hui Chang,Tony Q. S. Quek*

Main category: eess.SY

TL;DR: 提出一个基于参数解耦的轻量化个性化联邦学习框架：将模型分为全局共享子空间和私有子空间，分别对共享部分应用梯度稀疏化，对私有部分应用模型裁剪，以降低通信与计算；并给出收敛性分析及一个联合优化，按客户端单位选择稀疏度、裁剪率和带宽以缩短端到端训练时间。


<details>
  <summary>Details</summary>
Motivation: 在移动边缘计算场景中，联邦学习面临高通信和计算成本，且受统计和设备异质性的影响，影响实际部署。需要在保护个性化的同时提升效率，降低跨轮开销。

Method: 模型参数解耦成全局共享子空间和本地私有子空间。对共享子空间应用梯度稀疏化，对私有子空间进行模型裁剪，以实现全局知识交流的压缩和本地个性化的计算降低。给出在组合稀疏化与裁剪下的收敛性分析，揭示稀疏-裁剪的权衡与迭代复杂度的关系；据此提出联合优化问题，按客户端选择稀疏度、裁剪率和无线带宽以降低端到端训练时间。

Result: 通过仿真验证，该方法在收敛速度和总体通信/计算成本方面显著优于基线方法，同时精度损失可忽略，证明了在资源受限的异构环境中，协同和面向资源的个性化具有明显优势。

Conclusion: 参数解耦的联邦学习框架实现了对全局知识交换的有效压缩与对本地个性化的计算节约，适用于资源受限且异质性显著的边缘场景，揭示了稀疏化与裁剪之间的权衡，并提供了一个可用于跨设备优化的联合规划。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy, but high communication and computation costs, exacerbated by
statistical and device heterogeneity, limit its practicality in mobile edge
computing. Existing compression methods like sparsification and pruning reduce
per-round costs but may increase training rounds and thus the total training
cost, especially under heterogeneous environments. We propose a lightweight
personalized FL framework built on parameter decoupling, which separates the
model into shared and private subspaces, enabling us to uniquely apply gradient
sparsification to the shared component and model pruning to the private one.
This structural separation confines communication compression to global
knowledge exchange and computation reduction to local personalization,
protecting personalization quality while adapting to heterogeneous client
resources. We theoretically analyze convergence under the combined effects of
sparsification and pruning, revealing a sparsity-pruning trade-off that links
to the iteration complexity. Guided by this analysis, we formulate a joint
optimization that selects per-client sparsity and pruning rates and wireless
bandwidth to reduce end-to-end training time. Simulation results demonstrate
faster convergence and substantial reductions in overall communication and
computation costs with negligible accuracy loss, validating the benefits of
coordinated and resource-aware personalization in resource-constrained
heterogeneous environments.

</details>


### [13] [Optimal and Heuristic Approaches for Platooning Systems with Deadlines](https://arxiv.org/abs/2510.25564)
*Thiago S. Gomides,Evangelos Kranakis,Ioannis Lambadaris,Yannis Viniotis,Gennady Shaikhet*

Main category: eess.SY

TL;DR: 在有限容量L和截止时间T约束下，研究高速公路站点的卡车编队与调度问题，将其建模为离散时间MDP，给出L=3时的最优策略π*的结构性分析并推广至任意L，证实π*在状态空间上单调且存在不可达状态；由于状态量随L、T指数增长，提出利用结构特征的启发式方法与基于深度学习的策略以降低计算复杂度并保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 通过编队提升运输效率，降低燃料消耗与排放，同时满足Delivery deadlines并避免罚款；在考虑等待成本和截止时间违规成本的前提下，研究如何在有限容量和时间窗约束下形成与派遣最优编队。

Method: 将问题离散化建模为马尔可夫决策过程，首先对L=3情形的最优策略π*进行结构性分析，并尝试将结论推广至任意L；证明最优策略在状态空间上具有单调性，识别不可达状态集合；针对状态空间呈指数级增长的挑战，提出基于结构洞察的启发式算法（包括条件化策略）以及基于深度学习的近似策略，从而在保持低计算复杂度的同时获得可观性能。

Result: 得到的主要结果包括：1) 最优策略π*在状态空间上单调，便于近似与剪枝；2) 识别出若干不可达状态，显著减少搜索空间；3) 提出可扩展的启发式方法和深度学习基的近似策略，在较低复杂度下实现接近最优的调度与编队决策。

Conclusion: 通过揭示状态结构性特征，提供了在大规模L和T下的可行解框架；单调性与不可达状态等结构性结果为MDP解法提供简化途径，启发式和深度学习方法具备实际应用潜力与可扩展性。

Abstract: Efficient truck platooning is a key strategy for reducing freight costs,
lowering fuel consumption, and mitigating emissions. Deadlines are critical in
this context, as trucks must depart within specific time windows to meet
delivery requirements and avoid penalties. In this paper, we investigate the
optimal formation and dispatch of truck platoons at a highway station with
finite capacity $L$ and deadline constraints $T$. The system operates in
discrete time, with each arriving truck assigned a deadline of $T$ slot units.
The objective is to leverage the efficiency gains from forming large platoons
while accounting for waiting costs and deadline violations. We formulate the
problem as a Markov decision process and analyze the structure of the optimal
policy $\pi^\star$ for $L = 3$, extending insights to arbitrary $L$. We prove
that the $\pi^\star$ is monotone in the state space $\mathcal{S}$ and identify
classes of unreachable states. Moreover, since $\mathcal{S}$ grows
exponentially with $L$ and $T$, we propose heuristics-including conditional and
deep-learning based approaches-that exploit these structural insights while
maintaining low computational complexity.

</details>


### [14] [An OPF-based Control Framework for Hybrid AC-MTDC Power Systems under Uncertainty](https://arxiv.org/abs/2510.25671)
*Hongjin Du,Rahul Rane,Weijie Xia,Pedro P. Vergara,Aleksandra Lekić*

Main category: eess.SY

TL;DR: 提出一个基于预测的OPF自适应控制框架，用以在混合交流-高压直流系统中应对风力发电不确定性，通过将随机森林风速预测嵌入时域耦合OPF设定基准转换器并进行实时调整，发展自适应下垂控制以同时考虑直流电压和交流频率偏差，并通过硬件在环仿真验证其在高渗透率可再生能源条件下的稳定性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着海上风电等可再生能源接入增加，混合交流-HVDC系统的不确定性显著增大，预报误差和功率波动可能危及系统稳定性。传统控制多依赖固定设点且对频率偏差不敏感，需具备预测信息和时域耦合的自适应控制来提升鲁棒性。

Method: 使用随机森林风速预测作为输入，形成时域耦合的OPF以在风力波动前瞻性确定基线变换器设点；在实际运行工况下基于观测值进行实时调整。提出自适应下垂控制，联合考虑直流电压与交流频率的偏差以实现协同调节。通过硬件在环（HIL）仿真验证框架的有效性。

Result: 仿真结果表明该框架在高渗透率可再生能源条件下能够维持混合AC-HVDC系统的稳定性与鲁棒性，显示出对风力波动的适应能力。

Conclusion: 将预测信息融入OPF并通过自适应下垂实现直流－交流耦合控制，可显著提升混合AC-HVDC系统在可再生能源高渗透下的稳定性与鲁棒性。

Abstract: The increasing integration of renewable energy, particularly offshore wind,
introduces significant uncertainty into hybrid AC-HVDC systems due to forecast
errors and power fluctuations. Conventional control strategies typically rely
on fixed setpoints and neglect frequency deviations, which can compromise
system stability under rapid renewable variations. To address this challenge,
this paper presents a forecast-integrated, optimal power flow (OPF)-based
adaptive control framework. Wind speed forecasts generated using a Random
Forest model are incorporated into a time-coupled OPF to determine baseline
converter setpoints in anticipation of wind fluctuations, which are further
adjusted in real time based on actual operating conditions. An adaptive droop
control scheme is developed that jointly considers DC voltage and AC frequency
deviations. The effectiveness of the proposed control framework is validated
through hardware-in-the-loop (HIL) simulations, demonstrating its capability to
ensure stable and robust operation of hybrid AC-HVDC systems under high
penetration of renewable energy.

</details>


### [15] [Over 3 kV and Ultra-Low leakage Vertical (011) \b{eta}-Ga2O3 Power Diodes with Engineered Schottky Contact and High-permittivity Dielectric Field Plate](https://arxiv.org/abs/2510.25695)
*Emerson J. Hollar,Esmat Farzana*

Main category: eess.SY

TL;DR: 利用(011)β-Ga2O3的高耐压垂直结构，通过Pt cap/PtOx/Pt复合肖特基接触与高介电ZrO2场板实现3.7 kV击穿和极低泄漏的功率器件。


<details>
  <summary>Details</summary>
Motivation: 满足对高压、低泄漏β-Ga2O3功率器件的需求，尤其是多kV级垂直器件的可行性研究。

Method: 在(011)β-Ga2O3上实现薄漂移层和低背景掺杂；使用Pt cap/PtOx/Pt复合接触来提升反向阻断并保持低开启电压；以ZrO2场板实现高介电常数边缘场缓解；在同一晶圆上制备Pt/(011)β-Ga2O3 SBDs并比较击穿电压；通过结构优化提升击穿电压至3.7 kV并记录开通电压。

Result: 未加场板SBD击穿约1.5 kV；带场板的Pt/(011)β-Ga2O3 SBD击穿提升至2.75 kV；结合Pt cap/PtOx/Pt接触后击穿提升至3.7 kV，且开启电压保持接近原始Pt/(011) SBD的水平。

Conclusion: Pt cap/PtOx/Pt接触、场板(ZrO2)和(011)β-Ga2O3的组合为超低泄漏、以及多kV级垂直功率器件提供了一条有前景的实现路径。

Abstract: We report over 3 kV breakdown voltage and ultra-low leakage (011)
\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and
high-permittivity (\k{appa}) dielectric (ZrO2) field plate. The (011)
orientation of \b{eta}-Ga2O3 enabled low background doping and thick drift
layers which are promising to support kV-class vertical \b{eta}-Ga2O3 power
switches. The Schottky barrier engineering was performed with a composite Pt
cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse
blocking capabilities enabled by PtOx while allowing low turn-on voltage by the
interfacing thin Pt layer. We also performed a systematic study using a
co-processed Pt/(011) \b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same
wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the
field-plate Pt/(011) \b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage
of 2.75 kV owing to the edge field management. Further enhancement of the
breakdown voltage was achieved by tunneling leakage management using composite
Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown
voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt
(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)
\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management
by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,
edge field reduction by high-\k{appa} dielectric ZrO2 field plate, as well as
the advantageous material properties offered by (011) \b{eta}-Ga2O3 demonstrate
a promising strategy for developing ultra-low leakage and multi-kV class
vertical (011) \b{eta}-Ga2O3 power devices.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty](https://arxiv.org/abs/2510.24869)
*Mehrshad Eskandarpour,Hossein Soleimani*

Main category: cs.NI

TL;DR: PPO-based multi-objective reinforcement learning for QoS-aware load balancing in dense 5G RAN using a pure-Python simulator; achieves better KPIs and stable convergence than baselines.


<details>
  <summary>Details</summary>
Motivation: Sustain QoS and reduce handovers in dense, dynamic 5G RAN via autonomous, data-driven control.

Method: Model the control as an MDP; adjust Cell Individual Offset (CIO) to steer user associations; use PPO with actor-critic networks; multi-objective reward combining throughput, latency, jitter, packet loss, fairness (Jain index), and handover count; train in a Python simulator with configurable mobility and noise; compare against ReBuHa, A3, and CDQL across 500+ episodes.

Result: PPO policy yields higher throughput and fairness, lower delay, jitter, packet loss, and handovers, with rapid, stable convergence; outperforms baselines on all KPIs and shows smoother learning dynamics and better generalization as load increases.

Conclusion: Clip-based PPO with advantage training provides robust, deployable control for next-generation RAN load balancing in a fully Python-based toolchain.

Abstract: Efficient mobility management and load balancing are critical to sustaining
Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We
present a deep reinforcement learning framework based on Proximal Policy
Optimization (PPO) for autonomous, QoS-aware load balancing implemented
end-to-end in a lightweight, pure-Python simulation environment. The control
problem is formulated as a Markov Decision Process in which the agent
periodically adjusts Cell Individual Offset (CIO) values to steer user-cell
associations. A multi-objective reward captures key performance indicators
(aggregate throughput, latency, jitter, packet loss rate, Jain's fairness
index, and handover count), so the learned policy explicitly balances
efficiency and stability under user mobility and noisy observations. The PPO
agent uses an actor-critic neural network trained from trajectories generated
by the Python simulator with configurable mobility (e.g., Gauss-Markov) and
stochastic measurement noise. Across 500+ training episodes and stress tests
with increasing user density, the PPO policy consistently improves KPI trends
(higher throughput and fairness, lower delay, jitter, packet loss, and
handovers) and exhibits rapid, stable convergence. Comparative evaluations show
that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based
CDQL baseline across all KPIs while maintaining smoother learning dynamics and
stronger generalization as load increases. These results indicate that PPO's
clipped policy updates and advantage-based training yield robust, deployable
control for next-generation RAN load balancing using an entirely Python-based
toolchain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo proposes swarm inference, a decentralized, reputation-weighted, pairwise-ranking protocol for AI inference that scales horizontally and resists adversarial behavior, outperforming monolithic models on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Centralized AI systems face compute ceilings; scalable, reliable, and secure inference requires a distributed approach that leverages collective intelligence while mitigating Sybil attacks and low-quality participants.

Method: Swarm inference with peer-ranked, reputation-weighted consensus across heterogeneous models; pairwise ranking using a Bradley–Terry–style aggregation; on-chain reputation; proof-of-capability with calibration/test rounds and reputation staking; ranking rounds; six challenging benchmarks including GPQA Diamond, LiveCodeBench, and AIME.

Result: Swarm inference outperforms majority voting (GPQA Diamond: 85.90% vs 68.69%; +17.21 points, ~+25.1% relative). Across six benchmarks, higher accuracy and strong resilience to adversarial/noisy prompting (prompt-injection degradation 0.12% vs 6.20% for monolithic baseline). Demonstrates robustness and deployability of decentralized AI inference.

Conclusion: Establishes a foundation for decentralized AI systems enabling high-quality inference through collective intelligence while preserving reliability and security, potentially democratizing access to AI capabilities.

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [18] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA is a Python framework that constructs and analyzes large-scale mutational fitness landscapes, computing 20 features that capture landscape topography to better interpret and compare sequence-to-fitness models; validated on thousands of landscapes and released extensive combinatorially complete datasets; code available publicly.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for sequence–fitness modeling lack topographical information about fitness landscapes, limiting interpretation and cross-model comparisons beyond mean performance metrics.

Method: GraphFLA ingests mutagenesis data across modalities (DNA, RNA, protein, etc.) to construct fitness landscapes, computes 20 biologically relevant features describing 4 fundamental topography aspects, and analyzes thousands of landscapes (over 5,300 from ProteinGym, RNAGym, CIS-BP). It also releases 155 combinatorially complete empirical landscapes (over 2.2 million sequences).

Result: Demonstrates that topography-aware features help interpret and compare dozens of fitness-prediction models, reveals factors that influence model accuracy, and highlights advantages of different modeling approaches across modalities; provides a rich, open dataset suite for benchmarking.

Conclusion: GraphFLA provides a topography-aware benchmarking framework for sequence-to-fitness modeling, enabling more nuanced interpretation and fairer comparison of models, and supplies substantial public datasets and code to advance landscape-informed ML in genomics.

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [19] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [20] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 对现有的OOD泛化基准提出批评：在把异质的OOD样本聚合后，ID与OOD间的“在直线上的准确度”正相关性常被误解为鲁棒性好；作者提出基于梯度的OODSelect方法来识别语义上连贯的OOD子集，在这些子集中ID精度与OOD精度不再呈正相关，甚至在某些子集中表现出负相关。跨主流基准发现这些子集可能占据OOD集合的一半甚至更多，揭示聚合指标掩盖的关键失败模式。作者还发布代码与识别出的子集以促进行业研究。


<details>
  <summary>Details</summary>
Motivation: 旨在检验ID与OOD间的正相关性是否真的指示鲁棒性，以及聚合评估是否隐藏了对OOD鲁棒性的重要失败模式；通过可解释的子集分析揭示更细粒度的鲁棒性评估。

Method: 提出梯度驱动的OODSelect方法，基于语义连贯性筛选OOD子集；在广泛使用的分布迁移基准上应用，比较聚合与子集上的ID与OOD性能关系。

Result: 在OODSelect识别的子集里，准确率在直线上的关系不再成立，甚至在某些子集中ID越高，OOD越差；这些子集有时占据标准OOD集的一半以上；表明聚合度量掩盖了重要的失败模式；并提供代码与子集供复现。

Conclusion: 聚合指标可能掩盖关键的OOD鲁棒性问题，需通过语义上连贯的子集分析来评估模型的泛化能力，研究者应采用更细粒度、可重复的评估集合；研究组发布资源以推动后续研究。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [21] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN introduces a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator before graph convolution networks (GCNs) to create a fast and accurate ice-sheet emulator. It uses learnable 1D warps and a linear mixing step to improve feature conditioning without increasing message-passing depth. Trained and tested on 36 melting-rate simulations across 3 mesh sizes for Pine Island Glacier, it matches or exceeds pure GCN and MLP-GCN baselines across 2–5-layer architectures. It yields higher throughput on coarser meshes by replacing an edge-wise message-passing layer with a node-wise transform, with only modest costs at the finest mesh. Overall, KAN-first designs offer a favorable accuracy-efficiency trade-off for large transient scenario sweeps.


<details>
  <summary>Details</summary>
Motivation: 需要提升冰盖数值模拟的速度和可扩展性，尤其在大规模瞬态场景扫荡中，同时保持或提升预测精度；通过改进特征编码和前处理来减少对深度消息传递的依赖。

Method: 在GCN前端引入可学习的一维扭曲(KAN)与线性混合，将其作为特征前处理器应用于GCN架构。对 Pine Island Glacier 的36组 melting-rate 模拟及3种网格尺寸进行训练与测试，评估在2–5层结构中的性能。粗网格通过将一个边缘消息传递层替换为节点变换来提升推理吞吐量，最细网格成本仅为 modest。

Result: 在2–5层结构中，KAN-GCN 的准确性达到或超过纯GCN和MLP-GCN基线；在粗网格上显著提升推理吞吐量，且参数开销较小；仅在最细网格上成本略高。

Conclusion: KAN优先设计为大规模瞬态场景扫荡提供更佳的准确性与效率权衡，适用于高性能的冰盖模拟仿真。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [22] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 提出 True Thinking Score (TTS) 来衡量每一步推理对最终输出的因果贡献，揭示大语言模型的许多 CoT 步骤具有装饰性；仅极少数步骤真正驱动预测，并且可通过在潜在空间的 TrueThinking 方向进行干预来控制推理及其结果。


<details>
  <summary>Details</summary>
Motivation: 揭示 Chain-of-Thought (CoT) 解释的局限性与潜在的安全与效率问题；需要区分对最终预测有因果贡献的真实思维步骤与仅为表象的装饰性步骤，以提升模型的可信度与可控性。

Method: 定义并测量每一步推理的因果影响，提出 True Thinking Score (TTS)；在不同数据集（如 AIME）和模型（如 Qwen-2.5）上评估推理步骤的分布，区分 true-thinking 与 decorative-thinking；发现 Latent Space 中的 TrueThinking 方向并通过沿该方向的干预来推动或抑制某些 CoT 步骤；分析自我验证步骤的实证性，探讨是否真正在内部“思考”。

Result: 在 AIME 数据集上，平均只有 2.3% 的推理步骤具有 TTS ≥ 0.7；通过沿 TrueThinking 方向可强制模型进行或忽略某些推理步骤，进而改变最终结果；自我验证（aha moments）往往是装饰性的，推动内部思考的干预可显著影响输出。

Conclusion: LLMs 往往口头化推理步骤但不一定在内部执行，削弱了推理效率与对 CoT 的信任度。需要重新评估 CoT 的可解释性与安全性，并结合对 TrueThinking 的干预实现更可控的推理。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [23] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 提出并证实在视觉-语言模型中存在对特定文化具有偏好的神经元，这些神经元对文化上下文相关的问题具有选择性响应，且通过干预可以显著影响相关文化问答的表现。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何处理文化相关信息，以及内部表示是如何组织的，探索是否存在文化敏感的神经元及其定位。

Method: 在CVQA基准上识别文化选择性神经元，结合多种识别方法标记后，对被选中的神经元进行因果干预（关闭/抑制），评估对不同文化问答的影响。比较现有基于概率/熵的识别方法，提出新的对比激活选择（CAS）以更优地识别文化敏感神经元，并进行按层分析以确定其分布。

Result: 发现确有对特定文化相关问答造成显著损害的神经元，且对其他文化影响很小；CAS在文化敏感神经元识别方面优于概率和熵等方法；且这类神经元在解码器的某些层集中出现。

Conclusion: 为多模态表示的内部组织提供新见解，表明文化信息以层级化、部位化的方式嵌入模型内部，且可通过特定方法识别与干预，对提升或审查VLM的文化鲁棒性具有重要意义。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [24] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出 FedLap：在图结构联邦学习中通过拉普拉斯平滑实现隐私友好且可扩展的子图联邦学习框架，结合谱域结构信息，提供隐私分析并在基准数据上实现竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨客户的子图之间的互连关系对学习的影响，同时避免交换敏感节点嵌入和高计算成本。

Method: 在谱域通过 Laplacian 平滑捕捉跨节点依赖性，利用全局结构信息进行隐私保护的联邦学习框架；给出隐私性分析并声称具有强隐私保障。

Result: 在基准数据集上，与现有方法相比，具有竞争或更优的效用。

Conclusion: FedLap 提供了一个隐私可控、可扩展且有效的子图联邦学习框架，是首个具备强隐私保证的子图 FL 方案。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [25] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 这项研究揭示了大语言模型的低维结构：通过对不同提示-回应集合构建的 logits 矩阵表现出近似低秩，并可用与无关提示的线性组合生成目标提示的回答；提出一个与模型无关的抽象框架并给出理论学习保证。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型研究中，理解其固有的低维结构并提出一个模型无关、以序列概率模型为框架的分析方法。

Method: 通过对各种现代语言模型的 logits 构建矩阵，并对不同提示与回应集合进行实验评估，发现近似低秩结构；证明可用对无关或乃至无意义提示的输出进行线性组合来生成目标提示的回答；从理论角度引入近似秩的通用抽象，分析其表示能力并给出可证明的学习保证。

Result: 实证结果显示 logits 矩阵具有近似低秩；可以用来自不同提示的输出的线性组合来生成目标提示的回答；理论分析与实验结果一致，给出学习保证。

Conclusion: 低维、通用的抽象为理解 LLM 的结构与提升生成效率提供了新的理论框架，并建立了对表示能力和学习性的保障，未来可进一步探索其限制与应用。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [26] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出一个端到端的表格数据特征选择框架，基于合作博弈，将特征视为玩家，通过评估协同交互和边际贡献来确定特征重要性，进而实现显著的计算成本降低与预测性能保持。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的迅速增长，模型训练成本上升，许多特征对性能贡献有限且计算成本高。需要高效的特征选择方法来减少计算负担并提升训练效率。

Method: 将特征建模为博弈中的参与者，使用合作博弈理论来评估特征重要性，框架包含四个核心组件：样本选择、基于博弈论的特征重要性评估、冗余特征消除以及优化的模型训练。

Result: 实验表明该方法在显著降低计算量的同时，维持或改善预测性能，适用于大规模机器学习的计算挑战，源代码公开可用于复现实验。

Conclusion: 提出的端到端框架为大规模表格数据的高效特征选择提供一种可行解决方案，显著降低计算成本的同时保持模型性能。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [27] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: Sharp spectral-norm perturbation bounds for symmetric matrices with symmetric perturbations; refines Eckart–Young–Mirsky and improves DP-PCA utility.


<details>
  <summary>Details</summary>
Motivation: Understand how noise/measurement errors affect low-rank approximations in the spectral norm, especially for differential privacy where preserving top-p structure while ensuring privacy is crucial; spectral-norm provides strongest guarantees compared to Frobenius-based metrics.

Method: Develops high-probability spectral-norm perturbation bounds for symmetric A and symmetric perturbation E, under mild eigengap and norm conditions; explicit bound on ||(A+E)_p - A_p||; introduces a contour bootstrapping technique from complex analysis, extended to a broad class of spectral functionals (polynomials, matrix exponentials).

Result: Bounds yield sharp estimates with potential improvements up to sqrt(n) over classical results; enables improved utility guarantees for differentially private PCA; empirical results on real datasets show the bounds track actual spectral error across perturbation regimes.

Conclusion: Introduces a novel contour bootstrapping framework for spectral functionals that delivers tight spectral-norm control under perturbations and has practical impact on DP-PCA and beyond.

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [28] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion introduces a risk-aware, inference-time sampling rule for diffusion policies in offline RL by treating each denoising step as a sequential test between the unconditional prior and the conditional policy; it calibrates a logistic gate to a user-specified Type-I level alpha, enabling a principled risk budget without retraining.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a statistical notion of risk in diffusion-policy guidance for offline RL and to balance exploitation with controlled risk, particularly in out-of-distribution settings.

Method: Treat each denoising step as a sequential hypothesis test, accumulate a log-likelihood ratio, and gate the conditional mean with a logistic controller calibrated under H0 to threshold alpha. Training remains vanilla (two heads with standard epsilon-prediction). LRT guidance can be integrated with Q-gradients at various points, and states/actions are standardized; an OOD metric is reported alongside return.

Result: On D4RL MuJoCo tasks, LRT-Diffusion improves the return–OOD trade-off over strong Q-guided baselines while honoring the specified alpha. The paper provides level-alpha calibration, stability bounds, and a return comparison showing LRT surpasses Q-guidance particularly when off-support errors dominate.

Conclusion: LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL, harmonizing risk budgeting with exploitation and offering theoretical guarantees.

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [29] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个同时处理癫痫发作检测与预测的端到端方法，基于 EEG 数据在 CHB-MIT 数据集上评估，显示检测与预测的潜力，但也暴露了在类别不平衡下仅靠准确率评估的问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服仅在癫痫发作后进行干预的局限，提出将实时检测与预测整合的策略，以捕捉 EEG 中的潜在前驱时序模式，从而实现更早的干预。

Method: 在癫痫患者 EEG 数据上，使用多种监督学习算法进行发作检测（KNN、逻辑回归、随机森林、SVM），并使用 LSTM 进行发作预测；在 CHB-MIT 数据集上评估。

Result: 检测阶段：逻辑回归达到 90.9% 精度、89.6% 召回率；随机森林与 SVM 达到 94% 精度但 0% 召回，说明单靠准确率对不平衡数据的医疗评估无效。预测阶段：LSTM 达到 89.26% 的预测准确率。

Conclusion: 结果显示实时监测工具具有潜力实现发作的早期干预与预测，能够从被动治疗转向主动管理；然而需要在处理类别不平衡、提高召回等方面做进一步改进。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [30] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 矩阵白化优化器普遍优于元素级优化器（如Adam），其优势源于方差自适应而非单纯的谱归一；低秩方差估计可在不牺牲性能的前提下降低内存成本。


<details>
  <summary>Details</summary>
Motivation: 系统性分解并比较不同的矩阵白化近似，明确哪些成分真正驱动性能提升。

Method: 在多组超参数下对比矩阵白化变体与元素级优化器；对变体进行方差自适应、lookahead、低秩方差估计等消融实验；分析谱降方向与性能关系。

Result: 矩阵白化方法在多项指标上优于元素级方法；方差自适应是被忽视的关键因素，提升显著；SOAP 获得最大每步收益，尽管 Muon 更准确沿着谱降方向下降；低秩方差估计实现内存降成本且不损失性能；lookahead 不如其他策略。

Conclusion: 方差自适应是矩阵白化有效性的核心驱动，谱归一虽相关但不能充分解释提升；矩阵白化具双重作用，提升了稳定性与样本效率；未来工作可聚焦更高效的方差估计方案。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [31] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE disentangles shared versus region-specific neural dynamics with a deep multi-encoder autoencoder, robust to distortions, and generalizes stimulation signatures across sites.


<details>
  <summary>Details</summary>
Motivation: In multi-region neural data, it is crucial to separate universal network-wide dynamics from region-specific activity and to understand how external perturbations reorganize cross-regional structure.

Method: Introduce SPIRE, a deep multi-encoder autoencoder that factorizes recordings into shared and private latent subspaces using novel alignment and disentanglement losses; trained solely on baseline data.

Result: SPIRE recovers cross-regional structure, reveals perturbation-induced reorganizations; on synthetic benchmarks it outperforms classical probabilistic models under nonlinear distortions and temporal misalignments; on intracranial DBS data, shared latents encode stimulation-specific signatures that generalize across sites and frequencies.

Conclusion: SPIRE is a practical, reproducible tool for analyzing multi-region neural dynamics under stimulation.

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [32] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 在MRI序列的分布漂移下，基于放射组学的机器学习模型的鲁棒性受序列、分割和观察者变异影响，使用协议不变特征和数据增强可提升校准与稳定性。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在实际临床中易受成像协议和分割差异导致的分布漂移影响，需通过协议感知的特征选择和仿真 phantom 研究来评估与提高鲁棒性。

Method: 在包含16个水果的 phantom 上，针对T2-HASTE、T2-TSE、T2-MAP、T1-TSE、T2-FLAIR等五种MRI序列评估分布漂移，比较全特征与协议不变特征两组 XGBoost 分类器；引入分割变异、互观变异、数据增强，并评估在域内/域外表现与不确定性校准。

Result: 协议不变特征的模型在分布漂移下保持F1>0.85；使用所有特征的模型在协议变化时下降约40%。数据增强显著提升不确定性估计质量，ECE降低约35%；温度标定对校准收益有限，说明XGBoost天生可靠。

Conclusion: 通过协议感知的特征选择和受控 phantom 研究，可以预测与提升放射组学生成模型在真实协议变异下的鲁棒性，为开发对实际协议变化鲁棒的放射组学模型提供框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [33] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出一种面向因果效应推断的有向混合图（ADMG）的图距离度量，基于未观测混淆下因果效应估计的下游任务；通过固定法识别与符号验证器量化图差异对不同处理-结果对的因果效应 estimand 的扭曲程度，并分析在不同图扰动下的行为，且与现有距离度量进行比较。


<details>
  <summary>Details</summary>
Motivation: 在因果发现研究中，单纯比较结构相同/不同的图难以反映对实际因果效应推断的影响。由于潜在混淆存在，现有的距离度量往往无法反映对处理-结果对的因果效应的扰动程度。本工作提出一个以下游因果效应为目标的图距离，使度量更能反映实际推断的差异。

Method: 提出基于 ADMG 的图距离，通过可识别性固定（identification via fixing）和符号验证器（symbolic verifier）来量化不同图结构在处理-结果对上的因果效应 estimand 的偏差。研究在不同图扰动下该度量的行为，并将其与现有距离度量进行对比。

Result: 该距离在扰动下能够反映对因果效应估计的实际影响，且与现有距离在灵敏度与任务相关性上存在差异；在多种扰动情形下，与传统距离的相关性较低但对因果推断的风险识别更直观。

Conclusion: 该方法提供了一种与实际下游因果推断强相关的图结构距离，有助于评估和比较因果发现方法在面对潜在混淆时的鲁棒性和有效性。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [34] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出一种名为 DWMGrad 的自适应优化算法，通过基于历史数据的动态引导来更新动量和学习率，以提升在非凸和复杂模型中的收敛速度与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前常用优化算法如 SGD 与 Adam 在学习率选取、避免陷入局部最优以及在高维非凸问题中的表现存在局限性，亟需更灵活的自适应策略来应对复杂数据结构与任务变化。

Method: 在传统优化方法基础上引入动态引导机制，基于历史信息动态调整动量和学习率的更新，能够灵活改变对历史数据的依赖，以适应不同训练场景。

Result: 通过大量实验验证，DWMGrad 在多种情景下实现更快的收敛和更高的准确率，显示出对环境与任务复杂度的良好适应性。

Conclusion: 该方法提升了优化过程对变化环境的自适应性，证明在复杂深度学习优化任务中具有潜在的普适性和有效性。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [35] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: A framework for co-optimizing data processing and CPU resource allocation in distributed ML over time-varying networks with log-quantized communications, achieving consensus and convergence to optimal solutions; shows substantial cost-efficiency gains.


<details>
  <summary>Details</summary>
Motivation: In distributed ML, there is a need for fast, scalable, and resource-efficient solutions that can jointly optimize data processing and CPU usage across multiple nodes, even when network connectivity is time-varying or bandwidth-constrained.

Method: Proposes an all-time-feasible, distributed optimization algorithm with consensus constraints and balanced-weights time-varying networks. The method supports log-quantized data exchange, allows local training on each node (e.g., distributed SVM and regression), and leverages perturbation theory, Lyapunov stability, and eigen-spectrum analysis to establish convergence to the optimal solution.

Result: The algorithm converges to the optimal solution under the stated assumptions. Empirically, it improves the cost optimality gap by more than 50% compared to conventional CPU scheduling solutions, demonstrated on distributed SVM and regression scenarios.

Conclusion: The proposed co-optimization framework enables fast, scalable, and provably convergent resource allocation for distributed ML with resilient performance under log-quantized communication, offering meaningful improvements over standard CPU scheduling approaches.

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [36] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO introduces a proximal regularizer for LoRA adapters in recommender systems, enabling continual adaptation to new user behavior while anchoring to the latest frozen state to balance adaptation and preservation, outperforming prior LoRA continual methods.


<details>
  <summary>Details</summary>
Motivation: In recommender systems, user preferences and item interactions drift over time. Traditional LoRA-based continual learning focuses on preserving performance on past tasks but may hinder adapting to new interests, since outdated preferences can hurt current recommendations. A data-aware, directionally guided adaptation in the LoRA subspace is needed to capture recent behavior without catastrophically forgetting the past.

Method: Propose PESO (Proximally regularized Single evolving LoRA), which adds a proximal regularizer that anchors the current LoRA adapter to its most recent frozen state. This creates a balance between adapting to new data and preserving useful past parameters. Theoretical analysis argues that the proximal design provides data-aware, direction-wise guidance in the LoRA subspace. Empirically, PESO is evaluated on recommendation tasks and shows consistent improvements over existing LoRA-based continual learning methods.

Result: PESO achieves better adaptation to evolving user preferences and improved recommendation performance compared with prior LoRA-based continual learning approaches across evaluated datasets.

Conclusion: A proximal regularizer on the evolving LoRA adapter enables effective continual adaptation in recommendation by balancing plasticity and stability, with theoretical justification and empirical superiority over existing methods.

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [37] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: 提出了 FairMIB，一种多视角信息瓶颈框架，通过将图分解为特征、结构和扩散视图来缓解 GNN 的复杂性偏差；利用对比学习最大化跨视图互信息，并引入多视角信息瓶颈目标与 IPW 邻接修正，在五个真实数据集上实现了公允性与实用性并重的最优性能。


<details>
  <summary>Details</summary>
Motivation: GNN 在通过节点特征和结构传递信息时，可能放大训练数据中的偏见，导致歧视性属性和结构不平衡被传播至结果中。单一偏见源的处理往往忽略不同属性与结构效应，造成公允性与效用的权衡不佳。

Method: 将图分解为三个视图（特征、结构、扩散），通过对比学习最大化跨视图互信息以学习偏见消除的表示；引入多视角条件信息瓶颈目标，最小化与敏感属性的互信息以平衡任务效用与公平性；在扩散视图中加入逆概率加权（IPW）邻接修正，以减少消息传递过程中的偏见传播。

Result: 在五个真实数据集上，FairMIB 在公允性与效用指标上都实现了 STATE-OF-THE-ART 的性能。

Conclusion: 通过对图的多视图分解和基于互信息的学习目标，FairMIB 提供了一种稳健的公允性-效用权衡框架，有效缓解 GNN 的复杂性偏差并提升公平性与性能。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [38] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: Training/test distribution shift can be beneficial in mixture distributions: mismatched training proportions can improve test performance even when components are unrelated, with optimal training proportions identified; extends to compositional settings with different component skills at train/test.


<details>
  <summary>Details</summary>
Motivation: To understand how distribution shift between training and test data, specifically in mixture models, can affect performance and whether there are principled ways to choose training proportions for optimal test accuracy.

Method: Theoretical analysis of mixture distributions under varying training/test proportions, deriving conditions under which shift is beneficial, identifying optimal proportions; extension to a compositional setting with differing component skills at training and testing.

Result: Showcases that distribution shift can be advantageous in many settings; there exist optimal training proportions that maximize test performance; results extend to compositional settings with different distributions of component skills at training vs. test.

Conclusion: Distribution shift between training and test distributions in mixture models is not just a nuisance but can be exploited for improved test performance; optimal training proportions can be determined in many scenarios, and the same analysis applies to compositional skill distributions.

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [39] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出一个统一的双层优化框架用于对抗性学习，并从数据扰动角度研究聚类模型的对抗攻击。结果表明在扰动较小时聚类具有鲁棒性，而扰动较大时攻击会改变聚类结果；引入 delta- measure 以在该框架内量化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型结构复杂，对抗性攻击机制难以解释，缺乏统一的可量化评估框架，尤其是在聚类等无监督任务中。

Method: 提出一个统一的双层优化模型用于对抗性学习；在聚类模型中从数据扰动角度分析对抗攻击，并在所提框架中引入 delta-measure 以度量攻击效果。

Result: 在扰动较小的情况下，聚类模型表现出鲁棒性；扰动较大时聚类结果会改变，从而构成攻击。delta-measure 在该双层框架中具有清晰定义，能够用来量化攻击效果。

Conclusion: delta-measure 可以在对抗性学习的聚类问题中作为衡量攻击影响的有用度量；该双层框架提供了对抗性学习的统一解释与分析工具。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [40] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 将数据增强与因果推断整合，提出 IVL 回归框架，并通过将参数化 DA 视为 IVL 问题来提升跨干预的因果推断与泛化能力，理论与仿真及真实数据验证均给出支持。


<details>
  <summary>Details</summary>
Motivation: 在存在隐性混杂且 IVs 不易获得的情形下，利用 DA 作为对干预的近似，改进因果效应估计与跨干预泛化。

Method: 建立一个将 DA 理解为对治疗机制的干预的统一框架，提出 IV-like (IVL) 回归，通过对基于 IV 的估计进行正则化来减小偏差；将参数化 DA 视为 IVL 回归问题并通过组合实现对最差情形的仿真以提高性能；给出总体理论结果及线性示例的有限样本仿真，以及真实数据实验。

Result: 给出总体情形下的理论结论、线性模型的有限样本仿真结果，以及真实数据的实验结果，显示与简单 DA 相比，在估计因果效应和跨干预泛化方面的提升。

Conclusion: 将 DA 扩展到因果泛化的新框架，IVL 正则化能缓解隐性混淆带来的偏差，DA 与 IVL 的组合可对干预下的预测与推断表现出更强的鲁棒性和泛化能力。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [41] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: Linearity grafting on activations reduces dominant approximation errors in Lipschitz-based certified robustness, tightening the local Lipschitz constant and improving robustness, via a Lipschitz-aware method, even without certified training.


<details>
  <summary>Details</summary>
Motivation: Certified robustness depends on tight local Lipschitz constants; worst-case adversarial search is NP-hard, and current over-approximation methods introduce large errors that hinder robustness guarantees. While linearity grafting has been proposed to facilitate verification, its theoretical impact on robustness was unclear.

Method: Provide two theoretical contributions: (1) analyze how linearity grafting improves certified robustness through the l_infty local Lipschitz constant; (2) propose Lipschitz-aware linearity grafting that eliminates dominant approximation errors by grafting linear pieces into nonlinear activations, since linear functions avoid relaxation. This method aims to tighten the local Lipschitz constant and improve certified robustness, even without certified training. Empirical validation on standard benchmarks demonstrates tighter l_infty local Lipschitz constants and improved certified robustness.

Result: Theoretically, linearity grafting reduces dominant approximation errors, yielding tighter l_infty local Lipschitz constants and enhanced certified robustness. Empirically, grafting linearity tightens the local Lipschitz bound and improves certified robustness across experiments, with benefits observed even without certified training.

Conclusion: Linearity grafting is an effective strategy to reduce approximation errors and tighten local Lipschitz bounds, leading to improved certified robustness. The Lipschitz-aware grafting method provides a practical means to achieve these gains without requiring certified training.

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [42] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: ML-guided MILP framework for optimal power shutoffs to reduce wildfire ignition risk, exploiting shared instance structure to generate high-quality de-energization decisions quickly, outperforming traditional optimization on a large California test system.


<details>
  <summary>Details</summary>
Motivation: Wildfire risk motivates de-energization; OPS is an MILP hard to solve in real time; many instances share structure but vary in risk/load/generation; need fast, reliable solutions.

Method: Develop an ML-guided framework that extends existing ML-guided MILP methods and integrates domain knowledge about the targeted number of lines energized/de-energized; train on varying instance parameters; apply to large-scale synthetic California test system.

Result: The ML-guided approach yields high-quality solutions faster than traditional optimization on the test system.

Conclusion: Exploiting shared structure and domain knowledge in OPS problems via ML-guided MILP can improve speed and solution quality for real-time wildfire risk management.

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [43] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN 通过 Bayes Optimal Learning Threshold 的启发，在 WGAN 框架上进行简单修改，在 Lipschitz 判别器下隐式最小化不同于 Wasserstein 距离的度量，从而提升训练稳定性；在 CIFAR-10、CelebA-64、LSUN-64 上实现 10-60% 的 FID 提升，显示 BOLT 作为一种改进 GAN 训练的普适原则的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管 WGAN 及其变体提升了 GAN 的稳定性，但训练过程仍易出现不稳或模式崩溃。引入基于 Bayes Optimal Learning Threshold 的 BOLT 思路，旨在通过重新定义目标距离来改善训练动态和生成质量。

Method: 在 WGAN 框架上进行简单而有效的改动，使判别器具备 Lipschitz 连续性，并结合 BOLT 的思想引导训练以优化一种不同于 Wasserstein 的度量。对四个标准图像数据集进行实验评估。

Result: 在四个数据集上，BOLT-GAN 始终优于 WGAN，FID 指标降低幅度为 10-60%。并显示出更稳健的训练行为。

Conclusion: BOLT 提供一种广义可应用的改进 GAN 训练的原则，BOLT-GAN 证明了通过重新定义训练目标的距离度量可以提升生成质量和训练稳定性。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [44] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 本工作提出一种带双遮罩的选择性学习策略，通过筛选时间步来优化损失，显著提升深度时间序列预测的泛化性，对Informer、TimesNet、iTransformer等模型有明显的MSE提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列的噪声与异常使得对所有时间步统一使用MSE训练易造成过拟合，需要在训练中区分可泛化的时间步与非泛化的时间步。

Method: 提出双遮罩机制以实现选择性学习：1) 不确定性遮罩，基于残差熵筛选不确定的时间步；2) 异常遮罩，基于残差下界估计排除异常时间步。训练阶段仅对筛选出的子集时间步计算MSE损失，使模型关注可泛化的时间步。

Result: 在8个真实数据集上验证了该策略的有效性，显著提升深度TSF模型的预测性能：Informer的MSE降低约37.4%，TimesNet降低约8.4%，iTransformer降低约6.5%。

Conclusion: 选择性学习框架通过聚焦可泛化的时间步来缓解过拟合，提升多种深度时间序列预测模型的性能，具有较好的推广性与训练效率潜力。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [45] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的多类正无标签学习（MPU）方法，通过对正例与推断负样本的损失分量赋予数据相关权重，在经验风险框架下实现目标风险的无偏估计；给出理论的泛化误差界，并在八个公开数据集上的实验显示在准确性与稳定性方面优于强基线。


<details>
  <summary>Details</summary>
Motivation: 多类正无标签学习中，现有方法往往难以实现无偏的风险估计，导致训练不稳定、性能受限。需要一个能够提供无偏性并具备良好泛化能力的MPU方法。

Method: 在经验风险最小化框架内，针对正样本与从无标签数据中推断得到的负样本损失分量，分配具有数据依赖性的不同权重，从而使得到的经验目标成为目标风险的无偏估计。系统地给出MPU数据生成过程的建模，并推导出所提出估计量的泛化误差界。

Result: 在八个公开数据集上进行广泛实验，覆盖不同的类别先验和类别数量，所提方法在准确性和稳定性方面相较强基线表现出一致的提升。

Conclusion: 通过自适应损失加权实现MPU的无偏风险估计，理论上具有泛化界限，并在实证中展现出稳定且优越的性能。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [46] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [47] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [48] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈的归一化IBNorm，通过有界压缩在抑制噪声变量的同时保留任务相关信息，从而在维度不对齐的情况下提升表示质量，并在语言与视觉模型上超越BatchNorm、LayerNorm和RMSNorm。理论上获得更高的IB值和更紧的泛化界，实验上在LLaMA、GPT-2、ResNet、ViT等模型上表现更好，且将给出公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法（BatchNorm、LayerNorm、RMSNorm）以方差为导向，强制零均值、单位方差，未显式控制表示所捕获的任务相关信息。需一种在稳定性与信息表达能力之间取得平衡的归一化方法，使表示更具信息性且对下游任务更友好。

Method: 提出IB-Inspired Normalization (IBNorm)，在归一化中引入有界压缩操作，通过最小化冗余/噪声变量对预测信息的影响，同时保留预测相关信息。结合信息瓶颈原理，推导出能够提升IB值的正则化/变换，并给出理论上的泛化界界限。通过在大规模语言模型（如LLaMA、GPT-2）与视觉模型（ResNet、ViT）上进行实验，与BatchNorm、LayerNorm、RMSNorm进行对比，并进行互信息分析以验证信息瓶颈行为。

Result: IBNorm在广泛的语言和视觉模型上持续超过对比归一化方法，获得更高的信息瓶颈（IB）值和更紧的泛化界，互信息分析证实了更优的信息瓶颈行为。代码将公开发布。

Conclusion: IBNorm提供一个简单但强大的归一化族，基于信息瓶颈原理，能够在保持稳定性与兼容性的同时提升表示的信息含量，未来可用于更广泛的模型与任务；并计划公开代码以促进复现与应用。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [49] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出一个两级的物理嵌入学习框架，结合自适应傅里叶神经算子，在保证物理一致性和可解释性的前提下，能够从稀疏噪声数据实现PDE的前向预测与未知项的符号回归发现。


<details>
  <summary>Details</summary>
Motivation: 解决复杂时空动力学的PDE难以从第一性原理推导和纯数据驱动的局限性；需要高效整合先验物理知识、处理高阶算子和非局部依赖的模型。

Method: 两级架构：第一层学习PDE的符号成分（基本算子/项），第二层学习它们的组合；将已知物理定律嵌入计算图中以确保物理一致性；在模型基础上构建自适应傅里叶神经算子以捕捉非局部依赖和高阶算子；通过结构化解耦已知与未知项，利用符号回归实现对潜在方程的可解释发现。

Result: 理论上提升了前向预测与逆问题的数据效率和解释性，确保物理一致性，并可对未知项进行可解释的方程发现；适用于稀疏/带噪声数据场景。

Conclusion: 该框架通过分层嵌入物理先验，结合强大的算子表达与符号回归，为复杂时空系统的建模与方程发现提供了一个高效、可解释的新路径，克服了现有纯数据驱动和传统物理信息方法的局限。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [50] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出一个多目标强化学习框架，在最大化期望回报的同时，实现对目标状态集合的边际分布的均匀化覆盖；通过策略混合和离线RL更新，结合一个基于目标分类器的目标集合。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索中引入随机性或假设事先给出目标分布，但在大规模状态空间中难以获取目标集合信息；需要一种能同时最大化回报并保证对目标集的均匀覆盖的新方法。

Method: 提出基于策略混合的算法，构建自定义多目标奖励基于当前策略混合在采样轨迹上计算；利用离线RL算法更新策略混合；借助一个oracle分类器来界定目标状态集合；给出理论收敛与性能界限。

Result: 证明算法在同时优化回报与目标状态边际分布的自然目标函数方面具有收敛性和性能保证；在合成MDP和标准RL环境上验证其有效性，表现出高回报和目标分布的分散性/均匀性。

Conclusion: 提供一种可行的多目标RL框架，适用于大规模状态空间且需覆盖目标状态集的情境，并在实验中展示了对标准任务的提升。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [51] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出一种基于圆周矩阵与对角矩阵乘积的可逆线性层，显著降低参数与计算复杂度，并构建 Circulant-Diagonal Flow (CDFlow)，在自然图像数据上实现强密度估计并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 设计具有高表达力的可逆线性层，同时保持对数行列式和逆的高效计算，降低参数量与时间复杂度，并更好地建模具有周期结构的数据。

Method: 通过 m 个对角矩阵和 m-1 个圆周矩阵的乘积来近似一般线性变换；利用快速傅里叶变换（FFT）降低逆的时间复杂度到 O(m n log n) 和对数行列式到 O(m n)；将该层用于构建 Circulant-Diagonal Flow (CDFlow)。

Result: 在自然图像数据集上实现强密度估计，能够有效建模具有周期结构的数据，并显著加速了 normalizing flows 的关键运算，提升了可扩展性。

Conclusion: 圆周-对角线层为可逆线性层提供了一种高效且具表达力的选项，CDFlow 展示了在大规模生成建模中的潜力，特别适合具有周期结构的数据。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [52] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 通过时序分割评估、引入 Taoke 数据集和 CasTemp 框架，解决信息级联预测中的未来信息泄漏、缺乏下游转化信号和计算效率低下等问题，实现了在四个数据集上的最新性能，并在二次转化预测上展现实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决信息级联预测中的三大现实挑战：1) 评估过程的时序泄漏导致不可重复且偏乐观的结果；2) 数据集缺乏下游行为信号，限制实际落地；3) 复杂图模型训练成本高、效率低下。

Method: 1) 提出时序分割策略，将数据按时间顺序划分为连续窗口，防止未来信息泄漏。2) 构建 Taoke 大规模电商级级联数据集，包含丰富的推广/商品属性和真实购买转化信号，覆盖从传播到变现的全生命周期。3) 提出 CasTemp 框架：通过时序步行（temporal walks）建模级联动力学、使用基于 Jaccard 的跨级联依赖近邻选择、以及时间感知注意力的 GRU 编码，以实现轻量化但高效的预测。

Result: 在去泄漏的评估框架下，CasTemp 在四个数据集上达到最先进的性能并实现数量级级的训练速度提升；在预测第二阶段转化（如购买）方面具有突出表现，体现其实用性。

Conclusion: 提出的时序分割、Taoke 数据集与 CasTemp 框架共同构成一个更贴近实际场景的级联预测研究范式，可提升预测准确性与落地效率，并为未来在其他领域的级联分析提供可扩展的思路。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [53] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 对随机几何超图上的变分学习进行渐近一致性分析，给出良定义性条件并证明收敛至加权p-Laplacian方程；提出 Higher-Order Hypergraph Learning (HOHL)，通过骨架图的拉普拉斯幂实现多尺度平滑正则化，HOHL 收敛到高阶 Sobolev 半范数；在标准基线任务上实验表现强劲。


<details>
  <summary>Details</summary>
Motivation: 将高阶交互建模嵌入超图框架，并提供半监督学习的理论保障。通过对随机几何超图的渐近分析，确立一致性和良定义性等理论基石，并揭示其与偏微分算子（加权p-Laplacian）之间的联系，提升对超图学习的理解。

Method: 对随机几何超图上的变分学习进行渐近分析，给出使问题良定的充分条件，并证明解向加权p-Laplacian方程收敛；提出 HOHL，在骨架图上通过拉普拉斯算子的幂实现多尺度平滑正则化，且 HOHL 收敛到高阶 Sobolev 半范数。

Result: 理论层面给出一致性、良定义性及收敛到加权p-Laplacian的证据；HOHL 在理论上与高阶Sobolev范数建立联系；实验上在标准基线中表现出色。

Conclusion: HOHL 提供了一个具有理论保证的高阶正则化框架，能充分利用超图的高阶结构并实现多尺度平滑，同时在实验中展现出色表现，推动超图学习的理论与应用发展。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [54] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 通过权重平均的模型合并在知识图嵌入模型(KGE)中提高链接预测的性能，同时避免训练多个独立模型的高成本，提出两种权重平均策略：持续的全局参数滑动平均以及只在验证集泛化性能提升时更新的选择性更新策略；在链接预测、字面量增强的KGE以及多跳查询任务中均显示出性能提升。


<details>
  <summary>Details</summary>
Motivation: 集成方法能提高泛化能力，但训练多模型成本高、延迟和内存压力大。模型合并提供一种无需多模型训练的替代路线，尤其对KGE在链接预测等任务上的泛化能力具有潜在收益。

Method: 提出两种基于权重平均的模型合并方法。第一种：从训练过程中的某一时刻起对模型参数进行滑动平均，形成一个运行中的全局参数平均模型用于预测。第二种：仅在验证集上泛化性能提高时才更新运行中的平均参数，从而实现更具选择性的参数合并。对这两种方法在链接预测任务中与最先进的标准 ensemble 进行比较，并在字面量增强的KGE模型以及多跳查询任务中进一步评估。

Result: 实验结果显示，所提出的加权平均方法在多种评估设置中对比基线和现有集成方法具有一致的性能提升；在链接预测、字面量增强KGE以及多跳查询方面均表现出改进。

Conclusion: 通过模型参数的加权平均合并，可以在不训练多个独立模型的情况下提升KGE的泛化能力与预测性能。选择性更新策略进一步稳定性与性能提升，证明该方法在多样化任务中的适用性与有效性。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [55] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 一个两阶段优化框架：在损失函数从初期的非凸变为接近最优点的凸区间时，切换从非凸优化（如 Adam）到凸优化（如共轭梯度 CG），通过观察梯度范数对损失的依赖来检测切换点。


<details>
  <summary>Details</summary>
Motivation: 现实中的损失面在靠近最优解时往往呈现凸性；若能在不同阶段切换优化方法，可能提升收敛速度和精度。

Method: 提出一个两阶段算法，通过观察梯度范数与损失之间的关系来检测非凸到凸的切换点；在非凸区域使用 Adam 等非凸优化器，在凸区域使用 CG 等二阶/近似二阶方法进行更新。

Result: 计算实验支持假设，显示在该凸性结构下，收敛速度和准确性显著提升。

Conclusion: 真实任务中的简单凸性结构较为常见，能够被利用以显著提升优化性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [56] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt is an LLM-based optimizer for continuous black-box optimization, trained on synthetic BO data to generalize across tasks, reducing the need for parameter tuning and outperforming traditional optimizers.


<details>
  <summary>Details</summary>
Motivation: Global optimization of expensive, derivative-free black-box functions requires high sample efficiency. Classical Bayesian Optimization (BO) often needs careful, domain-specific hyperparameter tuning. Large Language Models (LLMs) offer broad reasoning abilities but are not yet proficient at continuous black-box optimization; this work aims to bridge that gap by making LLMs capable of continuous optimization without manual tuning.

Method: Fine-tune large language models on extensive synthetic datasets generated from diverse BO parameterizations to instill continuous optimization capabilities. The training leverages LLM pre-training to generalize to various optimization tasks, enabling the model to perform global optimization with minimal or no parameter tuning at deployment.

Result: GPTOpt surpasses traditional optimizers on a variety of black-box optimization benchmarks, demonstrating the power of LLMs in advanced numerical reasoning and providing a flexible, parameter-tuning-free framework for global optimization.

Conclusion: Pretraining/fine-tuning LLMs on synthetic BO data allows LLMs to perform effective continuous optimization, reducing the need for domain-specific hyperparameter tuning and broadening the applicability of global optimization methods.

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [57] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了一个名为“效用校准”的通用多类校准评估框架，允许以特定下游效用衡量校准误差，统一并扩展现有度量，提升对顶类和逐类校准的鲁棒性，并支持对更丰富下游决策的评估。


<details>
  <summary>Details</summary>
Motivation: 当前多类校准评估往往聚焦于特定方面（如顶类置信度、类级校准）或采用计算成本高的变分方法，缺乏一个可扩展、以决策目标为导向的评估框架来覆盖更丰富的下游任务。

Method: 提出效用校准框架，将校准误差相对于给定的效用函数来度量，能够统一并重新解释现有的多类校准度量，提供对顶部类别和类级别校准的鲁棒改进，并扩展到更丰富的下游效用。

Result: 该框架在理论与方法层面实现了对现有度量的统一与再解释，并提升了对顶类与类级别校准的鲁棒性，同时支持对更丰富下游效用的评估。

Conclusion: 以效用为驱动的校准评估能够直接服务于实际决策目标，增强分类器的可信度。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [58] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 梯度-权重对齐（GWA）是一种新颖的度量，用于衡量每个训练样本的梯度与模型权重之间的相干性。它能在训练过程中追踪泛化性能，归因到具体样本，并在无验证集的条件下实现模型比较和早停决策。


<details>
  <summary>Details</summary>
Motivation: 在监督分类中，需一类健壮的验证指标来检测过拟合、监控训练动态，并能够将性能归因于个别训练样本；现有方法通常依赖验证集，缺乏样本级别的分析能力。

Method: 定义并计算每个样本对损失的梯度，与当前模型权重向量之间的对齐度（如内积/夹角），形成GWA度量。该度量可在训练中高效计算，反映样本级贡献与数据集级学习动力学。通过广泛实验验证其与泛化的关系，且在训练阶段即可使用。

Result: 实验表明，GWA能够准确预测最佳早停点，便于进行基于原理的模型比较，并识别对训练数据具有影响力的样本；提供一种无需验证集的模型分析方法，直接从训练数据中获取信息。

Conclusion: GWA为训练过程中的泛化跟踪与样本层面的性能归因提供了一种高效、可计算的诊断工具，能够在没有验证集的情况下反映学习动力学。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [59] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 提出原型化神经符号架构，通过原型学习避免捷径推理，在极少监督下确保学习到正确概念并满足符号约束，在 rsbench 与多任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号AI中的捷径推理问题：模型容易通过虚假相关性满足符号约束，从而学习到错误概念，尤其在数据稀缺场景下。

Method: 引入基于原型学习的神经符号架构，训练过程考虑输入与少量标注样本的相似性，以确保模型学习正确概念并符合背景知识的约束。

Result: 在合成任务（MNIST-EvenOdd、Kand-Logic）和真实高风险任务（BDD-OIA）中，在极低监督下显著提升对正确概念的学习和鲁棒性。

Conclusion: 原型 grounding 提供了一种注释高效且安全可靠的神经符号学习路径，支持端到端训练的原型化神经符号模型。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [60] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN提出一种基于线性RNN的单变量时序基金会模型，在纯合成数据上预训练，实现零-shot长序列预测的高效且可重复性，采用GatedDeltaProduct+state-weaving实现全并行训练，并在Gift-Eval上领先于现有合成数据方法，且优于大多数用真实数据训练的模型，同时开源数据管线与训练代码。


<details>
  <summary>Details</summary>
Motivation: 解决零-shot长时序预测中高效性与可重复性挑战；现有以合成数据为主的方法在难 benchmark 上表现不足，需具备可扩展的并行训练/推理能力及统一的合成数据管线来提升性能。

Method: 基于线性RNN的单变量基金会模型TempoPFN，使用GatedDeltaProduct架构与state-weaving实现跨序列长度的完全并行训练；停止使用滑窗或摘要技巧；建立统一的合成数据管线，包含SDE、GP和音频合成等生成源，搭配新颖数据增广。

Result: 在Gift-Eval零-shot评估中，TempoPFN达到顶尖水平，超越所有仅合成数据方法，且优于大量用真实数据训练的模型；通过并行训练与推理相比现有基线更高效。

Conclusion: 开源完整的数据生成与训练代码，提供可重复的研究基础，便于未来在零-shot时序预测领域的进一步研究。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [61] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 用机器学习预测 burnout 风险，基于 HackerEarth 数据集；SVM 优于 KNN、RF，R2=0.84，并有配对 t 检验显著性；并开发 Streamlit 界面供非技术用户使用。


<details>
  <summary>Details</summary>
Motivation:  Burnout（倦怠）对个体幸福感和组织绩效有显著影响；需要可操作、数据驱动的早期预测工具以促进干预和管理。

Method: 比较三种有监督学习算法（KNN、随机森林、SVM）在 30 折交叉验证下的表现，使用决定系数 R2 作为性能指标；通过配对 t 检验比较模型差异；并开发基于 Streamlit 的交互界面以使非技术用户可输入数据并获得预测。

Result: 在所比较的模型中，SVM 获得最高预测性能 R2=0.84，且经配对 t 检验显著优于 KNN 和随机森林。

Conclusion: 机器学习具备在组织层面实现 burnout 早期检测的潜力，并可通过数据驱动的心理健康策略来促进干预，同时提供一个可交互的应用界面提升实际可用性。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [62] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: Proposes model-inherent, cross-class, mechanistic concept explanations that are faithfully traceable from any layer, along with a foundation-model–driven concept-consistency metric (C^2-Score); finds more consistent and interpretable concepts without sacrificing ImageNet performance.


<details>
  <summary>Details</summary>
Motivation: Address the faithfulness gap in post-hoc concept explanations and move beyond restrictive assumptions (class-specificity, small spatial extent, alignment with human expectations) by introducing model-inherent, cross-class concepts whose contributions and visualizations can be faithfully traced.

Method: Introduce a new model with shared, layer-wise concepts whose contributions to logits and input visualizations are traceable. Utilize foundation models to define and compute a concept-consistency metric (C^2-Score) for evaluating concept-based explanations.

Result: The proposed concepts are quantitatively more consistent and more interpretable to users, while maintaining competitive ImageNet accuracy.

Conclusion: Model-inherent, cross-class mechanistic concepts coupled with the C^2-Score provide faithful explanations without sacrificing performance, advancing interpretability in deep networks.

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [63] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Introduces kernel-guided mutual information (KG-MI) with multi-head attention to learn DAG structures with multiple parents; proves polynomial-time convergence to global optimum and, under KL divergence, exact recovery of the ground-truth adjacency, with empirical validation.


<details>
  <summary>Details</summary>
Motivation: To extend theoretical guarantees for transformer-based structure learning from tree-like graphs to general DAGs with multiple parents, by designing objectives that allow different attention heads to capture distinct parent relationships.

Method: Propose KG-MI based on f-divergence; pair each attention head with a distinct marginal transition kernel to model diverse parent-child dependencies; analyze training dynamics for sequences generated by a K-parent DAG; prove convergence of a single-layer, multi-head transformer under gradient ascent; characterize attention patterns at convergence; specialize to KL divergence to recover the adjacency matrix.

Result: Demonstrates polynomial-time convergence to the global optimum; characterizes attention score patterns at convergence; when using KL divergence, learned attention scores reveal the ground-truth adjacency, effectively recovering the underlying graph structure; experimental results support the theory.

Conclusion: The framework provides provable guarantees for learning complex DAG structures with attention heads encoding multiple parent relationships, bridging theory and empirical results in graph-aware transformers.

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [64] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出一个统一框架，从随机化的PAC-Bayes保证中提取对单一假设的保证；给出通用的oracle界、数值界及对多数票的特化；实验显示对确定性分类器的泛化界比常用基线提升可达2倍。


<details>
  <summary>Details</summary>
Motivation: 在传统PAC-Bayes框架中，保证仅针对随机抽样的假设的期望风险，需进行随机预测，难以在单一确定性分类器情境中使用。需要从随机PAC-Bayes预测中提取对单一假设的保证。

Method: 提出一个统一框架，构建通用的oracle界；从中推导数值界，并对多数票进行特化；给出一个适用于单一假设的界，并提供可直接用于确定性分类器的可计算界。

Result: 理论上给出通用守则/界与推导结果；在实验中，该方法对确定性分类器的泛化界优于流行基线，提升幅度可达2倍。

Conclusion: 该框架使PAC-Bayes在实践中能够为单一假设提供保证，尤其对多数票情形效果显著，提升对确定性分类器的界的实用性。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [65] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 对低秩伪逆在噪声环境下的谱范数鲁棒性进行系统性研究，给出与特征值间隙、谱衰减和噪声对低曲率方向的对齐相关的非渐进性界限；引入对函数f(z)=1/z的轮廓积分新应用，显著改善对比全逆的界限，并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 现实矩阵常带噪声（采样、草绘、量化等），需对低秩伪逆近似的谱范数误差给出可靠、可观测的界限，以支持在噪声计算环境中的可扩展性。现有基于全逆的鲁棒性界限对低秩近似缺乏谱结构敏感性和非渐近性强。

Method: 设A为对称n×n矩阵，A_p^{-1}为A^{-1}的最优秩-p近似的逆；tilde A = A + E为带噪声观测。给出非渐近的扰动界限，揭示误差如何随特征值间隙、谱衰减以及噪声在低曲率方向的对齐而缩放。核心在于将轮廓积分技术应用于非整函数f(z)=1/z，得到比经典全逆界限更优的上界（可提升至√n量级的改进）。通过对真实世界与合成矩阵的实验，界限与实际扰动误差紧密吻合，相较于传统结果往往过度预测误差。

Result: 得到一组针对A的谱分布和噪声特征的明确非渐进扰动界限，显示低秩逆近似在噪声环境中的鲁棒性受特征值间隙、谱衰减和噪声方向对齐的共同影响；轮廓积分新技巧对非整函数的处理显著提升界限的紧密性，实验验证界限的实用性与准确性。

Conclusion: 提供面向谱的实际保障，支持在噪声计算环境中对低秩逆近似的稳定性与准确性进行量化评估。这一方法与结论可用于高效机器学习、优化和科学计算中的大规模矩阵处理，且引入的轮廓积分技术扩展了对非整函数的扰动分析。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [66] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 提出并实现基于 Orlicz-几何结构的广义 Sobolev IPM 的 Musielak 正则化 GSI-M，在图结构上化简为单变量优化，显著提升计算效率，并在文档分类与拓扑数据分析任务中优于 OW。


<details>
  <summary>Details</summary>
Motivation: 现有 Sobolev IPM 受限于 L^p 结构，难以融入非 L^p 的几何先验。引入 Orlicz 几何结构及其与 Musielak范数的联系，以提供更灵活的几何先验，同时保持与 Orlicz-Wasserstein/广义 Sobolev 传输框架的一致性。

Method: 将 Sobolev IPM 泛化至 Orlicz-几何结构，构建广义 Sobolev IPM（GSI），并建立 Orlicz-Sobolev 范数与 Musielak 范数之间的理论联系；引入 Musielak 正则化，形成 GSI-M；利用图的结构将优化问题降表为单变量优化以提升计算效率。

Result: GSI-M 可以转化为单变量优化，计算复杂度显著降低，比 OW 快多数量级；在给定图上的分布比较中对文档分类及拓扑数据分析等任务表现出实际优势。

Conclusion: 提出的 Orlicz-几何结构统一并推广了 Sobolev IPM，使得能够引入更灵活的几何先验，并通过 Musielak 正则化实现高效计算；GSI-M 为大规模图数据的概率分布比较提供实用工具。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [67] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核分数的统一不确定性量测框架，覆盖全不确定性类型（总、不确定性、证据不确定性）并通过核的选择控制尾部敏感性、鲁棒性和OOD响应；给出设计准则并通过理论-实验验证其可设计性与权衡。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的回归任务中，量化不确定性比分类任务更复杂且文献匮乏，现有方法多聚焦分类场景，因此需要一个统一、可定制、面向回归的概率不确定性框架。

Method: 提出基于适当评分规则的核分数家族度量，用于总不确定性、 aleatoric 与 epistemic 不确定性的度量，构建一个统一框架；通过与核-评分特征之间的显式对应关系，给出面向任务的设计指南，并证明可通过核的选择来控制尾部敏感性、鲁棒性及对分布外数据的响应；进行大量实验验证其在下游任务中的有效性及各实现之间的权衡。

Result: 所提出的核-score 不确定性度量统一了多种已有度量，提供了可设计性强的准则，实验显示在不同任务下具有良好性能，并揭示鲁棒性与OOD检测等之间的权衡。

Conclusion: 通过核-score 的设计，可针对具体回归任务定制不确定性度量，核的选择直接影响尾部、鲁棒性与分布外响应等行为，理论-实验结合给出实用的设计指南与可观的应用潜力。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [68] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了不同粒度下的FP与INT量化，揭示在粗粒度下FP占优，但在细粒度（如8位，块大小32）情况下，MXINT8通常优于FP，4位格式下FP在多数情况下更具优势，然而通过Hadamard旋转等出错抑制策略，NVINT4可超越NVFP4。提出对称裁剪解决细粒度INT训练的梯度偏差，使MXINT8训练接近无损。结论是：细粒度INT格式，尤其MXINT8，在准确性与能效之间提供更好平衡，未来AI加速器设计应更多考虑细粒度INT而非统一FP路径。


<details>
  <summary>Details</summary>
Motivation: 当前高性能AI硬件（如Nvidia Blackwell）广泛采用低精度浮点格式来应对LLMs中的激活离群，但缺乏对FP与INT量化在不同粒度上的统一系统性对比，导致算法与硬件共设计缺乏清晰指引。需要跨粒度的公正比较来提供设计准则。

Method: 进行系统性比较，覆盖常见的8位细粒度格式（如MX，块大小为32），对比MXINT8与MXFP8；在4位场景中比较MXFP4、NVFP4、MXINT4、NVINT4等，评估在算法准确性与硬件效率上的差异。引入Hadamard旋转等出错抑制技术以提升INT在低位宽下的表现；提出对称裁剪以消除细粒度INT训练中的梯度偏差，实验验证其对MXINT8训练的近乎无损性能。

Result: 关键发现包括：1) 在8-bit的细粒度格式中，MXINT8在算法准确性与硬件效率上通常优于其FP对等格式；2) 在4-bit场景，FP往往具有更高的准确性，但通过Hadamard旋转等出错抑制技术，NVINT4可以超过NVFP4；3) 提出对称裁剪，显著降低细粒度INT训练的梯度偏差，使MXINT8训练接近无损；4) 这一系列结果挑战了“统一FP路径”的硬件设计路径，表明细粒度INT格式（尤其MXINT8）在未来AI加速器的能效与准确性之间提供更优平衡。

Conclusion: 研究表明，细粒度INT格式，特别是MXINT8，往往在准确性与功耗/带宽等硬件指标之间实现更优折中，应该成为未来AI加速器设计的重点取向。对于4-bit场景，FP仍具有竞争力，但通过有效的出错抑制和正则化策略（如对称裁剪）可提升INT的表现。总体上，FP并非在所有场景下最佳，细粒度INT格式给出更强的灵活性与潜在节能收益。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [69] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 系统性研究 Vision-Language-Action (VLA) 模型在进行动作任务微调时对视覺表示的保留情况，发现简单的动作微调会导致视觉表示退化；通过探针隐藏表示、分析注意力、设计对比任务等方法，揭示VL能力随动作微调的变化，并提出对齐视觉表征的策略以缓解退化、提升OOD泛化能力，最后强调在动作微调与VL表征保留之间的权衡与恢复VL能力的可行性。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉-语言模型（VLM）带来可迁移的世界知识和VL对齐能力，理论上应支撑更具泛化性的行动模型。然而，将VLMs应用到动作模态时，仍不清楚原始VL表示和知识在多大程度上被保留，需要系统地研究在动作微调中的表示保留和退化现象。

Method: 进行系统性的表示保留研究：对VLA微调过程中的隐藏表示进行探针分析，分析注意力图；设计一组有针对性的任务和对比方法，将VLA模型与其对应的VLM进行对照， isolating 动作微调对VL能力的影响；评估多种对齐视觉表征的策略，并提出一种简单但有效的方法来缓解退化、提升OOD泛化。

Result: 结果表明：未经适当对齐的动作微调会导致视觉表示退化；通过对齐策略的引入，能缓解这种退化并提升对OOD场景的泛化能力；研究揭示了动作微调与VL表示退化之间的权衡，并给出恢复继承VL能力的可行途径。

Conclusion: 在VLA微调中存在动作任务对VL表示的退化与知识丧失的权衡，但通过合适的对齐方法，可以部分恢复并保持VL能力，同时提升对新分布的鲁棒性。代码公开，方便复现与进一步研究。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [70] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: RNNs can emulate discrete latent dynamics similar to HMMs by organizing activity into slow, noise-driven regions with fast transitions, using a small set of “kick neurons” to initiate state changes; this motif supports noise-sustained, probabilistic computation and generalizes across different HMM architectures.


<details>
  <summary>Details</summary>
Motivation: To bridge continuous, attractive RNN dynamics with discrete, stochastic latent states observed in natural behavior; to understand how RNNs could generate spontaneous, probabilistic dynamics beyond input-driven, deterministic tasks.

Method: Train RNNs to reproduce HMM emission statistics; analyze trained networks to uncover internal mechanisms. In absence of input, activity collapses to a fixed point; with stochastic input, trajectories follow noise-sustained closed orbits. Transitions between slow regions are driven by fast deterministic jumps; a small set of kick neurons initiate these transitions. Examine multiple HMM architectures (fully connected, cyclic, linear-chain) to test generality.

Result: Networks develop highly structured connectivity that supports stochastic resonance and probabilistic computation. Dynamics reveal a modular, reusable motif that underlies transitions between latent states, enabling the RNNs to emulate complex discrete dynamics and generalize across architectures.

Conclusion: A compositional principle emerges: RNNs can implement discrete latent dynamics by reusing a common dynamical motif—slow regions connected by fast transitions—allowing probabilistic, stochastic behavior to be generated within continuous-state networks.

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [71] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了卷积脉冲GRU（CS-GRU）单元，将卷积运算与尖峰神经元的时序性结合，既保留局部结构也实现高效时序处理，在多种时空数据集上超越现有GRU变体、并显著提升效率。


<details>
  <summary>Details</summary>
Motivation: SNNs在处理时序与事件驱动数据方面高效，但传统RNN在处理长序列时易丢失局部细节；SpikGRU等方法未能捕捉细粒度局部依赖，因此需要一种既保留局部结构又具备时序精度的模型。

Method: 提出CS-GRU单元，通过卷积操作保持局部结构，并将脉冲神经元的时序信息与GRU门控机制相结合；在时序数据集NTIDIGITS、SHD和时空数据集MNIST、DVSGesture、CIFAR10DVS上验证。

Result: CS-GRU在平均4.35%的提升上优于最先进的GRU变体；在序列任务上达到>90%准确率，MNIST最高可达99.31%；相较于SpikGRU，效率提升约69%。

Conclusion: CS-GRU为时序与时空数据的序列处理提供了灵活高效的框架，成功融合了卷积局部性与尖峰神经元的时间精度，适用于多种数据类型；代码已开放。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [72] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: MIL在医疗影像中的上下文关系被低估；通过一个设计的合成任务强调邻近实例特征的重要性，揭示传统MIL的局限性，并与可闭式求解的贝叶斯最优对比，结果是新型相关MIL在大规模训练下仍未达到最佳泛化。


<details>
  <summary>Details</summary>
Motivation: 在高分辨率医疗影像的MIL任务中，实例之间的上下文关系（如相邻patch/切片的特征相关性）对预测至关重要，但多数MIL方法将实例独立处理而忽略此关系。作者通过设计一个需要利用邻近实例信息的合成分类任务，量化现有MIL方法与贝叶斯最优解之间的差距，并评估在大规模数据下相关MIL的泛化能力。

Method: 设计一个强调相邻实例特征相关性的合成分类任务；将传统独立-实例MIL方法与可闭式求解的贝叶斯最优进行对比；在数万实例规模下训练并评估新型相关MIL方法的泛化性能。

Result: 结果表明，常规MIL在该任务上受限，贝叶斯最优解在闭式解下达到理想性能；即便是新近的相关MIL方法，在从头训练于大规模数据时，也难以达到最优的泛化水平。

Conclusion: 结论是显式建模实例之间的上下文关系对MIL任务的性能提升至关重要；现有独立-实例MIL及部分相关MIL在复杂上下文依赖下存在显著泛化差距，需要发展更强的上下文建模与泛化能力的模型。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [73] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出 Neural Stochastic Flows（NSFs），直接学习条件正态化流下的 SDE 转移规律，实现任意时间点的一次性采样，并在大时间步长下达到显著加速，同时保持分布准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的 SDE 数值求解在跨越任意时间点的采样成本高，且对于不规则采样场景效率低。需要一种能够快速从一个状态直接采样到任意时间点的分布的方法，同时保持随机流的基本性质。

Method: 提出 NSFs 及其潜在变体，利用带有保留随机流性质的条件正态化流学习 (潜在) SDE 的转移规律；通过架构约束来维持随机流的理论属性，从而实现对任意状态之间的一次性采样。

Result: 在合成 SDE 与真实世界跟踪/视频数据上的实验表明：NSFs 在大时间间距下可实现最高约两数量级的加速；分布准确性与数值求解方法相当，同时显著降低任意时间点采样的计算成本。

Conclusion: NSFs 提供一种高效且保持分布一致性的任意时间点快速采样框架，适用于需要在不规则时间网格上进行快速、可靠采样的应用场景，如金融、物理与机器学习等领域。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: 本研究揭示顺序数据发布在单次发布隐私保护有效时仍可能泄露隐私，攻击者通过将隐马尔可夫模型与强化学习的双向推理结合，利用时间相关性推断敏感信息，尤其在轨迹数据领域。


<details>
  <summary>Details</summary>
Motivation: 现实系统常以连续或序列方式发布数据，且前后发布之间存在时间相关性。这些相关性可能导致即使每次发布都满足传统隐私保障，整体序列仍可能暴露敏感信息，因此需要评估和防护序列级隐私风险。

Method: 提出一个新的攻击模型，将隐马尔可夫模型与基于强化学习的双向推理机制相结合，利用先前与后续观测来推断私有信息；在轨迹数据场景（Geolife、Porto Taxi、SynMob）中实现并评估该框架。

Result: 实验结果显示，与独立处理每次发布的基线方法相比，该框架在多个数据集上表现更优，能够更准确地恢复敏感信息，揭示顺序数据发布中的隐私风险具有普遍性。

Conclusion: 需要新的隐私保护框架来显式建模时间依赖性，例如时间感知差分隐私或顺序数据混淆策略，以防止跨时序的隐私推断。

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [75] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: 提出 Med-Hammer 的硬件-软件协同威胁模型，将 Rowhammer 硬件故障注入与神经Trojan攻击结合，针对ViT类医学影像系统实现目标型错误分类或诊断抑制，实验显示在MobileViT与Swin Transformer上攻破性强、隐蔽性高。


<details>
  <summary>Details</summary>
Motivation: 随着 Vision Transformer 在医学影像分析中的领先地位，其对大规模注意力驱动模型的依赖使其易受硬件层面攻击的影响。尤其是在医疗场景中，诊断结果的可靠性至关重要，需对模型及底层硬件的安全性进行全面评估。

Method: 提出 Med-Hammer 威胁模型，结合 Rowhammer 硬件故障注入与神经Trojan 攻击，通过诱发比特翻转触发嵌入式 Trojan，导致对特定病灶的错误分类或抑制。对 ISIC、Brain Tumor、MedMNIST等基准数据集上的 MobileViT 与 SwinTransformer 进行大量实验，评估攻击成功率、隐蔽性及对模型结构（如稀疏性、注意力权重分布、层的特征数等）的影响以揭示攻击效果的决定因素。

Result: 在多个医学影像数据集上，攻击在 MobileViT 与 SwinTransformer 上实现较高的攻击成功率，分别约为 82.51% 与 92.56%，并对整体隐蔽性与鲁棒性进行了评估。实验还揭示了模型稀疏性、注意力权重分布以及层特征数量等 Architectural 属性对攻击效果的显著影响。

Conclusion: 提出的 Med-Hammer 将硬件级故障与深度学习安全问题结合，强调在医疗场景中需要跨越模型结构与底层硬件平台的鲁棒防御，以应对潜在的攻击向量并保障诊断的可信度。

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [76] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: 提出 FaRAccel FPGA 硬件加速器以降低 FaR 在 Transformer 上的推理延迟和能耗，同时保持对比特翻转攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决 FaR 在实际部署中的性能与内存开销，以及缺乏硬件级优化的问题。

Method: 在 FPGA 上实现可重配置逻辑用于动态激活重路由、轻量化 rewiring 配置存储，优化 FaR 运算的数据通路以实现低延迟推理。

Result: 在多款 Transformer 模型上实现并评估，显著减少 FaR 推理延迟、提升能效，并维持鲁棒性增益。

Conclusion: 首次在硬件层面加速对 BFAs 的防御，弥合算法韧性与现实 AI 部署之间的差距。

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [77] [SLIP-SEC: Formalizing Secure Protocols for Model IP Protection](https://arxiv.org/abs/2510.24999)
*Racchit Jain,Satya Lokam,Yehonathan Refael,Adam Hakim,Lev Greenberg,Jay Tenenbaum*

Main category: cs.CR

TL;DR: 提出 SLIP 的形式框架及安全基础，设计基于混成推理的全局模型保护协议，提供对诚实但好奇对手的信息论安全性及对恶意对手的鲁棒性证明；理论部分与伴随论文的经验部分并行。


<details>
  <summary>Details</summary>
Motivation: 在部署大语言模型于不完全可信的设备时，保护模型 IP 和推理过程免受盗取和滥用，需具备可证明的安全性保障。

Method: 提出模型分解和混合推理协议的形式定义，基于权重矩阵的加性分解、掩码与概率验证等技术，构造安全推理协议，给出安全性证明。

Result: 在信息论意义上对诚实但好奇的对手实现信息论安全，对恶意对手给出以极小的 soundness error 的鲁棒性保障。

Conclusion: 论文专注于 SLIP 的理论基础、精确定义、形式协议和安全性证明，强调与伴随论文的理论-实践共补，提供完整的保障框架。

Abstract: Large Language Models (LLMs) represent valuable intellectual property (IP),
reflecting significant investments in training data, compute, and expertise.
Deploying these models on partially trusted or insecure devices introduces
substantial risk of model theft, making it essential to design inference
protocols with provable security guarantees.
  We present the formal framework and security foundations of SLIP, a hybrid
inference protocol that splits model computation between a trusted and an
untrusted resource. We define and analyze the key notions of model
decomposition and hybrid inference protocols, and introduce formal properties
including safety, correctness, efficiency, and t-soundness. We construct secure
inference protocols based on additive decompositions of weight matrices,
combined with masking and probabilistic verification techniques. We prove that
these protocols achieve information-theoretic security against
honest-but-curious adversaries, and provide robustness against malicious
adversaries with negligible soundness error.
  This paper focuses on the theoretical underpinnings of SLIP: precise
definitions, formal protocols, and proofs of security. Empirical validation and
decomposition heuristics appear in the companion SLIP paper. Together, the two
works provide a complete account of securing LLM IP via hybrid inference,
bridging both practice and theory.

</details>


### [78] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: 提出RAGuard，一种用于检测RAG知识库污染文本的非参数检测框架。通过扩展检索范围、逐块困惑度筛选与文本相似度过滤来识别并缓解中毒文本，实验表明对强自适应攻击具有效果。


<details>
  <summary>Details</summary>
Motivation: LLMs结合外部知识的RAG系统面临数据中毒的安全风险，攻击者可向知识库注入污染文本以操纵输出。现有防御多对抗不充分，需鲁棒的检测框架。

Method: RAGuard的核心策略包括：1) 扩展检索范围以提高干净文本比例；2) 对检索文本按块进行困惑度筛选以发现异常变异；3) 使用文本相似度过滤来标记高度相似的污染文本；该方法为非参数化，适用于RAG安全场景。

Result: 在大规模数据集上实验表明，该框架能有效检测并缓解污染攻击，且对强自适应攻击具有鲁棒性。

Conclusion: RAGuard为RAG系统提供一个非参数化的污染文本检测能力，显著提升对潜在中毒文本的识别与防护水平，适合在实际系统中部署。

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [79] [Is Protective DNS Blocking the Wild West?](https://arxiv.org/abs/2510.25352)
*David Plonka,Branden Palacio,Debbie Perouli*

Main category: cs.CR

TL;DR: Protective DNS blocklists are heterogeneous and opaque, hindering cross‑organization comparison and scalable deployment.


<details>
  <summary>Details</summary>
Motivation: Assess how Protective DNS performs in a large Research & Education Network and examine governance/oversight challenges posed by freely available blocklists.

Method: A passive measurement study using real DNS traffic from a Research & Education Network. The study applies freely available blocklists to determine which queries would be blocked and analyzes hundreds of millions of user queries observed over one week.

Result: Blocklists vary widely in naming, goals, transparency, and provenance; they are difficult to compare and lack organized oversight, creating operational challenges and risks for large-scale Protective DNS.

Conclusion: Greater transparency, standardization, and governance are needed before large-scale deployment of Protective DNS to ensure reliable, accountable operation.

Abstract: We perform a passive measurement study investigating how a Protective DNS
service might perform in a Research & Education Network serving hundreds of
member institutions. Utilizing freely-available DNS blocklists consisting of
domain names deemed to be threats, we test hundreds of millions of users' real
DNS queries, observed over a week's time, to find which answers would be
blocked because they involve domain names that are potential threats. We find
the blocklists disorderly regarding their names, goals, transparency, and
provenance making them quite difficult to compare. Consequently, these
Protective DNS underpinnings lack organized oversight, presenting challenges
and risks in operation at scale.

</details>


### [80] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: 对Android手机恶意威胁检测的现有ML方法进行对比综述，基于Android Applications数据集，指出需要大量信息、存在假性反馈问题，鲁棒性和可扩展性挑战并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 全球范围内恶意软件威胁上升，Android平台出现加密型勒索软件等新威胁，影响隐私与用户体验，需要通过机器学习方法进行检测与评估。

Method: 对现有研究进行综合比较，分析所使用的机器学习技术、数据集（如Android Applications数据集）、评估指标（如准确率）及关注点（如用户反馈的可信度），并给出对比性结论。

Result: 总结不同ML方法在检测恶意威胁方面的性能差异，指出研究往往依赖大量数据，且需关注假评/自评反馈带来的挑战；提出现有方法在可扩展性、鲁棒性方面的局限。

Conclusion: 提供对当前研究的综合评价，强调需要高质量数据集、鲁棒的自动化反恶意软件系统，以及在大规模数据下的评估来提升Android端恶意威胁检测效果，并指向未来研究方向。

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [81] [A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs](https://arxiv.org/abs/2510.25477)
*Yi Chen,Bin Chen,Peichang Zhang,Da Che*

Main category: cs.CR

TL;DR: 基于DID与ZKP的去中心化奖学金评审框架，通过链下聚合的多维ZKP与智能合约实现对评审条件的验证，同时保护原始分数与计算细节的隐私。


<details>
  <summary>Details</summary>
Motivation: 传统集中评审存在数据泄露风险、隐私保护与可审计性难以兼顾的问题，亟需一个在保护隐私的同时实现透明评审的技术解决方案。

Method: 引入去中心化身份（DID）进行身份管理；使用零知识证明（ZKP）证明学生满足评审条件；将多维ZKP在链下聚合，智能合约对合规性进行验证但不泄露原始数据与计算过程。

Result: 实验结果表明该方案能高效自动化评审，同时最大限度地保护学生隐私与数据完整性，具备实用性与可信度。

Conclusion: 提出的去中心化、隐私保护的评审范式为高等教育奖学金项目提供可部署、可扩展的技术路径，平衡隐私、透明度与效率。

Abstract: Traditional centralized scholarship evaluation processes typically require
students to submit detailed academic records and qualification information,
which exposes them to risks of data leakage and misuse, making it difficult to
simultaneously ensure privacy protection and transparent auditability. To
address these challenges, this paper proposes a scholarship evaluation system
based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The
system aggregates multidimensional ZKPs off-chain, and smart contracts verify
compliance with evaluation criteria without revealing raw scores or
computational details. Experimental results demonstrate that the proposed
solution not only automates the evaluation efficiently but also maximally
preserves student privacy and data integrity, offering a practical and
trustworthy technical paradigm for higher education scholarship programs.

</details>


### [82] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: 提出一种面向模型反向攻击的形式化防御框架，利用模糊提取器作为核心构件，证明现有模糊提取器在ML面部认证中的不安全性，提出新的PIPE攻击以及第一个兼容欧几里得距离的模糊提取器L2FE-Hash，并给出理论安全性和经验评估，显示对现有攻击有抵御作用且无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 需要在隐私敏感的ML系统（如面部认证）中防止通过嵌入向量反推原始面部信息，缺乏对理想防御的系统性表征及与密码学概念的联系。

Method: 形式化理想防御属性并将其与模糊提取器联系起来；提出PIPE模型反向攻击；设计并实现L2FE-Hash模糊提取器，能够在存储秘密被完全暴露等极端威胁模型下给出计算安全保证；在面部认证数据分布上进行实验评估。

Result: PIPE在大多数情况下对现有方案的攻击成功率超过89%；L2FE-Hash实现了与欧几里得距离比较器兼容的安全特性，提供攻击无关的安全性且无需重新训练；对先前的最先进逆向攻击以及PIPE攻击均能无效化。

Conclusion: 给出一个可用于ML面部认证的可证明强防御框架，利用模糊提取器实现对逆向攻击的鲁棒性，并在理论与经验层面均证明其有效性和可用性。

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


### [83] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 给出若干常用机制在zCDP下的严格界，尤其证明Laplace的zCDP界为 ε + e^{-ε} - 1，并给出离散Laplace、k-Randomized Response（k≤6）、RAPPOR以及最坏情形有界区间机制的tight zCDP界；


<details>
  <summary>Details</summary>
Motivation: 虽然从ε-DP到zCDP存在通用的最坏情形转换，但许多常见算法具有更强的保证，需要更紧的zCDP表述以便准确的隐私预算管理。

Method: 对多种机制进行理论推导，利用隐私损失随机变量与zCDP定义之间的关系，逐一推导并证明tightness。

Result: 对ε-DP的Laplace机制的tight zCDP界为 ε + e^{-ε} - 1；还给出离散Laplace、k-Randomized Response（k≤6）、RAPPOR以及最坏情形有界区间机制的tight zCDP界；

Conclusion: 本文为若干广泛使用的机制提供了tight zCDP表述，提升了zCDP在预算分配中的应用精度和可控性。

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [84] [Ambient Backscatter Communication Assisted by Fluid Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2510.24725)
*Masoud Kaveh,Farshad Rostami Ghadi,Riku Jantti,Kai-Kit Wong,F. Javier Lopez-Martinez*

Main category: eess.SP

TL;DR: 引入流体可重构表面（FRIS）以提升 Ambient Backscatter Communication (AmBC) 的吞吐量；通过动态调整 FRIS 单元位置实现更好的信道增益，PSO 用于求解非凸优化。


<details>
  <summary>Details</summary>
Motivation: 解决直接标签到读取器的弱信道或被障碍物阻挡的问题，提升 AmBC 系统的可用性和吞吐性能。

Method: 建立 FRIS-aided AmBC 的系统模型，分析可实现的回传速率，将 FRIS 单元位置优化问题建模为非凸优化并使用粒子群优化（PSO）近似求解。

Result: 仿真表明，FRIS 辅助的 AmBC 在吞吐量方面显著优于传统 RIS 基 AmBC 系统。

Conclusion: 动态可调的 FRIS 提供更高的空间自适应性，显著提升边缘条件下的回传通信性能。

Abstract: This paper investigates the integration of a fluid reconfigurable intelligent
surface (FRIS) into ambient backscatter communication (AmBC) systems. Unlike
conventional reconfigurable intelligent surfaces (RISs) with fixed position
elements, FRIS employs fluidic elements that can dynamically adjust their
positions, offering enhanced spatial adaptability. We develop a system model
where an AmBC tag communicates with a reader through an FRIS, which is
particularly beneficial in scenarios where the direct tag-to-reader link is
weak or blocked by obstacles. The achievable backscatter rate is analyzed, and
the optimization of FRIS element positions is formulated as a non-convex
problem. To address this, we employ particle swarm optimization (PSO) to obtain
near-optimal configurations of the fluid elements. Simulation results
demonstrate that FRIS-aided AmBC significantly outperforms conventional
RIS-based AmBC systems in terms of achievable throughput.

</details>


### [85] [Modelling Real-Life Cycling Decisions in Real Urban Settings Through Psychophysiology and LLM-Derived Contextual Data](https://arxiv.org/abs/2510.24726)
*Maximiliano Rosadio Z.,Angel Jimenez-Molina,Bastián Henríquez,Paulina Leiva,Ricardo Hurtubia,Ricardo De La Paz Guala,Leandro Gayozo,C. Angelo Guevara*

Main category: eess.SP

TL;DR: 通过生理信号和基于视频的上下文描述，建立一个混合模型来解释环境与交通条件如何影响情绪状态及骑行行为的关系；结果表明压力相关情绪会影响骑行决策，城市特征和交通条件对骑行行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有以自评情绪为主的方法在粒度和记忆偏差方面有限；在现实世界交通环境中，需连续、细粒度的情绪测量并理解其触发因素，以解释骑行行为。

Method: 在圣地亚哥市城市骑行研究中，收集参与者的生理数据并通过路况视频提取图像序列，由大语言模型对环境进行语义描述，形成密集的上下文数据。将疲劳和唤醒作为潜变量，结合GPS推断出的待行、加速、制动等骑行行为，构建混合模型以解释情绪与行为的关系。

Result: 研究证实，骑行决策受压力相关情绪影响，城市特征与交通条件对骑行行为具有强烈影响。

Conclusion: 方法学上证明了将生理数据与来自多模态信息的丰富上下文结合以解释在真实世界中的情绪-行为关系的可行性，为城市规划和交通管理提供了对情绪驱动行为的新视角。

Abstract: Measuring emotional states in transportation contexts is an emerging field.
Methods based on self-reported emotions are limited by their low granularity
and their susceptibility to memory bias. In contrast, methods based on
physiological indicators provide continuous data, enabling researchers to
measure changes in emotional states with high detail and accuracy. Not only are
emotions important in the analysis, but understanding what triggers emotional
changes is equally important. Uncontrolled variables such as traffic
conditions, pedestrian interactions, and infrastructure remain a significant
challenge, as they can have a great impact on emotional states. Explaining the
reasons behind these emotional states requires gathering sufficient and proper
contextual data, which can be extremely difficult in real-world environments.
This paper addresses these challenges by applying an innovative approach,
extracting contextual data (expert annotator level) from recorded multimedia
using large language models (LLMs). In this paper, data are collected from an
urban cycling case study of the City of Santiago, Chile. The applied models
focus on understanding how different environments and traffic situations affect
the emotional states and behaviors of the participants using physiological
data. Sequences of images, extracted from the recorded videos, are processed by
LLMs to obtain semantic descriptions of the environment. These discrete,
although dense and detailed, contextual data are integrated into a hybrid
model, where fatigue and arousal serve as latent variables influencing observed
cycling behaviors (inferred from GPS data) like waiting, accelerating, braking,
etc. The study confirms that cycling decisions are influenced by stress-related
emotions and highlights the strong impact of urban characteristics and traffic
conditions on cyclist behavior.

</details>


### [86] [Decoding non-invasive brain activity with novel deep-learning approaches](https://arxiv.org/abs/2510.24733)
*Richard Csaky*

Main category: eess.SP

TL;DR: A thesis on non-invasive brain signal decoding (EEG/MEG) addressing inter- and intra-subject variability, using linear subject-specific models and cross-subject group decoding, plus Transformer-based MEG forecasting; introduces a high-trial inner speech dataset; inner-speech decoding largely unsuccessful.


<details>
  <summary>Details</summary>
Motivation: Tackle high variability in electrophysiological recordings to improve brain decoding and understanding of neural responses to visual stimuli and inner speech, leveraging deep learning and modern architectures.

Method: Two-part work: (1) methodological—linear models at the individual subject level for visual decoding; group decoding strategies to cope with between-subject variability; convolutional and Transformer-based forecasting models for MEG data. (2) experimental—new dataset with high-trial inner speech EEG/MEG and preliminary OP-Magnetometer data to push decoding of inner speech with many trials/sessions.

Result: Transformer-based models show improved capabilities in generating signals close to real brain data, enhancing modelling accuracy. For visual decoding, linear models work at the single-subject level; group decoding methods address cross-subject variability. Experimental inner speech decoding largely negative, underscoring difficulty in decoding inner speech.

Conclusion: Transformer-based forecasting and group-aware decoding strategies are promising directions for electrophysiological data analysis; progress depends on better handling of variability and richer datasets. Inner speech decoding remains challenging, calling for more data, novel representations, or multimodal integration.

Abstract: This thesis delves into the world of non-invasive electrophysiological brain
signals like electroencephalography (EEG) and magnetoencephalography (MEG),
focusing on modelling and decoding such data. The research aims to investigate
what happens in the brain when we perceive visual stimuli or engage in covert
speech (inner speech) and enhance the decoding performance of such stimuli. The
thesis is divided into two main sections, methodological and experimental work.
A central concern in both sections is the large variability present in
electrophysiological recordings, whether it be within-subject or
between-subject variability, and to a certain extent between-dataset
variability. In the methodological sections, we explore the potential of deep
learning for brain decoding. We present advancements in decoding visual stimuli
using linear models at the individual subject level. We then explore how deep
learning techniques can be employed for group decoding, introducing new methods
to deal with between-subject variability. Finally, we also explores novel
forecasting models of MEG data based on convolutional and Transformer-based
architectures. In particular, Transformer-based models demonstrate superior
capabilities in generating signals that closely match real brain data, thereby
enhancing the accuracy and reliability of modelling the brain's
electrophysiology. In the experimental section, we present a unique dataset
containing high-trial inner speech EEG, MEG, and preliminary optically pumped
magnetometer (OPM) data. Our aim is to investigate different types of inner
speech and push decoding performance by collecting a high number of trials and
sessions from a few participants. However, the decoding results are found to be
mostly negative, underscoring the difficulty of decoding inner speech.

</details>


### [87] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: Cardi-GPT通过深度学习与自然语言交互提升ECG解读与临床沟通效率，整合CNN、模糊化层与聊天机器人。


<details>
  <summary>Details</summary>
Motivation: ECG解读复杂且依赖专业知识，临床沟通需清晰、可转化为行动项；现有方法在解释性、跨语言/跨场景交互方面存在不足。

Method: 采用16残差块CNN处理12导联ECG；引入模糊化层将数值输出转为临床可用的语言类别；整合聊天机器人界面实现直观的诊断探索与沟通；在涵盖六家医院、四国的数据集上进行评估；与基线模型对比；使用覆盖、 grounding（证据支撑）与连贯性等维度的综合评估框架，获得73%响应质量分数。

Result: 在24种心源性条件上的加权准确率为0.6194；总体响应质量73%；数据集具多国多中心特征，表现优于基线模型。

Conclusion: Cardi-GPT有望缩短ECG解读与临床沟通之间的距离，提升诊断准确性、工作流效率及患者结局，在多元医疗环境中具有变革潜力。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [88] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: StrikeWatch is a compact wrist-worn, on-device, real-time gait recognition system leveraging IMU signals to detect heel vs forefoot strikes, enabling immediate visual/auditory feedback for runners. It evaluates four lightweight DL architectures (1D-CNN, 1D-SepCNN, LSTM, Transformer) on energy-efficient FPGA implementations (AMD Spartan-7 XC7S15 and Lattice iCE40UP5K). The study provides a hardware-aware comparison, showing 6-bit quantized 1D-SepCNN achieving F1=0.847 with 0.350 μJ per inference and 0.140 ms latency on iCE40UP5K at 20 MHz, supporting up to 13.6 days of continuous inference on a 320 mAh battery; all data and code are open-source.


<details>
  <summary>Details</summary>
Motivation: Existing gait analysis solutions are bulky, offline, or cloud-dependent. Wrist-worn devices offer practicality but face noisy IMU signals, limited compute, and connectivity constraints. There is a need for fully on-device, energy-efficient gait recognition to provide real-time feedback for injury prevention and gait correction during running.

Method: Design and implement four compact DL architectures (1D-CNN, 1D-SepCNN, LSTM, Transformer) optimized for energy-efficient inference on two FPGAs. Build a hardware prototype, collect outdoor running data, and evaluate models via an automated deployment pipeline including on-device inference and feedback delivery. Compare model complexity vs hardware efficiency and quantify trade-offs.

Result: Among 12 participants, 6-bit quantized 1D-SepCNN yields the best overall performance with an average F1 of 0.847, while consuming 0.350 μJ per inference and delivering 0.140 ms latency on iCE40UP5K at 20 MHz. The system can run up to 13.6 days on a 320 mAh battery. All datasets and code are available in the provided GitHub repository.

Conclusion: StrikeWatch demonstrates that real-time, on-device gait recognition from IMU data is feasible on ultra-low-power hardware, enabling immediate feedback to runners and offering favorable accuracy-efficiency trade-offs for wearable deployment; the approach is extensible to other gait-related analytics and can influence future wrist-worn health monitoring devices.

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [89] [Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR](https://arxiv.org/abs/2510.24740)
*Nader Nemati*

Main category: eess.SP

TL;DR: STAR：一种节拍级时-振幅重采样的ECG增强方法，在相邻R峰之间进行受控时间扭曲与振幅缩放，维持P-QRS-T顺序与记录头尾不变，提升跨来源泛化、对罕见类别的学习效果，并可与常用1D SE-ResNet ECG编码器无缝对接，且实现可复现、开源。


<details>
  <summary>Details</summary>
Motivation: 临床12导联ECG分类受制于多源录制条件、重叠病理及标签不平衡，常用全局或随机增强易扭曲心电形态，亟需在R-R区间内进行可控、保持关键波形的增广，以提高泛化性与跨设备鲁棒性。

Method: 提出Sinusoidal Time--Amplitude Resampling (STAR)，在逐拍的R-R区间内进行时序扭曲与振幅缩放，严格保持P-QRS-T的顺序、不改变信号头尾，避免破坏关键形态与相位关系；☆实现与常见1D SE–ResNet风格ECG编码器兼容；☆基于源感知、分层五折评估，采用多机构、跨源数据进行训练与验证；☆通过对信息量较多的心拍进行重采样来增强罕见类别，降低记录重复带来的过拟合。

Result: STAR提供形态保真且多样化的增广，提升源之间的鲁棒性与跨设备泛化能力；模型对不同数据源具有良好稳定性，且对罕见类别学习更有利，避免对峰位的破坏性干扰；实现与训练流程的开源发布便于复现与复用。

Conclusion: STAR为临床ECG分类提供一个简单、可控且对形态可信的增强方案，增强跨源耐久性与实用性，适合在现有1D卷积编码器框架中直接应用，并促成透明的实验复现与扩展。

Abstract: Clinical 12-lead ECG classification remains difficult because of diverse
recording conditions, overlapping pathologies, and pronounced label imbalance
hinder generalization, while unconstrained augmentations risk distorting
diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude
Resampling (STAR) is introduced as a beat-wise augmentation that operates
strictly between successive R-peaks to apply controlled time warping and
amplitude scaling to each R--R segment, preserving the canonical P--QRS--T
order and leaving the head and tail of the trace unchanged. STAR is designed
for practical pipelines and offers: (i) morphology-faithful variability that
broadens training diversity without corrupting peaks or intervals; (ii)
source-resilient training, improving stability across devices, sites, and
cohorts without dataset-specific tuning; (iii) model-agnostic integration with
common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on
rare classes via beat-level augmentation, reducing overfitting by resampling
informative beats instead of duplicating whole records. In contrast to global
crops, large shifts, or additive noise, STAR avoids transformations that
suppress or misalign clinical landmarks. A complete Python implementation and a
transparent training workflow are released, aligned with a source-aware,
stratified five-fold protocol over a multi-institutional 12-lead corpus,
thereby facilitating inspection and reuse. Taken together, STAR provides a
simple and controllable augmentation for clinical ECG classification where
trustworthy morphology, operational simplicity, and cross-source durability are
essential.

</details>


### [90] [Opportunistic Screening of Wolff-Parkinson-White Syndrome using Single-Lead AI-ECG Mobile System: A Real-World Study of over 3.5 million ECG Recordings in China](https://arxiv.org/abs/2510.24750)
*Shun Huang,Deyun Zhang,Sumei Fan,Shijia Geng,Yujie Xiao,Rui Zhang,Zhaoji Fu,Shenda Hong*

Main category: eess.SP

TL;DR: 一个单导联AI-ECG系统在真实世界大规模人群中用于WPW综合征的机会性筛查，表现为灵敏度45.5%、特异性95.9%；阳性结果约提升WPW风险210倍；基于AI筛选的 positives可将医生工作量降低99.5%，每确认一个WPW仅需12次审阅，与人口普查式和用户主导策略相比显著提高效率。


<details>
  <summary>Details</summary>
Motivation: WPW综合征的常规筛查成本高、规模受限，亟需可扩展且高效的筛查方法。基于以往单导联AI-ECG用于房颤筛查的研究，本文在真实世界数据中评估其对WPW的机会性检测能力，以提升早期发现与预防效果。

Method: 回顾性分析，来自中国87836名个体的3566626张单导联ECG记录，使用获批设备WenXinWuYang采集。AI系统与心血管科医生标注及随机抽样结果进行验证，评估AI对工作量的辅助降低及AI阳性与用户主动筛查在工作流中的效率差异。

Result: AI的性能为灵敏度45.5%、特异性95.9%。阳性AI结果对应约210倍的WPW confirmed风险。以AI筛选阳性为前提的工作流将医生工作量降低99.5%，仅需12次审阅即可确诊一个WPW，与全人群筛查和用户驱动路径相比分别需审阅909次和875次。

Conclusion: 大规模真实世界研究表明单导联AI-ECG系统可实现对WPW的高效机会性筛查，显著降低医生工作负担，支持基于人群的心血管疾病预防。

Abstract: Wolff-Parkinson-White (WPW) syndrome is a congenital cardiac condition
associated with sudden cardiac death, with a prevalence of 0.1-0.3%.
Conventional screening relies on electrophysiological testing or 12-lead
electrocardiography interpreted by cardiologists, which limits large-scale and
cost-effective screening. Building on our previous work developing a
single-lead AI-ECG mobile system for atrial fibrillation screening, this study
evaluates its efficiency and effectiveness for opportunistic detection of WPW
syndrome in real-world settings. This retrospective analysis included 3,566,626
single-lead ECG recordings from 87,836 individuals in China, collected using
the NMPA-approved portable ECG device WenXinWuYang. The AI system performance
was validated using cardiologist annotations and random sampling. We quantified
AI-assisted workload reduction and compared review efficiency across
AI-positive and user-initiated workflows. The AI system achieved 45.5%
sensitivity and 95.9% specificity. A positive AI result indicated about 210
times higher risk of confirmed WPW. Focusing on AI-selected positives reduced
physician workload by 99.5%, requiring only 12 reviews to confirm one WPW case,
compared with 909 and 875 in population-wide and user-driven approaches. In
conclusion, this large-scale real-world study demonstrates that a single-lead
AI-ECG system enables efficient and practical opportunistic screening for WPW
syndrome, significantly reducing physician workload and supporting
population-based cardiovascular prevention.

</details>


### [91] [Spectral and Energy Efficiency Tradeoff for Pinching-Antenna Systems](https://arxiv.org/abs/2510.25192)
*Zihao Zhou,Zhaolin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出一种针对 pinching-antenna 系统的发射与针夹式波束成形的联合设计，以权衡光谱效率与能量效率（SE-EE），覆盖单用户与多用户场景。结论是在单用户场景中，最优针夹天线位置与传输波束成形无关；提出两阶段设计（ICR对齐相位与针夹天线位置选取；给定位置后推导闭式发射波束）以及在多用户场景中基于交替优化的联合设计，且证明若针夹放置子问题条件不违反则算法收敛。数值结果显示显著提升 SE-EE 性能与收敛速度，且当针夹天线数量和覆盖范围增加时，PASS 相较于传统多天线系统的 SE-EE 区间扩大。


<details>
  <summary>Details</summary>
Motivation: 在无线通信场景中，针夹天线（PASS）提供灵活的天线放置以提升谱效和能效之间的权衡。然而，现有方法往往只聚焦其中一方面，缺乏对针夹天线位置与传输波束的联合优化。本文旨在通过同时设计针夹天线放置与传输波束，显著提升单用户与多用户场景下的 SE-EE 性能。

Method: 单用户：证明最优 PA 位置与传输波束成形无关；提出两阶段设计：第一阶段通过迭代闭式改进（ICR）对齐接收相位并据此给出 PA 放置框架；第二阶段在确定 PA 位置后推导最优传输波束的闭式解。多用户：基于交替优化（AO）进行联合波束设计以在满足 QoS 的前提下平衡 SE 与 EE；并证明在 PA 放置子问题不违反约束时算法收敛。

Result: 数值结果表明：1）所提算法显著提升联合 SE-EE 性能且收敛速度快；2）当 PA 数量增加、覆盖范围扩大时，PASS 与传统多天线系统之间的 SE-EE 区间差距扩大。

Conclusion: 该工作提出的联合发射与 PA 放置设计在单/users 与多用户场景下均有效地提升 SE-EE，并给出快速收敛的解法与理论收敛性保障，表明 PASS 在提高 SE-EE 的潜力及适用性。

Abstract: The joint transmit and pinching beamforming design for spectral efficiency
(SE) and energy efficiency (EE) tradeoff in pinching-antenna systems (PASS) is
proposed. Both PASS-enabled single- and multi-user communications are
considered. In the single-user scenario, it is proved that the optimal pinching
antenna (PA) positions are independent of the transmit beamforming. Based on
this insight, a two-stage joint beamforming design is proposed. Specifically,
in the first stage, an iterative closed-form refinement (ICR) scheme is
proposed to align the phases of the received signals, based on which a PA
placement framework is proposed. In the second stage, the closed-form solution
for the optimal transmit beamformer is derived given the optimal PA positions.
In the multi-user scenario, an alternating optimization (AO)-based joint
beamforming design is proposed to balance the SE-EE performance while taking
the quality-of-service (QoS) requirements into account. It is proved that the
proposed AO-based algorithm is guaranteed to converge when no constraints are
violated in PA placement subproblem. Numerical results demonstrate that: 1) the
proposed algorithms significantly improve joint SE-EE performance with fast
convergence speed; 2) the SE-EE tradeoff regime gap between PASS and
conventional multi-antenna system widens as the number of PAs and service
coverage increase.

</details>


### [92] [Cramér-Rao Bound Optimization for Movable Antenna-Empowered Integrated Sensing and Uplink Communication System](https://arxiv.org/abs/2510.25246)
*Yuan Guo,Wen Chen,Qingqing Wu,Yang Liu,Qiong Wu*

Main category: eess.SP

TL;DR: 通过移动天线（MA）提升ISAC系统的空间自由度，实现对目标角度CRB的最小化，同时保障通信性能；提出基于MM与PDD的迭代算法，联合优化波束形成、功率分配、接收滤波器和MA位置，具有低复杂度且避免依赖CVX。


<details>
  <summary>Details</summary>
Motivation: 解决固定定位天线在ISAC中无法充分利用空间自由度的问题，利用MA提升信道条件和综合感知、通信性能；目标是在保证通信约束下降低角度估计的CRB。

Method: 提出基于裁剪的MM和惩罚对偶分解的迭代框架，解析地交替优化具有分数型和四次项的目标/约束，优化波束配置、功率分配、接收滤波和MA位置。

Result: 数值仿真验证了算法的有效性与高效性，展示了MA在ISAC中的显著性能提升。

Conclusion: MA辅助的ISAC采用MM-PDD框架实现低复杂度的联合优化，显著提升角度估计CRB与通信性能，指示6G ISAC系统中移动天线的潜力。

Abstract: Integrated sensing and communication (ISAC) is a promising solution for the
future sixth-generation (6G) system. However, classical fixed-position antenna
(FPA) ISAC systems fail to fully utilize spatial degrees of freedom (DoFs),
resulting in limited gains for both radar sensing and communication
functionalities. This challenge can be addressed by the emerging novel movable
antenna (MA) technology, which can pursue better channel conditions and improve
sensing and communication performances. In this paper, we aim to minimize the
Cram\'er-Rao bound (CRB) for estimating the target's angle while guaranteeing
communication performance. This involves jointly optimizing active beamforming,
power allocation, receiving filters, and MA position configurations, which is a
highly non-convex problem. To tackle this difficulty, we propose an efficient
iterative solution that analytically optimizes all variables without relying on
numerical solvers, i.e., CVX. Specifically, by leveraging cutting-edge
majorization-minimization (MM) and penalty-dual-decomposition (PDD) methods, we
develop a low-complexity algorithm to solve the beamformer configuration
problem containing the fractional and quartic terms. Numerical simulation
results demonstrate the effectiveness and efficiency of our proposed algorithm,
highlighting significant performance improvements achieved by employing MA in
the ISAC system.

</details>


### [93] [Fair Rate Maximization for Multi-user Multi-cell MISO Communication Systems via Novel Transmissive RIS Transceiver](https://arxiv.org/abs/2510.25290)
*Yuan Guo,Wen Chen,Qingqing Wu,Zhendong Li,Kunlun Wang,Hongying Tang,Jun Li*

Main category: eess.SP

TL;DR: 在多蜂窝MISO下行通信中，提出一种基于可透射可重配置智能表面（TRTC）的新型RIS配置的低复杂度优化框架；通过分数编程（FP）对对数速率进行变换并用平滑近似与MM法得到解析更新，从而实现对每个小区的用户最小速率的最大化，且在功率约束下更新传输波束形成。


<details>
  <summary>Details</summary>
Motivation: 解决多小区MISO下行通信中以TRTC为核心的RIS系统中目标函数不可微且计算复杂度高的问题，追求在保证公平性的前提下实现高效且低复杂度的优化求解。

Method: 将对数速率的最大化（min-max / max-min）通过分数编程进行变形，并利用平滑近似将目标函数转化为可微形式；再结合大界定-最小化（MM）框架，推导出解析更新公式，整个过程不依赖数值求解器。

Result: 给出一种低复杂度的迭代算法，理论上具备收敛性；通过数值仿真验证收敛性和有效性，显示在相同性能下计算复杂度显著降低；并且仿真结果显示在TRTC部署下的性能优于基准方案。

Conclusion: 所提出的基于FP与MM的TRTC优化框架可在多小区MISO下行系统中实现对用户最小速率的有效提升，同时显著降低计算复杂度，且在仿真中相较基准方案具有明显的性能优势。

Abstract: This paper explores a multi-cell multiple-input single-output (MISO) downlink
communication system enabled by a unique transmissive reconfigurable
intelligent surface (RIS) transceiver (TRTC) configuration. Within this system
framework, we formulate an optimization problem for the purpose of maximizing
the minimum rate of users for each cell via designing the transmit beamforming
of the TRTC, subject to the power constraints of each TRTC unit. Since the
objective function is non-differentiable, the max-min rate problem is difficult
to solve. In order to tackle this challenging optimization problem, an
efficient low-complexity optimization algorithm is developed. Specifically, the
log-form rate function is transformed into a tractable form by employing the
fractional programming (FP) methodology. Next, the max-min objective function
can be approximated using a differentiable function derived from smooth
approximation theory. Moreover, by applying the majorization-minimization (MM)
technique and examining the optimality conditions, a solution is proposed that
updates all variables analytically without relying on any numerical solvers.
Numerical results are presented to demonstrate the convergence and
effectiveness of the proposed low-complexity algorithm. Additionally, the
algorithm can significantly reduce the computational complexity without
performance loss. Furthermore, the simulation results illustrate the clear
superiority of the deployment of the TRTC over the benchmark schemes.

</details>


### [94] [Millimeter-Wave Radar Sensing of Wombat Respiration](https://arxiv.org/abs/2510.25293)
*Marina Murakami,Ryoko Iwase,Chiemi Iba,Daisuke Ogura,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 使用79-GHz毫米波雷达对两只袋熊实现非接触呼吸监测，提出基于自相关函数中谐波分量叠加的呼吸间隔估计法；在不同角度和两次时间点的测量中获得约2.4%的呼吸间隔误差和2.2%的呼吸频率误差，显示方法具备潜在的非接触健康监测应用。


<details>
  <summary>Details</summary>
Motivation: 为野生动物提供非接触、低干扰的呼吸与健康监测手段，克服传统接触式测量在野外环境中的局限性，特别针对体表运动明显的物种如袋熊的监测需求。

Method: 在2024年6月和12月于两只袋熊身上使用两台79 GHz毫米波雷达进行测量；通过对呼吸引起的体表位移的准周期性特征，利用自相关函数的谐波分量叠加来估计呼吸间隔；通过两雷达在不同角度的并行观测评估估计误差。

Result: 呼吸间隔的估计误差为47.4 ms，相对误差2.44%；呼吸速率的误差为0.81 bpm，相对误差2.21%；比较两只袋熊和不同季节（6月与12月）的差异，表明方法在多角度观测下具备稳定性并可捕捉季节性差异。

Conclusion: 该雷达基非接触呼吸监测方法对袋熊等野生动物的健康监测具有潜在应用前景，且在不同角度观测下表现出鲁棒性；未来需扩大样本量、验证对其他物种与更复杂环境的鲁棒性，并进一步优化算法以提升对干扰信号的鲁棒性。

Abstract: This study demonstrates the feasibility of radar-based non-contact
respiratory monitoring for wombats. Two measurement experiments were conducted
in June and December 2024 using 79-GHz millimeter-wave radar systems to monitor
the respiration of two wombats. To estimate the respiratory interval, we used a
method based on summing harmonic components in the autocorrelation function,
capturing the quasi-periodic displacement of the body surface caused by
respiration. Estimation accuracy was evaluated through simultaneous
measurements from different angles using two radar units. The respiratory
interval and respiratory rate were measured with errors of 47.4 ms (2.44%) and
0.81 bpm (2.21%), respectively. We also discuss the differences in respiratory
rates between the two wombats, as well as seasonal variations between June and
December. The results support the potential application of this method to
non-contact health monitoring of wombats.

</details>


### [95] [Low-Overhead CSI Prediction via Gaussian Process Regression -- Part~I: Data-Driven Spatial Interpolation](https://arxiv.org/abs/2510.25390)
*Syed Luqman Shah,Nurul Huda Mahmood,Italo Atzeni*

Main category: eess.SP

TL;DR: 基于高斯过程回归的 CSI 预测框架，通过少量观测条目实现全 CSI 的预测，从而降低 pilot 开销；在三种核函数下对 Kronecker 和 Weichselberger 通道模型进行评估，结果显示在 50% pilot 保存下实现最低预测误差、最佳置信区间覆盖和信息保留。


<details>
  <summary>Details</summary>
Motivation: 随着天线数量增加，基于导频的估计开销高；需要更高效的 CSI 估计方法。通过 GPR 捕捉天线阵列几何导致的空间相关性，用少量观测推断完整 CSI。

Method: 采用高斯过程回归框架，使用三种核函数（径向基函数、Matérn、有理二次）来建模来自阵列几何的平滑且多尺度的空间相关性；在 Kronecker 和 Weichselberger 通道模型下，对三种探针（pilot）方案进行评估。

Result: 在 50% pilot saving 下，GPR 方案获得最低的预测误差、最高的 95% 置信区间覆盖以及最佳的信息保留，与基准相比优势明显。

Conclusion: 所提出的 GPR 基于 CSI 估计框架能够显著降低 pilot 开销，同时在保留>92% 链路容量的前提下保持良好估计性能。

Abstract: Accurate channel state information (CSI) is critical for current and
next-generation multi-antenna systems. Yet conventional pilot-based estimators
incur prohibitive overhead as antenna counts grow. In this paper, we address
this challenge by developing a novel framework based on Gaussian process
regression (GPR) that predicts full CSI from only a few observed entries,
thereby reducing pilot overhead. The correlation between data points in GPR is
defined by the covariance function, known as kernels. In the proposed GPR-based
CSI estimation framework, we incorporate three kernels, i.e., radial basis
function, Mat\'ern, and rational quadratic, to model smooth and multi-scale
spatial correlations derived from the antenna array geometry. The proposed
approach is evaluated across Kronecker and Weichselberger channel models with
three distinct pilot probing schemes. Results show that the proposed GPR with
50% pilot saving achieves the lowest prediction error, the highest empirical
95% credible-interval coverage, and the best preservation of mutual information
relative to benchmarks. This enables up to 50% pilot reduction while preserving
over 92% of the link capacity.

</details>


### [96] [Model-Free Robust Beamforming in Satellite Downlink using Reinforcement Learning](https://arxiv.org/abs/2510.25393)
*Alea Schröder,Steffen Gracla,Carsten Bockelmann,Dirk Wübben,Armin Dekorsy*

Main category: eess.SP

TL;DR: 基于强化学习的鲁棒下行波束成形，在卫星6G场景对抗信道信息不完备，利用Soft Actor-Critic自适应学习，适用于单卫星与多卫星协作，且在两种不确定性模型下优于解析基线，具备可复现性。


<details>
  <summary>Details</summary>
Motivation: 卫星通信的频率重用与用户干扰管理对光谱效率至关重要；在信道信息不完善（如位置估计误差与CSI迟滞）时，传统鲁棒预编码难以解析，需放宽优化或启发式约束，要求一种数据驱动、灵活的鲁棒策略。

Method: 将Soft Actor-Critic算法改造用于下行卫星波束成形，以数据驱动方式学习鲁棒预编码；覆盖单卫星与多卫星协同场景，支持全局或局部CSI；引入两种误差模型以模拟不同水平的不确定性；并给出公开实现以便重复结果。

Result: 所学算法在所有考察场景中达到或显著超过两种解析基线在和速率上的表现，具备对鲁棒性需求水平自适应的能力；揭示了成果背后的鲁棒性机制；代码公开，便于复现实验。

Conclusion: 数据驱动的强化学习方法能够在卫星下行中实现鲁棒波束成形，适应单/多卫星协同、全局/局部CSI与不同不确定性水平，并具有良好的可复现性和实用性。

Abstract: Satellite-based communications are expected to be a substantial future market
in 6G networks. As satellite constellations grow denser and transmission
resources remain limited, frequency reuse plays an increasingly important role
in managing inter-user interference. In the multi-user downlink, precoding
enables the reuse of frequencies across spatially separated users, greatly
improving spectral efficiency. The analytical calculation of suitable
precodings for perfect channel information is well studied, however, their
performance can quickly deteriorate when faced with, e.g., outdated channel
state information or, as is particularly relevant for satellite channels, when
position estimates are erroneous. Deriving robust precoders under imperfect
channel state information is not only analytically intractable in general but
often requires substantial relaxations of the optimization problem or heuristic
constraints to obtain feasible solutions. Instead, in this paper we flexibly
derive robust precoding algorithms from given data using reinforcement
learning. We describe how we adapt the applied Soft Actor-Critic learning
algorithm to the problem of downlink satellite beamforming and show numerically
that the resulting precoding algorithm adjusts to all investigated scenarios.
The considered scenarios cover both single satellite and cooperative
multi-satellite beamforming, using either global or local channel state
information, and two error models that represent increasing levels of
uncertainty. We show that the learned algorithms match or markedly outperform
two analytical baselines in sum rate performance, adapting to the required
level of robustness. We also analyze the mechanisms that the learned algorithms
leverage to achieve robustness. The implementation is publicly available for
use and reproduction of the results.

</details>


### [97] [Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems](https://arxiv.org/abs/2510.25416)
*Jiaming Cheng,Wei Chen,Bo Ai*

Main category: eess.SP

TL;DR: 提出一种自适应端到端无线架构，结合AI驱动的星座成形与神经接收器，支持无 pilots/CP、轻量通道适配器、统一多调制模型，以及在训练中实现PAPR约束，显著提升BER、吞吐量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM系统对导频与循环前缀高度依赖，造成开销大、频谱效率降低。在下一代无线系统中需要无导频/无CP、且能在动态信道中快速自适应的端到端AI解决方案。

Method: 在端到端层面联合训练AI驱动的星座成形与神经接收器；引入轻量级通道适配器以仅更新CA参数实现对时变信道的快速适应；提出可扩展的统一模型以覆盖多种调制阶数并显著减少模型存储；通过约束的端到端训练实现PAPR目标而不增加传输开销。

Result: 仿真实验表明该框架在多种信道场景下提升BER与吞吐量并增强鲁棒性，相较传统方案具备更强的适应性与更低的存储需求。

Conclusion: 该AI原生端到端架构对下一代无线通信具有潜在应用前景，能够在无导频/无CP条件下实现高效、鲁棒的通信，并兼容PAPR约束。

Abstract: The advent of artificial intelligence (AI)-native wireless communication is
fundamentally reshaping the design paradigm of next-generation (NextG) systems,
where intelligent air interfaces are expected to operate adaptively and
efficiently in highly dynamic environments. Conventional orthogonal frequency
division multiplexing (OFDM) systems rely heavily on pilots and the cyclic
prefix (CP), resulting in significant overhead and reduced spectral efficiency.
To address these limitations, we propose an adaptive end-to-end (E2E)
transceiver architecture tailored for pilot-free and CP-free wireless systems.
The architecture combines AI-driven constellation shaping and a neural receiver
through joint training. To enhance robustness against mismatched or
time-varying channel conditions, we introduce a lightweight channel adapter
(CA) module, which enables rapid adaptation with minimal computational overhead
by updating only the CA parameters. Additionally, we present a framework that
is scalable to multiple modulation orders within a unified model, significantly
reducing model storage requirements. Moreover, to tackle the high
peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained
E2E training, achieving compliance with PAPR targets without additional
transmission overhead. Extensive simulations demonstrate that the proposed
framework delivers superior bit error rate (BER), throughput, and resilience
across diverse channel scenarios, highlighting its potential for AI-native
NextG.

</details>


### [98] [Echo-Conditioned Denoising Diffusion Probabilistic Models for Multi-Target Tracking in RF Sensing](https://arxiv.org/abs/2510.25464)
*Amirhossein Azarbahram,Onel L. A. López*

Main category: eess.SP

TL;DR: 提出一种条件去噪扩散概率模型(C-DDPM)辅以VAE的动态射频感知框架，用以多目标时序跟踪并进行码本波束选择，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态射频感知中多目标跟踪对时序建模和噪声鲁棒性的挑战，利用生成模型提升回波压缩与未来状态预测，以改善感知与通信一体化(ISAC)性能。

Method: 采用条件DDPM，在回波观测作为条件信息的基础上进行去噪扩散；引入VAE对回波进行潜在表示压缩作为环境条件；通过 classifier-free guidance 增强条件去噪；在每个传输块，VAE编码接收回波，条件DDPM预测未来目标状态，并用于码本波束选择。

Result: 仿真实验显示在角度和距离跟踪误差上显著低于传统信号处理、滤波和深度学习基准，证明生成模型在ISAC中的潜力。

Conclusion: 验证了生成模型在集成感知与通信中的应用潜力，显示C-DDPM框架能显著提升多目标动态跟踪性能，并为未来的ISAC系统提供新的研究方向。

Abstract: In this paper, we consider a dynamic radio frequency sensing system aiming to
spatially track multiple targets over time. We develop a conditional denoising
diffusion probabilistic model (C-DDPM)-assisted framework that learns the
temporal evolution of target parameters by leveraging the noisy echo
observations as conditioning features. The proposed framework integrates a
variational autoencoder (VAE) for echo compression and utilizes classifier-free
guidance to enhance conditional denoising. In each transmission block, VAE
encodes the received echo into a latent representation that conditions DDPM to
predict future target states, which are then used for codebook beam selection.
Simulation results show that the proposed approach outperforms classical signal
processing, filtering, and deep learning benchmarks. The C-DDPM-assisted
framework achieves significantly lower estimation errors in both angle and
distance tracking, demonstrating the potential of generative models for
integrated sensing and communications.

</details>


### [99] [Dynamic Beamforming and Power Allocation in ISAC via Deep Reinforcement Learning](https://arxiv.org/abs/2510.25496)
*Duc Nguyen Dao,André B. J. Kokkeler,Haibin Zhang,Yang Miao*

Main category: eess.SP

TL;DR: A DRL-based approach for dynamic beamforming and power allocation in ISAC that delivers real-time decision making with competitive performance.


<details>
  <summary>Details</summary>
Motivation: In ISAC for 6G, resource allocation is challenging due to dynamics; need real-time, efficient optimization that jointly improves sensing and communication.

Method: A DRL agent interacts with the ISAC environment to learn beamforming and power allocation policies; compared against SDR and DQN baselines; trained/evaluated in dynamic settings.

Result: Converges within 2000 episodes; achieves up to 80% of SDR benchmark in spectral efficiency; runtime ~20 ms per decision vs 4500 ms for SDR; ~30% higher sum-rate than DQN with comparable runtime.

Conclusion: DRL is a promising tool for real-time, high-performance ISAC in dynamic scenarios, enabling adaptive resource management.

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler in 6G networks,
where sensing and communication capabilities are designed to complement and
enhance each other. One of the main challenges in ISAC lies in resource
allocation, which becomes computationally demanding in dynamic environments
requiring real-time adaptation. In this paper, we propose a Deep Reinforcement
Learning (DRL)-based approach for dynamic beamforming and power allocation in
ISAC systems. The DRL agent interacts with the environment and learns optimal
strategies through trial and error, guided by predefined rewards. Simulation
results show that the DRL-based solution converges within 2000 episodes and
achieves up to 80\% of the spectral efficiency of a semidefinite relaxation
(SDR) benchmark. More importantly, it offers a significant improvement in
runtime performance, achieving decision times of around 20 ms compared to 4500
ms for the SDR method. Furthermore, compared with a Deep Q-Network (DQN)
benchmark employing discrete beamforming, the proposed approach achieves
approximately 30\% higher sum-rate with comparable runtime. These results
highlight the potential of DRL for enabling real-time, high-performance ISAC in
dynamic scenarios.

</details>


### [100] [Low Probability of Detection Communication Using Noncoherent Grassmannian Signaling](https://arxiv.org/abs/2510.25751)
*Diego Cuevas,Mikel Gutiérrez,Jesús Ibáñez,Ignacio Santamaria*

Main category: eess.SP

TL;DR: 提出基于直接序列扩频和Grassmannian信号的非相干LPD通信，用Grassmannian星座来提高覆盖隐蔽性，在低SNR下对比QPSK/QAM等有 pilots 的相干方案，BER竞争力强，且对未授权接收端的探测更隐蔽。


<details>
  <summary>Details</summary>
Motivation: 在低概率探测和隐蔽通信需求下，减少或消除通道估计开销，通过非相干Grassmannian信号提高 covertness 和实现可行的LPD通信。

Method: 将直接序列扩频与 Grassmannian constellations 相结合，使用非相干信号传输，避免Pilots；与使用QPSK/QAM并需要信道估计的相干方案进行对比；通过仿真评估BER和LPD性能。

Result: 仿真结果显示，在低SNR下 Grassmannian信号可提供竞争性的BER，同时对未授权接收端的LPD有改善；相较于传统的 Pilots 依赖的相干方案，非相干Grassmannian在隐蔽性和性能上具有优势。

Conclusion: 非相干Grassmannian信号在LPD通信中具有实际可行性和安全收益，因其更好的覆盖隐蔽性和在低SNR下的性能表现。

Abstract: This paper proposes a noncoherent low probability of detection (LPD)
communication system based on direct sequence spread spectrum (DSSS) and
Grassmannian signaling. Grassmannian constellations enhance covertness because
they tend to follow a noise-like distribution. Simulations showed that
Grassmannian signaling provides competitive bit error rates (BER) at low
signal-to-noise ratio (SNR) regimes with low probability of detection at the
unintended receiver compared to coherent schemes that use QPSK or QAM
modulation formats and need pilots to perform channel estimation. The results
suggest the practicality and security benefits of noncoherent Grassmannian
signaling for LPD communications due to their improved covertness and
performance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [101] [Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications](https://arxiv.org/abs/2510.24763)
*Tingting Huang,Jundong Chen,Huanqiang Zeng,Guofa Cai,Georges Kaddoum*

Main category: cs.IT

TL;DR: 提出一种基于深度学习的功率域非正交多址（NOMA）-chaos shift keying（CSK）系统用于 vehicular 通信。通过DNN解调器学习混沌信号的内在特征，无需同步与参考信号，结合双域特征提取在时域与频域提升鲁棒性，并嵌入SIC框架以抑制错误传播。相比传统MU-DCSK与现有DL方法，具有更高的光谱效率、能量效率、BER鲁棒性与安全性，且计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 解决车辆通信中多用户传输的安全性与效率瓶颈。现有基于混沌的多址系统在非同相检测下存在光谱效率低、需要参考信号且对OMM/R多址不灵活等问题；非正交方案如SCMA-DCSK虽有优势但计算复杂且codebook固定，需要更灵活的可扩展性与低复杂度解调。

Method: 提出DL-NOMA-CSK系统。采用DNN解调器在离线训练阶段学习混沌信号特征，消除对混沌同步和参考信号的需求；构建双域特征提取架构，联合处理时域与频域信息以提升特征学习在动态信道中的鲁棒性；在SIC框架中结合DNN解调以减缓误差传播。

Result: 理论分析与大量仿真表明：所提系统在光谱效率、能量效率、比特误差率、安全性与鲁棒性方面优于传统MU-DCSK及现有DL方法，且计算复杂度更低，具备在实际车辆通信场景中的可行性。

Conclusion: DL-NOMA-CSK对安全的车辆通信具有较高的实用性，提供更优的综合性能与可扩展性，降低对同步和参考信号的依赖，适应动态信道环境。

Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for
vehicular communication systems. Chaos-based modulation schemes have garnered
considerable interest due to their benefits in physical layer security.
However, most existing MU chaotic communication systems, particularly those
based on non-coherent detection, suffer from low spectral efficiency due to
reference signal transmission, and limited user connectivity under orthogonal
multiple access (OMA). While non-orthogonal schemes, such as sparse code
multiple access (SCMA)-based DCSK, have been explored, they face high
computational complexity and inflexible scalability due to their fixed codebook
designs. This paper proposes a deep learning-assisted power domain
non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for
vehicular communications. A deep neural network (DNN)-based demodulator is
designed to learn intrinsic chaotic signal characteristics during offline
training, thereby eliminating the need for chaotic synchronization or reference
signal transmission. The demodulator employs a dual-domain feature extraction
architecture that jointly processes the time-domain and frequency-domain
information of chaotic signals, enhancing feature learning under dynamic
channels. The DNN is integrated into the successive interference cancellation
(SIC) framework to mitigate error propagation issues. Theoretical analysis and
extensive simulations demonstrate that the proposed system achieves superior
performance in terms of spectral efficiency (SE), energy efficiency (EE), bit
error rate (BER), security, and robustness, while maintaining lower
computational complexity compared to traditional MU-DCSK and existing DL-aided
schemes. These advantages validate its practical viability for secure vehicular
communications.

</details>


### [102] [Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission](https://arxiv.org/abs/2510.25002)
*Zhenyu Liu,Yi Ma,Rahim Tafazolli,Zhi Ding*

Main category: cs.IT

TL;DR: A lightweight, token-based video transmission framework (Resi-VidTok) for ultra-low-rate wireless channels, enabling progressive, prefix-decodable reconstruction with graceful degradation and channel-adaptive coding; demonstrates real-time performance and robustness under severe bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: Real-time video transmission over wireless networks remains challenging under severe channel conditions (limited bandwidth, weak connectivity). There is a need for low-complexity, energy-efficient solutions that preserve perceptual and semantic fidelity while ensuring robustness.

Method: Introduce Resi-VidTok with a resilient 1D tokenization pipeline that encodes video into an importance-ordered stream of key and refinement tokens; employs differential temporal token coding, prefix-decodable reconstruction using a single shared framewise decoder; uses stride-controlled frame sparsification with a lightweight decoder-side interpolator; and applies channel-adaptive source–channel coding and modulation that allocate rate and protection based on token importance and channel conditions.

Result: Empirical evaluations show robust visual and semantic fidelity at extremely low bandwidth ratios (CBR as low as 0.0004) and real-time reconstruction at over 30 fps, indicating practicality for energy-efficient, latency-sensitive, and reliability-critical wireless applications.

Conclusion: Resi-VidTok offers a practical, resilient solution for ultra-low-rate video transmission over wireless networks, enabling progressive quality, robust reconstruction, and adaptive protection without heavy generative models or auxiliary temporal extractors.

Abstract: Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.

</details>


### [103] [Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder](https://arxiv.org/abs/2510.25181)
*Yixiang Zhou,Tong Wu,Meixia Tao,Jianhua Mo*

Main category: cs.IT

TL;DR: 提出 Fed-PELAD 的联邦学习框架用于 MASSIVE MIMO 的 CSI 反馈，通过在每个用户设备上训练个性化编码器、在基站全局协调下使用 LoRA 自适应的共享解码器，使仅传输 LoRA 参数而非全量更新，从而降低上行通信开销并在异质条件下提升 CSI 反馈精度；通过交替冻结策略与校准学习率比提高收敛性。实验证明在 3GPP 通道模型下上行通信成本降低约 42.97%，CSI 反馈精度提升约 1.2 dB。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多输入多输出（mMIMO）场景下深度学习的通信开销、数据异质性与隐私保护问题；不同用户设备存在信道特性差异，需要在保护隐私的前提下实现高效的模型更新与收敛。

Method: 提出 Fed-PELAD：在用户设备端本地训练个性化编码器以捕获设备特有的信道特征；在基站端使用全局协调、采用 LoRA 的共享解码器，传输的是 LoRA 适配器参数而非完整模型更新；为提升收敛稳定性，采用带校准学习率比的交替冻结策略进行 LoRA 聚合。通过 3GPP 标准信道模型的广泛仿真评估。

Result: 相比传统方法，Fed-PELAD 的上行通信成本降低约 42.97%，在异质条件下 CSI 反馈精度提高约 1.2 dB。仿真结果支持该方法在隐私保护和数据异质性环境中具备更高的现实可行性与鲁棒性。

Conclusion: Fed-PELAD 能够在保持较低上行通信开销的同时提升 CSI 反馈性能，且通过个性化编码器与 LoRA 适配的结合在异质环境中实现更稳健的收敛与高效更新，具有较强的实际落地潜力。

Abstract: This paper addresses the critical challenges of communication overhead, data
heterogeneity, and privacy in deep learning for channel state information (CSI)
feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel
federated learning framework that incorporates personalized encoders and a
LoRA-adapted shared decoder. Specifically, personalized encoders are trained
locally on each user equipment (UE) to capture device-specific channel
characteristics, while a shared decoder is updated globally via the
coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This
design ensures that only compact LoRA adapter parameters instead of full model
updates are transmitted for aggregation. To further enhance convergence
stability, we introduce an alternating freezing strategy with calibrated
learning-rate ratio during LoRA aggregation. Extensive simulations on
3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\%
of the uplink communication cost compared to conventional methods while
achieving a performance gain of 1.2 dB in CSI feedback accuracy under
heterogeneous conditions.

</details>


### [104] [General Coverage Models: Structure, Monotonicity, and Shotgun Sequencing](https://arxiv.org/abs/2510.25305)
*Yitzchak Grunbaum,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出一种将覆盖过程的求解从概率问题转化为对覆盖[n]的子集族计数的统一组合框架；给出窗口模型（循环与非循环）的精确表达式及其渐近行为；引入 uniform ℓ-regular 模型并与批量采样比较，给出上/下界与普遍上界，揭示 leading-order 的通用性与不同的低阶项。


<details>
  <summary>Details</summary>
Motivation: 研究覆盖时间的本质及其与组合计数之间的联系。以香肠般的覆盖模型（如窗口覆盖）为主线，源自香槟式的连锁问题（Coupon Collector 及其扩展），并以 blasts DNA 测序等实际问题为驱动，期望通过一个统一的计数工具将概率分析转化为组合计数，并解析不同模型的渐近与普适性。

Method: 提出一个统一的组合工具，将覆盖时间问题转化为统计覆盖 [n] 的子集族计数问题；对窗口模型（循环与非循环）给出等价的精确表达式；借助连续模型的结果分析两种窗口模型的渐近行为；定义并研究 uniform ℓ-regular 模型，比较其与批量抽样的关系，给出界与某些特殊情形下的定理；提出并验证对 uniform ℓ-regular 的普遍上界及相关结论。

Result: 获得窗口模型的精确表达式；确立两类模型的渐近性质；在 uniform ℓ-regular 框架下给出与批量采样的上下界，并在特定情形和一般情形下给出普遍上界，证明在该族中领先渐近阶数相同但低阶项可能不同；并且提出并在特殊情形下证明了关于最大化覆盖时间的猜想（对某些模型成立）。

Conclusion: 构建了一个将覆盖时间的概率分析转为组合计数的普适框架，揭示了多种覆盖模型在 leading-order 行为上的一致性与在低阶项上的差异；对窗口模型提供了精确表达与渐近分析，并对 uniform ℓ-regular 模型的极值性质及与批量抽样的关系给出了清晰界限，为后续的广义覆盖过程研究建立了坚实基础。

Abstract: We study coverage processes in which each draw reveals a subset of $[n]$, and
the goal is to determine the expected number of draws until all items are seen
at least once. A classical example is the Coupon Collector's Problem, where
each draw reveals exactly one item. Motivated by shotgun DNA sequencing, we
introduce a model where each draw is a contiguous window of fixed length, in
both cyclic and non-cyclic variants. We develop a unifying combinatorial tool
that shifts the task of finding coverage time from probability, to a counting
problem over families of subsets of $[n]$ that together contain all items,
enabling exact calculation. Using this result, we obtain exact expressions for
the window models. We then leverage past results on a continuous analogue of
the cyclic window model to analyze the asymptotic behavior of both models. We
further study what we call uniform $\ell$-regular models, where every draw has
size $\ell$ and every item appears in the same number of admissible draws. We
compare these to the batch sampling model, in which all $\ell$-subsets are
drawn uniformly at random and present upper and lower bounds, which were also
obtained independently by Berend and Sher. We conjecture, and prove for special
cases, that this model maximizes the coverage time among all uniform
$\ell$-regular models. Finally, we prove a universal upper bound on the entire
class of uniform $\ell$-regular models, which illuminates the fact that many
sampling models share the same leading asymptotic order, while potentially
differing significantly in lower-order terms.

</details>


### [105] [Several classes of $p$-ary linear codes with few-weights derived from Weil sums](https://arxiv.org/abs/2510.25578)
*Mrinal Kanti Bose,Abhay Kumar Singh*

Main category: cs.IT

TL;DR: 通过两组定义集构造了若干少重线性码：第一组定义集给出五类4重码和一类2重码，第二组定义集结合弱规则双弯函数得到两类6重码、两类8重码、以及一类9重码；码的参数与重分布通过对有限域上的 Weil 和数计算完整确定，且存在一类达到 Griesmer 上界的最优2重码。


<details>
  <summary>Details</summary>
Motivation: 研究少重线性码及其在秘密分享、认证码、关联结构和强正则图中的应用；在 Cheng–Gao 与 Wu–Li–Zeng 的基础工作上，提出两种新的定义集并利用 Weil 和数以及弱规则 Bent 函数来系统构造和分析码的参数与重分布。

Method: 通过选取两组特定的定义集构造线性码；第一组定义集在 F_p 上直接得到多达五类4重码和一类2重码；第二组定义集引入弱规则 Bent 函数，以得到额外的6重、8重、9重码类，并通过对相关 Weil 和数的逐项计算来确定码的参数与重分布。

Result: 得到五类4重量码和一类2重量码来自第一定义集；通过第二定义集，得到两类6重量码、两类8重量码、以及一类9重量码；所有参数和重分布均由对有限场上 Weil 和数的详细计算确定；并识别出一类达到 Griesmer 界的最优2重量码。

Conclusion: 该工作给出了一系列新颖的少重线性码构造及其精确的重量分布，且存在一类最优2重量码，展示了定义集选择与 Bent 函数在精确重量分析中的有效性，具有潜在的应用与后续扩展空间。

Abstract: Linear codes with few weights have been a significant area of research in
coding theory for many years, due to their applications in secret sharing
schemes, authentication codes, association schemes, and strongly regular
graphs. Inspired by the works of Cheng and Gao \cite{P8} and Wu, Li and Zeng
\cite{P12}, in this paper, we propose several new classes of few-weight linear
codes over the finite field $\mathbb{F}_{p}$ through the selection of two
specific defining sets. Consequently, we obtain five classes of $4$-weight
linear codes and one class of $2$-weight linear codes from our first defining
set. Furthermore, by employing weakly regular bent functions in our second
defining set, we derive two classes of $6$-weight codes, two classes of
$8$-weight codes, and one class of $9$-weight codes. The parameters and weight
distributions of all these constructed codes are wholly determined by detailed
calculations on certain Weil sums over finite fields. In addition, we identify
an optimal class of $2$-weight codes that meet the Griesmer bound.

</details>


### [106] [On Multidimensional 2-Weight-Limited Burst-Correcting Codes](https://arxiv.org/abs/2510.25592)
*Hagai Berend,Ohad Elishco,Moshe Schwartz*

Main category: cs.IT

TL;DR: 给出多维码在爆发错误权重至多为2的情形下的三种相对位置受限的构造：通过显式构造实现两点之间在某种距离度量下有界，并与下界的冗余进行比较。


<details>
  <summary>Details</summary>
Motivation: 研究多维码在爆发错误（burst error）权重受限的场景，特别是当仅有两个错误位置时，要求这两个位置的相对位置受某种几何/距离约束，以便设计有效的纠错码并量化冗余。

Method: 分别考虑三种限制：1) 两个错误位置的 L∞ 距离有界；2) 两个错误位置的 L1 距离有界；3) 两个错误位置位于轴平行的直线上且距离有界。给出这三种情形下的显式码构造，并分析它们的冗余与一个下界的差距。

Result: 给出三类有限距离约束下的显式码构造，并与相应的冗余下界进行比较，给出冗余的优劣势以及不同限制条件下的性能差异。

Conclusion: 在三种限制下实现对权重为2的爆发错误的纠错能力，且给出显式构造与下界比较，表明某些情形的冗余率接近下界或达到近似最优。

Abstract: We consider multidimensional codes capable of correcting a burst error of
weight at most $2$. When two positions are in error, the burst limits their
relative position. We study three such limitations: the $L_\infty$ distance
between the positions is bounded, the $L_1$ distance between the positions is
bounded, or the two positions are on an axis-parallel line with bounded
distance between them. In all cases we provide explicit code constructions, and
compare their excess redundancy to a lower bound we prove.

</details>


### [107] [Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems](https://arxiv.org/abs/2510.25736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 在图结构的数据库复制模型中研究对称私信息检索（SPIR）。提出一种把PIR方案转换为SPIR方案的算法，给出适用于某些图（包括路径图和循环图）的SPIR容量下界与更紧的上界；在三点路径图的特殊情形下，SPIR容量为1/2。


<details>
  <summary>Details</summary>
Motivation: 探究复制拓扑（由图决定的服务器副本关系）与公共随机数对SPIR容量的影响，以及将PIR与SPIR之间的连接。目标是量化相对于仅使用图复制的公共随机数的SPIR容量提升。

Method: 提出将一类PIR方案转化为等价SPIR方案的算法，从而在存在这类PIR-到-SPIR转化的图上确立SPIR容量的下界；对路径图和循环图等拓扑给出容量上界（比对应的PIR容量给出的界更紧）；在路径图三点情形中，推导出SPIR容量为1/2。

Result: 建立了将PIR-到-SPIR的转化工具，给出适用于特定图的SPIR容量下界；对路径与循环图给出比PIR容量更紧的上界；特例路径图三点的SPIR容量精确值为1/2。

Conclusion: 该工作揭示了在图型复制数据库中，PIR到SPIR的转换性及其对容量界的影响。对某些拓扑（如路径和循环图）可以给出更紧的容量界，且在特定情形（如路径图三点）得到精确容量，从而为SPIR在图结构下的容量研究提供了新的工具与方向。

Abstract: We revisit the problem of symmetric private information retrieval (SPIR) in
settings where the database replication is modeled by a simple graph. Here,
each vertex corresponds to a server, and a message is replicated on two servers
if and only if there is an edge between them. To satisfy the requirement of
database privacy, we let all the servers share some common randomness,
independent of the messages. We aim to quantify the improvement in SPIR
capacity, i.e., the maximum ratio of the number of desired and downloaded
symbols, compared to the setting with graph-replicated common randomness.
Towards this, we develop an algorithm to convert a class of PIR schemes into
the corresponding SPIR schemes, thereby establishing a capacity lower bound on
graphs for which such schemes exist. This includes the class of path and cyclic
graphs for which we derive capacity upper bounds that are tighter than the
trivial bounds given by the respective PIR capacities. For the special case of
path graph with three vertices, we identify the SPIR capacity to be
$\frac{1}{2}$.

</details>


### [108] [A mathematical study of the excess growth rate](https://arxiv.org/abs/2510.25740)
*Steven Campbell,Ting-Kam Leonard Wong*

Main category: cs.IT

TL;DR: excess growth rate 在信息理论视角下的三条公理化刻画，揭示其与 Renyi、互信息、热力学自由能等的联系，并与增长最优投资组合比较。


<details>
  <summary>Details</summary>
Motivation: 将信息理论的度量引入量化金融中的增长率概念，解释为何该指标具有理论与实践意义，并建立与熵、大偏差等信息量之间的对应关系。

Method: 通过三条公理化刻画，对 excess growth rate 与相对熵、Jensen 不等式的间隙、以及对数散度的联系进行 formal 化分析；比较其与增长最优投资组合的关系。

Result: 给出三个以相对熵、Jensen 不等式差值、对数散度（广义 Bregman 散度）为基础的公理化刻画定理；揭示与 Renyi 与互信息、Helmholtz 自由能、Campbell 的平均码长度等的联系；以及对 excess growth rate 的最大化及其与增长最优组合的比较结果。

Conclusion: 确立信息理论与定量金融之间的新联系，为理解 excess growth rate 的理论基础提供支撑，并为投资组合设计提供新的信息理论工具。

Abstract: We study the excess growth rate -- a fundamental logarithmic functional
arising in portfolio theory -- from the perspective of information theory. We
show that the excess growth rate can be connected to the R\'{e}nyi and cross
entropies, the Helmholtz free energy, L. Campbell's measure of average code
length and large deviations. Our main results consist of three axiomatic
characterization theorems of the excess growth rate, in terms of (i) the
relative entropy, (ii) the gap in Jensen's inequality, and (iii) the
logarithmic divergence that generalizes the Bregman divergence. Furthermore, we
study maximization of the excess growth rate and compare it with the growth
optimal portfolio. Our results not only provide theoretical justifications of
the significance of the excess growth rate, but also establish new connections
between information theory and quantitative finance.

</details>
