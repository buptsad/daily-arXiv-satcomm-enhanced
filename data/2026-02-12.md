<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.CR](#cs.CR) [Total: 25]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.LG](#cs.LG) [Total: 107]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Boiling flow parameter estimation from boundary layer data](https://arxiv.org/abs/2602.10394)
*Jeffrey W. Utley,Gregery T. Buzzard,Charles A. Bouman,Matthew R. Kemnetz*

Main category: eess.SP

TL;DR: 利用测量数据估计 boili...


<details>
  <summary>Details</summary>
Motivation: 实验诱导大气湍流引起的相位畸变成本高，寻求低成本、低耗时的数值模拟替代方案

Method: 采用实验测量的航空光学相位畸变量，利用参数估计算法（最小化空间和时间统计误差）拟合 Boiling Flow 仿真模型

Result: 时间功率谱密度相当吻合，误差 8-9%；空间结构函数误差超 28%，显示空间统计匹配不足

Conclusion: 在拟合高对流数据时，boiling flow 参数可较好匹配时间统计，但在空间统计上仍表现不佳，提示该模型无法完整描述航空光学相位畸变

Abstract: Atmospheric turbulence and aero-optic effects cause phase aberrations in propagating light waves, thereby reducing effectiveness in transmitting and receiving coherent light from an aircraft. Existing optical sensors can measure the resulting phase aberrations, but the physical experiments required to induce these aberrations are expensive and time-intensive. Simulation methods could provide a less expensive alternative. For example, an existing simulation algorithm called boiling flow, which generalizes the Taylor frozen-flow method, can generate synthetic phase aberration data (i.e., phase screens) induced by atmospheric turbulence. However, boiling flow depends on physical parameters, such as the Fried coherence length r0, which are not well-defined for aero-optic effects. In this paper, we introduce a method to estimate the parameters of boiling flow from measured aero-optic phase aberration data. Our algorithm estimates these parameters to fit the spatial and temporal statistics of the measured data. This method is computationally efficient and our experiments show that the temporal power spectral density of the slopes of the synthetic phase screens reasonably matches that of the measured phase aberrations from two turbulent boundary layer data sets, with errors between 8-9%. However, the Kolmogorov spatial structure function of the phase screens does not match that of the measured phase aberrations, with errors above 28%. This suggests that, while the parameters of boiling flow can reasonably fit the temporal statistics of highly convective data, they cannot fit the complex spatial statistics of aero-optic phase aberrations.

</details>


### [2] [RadarEye: Robust Liquid Level Tracking Using mmWave Radar in Robotic Pouring](https://arxiv.org/abs/2602.10417)
*Hongyu Deng,He Chen*

Main category: eess.SP

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transparent liquid manipulation in robotic pouring remains challenging for perception systems: specular/refraction effects and lighting variability degrade visual cues, undermining reliable level estimation. To address this challenge, we introduce RadarEye, a real-time mmWave radar signal processing pipeline for robust liquid level estimation and tracking during the whole pouring process. RadarEye integrates (i) a high-resolution range-angle beamforming module for liquid level sensing and (ii) a physics-informed mid-pour tracker that suppresses multipath to maintain lock on the liquid surface despite stream-induced clutter and source container reflections. The pipeline delivers sub-millisecond latency. In real-robot water-pouring experiments, RadarEye achieves a 0.35 cm median absolute height error at 0.62 ms per update, substantially outperforming vision and ultrasound baselines.

</details>


### [3] [Nonparametric Variational Bayesian Learning for Channel Estimation with OTFS Modulation](https://arxiv.org/abs/2602.10438)
*Chong Cao,Zhuyu Liu,Zheng Dong,Yong Zhou,He Chen*

Main category: eess.SP

TL;DR: 提出一种NPBL框架，利用条形分割和高斯混合捕捉信道聚簇结构，并通过剪枝提高OTFS信道估计的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有OTFS信道估计方案普遍忽略了CDL信道中本质的结构稀疏和聚簇特性，导致实际系统表现不足。

Method: 采用条形分割过程自动确定多径数量并将每条路径归属到对应的聚簇；在每个聚簇内对信道系数建模为高斯混合分布；设计裁剪准则剔除无效多径，进一步提升精度并降低复杂度。

Result: 仿真显示，该方法在归一化均方误差（NMSE）方面明显优于传统方案，并在处理复杂衰落统计时保持稳健性。

Conclusion: 该研究提出的基于非参数贝叶斯学习的OTFS信道估计框架在具有聚簇延迟线特征的高移动环境中，能显著提升估计精度并降低计算复杂度，优于现有方法。

Abstract: Orthogonal time frequency space (OTFS) modulation has demonstrated significant advantages in high-mobility scenarios in future 6G networks. However, existing channel estimation methods often overlook the structured sparsity and clustering characteristics inherent in realistic clustered delay line (CDL) channels, leading to degraded performance in practical systems. To address this issue, we propose a novel nonparametric Bayesian learning (NPBL) framework for OTFS channel estimation. Specifically, a stick-breaking process is introduced to automatically infer the number of multipath components and assign each path to its corresponding cluster. The channel coefficients within each cluster are modeled by a Gaussian mixture distribution to capture complex fading statistics. Furthermore, an effective pruning criterion is designed to eliminate spurious multipath components, thereby enhancing estimation accuracy and reducing computational complexity. Simulation results demonstrate that the proposed method achieves superior performance in terms of normalized mean squared error compared to existing methods.

</details>


### [4] [Clutter-Aware Integrated Sensing and Communication: Models, Methods, and Future Directions](https://arxiv.org/abs/2602.10537)
*Rang Liu,Peishi Li,Ming Li,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 在复杂环境中，通过统一建模和多域抑制，设计协同工作、QoS兼顾的ISAC系统，可显著提升感知与通信效率


<details>
  <summary>Details</summary>
Motivation: 宽带散射丰富环境下，杂波主导弱目标回波，严重限制了ISAC的感知可靠性，需系统化建模与抑制方法

Method: 提出统一的MIMO-OFDM信号模型，结合幅度统计、SIRV模型以及结构化协方差工程，多尺度（时间、空间、频率）消声技术，探讨空间-时-频适配处理与波束/波形协同优化

Result: 综述并指导不同波形、干扰情形下的抑制手段，为信号处理与波束设计提供技术路径和实践建议

Conclusion: 本文总结了在宽带场景下，针对冷杂波和热杂波的ISAC系统的建模与消除方案，并指出未来需要环境自适应与抗杂波的协同设计

Abstract: Integrated sensing and communication (ISAC) can substantially improve spectral, hardware, and energy efficiency by unifying radar sensing and data communications. In wideband and scattering-rich environments, clutter often dominates weak target reflections and becomes a fundamental bottleneck for reliable sensing. Practical ISAC clutter includes "cold" clutter arising from environmental backscatter of the probing waveform, and "hot" clutter induced by external interference and reflections from the environment whose statistics can vary rapidly over time. In this article, we develop a unified wideband multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) signal model that captures both clutter types across the space, time, and frequency domains. Building on this model, we review clutter characterization at multiple levels, including amplitude statistics, robust spherically invariant random vector (SIRV) modeling, and structured covariance representations suitable for limited-snapshot regimes. We then summarize receiver-side suppression methods in the temporal and spatial domains, together with extensions to space-time adaptive processing (STAP) and space-frequency-time adaptive processing (SFTAP), and we provide guidance on selecting techniques under different waveform and interference conditions. To move beyond reactive suppression, we discuss clutter-aware transceiver co-design that couples beamforming and waveform optimization with practical communication quality-of-service (QoS) constraints to enable proactive clutter avoidance. We conclude with open challenges and research directions toward environment-adaptive and clutter-resilient ISAC for next-generation networks.

</details>


### [5] [Transfer to Sky: Unveil Low-Altitude Route-Level Radio Maps via Ground Crowdsourced Data](https://arxiv.org/abs/2602.10736)
*Wenlihan Lu,Huacong Chen,Ruiyang Duan,Weijie Yuan,Shijian Gao*

Main category: eess.SP

TL;DR: 通过仿真预训练、对抗对齐以及有限真实测量的微调，本文实现了高精度的无人机航线级射频图预测，实测准确性提升超过 50%。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速发展需要可靠的蜂窝连通性，然而现有无线信号图方法受限于测量稀疏，难以在预飞计划中精准预估链路质量，亟需更精准的数据预测手段。

Method: 采用仿真获取通用传播先验，利用对抗式特征空间对齐弥补地面与空中数据域差异，随后在有限真实无人机测量数据上微调模型，实现航线级 RSRP 预测。

Result: 在美团真实数据集上，所提出的方法在预测航线 RSRP 的准确率上比现有最先进基线高 50% 以上。

Conclusion: 本文提出的迁移学习框架显著提升了无人机航线级射频图预测的精度，验证了利用大量地面众包信号与仿真预训练相结合的有效性。

Abstract: The expansion of the low-altitude economy is contingent on reliable cellular connectivity for unmanned aerial vehicles (UAVs). A key challenge in pre-flight planning is predicting communication link quality along proposed and pre-defined routes, a task hampered by sparse measurements that render existing radio map methods ineffective. This paper introduces a transfer learning framework for high-fidelity route-level radio map prediction. Our key insight is to leverage abundant crowdsourced ground signals as auxiliary supervision. To bridge the significant domain gap between ground and aerial data and address spatial sparsity, our framework learns general propagation priors from simulation, performs adversarial alignment of the feature spaces, and is fine-tuned on limited real UAV measurements. Extensive experiments on a real-world dataset from Meituan show that our method achieves over 50% higher accuracy in predicting Route RSRP compared to state-of-the-art baselines.

</details>


### [6] [Stochastic Design of Active RIS-Assisted Satellite Downlinks under Interference, Folded Noise, and EIRP Constraints](https://arxiv.org/abs/2602.10742)
*Muhammad Khalil,Ke Wang,Jinho Choi*

Main category: eess.SP

TL;DR: 本文提出针对主动RIS卫星链路的可靠性设计：通过随机SINR模型与SAA，将目标落差转化为MISOCP求解；得到的解决方案在真实随机环境与硬件约束下实现目标可靠性，配套的SINR包络实现快速性能诊断与验证。


<details>
  <summary>Details</summary>
Motivation: 主动RIS在卫星下行链路中受随机共信道干扰、放大器噪声及法规限制等因素限制，需制定可靠性的设计框架以保证目标的落差性能。

Method: 构建随机SINR模型，利用样本平均近似（SAA）将目标落差约束化为混合整数二阶锥问题（MISOCP），随后通过双向搜索求解SINR阈值；同时给出基于本征值和l1范数的SINR包络，以快速诊断性能。

Result: 通过蒙特卡洛仿真验证，给出的SINR包络紧贴实际SINR，能够准确预测饱和行为并量化干扰导致的性能退化；所构建的MISOCP求解器完善实现可在实际硬件条件下达到设定可靠性。

Conclusion: 本文提出一种针对主动可重构智能表面（RIS）卫星下行链路的可靠性设计方法，能够在随机干扰、放大器噪声和辐射功率限制下保证目标落差概率；实现通过混合整数二阶锥规划求解问题，验证方案在实际随机性和硬件约束下具备高可靠性。

Abstract: Active reconfigurable intelligent surfaces (RISs) can mitigate the double-fading loss of passive reflection in satellite downlinks. However, their gains are limited by random co-channel interference, gain-dependent amplifier noise, and regulatory emission constraints. This paper develops a stochastic reliability framework for active RIS-assisted satellite downlinks by modeling the desired and interfering channels, receiver noise, and RIS amplifier noise as random variables. The resulting instantaneous signal-to-interference-plus-noise ratio (SINR) model explicitly captures folded cascaded amplifier noise and reveals a finite high-gain SINR ceiling.
  To guarantee a target outage level, we formulate a chance-constrained max-SINR design that jointly optimizes the binary RIS configuration and a common amplification gain. The chance constraint is handled using a sample-average approximation (SAA) with a violation budget. The resulting feasibility problem is solved as a mixed-integer second-order cone program (MISOCP) within a bisection search over the SINR threshold. Practical implementation is enforced by restricting the gain to an admissible range determined by small-signal stability and effective isotropic radiated power (EIRP) limits.
  We also derive realization-wise SINR envelopes based on eigenvalue and l1-norm bounds, which provide interpretable performance limits and fast diagnostics. Monte Carlo results show that these envelopes tightly bound the simulated SINR, reproduce the predicted saturation behavior, and quantify performance degradation as interference increases. Overall, the paper provides a solver-ready, reliability-targeting design methodology whose achieved reliability is validated through out-of-sample Monte Carlo testing under realistic randomness and hardware constraints.

</details>


### [7] [Constellation Design for Robust Interference Mitigation](https://arxiv.org/abs/2602.10767)
*Athanasios T. Papadopoulos,Thrassos K. Oikonomou,Dimitrios Tyrovolas,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,Panagiotis Sarigiannidis,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 针对Nakagami‑m干扰，提出低复杂度ML‑G检测器及干扰友好的星座设计，在数值仿真中显著提升符号误差率。


<details>
  <summary>Details</summary>
Motivation: 传统检测依赖高斯噪声模型，而实际存在随机幅度、非均匀相位的Nakagami‑m干扰，导致检测区域失配，影响性能。

Method: 构造低复杂度ML‑G检测规则，加入干扰统计特性；在此基础上建立星座优化问题，求解满足能量约束的最小符号错误率星座。

Result: 仿真表明，ML‑G检测器在各种干扰强度下显著优于已有方案，尤其在主导性加性干扰场景；依赖干扰的星座显示非标准非对称结构。

Conclusion: 论文通过提出最大似然高斯相位近似（ML‑G）检测器以及针对干扰的星座设计，有效解决了在Nakagami-m干扰下符号检测误差率不匹配的问题。

Abstract: This paper investigates symbol detection for single-carrier communication systems operating in the presence of additive interference with Nakagami-m statistics. Such interference departs from the assumptions underlying conventional detection methods based on Gaussian noise models and leads to detection mismatch that fundamentally affects symbol-level performance. In particular, the presence of random interference amplitude and non-uniform phase alters the structure of the optimal decision regions and renders standard Euclidean distance-based detectors suboptimal. To address this challenge, we develop the maximum-likelihood Gaussian-phase approximate (ML-G) detector, a low-complexity detection rule that accurately approximates maximum-likelihood detection while remaining suitable for practical implementation. The proposed detector explicitly incorporates the statistical properties of the interference and induces decision regions that differ significantly from those arising under conventional metrics. Building on the ML-G framework, we further investigate constellation design under interference-aware detection and formulate an optimization problem that seeks symbol placements that minimize the symbol error probability subject to an average energy constraint. The resulting constellations are obtained numerically and adapt to the interference environment, exhibiting non-standard and asymmetric structures as interference strength increases. Simulation results demonstrate clear symbol error probability gains over established benchmark schemes across a range of interference conditions, particularly in scenarios with dominant additive interference.

</details>


### [8] [Bayesian Signal Component Decomposition via Diffusion-within-Gibbs Sampling](https://arxiv.org/abs/2602.10792)
*Yi Zhang,Rui Guo,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 本文提出的DiG采样器通过Gibbs + PnP扩散先验实现了贝叶斯信号分解，理论可行且实验优越。


<details>
  <summary>Details</summary>
Motivation: 在信号处理中，传感器数据往往是多重成分的噪声线性叠加，准确分解成分是关键预处理步骤；现行方法难以统一引入模型驱动与数据驱动先验知识。

Method: 构建Bayesian框架，将Gibbs采样与Plug‑and‑Play扩散先验相结合，形成DiG（Diffusion‑in‑Gibbs）采样器；先验可单独学习后灵活组合，不必重新训练。

Result: 在满足合理假设下，DiG采样器理论上可产生后验分布样本；对合适的感知算子能更好地利用测量模型结构；数值实验显示其性能优于现有方案。

Conclusion: DiG采样器实现了模型驱动与数据驱动先验知识的统一整合，并在理论与实验上证明其能够得到后验分布样本且在测量模型上更有效地利用结构，显著优于现有方法。

Abstract: In signal processing, the data collected from sensing devices is often a noisy linear superposition of multiple components, and the estimation of components of interest constitutes a crucial pre-processing step. In this work, we develop a Bayesian framework for signal component decomposition, which combines Gibbs sampling with plug-and-play (PnP) diffusion priors to draw component samples from the posterior distribution. Unlike many existing methods, our framework supports incorporating model-driven and data-driven prior knowledge into the diffusion prior in a unified manner. Moreover, the proposed posterior sampler allows component priors to be learned separately and flexibly combined without retraining. Under suitable assumptions, the proposed DiG sampler provably produces samples from the posterior distribution. We also show that DiG can be interpreted as an extension of a class of recently proposed diffusion-based samplers, and that, for suitable classes of sensing operators, DiG better exploits the structure of the measurement model. Numerical experiments demonstrate the superior performance of our method over existing approaches.

</details>


### [9] [Physically Consistent Evaluation of Commonly Used Near-Field Models](https://arxiv.org/abs/2602.10976)
*Georg Schwan,Alexander Stutz-Tirri,Christoph Studer*

Main category: eess.SP

TL;DR: 简化近场模型对基本聚焦有效，但在旁瓣和频率响应上不够准确；物理一致模型揭示了这些缺陷。


<details>
  <summary>Details</summary>
Motivation: 缺乏可靠的全息近场模型导致现有天线与反射结构模型在真实系统中的有效性不明确，亟需一种物理一致的近场模型来评估这些简化模型。

Method: 本文提出一种物理一致的近场模型，并利用该模型对常用的简化近场模型进行评估和对比。

Result: 结果显示，常用模型在实现基本波束聚焦方面足够，但在预测反射结构的旁瓣和频率依赖性方面存在显著误差。

Conclusion: 基于物理一致的近场模型可以准确识别简化模型的局限性，为未来系统设计提供更可靠的预测手段。

Abstract: Near-field multi-antenna wireless communication has attracted growing research interest in recent years. Despite this development, most of the current literature on antennas and reflecting structures relies on simplified models, whose validity for real systems remains unclear. In this paper, we introduce a physically consistent near-field model, which we use to evaluate commonly used models. Our results indicate that common models are sufficient for basic beamfocusing, but fail to accurately predict the sidelobes and frequency dependence of reflecting structures.

</details>


### [10] [Reed-Muller Error-Correction Code Encoder for SFQ-to-CMOS Interface Circuits](https://arxiv.org/abs/2602.11140)
*Yerzhan Mustafa,Berker Peköz,Selçuk Köse*

Main category: eess.SP

TL;DR: 用RM(1,3)代码在SFQ逻辑中实现8位码字，能在±20% PPV下提高6.7%无错误率，±15% PPV下达到99.1%纠错率，且该方案硬件轻量、易实现。


<details>
  <summary>Details</summary>
Motivation: 解决超导数字电子与半导体CMOS之间数据传输时因磁通捕获、工艺参数波动( PPV)及制造缺陷导致的比特错误问题

Method: 实现轻量级、硬件高效的错误纠正编码器，采用RM(1,3)码，将4位信息映射为8位码字，并使用MIT-LL SFQ5ee工艺和SuperTools/ColdFlux RSFQ库完成电路设计；开发结合JoSIM模拟器与MATLAB脚本的自动数据收集和分析框架

Result: 在±20% PPV下，编码器将无比特错误概率提升6.7%；在±15%及更小PPV下，编码器能以≥99.1%的概率纠正所有错误；框架还评估了开路故障等制造缺陷对编码器的影响

Conclusion: 本文提出的RM(1,3)编码器实现了高效的错误检测与纠正，并在超导数字逻辑与CMOS之间的数据兼容性方面表现显著提升，验证了轻量级编码在保持硬件友好性下提升信号可靠性的可行性

Abstract: Data transmission from superconducting digital electronics such as single flux quantum (SFQ) logic to semiconductor (CMOS) circuits is subject to bit errors due to, e.g., flux trapping, process parameter variations (PPV), and fabrication defects. In this paper, a lightweight hardware-efficient error-correction code encoder is designed and analyzed. Particularly, a Reed-Muller code RM(1,3) encoder is implemented with SFQ digital logic. The proposed RM(1,3) encoder converts a 4-bit message into an 8-bit codeword and can detect and correct up to 3- and 1-bit errors, respectively. This encoder circuit is designed using MIT-LL SFQ5ee process and SuperTools/ColdFlux RSFQ cell library. A simulation framework integrating JoSIM simulator and MATLAB script for automated data collection and analysis, is proposed to study the performance of RM(1,3) encoder. The proposed encoder improves the probability of having no bit errors by 6.7% as compared to an encoder-less design under $\pm20\%$ PPV. With $\pm15\%$ and lower PPV, the proposed encoder could correct all errors with at least 99.1% probability. The impact of fabrication defects such as open circuit faults on the encoder circuit is also studied using the proposed framework.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [Reverse-Engineering Model Editing on Language Models](https://arxiv.org/abs/2602.10134)
*Zhiyu Sun,Minrui Luo,Yu Wang,Zhili Chen,Tianxing He*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \textit{KSTER} (\textbf{K}ey\textbf{S}paceRecons\textbf{T}ruction-then-\textbf{E}ntropy\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.

</details>


### [12] [Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants](https://arxiv.org/abs/2602.10142)
*Molly Campbell,Ajay Kumar Shrestha*

Main category: cs.CR

TL;DR: 针对加拿大青年，隐私自效感是使用智能语音助手时采取保护行为的关键；算法透明度通过自效感间接影响，感知利益则削弱直接行动。设计需提升青年自信，兼顾助手便利。


<details>
  <summary>Details</summary>
Motivation: 深入探究青年在智能语音助手中的隐私保护行为背后的机制，填补关于青年隐私风险感知与保护行动之间关系的研究空白。

Method: 对加拿大16-24岁青少年共469人进行横断面问卷调查，使用偏最小二乘结构方程模型（PLS-SEM）检验以感知隐私风险、感知利益、算法透明度与信任、自我效能和隐私保护行为为核心的五因素结构模型。

Result: 结果显示，隐私自效感是对隐私保护行为最强的预测因子；算法透明度对保护行为的影响被自效感完全中介；感知利益直接抑制保护行动，但通过轻度提升自效感亦可间接促进。研究揭示政策过载和隐藏控制削弱青年所需的自效感。

Conclusion: 本研究实证验证并扩展了先前的质性工作，揭示了青年在使用智能语音助手时，隐私自效感（PSE）是促进隐私保护行为（PPB）的核心驱动因素，算法透明度等因素通过PSE间接发挥效应。基于此，提出了在不牺牲助手实用性的前提下，通过设计增强青年数字公民信心的路径。

Abstract: Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.

</details>


### [13] [Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks](https://arxiv.org/abs/2602.10148)
*Yu Yan,Sheng Sun,Shengjia Cheng,Teli Liu,Mingfeng Li,Min Liu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention and bypass safety alignment mechanisms. However, these adversarial attacks rely on simple and fixed image-text combinations that lack attack complexity scalability, limiting their effectiveness for red-teaming VLMs' continuously evolving reasoning capabilities. We propose \textbf{CrossTALK} (\textbf{\underline{Cross}}-modal en\textbf{\underline{TA}}ng\textbf{\underline{L}}ement attac\textbf{\underline{K}}), which is a scalable approach that extends and entangles information clues across modalities to exceed VLMs' trained and generalized safety alignment patterns for jailbreak. Specifically, {knowledge-scalable reframing} extends harmful tasks into multi-hop chain instructions, {cross-modal clue entangling} migrates visualizable entities into images to build multimodal reasoning links, and {cross-modal scenario nesting} uses multimodal contextual instructions to steer VLMs toward detailed harmful outputs. Experiments show our COMET achieves state-of-the-art attack success rate.

</details>


### [14] [Exploring Semantic Labeling Strategies for Third-Party Cybersecurity Risk Assessment Questionnaires](https://arxiv.org/abs/2602.10149)
*Ali Nour Eldin,Mohamed Sellami,Walid Gaaloul*

Main category: cs.CR

TL;DR: 利用语义标签改进TPRA问题检索；半监督SSSL管道能以更低成本、少量标记实现全库标签化，提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有TPRA检索方法多依赖关键词或表层相似度，往往无法捕捉隐含的评估范围与控制语义，手工定制过程繁琐，亟需更智能且语义化的检索方案。

Method: ①比较两种标记策略：直接对每个问题使用LLM生成标签；②构建半监督SSSL管道：将问题在嵌入空间聚类，挑选少量代表性问题交给LLM标记，然后利用k-最近邻(kNN)将标签传播到未标记问题。③在两种标记空间（问题级标签 vs 原始问题文本）下分别进行检索，评估检索对齐度。

Result: 实验显示：当语义标签具有良好的区分度和一致性时，基于标签空间的检索与直接问题相似度检索相比，能更好地匹配评估需求；同时，SSSL管道在仅标记少量代表性问题的情况下即可为大规模问题库生成有效标签，显著降低LLM调用量与费用。

Conclusion: 语义标签可在标签具有判别力和一致性时提升第三方风险评估问题的检索对齐度；半监督语义标记(SSSL)通过在嵌入空间聚类、对少量代表性问题使用LLM标记并将标签传播给其余问题，能够从小样本泛化标签，并显著降低LLM使用量与成本。

Abstract: Third-Party Risk Assessment (TPRA) is a core cybersecurity practice for evaluating suppliers against standards such as ISO/IEC 27001 and NIST. TPRA questionnaires are typically drawn from large repositories of security and compliance questions, yet tailoring assessments to organizational needs remains a largely manual process. Existing retrieval approaches rely on keyword or surface-level similarity, which often fails to capture implicit assessment scope and control semantics.
  This paper explores strategies for organizing and retrieving TPRA cybersecurity questions using semantic labels that describe both control domains and assessment scope. We compare direct question-level labeling with a Large Language Model (LLM) against a hybrid semi-supervised semantic labeling (SSSL) pipeline that clusters questions in embedding space, labels a small representative subset using an LLM, and propagates labels to remaining questions using k-Nearest Neighbors; we also compare downstream retrieval based on direct question similarity versus retrieval in the label space. We find that semantic labels can improve retrieval alignment when labels are discriminative and consistent, and that SSSL can generalize labels from a small labeled subset to large repositories while substantially reducing LLM usage and cost.

</details>


### [15] [Basic Legibility Protocols Improve Trusted Monitoring](https://arxiv.org/abs/2602.10153)
*Ashwin Sreevatsa,Sebastian Prasanna,Cody Rushing*

Main category: cs.CR

TL;DR: 作者提出在AI控制中加入注释协议，让不受信模型输出时配上易于评估的注释，实验证明此法能提升监控模型识别后门代码的能力，并且在监控强度提高时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 传统可信监控在不受信模型产生难以理解或比喻的输出时往往失效，需引入更易评估的行为以减少受信模型的误导。

Method: 在APPS编码任务中实施注释协议，将不受信模型在生成代码时提供详细注释，随后让监控模型评估其可解释性，比较与去除注释基线的效果。

Result: 1）注释协议在保持任务性能的前提下提升了安全性；2）对诚实代码有更大益处，因为其具有自然解释；3）强监控模型更能识别真正的说明，注释协议增益随监控模型力量提升而增加。

Conclusion: 通过在不受信模型中加入可解释性的注释协议，可显著提升监控模型对后门代码的识别能力，同时保持任务性能。

Abstract: The AI Control research agenda aims to develop control protocols: safety techniques that prevent untrusted AI systems from taking harmful actions during deployment. Because human oversight is expensive, one approach is trusted monitoring, where weaker, trusted models oversee stronger, untrusted models$\unicode{x2013}$but this often fails when the untrusted model's actions exceed the monitor's comprehension. We introduce legibility protocols, which encourage the untrusted model to take actions that are easier for a monitor to evaluate.
  We perform control evaluations in the APPS coding setting, where an adversarial agent attempts to write backdoored code without detection. We study legibility protocols that allow the untrusted model to thoroughly document its code with comments$\unicode{x2013}$in contrast to prior work, which removed comments to prevent deceptive ones. We find that: (i) commenting protocols improve safety without sacrificing task performance relative to comment-removal baselines; (ii) commenting disproportionately benefits honest code, which typically has a natural explanation that resolves monitor suspicion, whereas backdoored code frequently lacks an easy justification; (iii) gains from commenting increase with monitor strength, as stronger monitors better distinguish genuine justifications from only superficially plausible ones.

</details>


### [16] [MalMoE: Mixture-of-Experts Enhanced Encrypted Malicious Traffic Detection Under Graph Drift](https://arxiv.org/abs/2602.10157)
*Yunpeng Tan,Qingyang Li,Mingxin Yang,Yannan Hu,Lei Zhang,Xinggong Zhang*

Main category: cs.CR

TL;DR: An MoE-based graph neural net that adapts to changing network traffic patterns, reliably detecting encrypted threats in real time.


<details>
  <summary>Details</summary>
Motivation: Encrypted traffic impedes payload‑level inspection, while graph-based approaches suffer from graph drift—changes in flow statistics/topology over time—hindering sustained accuracy.

Method: It uses a Mixture‑of‑Experts framework with 1‑hop‑GNN‑like experts, each tailored to specific graph drift scenarios, and a gate model that routes inputs based on detected drift, trained via a two‑stage stable strategy with data augmentation.

Result: Experiments on open‑source, synthetic, and real‑world datasets confirm that MalMoE achieves accurate, real‑time detection, outperforming existing methods in drift‑resilient environments.

Conclusion: MalMoE introduces a drift-aware graph-based detection system for encrypted traffic, demonstrating precise real‑time performance and robust adaptability to evolving network conditions.

Abstract: Encryption has been commonly used in network traffic to secure transmission, but it also brings challenges for malicious traffic detection, due to the invisibility of the packet payload. Graph-based methods are emerging as promising solutions by leveraging multi-host interactions to promote detection accuracy. But most of them face a critical problem: Graph Drift, where the flow statistics or topological information of a graph change over time. To overcome these drawbacks, we propose a graph-assisted encrypted traffic detection system, MalMoE, which applies Mixture of Experts (MoE) to select the best expert model for drift-aware classification. Particularly, we design 1-hop-GNN-like expert models that handle different graph drifts by analyzing graphs with different features. Then, the redesigned gate model conducts expert selection according to the actual drift. MalMoE is trained with a stable two-stage training strategy with data augmentation, which effectively guides the gate on how to perform routing. Experiments on open-source, synthetic, and real-world datasets show that MalMoE can perform precise and real-time detection.

</details>


### [17] [Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment](https://arxiv.org/abs/2602.10161)
*Kun Wang,Zherui Li,Zhenhong Zhou,Yitong Zhang,Yan Mi,Kun Yang,Yiming Zhang,Junhao Dong,Zhongxiang Sun,Qiankun Li,Yang Liu*

Main category: cs.CR

TL;DR: 通过模态解耦与黄金拒绝向量提取，改进全模态LLM的拒绝机制，显著提高拒绝率并保持通用性


<details>
  <summary>Details</summary>
Motivation: 探究全模态大语言模型在跨模态交互中的安全隐患

Method: 提出模态-语义解耦原理，构造AdvBench-Omni数据集，分析中间层溶解等机制，并利用奇异值分解提取黄金拒绝向量，设计OmniSteer轻量化适应性介入控制

Result: 在有害输入上的拒绝成功率从69.9%提升至91.2%，并保持多模态下通用能力

Conclusion: 证明全模态LLM存在显著安全漏洞；通过理论分析与轻量化适配器可有效提升安全性，同时保持性能

Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling principle and construct the AdvBench-Omni dataset, which reveals a significant vulnerability in OLLMs. Mechanistic analysis uncovers a Mid-layer Dissolution phenomenon driven by refusal vector magnitude shrinkage, alongside the existence of a modal-invariant pure refusal direction. Inspired by these insights, we extract a golden refusal vector using Singular Value Decomposition and propose OmniSteer, which utilizes lightweight adapters to modulate intervention intensity adaptively. Extensive experiments show that our method not only increases the Refusal Success Rate against harmful inputs from 69.9% to 91.2%, but also effectively preserves the general capabilities across all modalities. Our code is available at: https://github.com/zhrli324/omni-safety-research.

</details>


### [18] [Limits of Residual-Based Detection for Physically Consistent False Data Injection](https://arxiv.org/abs/2602.10162)
*Chenhan Xiao,Yang Weng*

Main category: cs.CR

TL;DR: 残差检测无法可靠识别满足物理约束的FDIA，需对该检测方法的不足进行补救。


<details>
  <summary>Details</summary>
Motivation: 基于现行残差检测方法在面对物理一致的攻击时可能失效，揭示其根本缺陷，并在此基础上寻找能弥补不足的防御技术。

Method: 构造了一种基于数据驱动的机制，将AC流动的通用功能结构融入扰动生成，以产生符合物理约束的测量流形扰动，并在多种AC测试系统中通过数值实验验证其可行性。

Result: 通过多套AC测试系统的仿真，表明当攻击信号充分配合测量流形时残差检测难以检出；实验成功展示了威胁的可行性和失败模式。

Conclusion: 本文指出，在AC电力系统状态估计中，仅靠拓扑感知的残差检测无法总能识别伪造数据注入攻击（FDIA），因为一些大气测量误差可以在AC流动关系产生的测量流形上保持物理一致性；从而揭示了残差检测方法的根本限制，并呼吁使用补充防御措施。

Abstract: False data injection attacks (FDIAs) pose a persistent challenge to AC power system state estimation. In current practice, detection relies primarily on topology-aware residual-based tests that assume malicious measurements can be distinguished from normal operation through physical inconsistency reflected in abnormal residual behavior. This paper shows that this assumption does not always hold: when FDIA scenarios produce manipulated measurements that remain on the measurement manifold induced by AC power flow relations and measurement redundancy, residual-based detectors may fail to distinguish them from nominal data. The resulting detectability limitation is a property of the measurement manifold itself and does not depend on the attacker's detailed knowledge of the physical system model. To make this limitation observable in practice, we present a data-driven constructive mechanism that incorporates the generic functional structure of AC power flow to generate physically consistent, manifold-constrained perturbations, providing a concrete witness of how residual-based detectors can be bypassed. Numerical studies on multiple AC test systems characterize the conditions under which detection becomes challenging and illustrate its failure modes. The results highlight fundamental limits of residual-based detection in AC state estimation and motivate the need for complementary defenses beyond measurement consistency tests.

</details>


### [19] [MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments](https://arxiv.org/abs/2602.10166)
*Tatsunori Ono*

Main category: cs.CR

TL;DR: 开发了可公钥验证的语音归因系统MerkleSpeech，通过Merkle树+签名+水印，能在加工变换后辨别语音块来源，实验显示鲁棒性好且误报低。


<details>
  <summary>Details</summary>
Motivation: 改进语音内容的真实性验证，兼顾鲁棒性和可验证性，满足真实工作流程中的剪辑、引用、截断及平台层处理等复杂场景。

Method: 构建MerkleSpeech系统：先计算短语音块的感知指纹，构建Merkle树，根签名；然后嵌入紧凑的水印负载，携带随机内容标识符和块元数据；验证时通过公钥签名、指纹重算与Merkle包含检验实现两层可信度。

Result: 实验在重采样、带通滤波、加噪等变换下保持低误报率；系统能提供分块级的痕迹时间线，明确哪些区域通过哪一层验证。

Conclusion: MerkleSpeech 提供了一种可公开验证的、局部级别的语音产生链验证方案，兼顾鲁棒性与完整性，可用于鉴别剪辑、合成和篡改的语音片段。

Abstract: Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing.
  We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers "was this chunk issued by a known party?". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.

</details>


### [20] [Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces](https://arxiv.org/abs/2602.10169)
*Nicolai Maisch,Shengjian Chen,Alexander Robertus,Samed Ajdinović,Armin Lechler,Alexander Verl,Oliver Riedel*

Main category: cs.CR

TL;DR: 通过在以太坊上发行 NFT 指向 IPFS 上的 AAS 元数据，实现了金属加工品质检数据的安全存储、可追溯及灵活更新，降低人工复检成本。


<details>
  <summary>Details</summary>
Motivation: 在在线质量保证过程中，实时仿真模型生成的大量质检数据需要可追溯、可验证且安全存储，避免手工复检繁琐且易出错，迫切需要一种统一、可靠的数字化溯源方案。

Method: 采用非同质化代币（NFT）和自定义智能合约生成代币，将元数据存入IPFS，NFT 作为链上引用，支持后续加工步骤动态更新数据，形成可扩展的链上质检模块。

Result: 构建的系统实现了质检数据在整个价值链中的自动化追踪，显著减少了重复人工检查的时间与成本，并提升了数据的可信度与可交换性。

Conclusion: 本文通过使用 NFT、智能合约与 IPFS，将质检数据以 Asset Administration Shell 的形式安全存储在以太坊区块链上，实现了对金属加工件质检数据的安全可追溯存储与传输，降低了人工复检成本。

Abstract: This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.

</details>


### [21] [5Gone: Uplink Overshadowing Attacks in 5G-SA](https://arxiv.org/abs/2602.10272)
*Simon Erni,Martin Kotuliak,Marc Roeschlin,Richard Baker,Srdjan Capkun*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: 5G presents numerous advantages compared to previous generations: improved throughput, lower latency, and improved privacy protection for subscribers. Attacks against 5G standalone (SA) commonly use fake base stations (FBS), which need to operate at a very high output power level to lure victim phones to connect to them and are thus highly detectable. In this paper, we introduce 5Gone, a powerful software-defined radio (SDR)-based uplink overshadowing attack method against 5G-SA. 5Gone exploits deficiencies in the 3GPP standard to perform surgical, covert denial-of-service, privacy, and downgrade attacks. Uplink overshadowing means that an attacker is transmitting at exactly the same time and frequency as the victim UE, but with a slightly higher output power. 5Gone runs on a COTS x86 computer without any need for dedicated hardware acceleration and can overshadow commercial 100 MHz cells with an E2E latency of less than 500$μ$s, which up to now has not been possible with any software-based UE implementation. We demonstrate that 5Gone is highly scalable, even when many UEs are connecting in parallel, and finally evaluate the attacks end-to-end against 7 phone models and three different chipset vendors both in our lab and in the real-world on public gNodeBs.

</details>


### [22] [SecCodePRM: A Process Reward Model for Code Security](https://arxiv.org/abs/2602.10418)
*Weichen Yu,Ravi Mangal,Yinyi Luo,Kai Hu,Jingxuan He,Corina S. Pasareanu,Matt Fredrikson*

Main category: cs.CR

TL;DR: SecCodePRM是一个基于步级奖励的安全评估模型，使用静态分析器和专家标注训练，能够在实时交互式编码中提供细粒度的安全评分，提升全代码和局部代码漏洞检测以及安全代码生成的性能，同时保持功能正确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件开发中日益重要，但传统的漏洞检测手段需完整上下文、只能提供稀疏的终端反馈，且当代码长度增加时效果下降；缺乏能够在交互式或流式生成过程中进行前缀级别、实时安全评估的工具。

Method: 利用静态分析器和专家标注生成逐步监督标签，对代码路径逐步评估安全风险；通过风险敏感聚合和排名策略，将结果用于全代码、局部代码的漏洞检测以及生成阶段候选序列的奖励排序；支持在推理时对候选继续序列进行缩放和排序，以实现实时反馈。

Result: 在全代码漏洞检测、部分代码检测和安全代码生成三项任务上，SecCodePRM均表现出比现有静态分析或基于LLM/GNN的模型更高的检测率和准确率，并保持了代码功能完整性；实验表明模型在长程生成时仍能提供密集、实时的安全反馈。

Conclusion: SecCodePRM提供了一种上下文感知、逐步安全评分的过程奖励模型，能够在交互式编码和流式生成中实现实时、细粒度的安全评估，并在完整代码、部分代码漏洞检测以及安全代码生成三种应用场景中都优于现有方法；同时保持了代码功能正确性，表明安全性提升并未以牺牲效用为代价。

Abstract: Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.

</details>


### [23] [Authenticated Workflows: A Systems Approach to Protecting Agentic AI](https://arxiv.org/abs/2602.10465)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 本工作提出“Authenticated Workflows”，实现对创建、调用、数据、上下文四个边界的加密安全检测，并配合 MAPL 策略语言，取得 100% 真阳性、零误报、覆盖多项 OWASP 风险及 CVE 缓解的实验结果。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化企业工作流（agentic AI）在安全层面存在缺陷，现有的防御机制（如守门闸、语义过滤）为概率性方法，易被绕过。科研的核心需求是构建一层完整可信的安全防护，使得每一次工作流交互都能可验证、可被审计。

Method: 提出“Authenticated Workflows”，通过四个基本边界（prompt、tool、data、context）来实现安全。每一次边界交叉都需满足意图与完整性：意图要求操作符合法规政策，完整性要求操作具备加密签名。结合 MAPL（AI 原生策略语言）实现动态、层级化的策略编写，并通过加密证明协同验证工作流依赖。最后，构建通用安全运行时，利用轻量头部适配器，将九个主要框架（MCP、A2A、OpenAI、Claude、LangChain、CrewAI、AutoGen、LlamaIndex、Haystack）接入，且不需任何协议改动。

Result: 实验验证：在 174 个测试案例中，检索率 100% 并且没有误报；在 OWASP Top 10 风险中成功检验 9/10；对两起高影响生产 CVE 做到完整缓解。

Conclusion: 通过结合意图检测、完整性验证与 MAPL 语言，构建了可扩展、可验证、deterministic 的企业 Agentic AI 运行时，提供了全流程的可信保障。

Abstract: Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.

</details>


### [24] [GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks](https://arxiv.org/abs/2602.10478)
*Zihao Li,Hongyi Lu,Yanan Guo,Zhenkai Zhang,Shuai Wang,Fengwei Zhang*

Main category: cs.CR

TL;DR: GPU-Fuzz通过约束求解模糊测试发现DL框架GPU内存错误，已测试出13个漏洞。


<details>
  <summary>Details</summary>
Motivation: GPU内存错误对深度学习框架构成严重威胁，需有效发现并修复。

Method: 使用约束求解器将算子参数建模为形式约束，生成测试用例系统探测GPU内核边界条件。

Result: 在PyTorch、TensorFlow和PaddlePaddle上发现13个未知的内存错误。

Conclusion: GPU-Fuzz能高效定位GPU内存错误，证明了基于约束求解的模糊测试方法的有效性。

Abstract: GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.

</details>


### [25] [Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI](https://arxiv.org/abs/2602.10481)
*Mohan Rajagopalan,Vinay Rao*

Main category: cs.CR

TL;DR: 提出身份认证的提示与上下文技术，结合策略代数与多层防御，实现 100% 的攻击检测、零误报、低开销，为 LLM 提供了拜占庭级别的安全防护。


<details>
  <summary>Details</summary>
Motivation: LLM 应用易受提示注入和上下文操纵攻击，传统安全模型难以防御，需要新的防御机制。

Method: 提出两种新原语：身份认证提示（authenticated prompts）和身份认证上下文（authenticated context）。通过可验证的提示线性化和基于可篡改哈希链的动态输入完整性，构建四个定理证明的策略代数，实现拜占庭级别的政策抵抗，并提供轻量级资源控制、LLM语义校验等五类互补防御。

Result: 在六个攻击类别的实测中，检测率 100%，无误报，且性能开销轻微；首次实现结合加密提示线性、不可篡改上下文及可证明政策推理的防御方案。

Conclusion: 该工作将 LLM 安全从被动检测转向主动预防，提供了加密保证、完整性验证和可证明的政策执行。

Abstract: Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.

</details>


### [26] [Following Dragons: Code Review-Guided Fuzzing](https://arxiv.org/abs/2602.10487)
*Viet Hoang Luu,Amirmohammad Pasdar,Wachiraphan Charoenwet,Toby Murray,Shaanan Cohney,Van-Thuan Pham*

Main category: cs.CR

TL;DR: EyeQ通过提取代码评审中的安全信号并转化为注解，引导模糊测试聚焦关键程序状态，显著提升PHP代码库的漏洞发现率，且实现高度自动化。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试往往无法触及程序中最脆弱或安全关键的状态，而这些状态在代码评审时已被开发者暴露，但自动化测试工具常忽略这些信息。

Method: EyeQ从代码评审讨论中提取安全相关信号，定位相关程序区域，并将其转换为注解式语义引导，供已有的支持注解的模糊器使用。随后通过设计好的提示自动化该流程，利用大型语言模型完成信号抽取与语义映射。

Result: 在面向PHP安全评审数据集的人机可行性研究中，EyeQ形成了强大的基线。通过自动化流程，EyeQ在实际使用中发现了40多个之前未知的漏洞，显著提升了标准模糊配置的漏洞挖掘效果。

Conclusion: EyeQ证明了将代码评审中的智能洞察融入模糊测试可显著提升安全漏洞发现率，且无需改动程序语义或开发者工作流程，可与现有注解敏感模糊器无缝集成。

Abstract: Modern fuzzers scale to large, real-world software but often fail to exercise the program states developers consider most fragile or security-critical. Such states are typically deep in the execution space, gated by preconditions, or overshadowed by lower-value paths that consume limited fuzzing budgets. Meanwhile, developers routinely surface risk-relevant insights during code review, yet this information is largely ignored by automated testing tools. We present EyeQ, a system that leverages developer intelligence from code reviews to guide fuzzing. EyeQ extracts security-relevant signals from review discussions, localizes the implicated program regions, and translates these insights into annotation-based guidance for fuzzing. The approach operates atop existing annotation-aware fuzzing, requiring no changes to program semantics or developer workflows. We first validate EyeQ through a human-guided feasibility study on a security-focused dataset of PHP code reviews, establishing a strong baseline for review-guided fuzzing. We then automate the workflow using a large language model with carefully designed prompts. EyeQ significantly improves vulnerability discovery over standard fuzzing configurations, uncovering more than 40 previously unknown bugs in the security-critical PHP codebase.

</details>


### [27] [CryptoCatch: Cryptomining Hidden Nowhere](https://arxiv.org/abs/2602.10573)
*Ruisheng Shi,Ziding Lin,Haoran Sun,Qin Wang,Shihan Zhang,Lina Lan,Zhiyuan Peng,Chenfeng Wang*

Main category: cs.CR

TL;DR: 本文提出结合机器学习和主动探测的两阶段加密挖矿流量检测方法，显著提升检测精度并降低误报，实验验证显示在多矿池环境中有效。


<details>
  <summary>Details</summary>
Motivation: 传统黑名单和深度包检测在处理加密流量时效果有限，误报率高；迫切需要一种可靠且精准的检测手段。

Method: 采用两阶段检测框架：第一阶段使用机器学习模型进行细粒度检测，第二阶段通过主动探测验证分类结果以减少误报。

Result: 系统的F1-得分达到0.99，针对特定加密货币的识别准确率为99.39%，并在各类矿池环境中得到验证。

Conclusion: 提供了一种高精度的加密挖矿流量检测框架，能够显著降低误报率并实现精确识别。

Abstract: Cryptomining poses significant security risks, yet traditional detection methods like blacklists and Deep Packet Inspection (DPI) are often ineffective against encrypted mining traffic and suffer from high false positive rates. In this paper, we propose a practical encrypted cryptomining traffic detection mechanism. It consists of a two-stage detection framework, which can effectively provide fine-grained detection results by machine learning and reduce false positives from classifiers through active probing. Our system achieves an F1-score of 0.99 and identifies specific cryptocurrencies with a 99.39\% accuracy rate. Extensive testing across various mining pools confirms the effectiveness of our approach, offering a more precise and reliable solution for identifying cryptomining activities.

</details>


### [28] [Invisible Trails? An Identity Alignment Scheme based on Online Tracking](https://arxiv.org/abs/2602.10626)
*Ruisheng Shi,Zhiyuan Peng,Tong Fu,Lina Lan,Qin Wang,Jiaqi Zeng*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many tracking companies collect user data and sell it to data markets and advertisers. While they claim to protect user privacy by anonymizing the data, our research reveals that significant privacy risks persist even with anonymized data. Attackers can exploit this data to identify users' accounts on other websites and perform targeted identity alignment. In this paper, we propose an effective identity alignment scheme for accurately identifying targeted users. We develop a data collector to obtain the necessary datasets, an algorithm for identity alignment, and, based on this, construct two types of de-anonymization attacks: the \textit{passive attack}, which analyzes tracker data to align identities, and the \textit{active attack}, which induces users to interact online, leading to higher success rates. Furthermore, we introduce, for the first time, a novel evaluation framework for online tracking-based identity alignment. We investigate the key factors influencing the effectiveness of identity alignment. Additionally, we provide an independent assessment of our generated dataset and present a fully functional system prototype applied to a cryptocurrency use case.

</details>


### [29] [GoodVibe: Security-by-Vibe for LLM-Based Code Generation](https://arxiv.org/abs/2602.10778)
*Maximilian Thang,Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Jona te Lintelo,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoodVibe通过识别并微调少量安全相关神经元，低成本高效提升LLM代码生成的安全性，性能优于全微调且计算成本显著下降。


<details>
  <summary>Details</summary>
Motivation: 在vibe式开发中LLM生成的代码往往安全风险大，传统的全参数或粗粒度的参数高效微调既昂贵又缺陷，缺乏对安全问题的精细控制。

Method: 首先利用基于梯度的归因方法在有监督的安全任务中挑选与安全相关的神经元；接着对这些神经元进行仅更新的微调；最后采用激活驱动的神经元聚类，实现结构化、低负担的参数更新。

Result: 在六款LLM（C++、Java、Swift、Go等语言）上评测，GoodVibe相较基模型提升多达2.5倍，参量量比完整微调少万倍，训练计算量比LoRA降低3.6倍，且保持了模型的整体实用性。

Conclusion: GoodVibe通过在神经元级别选择对安全敏感的子空间进行微调，显著提升了大语言模型在代码生成中的安全性，同时保持了模型的通用性能，证明了神经元级优化是一种可扩展、低成本的安全提升策略。

Abstract: Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.
  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.

</details>


### [30] [Beyond Permissions: An Empirical Static Analysis of Privacy and Security Risks in Children-Oriented and General-Audience Mobile Apps for Gaming](https://arxiv.org/abs/2602.10877)
*Bakheet Aljedaani*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mobile gaming applications (apps) have become increasingly pervasive, including a growing number of games designed for children. Despite their popularity, these apps often integrate complex analytics, advertising, and attribution infrastructures that may introduce privacy and security risks. Existing research has primarily focused on tracking behaviors or monetization models, leaving configuration-level privacy exposure and children-oriented apps underexplored. In this study, we conducted a comparative static analysis of Android mobile games to investigate privacy and security risks beyond permission usage. The analysis follows a three-phase methodology comprising (i) designing study protocol, (ii) Android Package Kit (APK) collection and static inspection, and (iii) data analysis. We examined permissions, manifest-level configuration properties (e.g., backup settings, cleartext network traffic, and exported components), and embedded third-party Software Development Kit (SDK) ecosystems across children-oriented and general-audience mobile games. The extracted indicators are synthesized into qualitative privacy-risk categories to support comparative reporting. The results showed that while children-oriented games often request fewer permissions, they frequently exhibit configuration-level risks and embed third-party tracking SDKs similar to general-audience games. Architectural and configuration decisions play a critical role in shaping privacy risks, particularly for apps targeting children. This study contributes a holistic static assessment of privacy exposure in mobile games and provides actionable insights for developers, platform providers, and researchers seeking to improve privacy-by-design practices in mobile applications.

</details>


### [31] [Resilient Alerting Protocols for Blockchains](https://arxiv.org/abs/2602.10892)
*Marwa Moullem,Lorenz Breidenbach,Ittay Eyal,Ari Juels*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Smart contracts are stateful programs deployed on blockchains; they secure over a trillion dollars in transaction value per year. High-stakes smart contracts often rely on timely alerts about external events, but prior work has not analyzed their resilience to an attacker suppressing alerts via bribery. We formalize this challenge in a cryptoeconomic setting as the \emph{alerting problem}, giving rise to a game between a bribing adversary and~$n$ rational participants, who pay a penalty if they are caught deviating from the protocol. We establish a quadratic, i.e.,~$O(n^2)$, upper bound, whereas a straightforward alerting protocol only achieves~$O(n)$ bribery cost.
  We present a \emph{simultaneous game} that asymptotically achieves the quadratic upper bound and thus asymptotically-optimal bribery resistance. We then present two protocols that implement our simultaneous game: The first leverages a strong network synchrony assumption. The second relaxes this strong assumption and instead takes advantage of trusted hardware and blockchain proof-of-publication to establish a timed commitment scheme. These two protocols are constant-time but incur a linear storage overhead on the blockchain. We analyze a third, \emph{sequential alerting} protocol that optimistically incurs no on-chain storage overhead, at the expense of~$O(n)$ worst-case execution time. All three protocols achieve asymptotically-optimal bribery costs, but with different resource and performance tradeoffs. Together, they illuminate a rich design space for practical solutions to the alerting problem.

</details>


### [32] [CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data](https://arxiv.org/abs/2602.11015)
*Valery Khvatov,Alexey Neyman*

Main category: cs.CR

TL;DR: CVPL是一个几何框架，用于后验评估原始与保护数据之间的链路风险，提供连续风险度量，揭示k-匿名性下的大量真实链接潜力，并帮助评估不同隐私保护机制的效用与风险。


<details>
  <summary>Details</summary>
Motivation: 现有隐私度量虽提供合规保证，却难以定量评估发布数据中的实际链接性，需要一种能够在各种保护强度和攻击严格度下给出连续风险估计的框架。

Method: 将链接分析拆解为阻止、向量化、潜在投影和相似度评估的操作流水线，定义阈值感知的风险曲面R(λ,τ)，并通过单调递进的阻止策略实现任意时刻有效下界估计。

Result: 实验在1万条记录和19种保护配置中显示，尽管满足k-匿名性，仍存在大量实证可链接性，主要源于非量化标识的行为模式；CVPL能够可视化哪些特征驱动链接可行性。

Conclusion: CVPL提供了连续、情景依赖的链路风险估计，弥补了传统二元合规评估的不足，并揭示正式k-匿名性可能伴随显著实际链接风险。

Abstract: Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.

</details>


### [33] [Mask-Based Window-Level Insider Threat Detection for Campaign Discovery](https://arxiv.org/abs/2602.11019)
*Jericho Cain,Hayden Beadles*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: User and Entity Behavior Analytics (UEBA) systems commonly detect insider threats by scoring fixed time windows of user activity for anomalous behavior. While this window-level paradigm has proven effective for identifying sharp behavioral deviations, it remains unclear how much information about longer-running attack campaigns is already present within individual windows, and how such information can be leveraged for campaign discovery. In this work, we study unsupervised window-level insider threat detection on the CERT r4.2 dataset and show that explicitly separating activity presence from activity magnitude yields substantial performance gains. We introduce a dual-channel convolutional autoencoder that reconstructs both a binary activity mask and corresponding activity values, allowing the model to focus representational capacity on sparse behavioral structure rather than dense inactive baselines. Across multiday attack campaigns lasting between one and seven days, the proposed approach achieves a window-level precision-recall AUC of 0.71, substantially exceeding standard unsupervised autoencoder baselines and enabling high-precision operating points with zero false alarms.

</details>


### [34] [IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing](https://arxiv.org/abs/2602.11023)
*Shaoyu Li,Hexuan Yu,Shanghao Shi,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: 提出IU-GUARD框架：利用VC与ZKP，在不泄露身份的前提下实现主角用户认证，低开销、实时可行。


<details>
  <summary>Details</summary>
Motivation: 现有的环境感知与信息提供机制均存在昂贵部署、易受干扰或隐私泄露等限制，亟需一种既能保护主角用户隐私又能实现有效频谱共享的新框架。

Method: 采用可验证凭证(VC)与零知识证明(ZKP)相结合，主角用户在与频谱协调系统(SCS)交互时仅披露必要的操作参数，从而解耦身份与频谱访问。

Result: 原型实验表明，IU-GUARD在提供强隐私保证的同时，计算与通信开销在可接受范围内，适用于实时动态频谱共享部署。

Conclusion: 通过使用可验证凭证和零知识证明，IU-GUARD在不暴露主要用户身份的前提下实现授权验证，显著提升了动态频谱共享过程中的隐私保护。

Abstract: With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.

</details>


### [35] [Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise](https://arxiv.org/abs/2602.11088)
*Abhishek Saini,Haolin Jiang,Hang Liu*

Main category: cs.CR

TL;DR: TEE 静态安全基导致 LLM 安全缺陷，攻击者可在几分钟内破解 8B 模型，并可扩展至 405B 模型。


<details>
  <summary>Details</summary>
Motivation: 在第三方设备上部署大型语言模型（LLMs）时，需要新的方法来保护模型的知识产权。尽管受信任执行环境（TEE）提供了一种有前景的解决方案，但其性能限制可能导致关键妥协：使用预计算的静态安全基来加速密码操作。我们证明这种主流设计模式向系统协议引入了经典密码学缺陷——秘密密钥材料的重复使用。

Method: 通过两种不同攻击检验其脆弱性：1）对模型机密性系统的攻击，实现完整的机密性破坏，恢复其秘密置换和模型权重；2）对像Soter和TSQP等系统的完整性攻击，完全绕过完整性检查。我们用 LLaMA-3 8B 模型演示了实践性，约 6 分钟即可恢复一个层的秘密，并展示该攻击可扩展到 405B 参数的 LLM，跨多种配置。

Result: 成功实现完整机密性突破与完整性 bypass；在 6 分钟内从 8B LLaMA-3 模型恢复层级秘密；攻击可扩展至 405B 参数模型，影响多种配置。

Conclusion: 使用预计算静态安全基的 TEE 设计模式在 LLM 部署中存在严重安全缺陷，攻击者能快速恢复模型秘密，危及知识产权与完整性。

Abstract: The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [36] [Efficient Policy Adaptation for Voltage Control Under Unknown Topology Changes](https://arxiv.org/abs/2602.10355)
*Jie Feng,Yuanyuan Shi,Deepjyoti Deka*

Main category: eess.SY

TL;DR: Fast topology‑aware sensitivity updates enable online RL policy adaptation, improving voltage control after topology changes.


<details>
  <summary>Details</summary>
Motivation: RL voltage‑control policies degrade when system topology or load changes; fast adaptation is needed.

Method: Estimate voltage–reactive‑power sensitivities via data‑driven sparse detection of topology changes, then perform online‑policy optimization on a pre‑trained neural‑network RL controller.

Result: On IEEE 13‑bus and SCE 56‑bus systems, the method identifies >90% of switched lines with 15 data points, and yields superior voltage regulation compared to baseline approaches.

Conclusion: A topology‑aware online policy‑optimization framework achieves rapid and accurate voltage regulation, outperforming non‑adaptive and regression‑based adaptive controllers.

Abstract: Reinforcement learning (RL) has shown great potential for designing voltage control policies, but their performance often degrades under changing system conditions such as topology reconfigurations and load variations. We introduce a topology-aware online policy optimization framework that leverages data-driven estimation of voltage-reactive power sensitivities to achieve efficient policy adaptation. Exploiting the sparsity of topology-switching events, where only a few lines change at a time, our method efficiently detects topology changes and identifies the affected lines and parameters, enabling fast and accurate sensitivity updates without recomputing the full sensitivity matrix. The estimated sensitivity is subsequently used for online policy optimization of a pre-trained neural-network-based RL controller. Simulations on both the IEEE 13-bus and SCE 56-bus systems demonstrate over 90 percent line identification accuracy, using only 15 data points. The proposed method also significantly improves voltage regulation performance compared with non-adaptive policies and adaptive policies that rely on regression-based online optimization methods for sensitivity estimation.

</details>


### [37] [Resilient Voltage Estimation for Battery Packs Using Self-Learning Koopman Operator](https://arxiv.org/abs/2602.10397)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TL;DR: 云端BMS受传感器攻击导致的电压测量错误会破坏EV充电。本文提出二阶段自学习Koopman运算符校正估计，结合经验策略与高斯过程回归，验证在多种环境下可靠估计电压，支持安全充电。


<details>
  <summary>Details</summary>
Motivation: 保障云端电池管理系统在面对传感器攻击时的电压测量数据可靠性，避免电动车充电中断；

Method: 采用两阶段自学习 Koopman 运算符的误差校正安全电压估计：第一阶段补偿 Koopman 近似误差；第二阶段通过两种方法恢复因缺少高阶电池动态信息导致的误差——（1）基于单体开路电压与荷电状态映射的自适应经验策略；（2）利用最小数据训练的高斯过程回归数据驱动方法。

Result: 在 PyBaMM‑liionpack 高保真电池仿真中，所提估计器在不同包拓扑、充电设置、电池老化程度以及攻击策略下均能实时产生高精度电压估计，验证了其可扩展性与适应性。

Conclusion: 本算法可在多种电池配置与运行状态下广泛部署，无需大量修改、额外数据或传感器冗余，能在传感器受攻击时保障电动车最佳充电。

Abstract: Cloud-based battery management systems (BMSs) rely on real-time voltage measurement data to ensure coordinated bi-directional charging of electric vehicles (EVs) with vehicle-to-grid technology. Unfortunately, an adversary can corrupt the measurement data during transmission from the local-BMS to the cloud-BMS, leading to disrupted EV charging. Therefore, to ensure reliable voltage data under such sensor attacks, this paper proposes a two-stage error-corrected self-learning Koopman operator-based secure voltage estimation scheme for large-format battery packs. The first stage of correction compensates for the Koopman approximation error. The second stage aims to recover the error amassing from the lack of higher-order battery dynamics information in the self-learning feedback, using two alternative methods: an adaptable empirical strategy that uses cell-level knowledge of open circuit voltage to state-of-charge mapping for pack-level estimation, and a Gaussian process regression-based data-driven method that leverages minimal data-training. During our comprehensive case studies using the high-fidelity battery simulation package 'PyBaMM-liionpack', our proposed secure estimator reliably generated real-time voltage estimation with high accuracy under varying pack topologies, charging settings, battery age-levels, and attack policies. Thus, the scalable and adaptable algorithm can be easily employed to diverse battery configurations and operating conditions, without requiring significant modifications, excessive data or sensor redundancy, to ensure optimum charging of EVs under compromised sensing.

</details>


### [38] [Rapid Boundary Stabilization of Two-Dimensional Elastic Plates with In-Domain Aeroelastic Instabilities](https://arxiv.org/abs/2602.10567)
*Xingzhi Huang,Ji Wang*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Motivated by active wing flutter suppression in high-Mach-number flight, this paper presents a rapid boundary stabilization strategy for a two-dimensional PDE-modeled elastic plate with in-domain instabilities, where the exponential stability is achieved with a decay rate that can be arbitrarily assigned by the users. First, the aeroelastic system is modeled as two-dimensional coupled wave PDEs with internal anti-damping terms, derived by Piston theory and Hamilton's principle. Using Fourier series expansion, the 2-D problem is decomposed into a parameterized family of 1-D systems. For each mode, a full-state boundary feedback controller is designed via PDE backstepping transformation. To enable output-feedback implementation, a state observer is further designed to estimate the distributed states over the two-dimensional spatial domain. Through Lyapunov analysis, the exponential stability of the 2-D elastic plate PDE under the proposed boundary control is established with a designer-tunable decay rate. Numerical simulations verify the effectiveness of the control strategy in suppressing flow-induced vibrations.

</details>


### [39] [Integrating Active Damping with Shaping-Filtered Reset Tracking Control for Piezo-Actuated Nanopositioning](https://arxiv.org/abs/2602.10724)
*Aditya Natu,Xiaozhe Hu,Hassan HosseinNia*

Main category: eess.SY

TL;DR: 重置双环控制+波形滤波器提升压电纳米定位器带宽≈34 Hz，跨越点≈55 Hz，验证了在阻尼谐振挑战下的性能突破。


<details>
  <summary>Details</summary>
Motivation: 传统线性闭环控制受限于结构轻度阻尼谐振与增益-相位约束，导致腔压级精度定位器的带宽和跟踪性能受限，亟需突破这一瓶颈。

Method: 提出双环架构：内部环使用非最小相位谐振控制器（NRC）主动阻尼，外部环采用常数增益、前馈相位（CgLp）重置元件实现跨越点相位前置且不提升环增益，并在重置触发路径加入波形滤波器调节重置行为，抑制高阶谐波。

Result: 在工业压电纳米定位器上实现实时控制，实验显示开环跨越点提升约55 Hz，闭环带宽提升约34 Hz，相较于精调线性基线。

Conclusion: 双环重置 + 调节滤波方案在维持相位补偿的同时，有效抑制谐波引起的误差敏感性，显著提高了压电定位器的动态性能，可推广至其他受阻尼谐振限制的高精度定位系统。

Abstract: Piezoelectric nanopositioning systems are often limited by lightly damped structural resonances and the gain--phase constraints of linear feedback, which restrict achievable bandwidth and tracking performance. This paper presents a dual-loop architecture that combines an inner-loop non-minimum-phase resonant controller (NRC) for active damping with an outer-loop tracking controller augmented by a constant-gain, lead-in-phase (CgLp) reset element to provide phase lead at the targeted crossover without increasing loop gain. We show that aggressively tuned CgLp designs with larger phase lead can introduce pronounced higher-order harmonics, degrading error sensitivity in specific frequency bands and causing multiple-reset behavior. To address this, a shaping filter is introduced in the reset-trigger path to regulate the reset action and suppress harmonic-induced effects while preserving the desired crossover-phase recovery. The proposed controllers are implemented in real time on an industrial piezo nanopositioner, demonstrating an experimental open-loop crossover increase of approximately 55~Hz and a closed-loop bandwidth improvement of about 34~Hz relative to a well-tuned linear baseline.

</details>


### [40] [Improving CACC Robustness to Parametric Uncertainty via Plant Equivalent Controller Realizations](https://arxiv.org/abs/2602.10752)
*Mischa Huisman,Thomas Arnold,Erjen Lefeber,Nathan van de Wouw,Carlos Murguia*

Main category: eess.SY

TL;DR: 改进进化CACC控制器，利用 PEC + LMI 优化在参数不确定性下显著提升鲁棒性与性能，实验验证成功。


<details>
  <summary>Details</summary>
Motivation: 传统 CACC 假设车辆参数精确，与实际车辆非线性动力学及参数不确定性不符，导致反馈线性化误差、模型失配，引起性能下降。

Method: 基于 \(\mathcal{L}_2\) 路径匹配的鲁棒性框架，采用 PEC 实现技术，结合线性矩阵不等式（LMI）对齐目标函数，形成凸优化问题以求解控制器参数。

Result: 实验验证显示，该方法在保持原有 CACC 结构的同时，能在存在参数不确定性的情况下提升鲁棒性与整体性能。

Conclusion: 通过显式建模闭环误差并利用 PEC 优化求解，可在保持原始 CACC 设计不变的前提下显著提升在参数不确定性下的鲁棒性和性能。

Abstract: Cooperative Adaptive Cruise Control (CACC) enables vehicle platooning through inter-vehicle communication, improving traffic efficiency and safety. Conventional CACC relies on feedback linearization, assuming exact vehicle parameters; however, longitudinal vehicle dynamics are nonlinear and subject to parametric uncertainty. Applying feedback linearization with a nominal model yields imperfect cancellation, leading to model mismatch and degraded performance with off-the-shelf CACC controllers. To improve robustness without redesigning the CACC law, we explicitly model the mismatch between the ideal closed-loop dynamics assumed by the CACC design and the actual dynamics under parametric uncertainties. Robustness is formulated as an $\mathcal{L}_2$ trajectory-matching problem, minimizing the energy of this mismatch to make the uncertain system behave as closely as possible to the ideal model. This objective is addressed by optimizing over plant equivalent controller (PEC) realizations that preserve the nominal closed-loop behavior while mitigating the effects of parametric uncertainty. Stability and performance are enforced via linear matrix inequalities, yielding a convex optimization problem applicable to heterogeneous platoons. Experimental results demonstrate improved robustness and performance under parametric uncertainty while preserving nominal CACC behavior.

</details>


### [41] [Singular Port-Hamiltonian Systems Beyond Passivity](https://arxiv.org/abs/2602.10855)
*Henrik Sandberg,Kamil Hassan,Heng Wu*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study a class of port-Hamiltonian systems whose vector fields exhibit singularities. A representative example of this class has recently been employed in the power electronics literature to implement a grid-forming controller. We show that, under certain conditions, these port-Hamiltonian systems, when interconnected with passive systems, converge to a prescribed non-equilibrium steady state. At first glance, the apparently passive nature of the port-Hamiltonian system seems incompatible with the active power injection required to sustain this non-equilibrium condition. However, we demonstrate that the discontinuity inherent in the vector field provides the additional energy needed to maintain this operating point, indicating that the system is not globally passive. Moreover, when the discontinuity is replaced by a continuous approximation, the resulting system becomes cyclo-dissipative while still capable of supplying the required power.

</details>


### [42] [Backstepping Control of PDEs on Domains with Graph-Monotone Boundaries](https://arxiv.org/abs/2602.10876)
*Mohamed Camil Belhadjoudja*

Main category: eess.SY

TL;DR: 本文证明，对于非对称、非平行六面体的偏微分方程（如钢琴形热方程），可以不使用域扩展，直接构造闭式反馈；这为更广泛的高维背向抛物控制提供了新的思路，并将在后续论文中进一步系统化。


<details>
  <summary>Details</summary>
Motivation: 现有的高维背向抛物方程控制多限于需要对称性或平行六面体域；域扩展方法存在实时模拟、闭式反馈难实现、以及输出反馈/自适应控制等局限，亟需更普适的方案。

Method: 在非平行六面体区间采用类似于平行六面体域方法的控制策略，利用图单调边界的几何特性，直接构造闭形式反馈，而非通过在扩展域上模拟 PDE 来获得控制输入。

Result: 以钢琴形域的热方程为例，证明域扩展方法不是必需的，采用上述图单调边界策略即可得到反馈控制，取得同等甚至更优效果。

Conclusion: 本文提出了一个在非平行六面体、无对称性边界（图单调边界）区域上实现背向抛物方程控制的新框架，避免了已知的域扩展技术的缺陷，并在钢琴形域的热方程实例中验证了其可行性；未来将进一步完善此框架。

Abstract: Despite the extensive body of work on backstepping for one-dimensional PDEs, results in higher dimensions remain comparatively limited. Most available methods either exploit particular symmetries of the PDE or address problems posed on parallelepiped domains. To the best of our knowledge, the only approach that enables the design of backstepping controllers on non-parallelepiped regions without symmetry assumptions is the domain extension technique. This method, however, presents several drawbacks. In particular, the control input at each time instant is obtained by simulating a PDE on an extended domain, from which the actual input on the original domain is approximated. By contrast, in the one-dimensional setting, once the time-independent backstepping gain kernel is known, the control input can be computed in closed form as a feedback depending solely on the state at that same instant. Moreover, problems such as output-feedback design or adaptive and robust control do not appear straightforward to address with the domain extension method, at least to the best of our knowledge. These considerations motivate the search, whenever possible, for alternatives that preserve the main advantages of one-dimensional backstepping. A motivating example for the domain extension method is the control of the heat equation on a piano-shaped domain, with actuation applied at the tail of the piano. In this extended abstract, we show through a simple calculation that the domain extension method is not required in this setting. Instead, a strategy akin to that used for parallelepiped domains can be adopted. This result constitutes a first instance of a broader framework for backstepping control of asymmetric PDEs posed on non-parallelepiped regions, which we refer to as domains with graph-monotone boundaries. The general framework is developed in a forthcoming paper.

</details>


### [43] [Anomaly Detection with Machine Learning Algorithms in Large-Scale Power Grids](https://arxiv.org/abs/2602.10888)
*Marc Gillioz,Guillaume Dubuis,Étienne Voutaz,Philippe Jacquod*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We apply several machine learning algorithms to the problem of anomaly detection in operational data for large-scale, high-voltage electric power grids. We observe important differences in the performance of the algorithms. Neural networks typically outperform classical algorithms such as k-nearest neighbors and support vector machines, which we explain by the strong contextual nature of the anomalies. We show that unsupervised learning algorithm work remarkably well and that their predictions are robust against simultaneous, concurring anomalies.

</details>


### [44] [Trajectory-based data-driven predictive control and the state-space predictor](https://arxiv.org/abs/2602.10936)
*Levi D. Reyes Premer,Arash J. Khabbazi,Kevin J. Kircher*

Main category: eess.SY

TL;DR: 本文提出的轨迹预测控制统合法多种数据驱动控制方法，并通过状态空间预测器在有限数据情景下逼近 H₂ 最优控制，提升了性能与理论基础。


<details>
  <summary>Details</summary>
Motivation: 在缺乏完整系统模型下，统一并改进多种数据驱动预测控制（DDPC）方法，尤其在训练数据有限的场景中实现高效控制。

Method: 我们提出轨迹预测控制（TPC）框架：通过将系统输出轨迹表示为最近输入/输出历史与计划输入轨迹的线性函数，并采用基于最近输入/输出历史的线性状态空间预测器，使TPC成为线性模型预测控制的特例，从而利用成熟的MPC理论。

Result: 数值实验表明，TPC 的性能接近具有完备模型的 H₂ 最优控制，并且在训练样本较少时，状态空间预测器由于参数更少，优于其他预测器。

Conclusion: TPC 提供了一个统一、理论可靠且在小数据集上表现出色的 DDPC 框架，具有广泛的应用前景。

Abstract: We define trajectory predictive control (TPC) as a family of output-feedback indirect data-driven predictive control (DDPC) methods that represent the output trajectory of a discrete-time system as a linear function of the recent input/output history and the planned input trajectory. This paper shows that for different choices of the trajectory predictor, TPC encompasses a wide variety of DDPC methods, including subspace predictive control (SPC), closed-loop SPC, $γ$-DDPC, causal-$γ$-DDPC, transient predictive control, and others. This paper introduces a trajectory predictor that corresponds to a linear state-space model with the recent input/output history as the state. With this state-space predictor, TPC is a special case of linear model predictive control and therefore inherits its mature theory. In numerical experiments, TPC performance approaches the limit of oracle $H_2$-optimal control with perfect knowledge of the underlying system model. For TPC with small training datasets, the state-space predictor outperforms other predictors because it has fewer parameters.

</details>


### [45] [Lie Group Variational Integrator for the Geometrically Exact Rod with Circular Cross-Section Incorporating Cross-Sectional Deformation](https://arxiv.org/abs/2602.10963)
*Srishti Siddharth,Vivek Natarajan,Ravi N. Banavar*

Main category: eess.SY

TL;DR: The paper presents a continuous Cosserat rod formulation with cross‑sectional deformation and a Lie‑group variational integrator that conserves key physical properties; simulations confirm its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To capture the intricate dynamics of a 3‑D Cosserat rod while accounting for planar cross‑sectional deformation and to provide a numerically robust integrator that respects geometric constraints.

Method: Derivation of continuous space–time equations for a three‑dimensional geometrically exact rod with planar cross‑sectional deformation, followed by the application of a Lie‑group variational integrator to construct a discrete model incorporating rotational motion and cross‑sectional deformation.

Result: The resulting discrete model conserves volume via a local dilatation factor, maintains rotational configuration, preserves energy with bounded error, and successfully reproduces the rod’s physics across a spectrum of initial conditions.

Conclusion: A continuous and discretized Cosserat rod model that faithfully preserves volume, rotational states, and energy with bounded error, demonstrating accurate physical replication under various initial conditions.

Abstract: In this paper, we derive the continuous space-time equations of motion of a three-dimensional geometrically exact rod, or the Cosserat rod, incorporating planar cross-sectional deformation. We then adopt the Lie group variational integrator technique to obtain a discrete model of the rod incorporating both rotational motion and cross-sectional deformation as well. The resulting discrete model possesses several desirable features: it ensures volume conservation of the discrete elements by considering cross-sectional deformation through a local dilatation factor, it demonstrates the beneficial properties associated with the variational integrator technique, such as the preservation of the rotational configuration, and energy conservation with a bounded error. An exhaustive set of numerical results under various initial conditions of the rod demonstrates the efficacy of the model in replicating the physics of the system.

</details>


### [46] [Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing](https://arxiv.org/abs/2602.11076)
*Kavan Fatehi,Mostafa Rahmani Ghourtani,Amir Sonee,Poonam Yadav,Alessandra M Russo,Hamed Ahmadi,Radu Calinescu*

Main category: eess.SY

TL;DR: AE-MAPPO 通过注意力增强的多智能体 PPO，提供零成本解释且在 6G RAN 中快速消除延迟峰值，实现高可靠性与大幅缩短排障时间。


<details>
  <summary>Details</summary>
Motivation: 6G RANs 必须满足严格的 SLA，突发延迟峰值难以用传统 DRL 或可解释 RL 诊断与解决，需要更透明、高效的控制方法。

Method: 构建 Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO)，在多智能体切片控制中引入六种专用注意力机制，并将其解释结果以零成本方式呈现；该框架按预测、反应与切片间优化三阶段，在 O-RAN 时延尺度内实现自适应控制。

Result: 在 URLLC 场景中，AE-MAPPO 在 18 ms 内解决延迟峰值，将延迟恢复到 0.98 ms，99.9999 % 的可靠性，并将故障排除时间缩短 93%，同时不影响 eMBB 与 mMTC 的连续性。

Conclusion: AE-MAPPO能在保持 SLA 合规性的同时提供可解释的决策支持，提升 6G RAN 切片控制的可信度和实时性。

Abstract: Sixth-generation (6G) radio access networks (RANs) must enforce strict service-level agreements (SLAs) for heterogeneous slices, yet sudden latency spikes remain difficult to diagnose and resolve with conventional deep reinforcement learning (DRL) or explainable RL (XRL). We propose \emph{Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO)}, which integrates six specialized attention mechanisms into multi-agent slice control and surfaces them as zero-cost, faithful explanations. The framework operates across O-RAN timescales with a three-phase strategy: predictive, reactive, and inter-slice optimization.
  A URLLC case study shows AE-MAPPO resolves a latency spike in $18$ms, restores latency to $0.98$ms with $99.9999\%$ reliability, and reduces troubleshooting time by $93\%$ while maintaining eMBB and mMTC continuity. These results confirm AE-MAPPO's ability to combine SLA compliance with inherent interpretability, enabling trustworthy and real-time automation for 6G RAN slicing.

</details>


### [47] [Credit-Based vs. Discount-Based Congestion Pricing: A Comparison Study](https://arxiv.org/abs/2602.11077)
*Chih-Yuan Chiu,Devansh Jalota,Marco Pavone*

Main category: eess.SY

TL;DR: 比较 CBCP 与 DBCP 的性能，证明在特定条件下 DBCP 更优。


<details>
  <summary>Details</summary>
Motivation: 解决拥堵收费导致的社会不公平问题，评估 Crédit 与折扣两种补贴机制的相对效益。

Method: 构建无原子拥堵博弈模型，证明纳什均衡存在，可通过凸规划计算，随后在实际案例中验证理论。

Result: 在满足可选条件时，折扣式拥堵收费在用户成本与收入之间取得更佳平衡；案例研究确认理论结论。

Conclusion: 本文比较了 CBCP 与 DBCP 对降低用户成本和增加收费收入的有效性，发现在线性条件下 DBCP 能更优地诱导均衡并最小化社会成本。

Abstract: Credit-based congestion pricing (CBCP) and discount-based congestion pricing (DBCP), which respectively allot travel credits and toll discounts to subsidize low-income users' access to tolled roads, have emerged as promising policies for alleviating the societal inequity concerns of congestion pricing. However, since real-world implementations of CBCP and DBCP are nascent, their relative merits remain unclear. In this work, we compare the efficacy of deploying CBCP and DBCP in reducing user costs and increasing toll revenues. We first formulate a non-atomic congestion game in which low-income users receive a travel credit or toll discount for accessing tolled lanes. We establish that, in our formulation, Nash equilibrium flows always exist and can be computed or well approximated via convex programming. Our main result establishes a set of practically relevant conditions under which DBCP provably outperforms CBCP in inducing equilibrium outcomes that minimize a given societal cost, which encodes user cost reduction and toll revenue maximization. Finally, we validate our theoretical contributions via a case study of the 101 Express Lanes Project, a CBCP program implemented in the San Francisco Bay Area.

</details>


### [48] [Multi-UAV Trajectory Optimization for Bearing-Only Localization in GPS Denied Environments](https://arxiv.org/abs/2602.11116)
*Alfonso Sciacchitano,Liraz Mudrik,Sean Kragelund,Isaac Kaminer*

Main category: eess.SY

TL;DR: 本研究提出一种基于估计感知的轨迹优化方法，使固定摄像头UAV在与船舶协同定位时，误差下降一倍，成本与复杂性降低，提升多UAV作战韧性。


<details>
  <summary>Details</summary>
Motivation: 精准定位海上目标在无GPS环境下仍具挑战；传统使用万向电光传感器的UAV复杂且成本高，易单点失效。

Method: 提出估计感知轨迹优化框架，利用固定非万向摄像头UAV与地面船舶协同，生成满足动力学与任务约束、可动态执行的轨迹。

Result: 估计感知轨迹相较启发式路径误差降低一倍以上；协同固定摄像头UAV可匹敌或优于单一万向系统，且复杂度、成本均显著降低。

Conclusion: 该框架提升了多UAV协同定位的可扩展性与韧性，可在GPS遮断环境下实现高精度目标定位。

Abstract: Accurate localization of maritime targets by unmanned aerial vehicles (UAVs) remains challenging in GPS-denied environments. UAVs equipped with gimballed electro-optical sensors are typically used to localize targets, however, reliance on these sensors increases mechanical complexity, cost, and susceptibility to single-point failures, limiting scalability and robustness in multi-UAV operations. This work presents a new trajectory optimization framework that enables cooperative target localization using UAVs with fixed, non-gimballed cameras operating in coordination with a surface vessel. This estimation-aware optimization generates dynamically feasible trajectories that explicitly account for mission constraints, platform dynamics, and out-of-frame events. Estimation-aware trajectories outperform heuristic paths by reducing localization error by more than a factor of two, motivating their use in cooperative operations. Results further demonstrate that coordinated UAVs with fixed, non-gimballed cameras achieve localization accuracy that meets or exceeds that of single gimballed systems, while substantially lowering system complexity and cost, enabling scalability, and enhancing mission resilience.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [49] [Scaling Routers with In-Package Optics and High-Bandwidth Memories](https://arxiv.org/abs/2602.10505)
*Isaac Keslassy,Ilay Yavlovich,Jose Yallouz,Tzu-Chien Hsueh,Yeshaiahu Fainman,Bill Lin*

Main category: cs.NI

TL;DR: 本文提出在单体封装内实现petabit/秒级路由器的方案，利用HBM、片上模块、光集成和空间拆分交换架构，在功耗受限下保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有路由器面临带宽与能耗双重瓶颈；需借助半导体行业的规模化技术（HBM、片上模块、光集成）突破单芯片集成化优势。

Method: 通过引入空间拆分的分并行交换机架构，将纤维流量分配给若干独立交换机；利用HBM共享内存实现交换机实现；设计并行帧交错算法，使H​BM按峰值速率循环访问。

Result: 评估表明，凭借空间拆分实现的粗粒度负载平衡与传统精细调度差距不大；HBM共享内存和帧交错方案在缓存访问性能方面达到峰值。

Conclusion: 本文提出了一种利用高带宽内存（HBM）、片上集成光学和片上模块技术，构建单一集成包内 petabit/秒级互联网路由器的架构，并证明了其在功耗约束下可实现高性能与可扩展性。

Abstract: This paper aims to apply two major scaling transformations from the computing packaging industry to internet routers: the heterogeneous integration of high-bandwidth memories (HBMs) and chiplets, as well as in-package optics. We propose a novel internet router architecture that employs these technologies to achieve a petabit/sec router within a single integrated package. At the top-level, we introduce a novel split-parallel switch architecture that spatially divides (without processing) the incoming fibers and distributes them across smaller independent switches without intermediate OEO conversions or fine-tuned per-packet load-balancing. This passive spatial division enables scaling at the cost of a coarser traffic load balancing. Yet, through extensive evaluations of backbone network traffic, we demonstrate that differences with fine-tuned approaches are small. In addition, we propose a novel HBM-based shared-memory architecture for the implementation of the smaller independent switches, and we introduce a novel parallel frame interleaving algorithm that packs traffic into frames so that HBM banks are accessed at peak HBM data rates in a cyclical interleaving manner. We further discuss why these new technologies represent a paradigm shift in the design of future internet routers. Finally, we emphasize that power consumption may constitute the primary bottleneck to scaling.

</details>


### [50] [SplitCom: Communication-efficient Split Federated Fine-tuning of LLMs via Temporal Compression](https://arxiv.org/abs/2602.10564)
*Tao Li,Yulin Tang,Yiyang Song,Cong Wu,Xihui Liu,Pan Li,Xianhao Chen*

Main category: cs.NI

TL;DR: SplitCom利用激活时序冗余和自适应阈值，减少98%以上的上行通信，减轻设备负担，并在U‑shape设置下保持大幅通信节省。


<details>
  <summary>Details</summary>
Motivation: 在保持用户隐私的同时，边缘设备受限的计算与内存资源限制了LLM的联邦微调，需要一种既节省通信又轻量的解决方案。

Method: 1) 分割联邦学习（SFL）将模型分为轻量客户端子模型和高效力服务器子模型；2) 采用视频压缩思想，仅在激活值显著偏离前期时上传激活；3) 通过Bang‑Bang控制或DDPG强化学习自适应阈值来平衡通信与学习；4) 采用降维技术减轻客户端内存压力；5) 对U‑shape架构扩展，保证服务器不访问客户端标签。

Result: 实验表明，标准配置下通信量可降至1.4%原值（降幅98.6%），U‑shape变种总通信量仅 4.2%（降幅95.8%），且模型性能基本保持不变。

Conclusion: SplitCom通过在连续训练周期中利用激活的时序冗余，显著降低联邦学习中的通信成本，而不显著影响模型性能。

Abstract: Federated fine-tuning of on-device large language models (LLMs) mitigates privacy concerns by preventing raw data sharing. However, the intensive computational and memory demands pose significant challenges for resource-constrained edge devices. To overcome these limitations, split federated learning (SFL) emerges as a promising solution that partitions the model into lightweight client-side and compute-intensive server-side sub-models, thus offloading the primary training workload to a powerful server. Nevertheless, high-dimensional activation exchanges in SFL lead to excessive communication overhead. To overcome this, we propose SplitCom, a communication-efficient SFL framework for LLMs that exploits temporal redundancy in activations across consecutive training epochs. Inspired by video compression, the core innovation of our framework lies in selective activation uploading only when a noticeable deviation from previous epochs occurs. To balance communication efficiency and learning performance, we introduce two adaptive threshold control schemes based on 1) bang-bang control or 2) deep deterministic policy gradient (DDPG)-based reinforcement learning. Moreover, we implement dimensionality reduction techniques to alleviate client-side memory requirements. Furthermore, we extend SplitCom to the U-shape architecture, ensuring the server never accesses clients' labels. Extensive simulations and laboratory experiments demonstrate that SplitCom reduces uplink communication costs by up to 98.6\,\% in its standard configuration and total communication costs by up to 95.8\,\% in its U-shape variant without noticeably compromising model performance.

</details>


### [51] [AI Infrastructure Sovereignty](https://arxiv.org/abs/2602.10900)
*Sergio Cruzes*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial intelligence has shifted from a software-centric discipline to an infrastructure-driven system. Large-scale training and inference increasingly depend on tightly coupled data centers, high-capacity optical networks, and energy systems operating close to physical and environmental limits. As a result, control over data and algorithms alone is no longer sufficient to achieve meaningful AI sovereignty. Practical sovereignty now depends on who can deploy, operate, and adapt AI infrastructure under constraints imposed by energy availability, sustainability targets, and network reach. This tutorial-survey introduces the concept of AI infrastructure sovereignty, defined as the ability of a region, operator, or nation to exercise operational control over AI systems within physical and environmental limits. The paper argues that sovereignty emerges from the co-design of three layers: AI-oriented data centers, optical transport networks, and automation frameworks that provide real-time visibility and control. We analyze how AI workloads reshape data center design, driving extreme power densities, advanced cooling requirements, and tighter coupling to local energy systems, with sustainability metrics such as carbon intensity and water usage acting as hard deployment boundaries. We then examine optical networks as the backbone of distributed AI, showing how latency, capacity, failure domains, and jurisdictional control define practical sovereignty limits. Building on this foundation, the paper positions telemetry, agentic AI, and digital twins as enablers of operational sovereignty through validated, closed-loop control across compute, network, and energy domains. The tutorial concludes with a reference architecture for sovereign AI infrastructure that integrates telemetry pipelines, agent-based control, and digital twins, framing sustainability as a first-order design constraint.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [52] [Predictive-State Communication: Innovation Coding and Reconciliation under Delay](https://arxiv.org/abs/2602.10542)
*Ozgur Ercetin,Mohaned Chraiti*

Main category: cs.IT

TL;DR: 在通信双方具备强大预测模型时，引入共享预测状态并通过创新信息进行纠正，形成新的预测状态通信协议。该协议通过交叉熵计量替代熵率，并产生受限的感知-容量带，可视化其对预测质量的依赖。


<details>
  <summary>Details</summary>
Motivation: 本文提出在通信双方均拥有强大预测模型的场景下，传统的可靠符号传输不再是唯一操作模式，提出需要共享预测状态的思路，以适应大型语言模型等现代生成性预测先验。

Method: 设计一种预测状态通信（PSC）协议，双方维护共享预测状态，物理信道主要用于传输创新信息，从而纠正接收方的临时轨迹与发送方实际轨迹的偏差。通过交叉熵跨模型失配进行计量，并引入同时满足容量、延迟及感知连续性约束的可行性集合。

Result: 在示例中可视化了可行性区域及其随预测质量变化的表现，得到一个受限的感知-容量带而非单侧阈值。

Conclusion: PSC实现了跨熵计量与模型失配约束的联合，可为现代通信系统提供更灵活的操作平台，并揭示容量与延迟与感知连续性的相互作用。

Abstract: Shannon theory models communication as the reliable transfer of symbol sequences, with performance governed by capacity and rate-distortion limits. When both endpoints possess strong predictors -- as in modern large language models and related generative priors -- literal symbol transport is no longer the only operational regime. We propose predictive-state communication (PSC), in which the transmitter and receiver maintain an explicit shared predictive state, and the physical channel is used primarily to convey innovations, i.e., corrective information that reconciles the receiver's provisional trajectory with the transmitter's realized trajectory. This viewpoint replaces entropy-rate accounting by cross-entropy accounting under model mismatch, and it introduces feasibility constraints that depend jointly on capacity, delay, and perceptual continuity requirements; the resulting operating set is typically a bounded perception-capacity band rather than a one-sided threshold. We outline the protocol and architectural implications (state identifiers, anchors, bounded rollback, and patch-based updates) and provide a stylized illustrative example to visualize the induced feasibility region and its dependence on predictive quality.

</details>


### [53] [MacWilliams identities for the generalized rank weights](https://arxiv.org/abs/2602.10929)
*Julien Molina*

Main category: cs.IT

TL;DR: 研究了一般化秩权分布，建立其与对偶码间的等价关系，推导枚举多项式，并计算MRD码的权分布。


<details>
  <summary>Details</summary>
Motivation: 深入理解线性码的秩权分布特性，有助于评估码的纠错能力及对偶关系，为码设计与分析提供理论工具。

Method: 通过构造一般化秩权分布，利用对偶关系推导等价式，进而推导枚举多项式表达式，并在MRD码中进行特殊化计算。

Result: 提出并证明了MacWilliams式；给出了枚举多项式的显式公式；并给出了最大秩距离码（MRD码）的具体权分布。

Conclusion: 后文得出了一般化秩权分布与其对偶代码的分布之间的马可维斯型等价关系，并给出枚举多项式的计算公式，最终对MRD码的分布做了明确计算。

Abstract: We study the generalized rank weight distribution of a linear code. First, we provide a MacWilliams-type identity which relates the distributions of a code and its dual. Then, we give a formula for the enumerator polynomial. Finally, we explicitly compute the distribution of an MRD code.

</details>


### [54] [Enormous Fluid Antenna Systems (E-FAS) for Multiuser MIMO: Channel Modeling and Analysis](https://arxiv.org/abs/2602.11099)
*Farshad Rostami Ghadi,Kai-Kit Wong,Masoud Kaveh,Wee Kiat New,Chan-Byoung Chae,Lajos Hanzo*

Main category: cs.IT

TL;DR: E‑FAS通过在墙面/天花板上托载圆柱面波，实现更高编码增益且不牺牲多样性，单用户/多用户信道容量均可提升，并给出闭式性能评价方法。


<details>
  <summary>Details</summary>
Motivation: 利用大型可重定位流体天线系统（E‑FAS）将多功能EM接口实现墙壁/天花板导波与空间波射频共存，探索其在无线链路性能上的潜力与优势。

Method: 构建物理一致的端到端信道模型，结合表面阻抗波与基站-表面、发射器-用户段的短程衰落；在单用户线性前向与多用户ZF前向中，获得闭式或可近似的出局概率、容量及信噪比分布。

Result: 证明有效BS-用户信道仍为循环对称复高斯，平均功率提升可视为表面波衰减与接点损耗；在高SNR下单用户显著获益；多用户ZF下SINR分布可近似，获得可行和的总速率表达式，展示E‑FAS宏增益与BS空间自由度的交互。

Conclusion: E‑FAS在保持由小尺度衰落决定的多样性阶数的同时，通过圆柱面波传播显著提升编码增益，可显著提升单/多用户下的容量与可靠性。

Abstract: Enormous fluid antenna systems (E-FAS), the system concept that utilizes position reconfigurability in the large scale, have emerged as a new architectural paradigm where intelligent surfaces are repurposed from passive smart reflectors into multi-functional electromagnetic (EM) interfaces that can route guided surface waves over walls, ceilings, and building facades, as well as emit space waves to target receivers. This expanded functionality introduces a new mode of signal propagation, enabling new forms of wireless communication. In this paper, we provide an analytical performance characterization of an E-FAS-enabled wireless link. We first develop a physics-consistent end-to-end channel model that couples a surface-impedance wave formulation with small-scale fading on both the base station (BS)-surface and launcher-user segments. We illustrate that the resulting effective BS-user channel remains circularly symmetric complex Gaussian, with an enhanced average power that explicitly captures surface-wave attenuation and junction losses. For single-user cases with linear precoding, we derive the outage probability and ergodic capacity in closed forms, together with high signal-to-noise ratio (SNR) asymptotics that quantify the gain of E-FAS over purely space-wave propagation. For the multiuser case with zero-forcing (ZF) precoding, we derive the distribution of the signal-to-interference-plus-noise ratio (SINR) and obtain tractable approximations for the ergodic sum-rate, explicitly revealing how the E-FAS macro-gain interacts with the BS spatial degrees of freedom (DoF). In summary, our analysis shows that E-FAS preserves the diversity order dictated by small-scale fading while improving the coding gain enabled by cylindrical surface-wave propagation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: 本文验证了大型语言模型可通过入院记录文本预测卒后功能结果，表现与传统基于结构化数据的模型相当，提示可开发无须人工抽象的文本预测工具。


<details>
  <summary>Details</summary>
Motivation: 现有mRS预测多依赖结构化变量与传统机器学习，尚未充分挖掘大型语言模型从临床自然语言文本中直接推断未来功能结果的潜力。

Method: 对NYU Langone Stroke Registry中的9,485份入住物理记录和1,898份90天记录进行实验，评估BERT、NYUTron等编码模型以及Llama‑3.1‑8B、MedGemma‑4B等生成模型的冻结和微调版本；使用7级mRS精确率与二进制功能结果准确率进行性能评估，并与包含NIHSS与年龄的结构化基线模型进行比较。

Result: 微调后Llama在90天精确预判mRS准确率为33.9%（95%CI 27.9-39.9%），二进制准确率76.3%（95%CI 70.7-81.9%）；出院时准确率42.0%（95%CI 39.0-45.0%）与75.0%（95%CI 72.4-77.6%）的二进制准确率。其80天预测性能与结构化基线相当。

Conclusion: Llama等大型语言模型在仅依赖入院记录的基础上，可实现与传统结构化变量模型相当的卒中功能预后预测，证明了基于文本的预测工具可顺利集成入临床工作流程。

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [56] [Towards Autonomous Mathematics Research](https://arxiv.org/abs/2602.10177)
*Tony Feng,Trieu H. Trinh,Garrett Bingham,Dawsen Hwang,Yuri Chervonyi,Junehyuk Jung,Joonkyung Lee,Carlo Pagano,Sang-hyun Kim,Federico Pasqualotto,Sergei Gukov,Jonathan N. Lee,Junsu Kim,Kaiying Hou,Golnaz Ghiasi,Yi Tay,YaGuang Li,Chenkai Kuang,Yuan Liu,Hanzhao,Lin,Evan Zheran Liu,Nigamaa Nayakanti,Xiaomeng Yang,Heng-tze Cheng,Demis Hassabis,Koray Kavukcuoglu,Quoc V. Le,Thang Luong*

Main category: cs.LG

TL;DR: Aletheia是一个自迭代的自然语言数学研究AI，已在从奥赛到博士级别的各类问题中表现出高自治和创新，产生了全自动论文、协作论文与大型问题评估。


<details>
  <summary>Details</summary>
Motivation: 从国际数学竞赛水平到专业研究，需要跨越庞大文献与长期证明的障碍，构建可在研究层面独立工作的AI助手。

Method: 利用Aletheia——一个基于Gemini Deep Think的自然语言推理模型，结合工具使用和推理时间扩展法，对数学问题进行迭代生成、验证、修订。

Result: 在奥赛题目到博士级难题的多维实验中，Aletheia实现：1) 完全由AI生成的Feng26论文，计算算术几何中结构常数“eigenweights”；2) AI与人类合作完成LeeSeo26论文，证明独立集的上界；3) 对Bloom’s Erdős猜想数据库700个开放问题进行半自治评估，自动求解四个未决问题。

Conclusion: 我们展示了AI在数学研究中的自治和创新能力，并呼吁建立标准量化指标以促进人机协作。

Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>


### [57] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: 提出Sig‑MMD和CSig‑MMD，解决传统评估框架缺陷，提升尾部事件预测评价，并促进更稳健的多步概率预测算法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，现有评估框架缺乏统一指标，并假定不同时间步或变量独立，且对尾部事件灵敏度不足。

Method: 提出基于签名核的Sig‑MMD和新的CSig‑MMD度量，利用签名核捕捉多变量间的时序相关并支持缺失数据处理；CSig‑MMD通过裁剪方案强调对尾部事件的预测能力，并保证得分规则的适度性。

Result: 这两种新指标能更可靠地评估多步直接预测，并促进更鲁棒的概率预测算法的开发。

Conclusion: Sig‑MMD与CSig‑MMD通过完善评估方法，提高可解释性与稳健性，为高风险决策提供更有效的概率预测评估手段。

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [58] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 提出 MVN-Grad，一种先通过梯度方差归一化再添加动量的 Adam 风格优化器。该方法消除了动量与归一化间的耦合，降低更新方差、提升稳健性，并在低方差场景避免符号崩塌，最终在 CIFAR-100 与 GPT 语言建模任务中匹配或超越 Adam 等主流优化器，且无额外开销。


<details>
  <summary>Details</summary>
Motivation: Adam 等基于动量的优化器受到随机归一化器与旧动量耦合的影响，导致训练不稳定、易受离群梯度影响，缺乏对低方差环境下的符号崩塌的处理。

Method: 采用指数移动平均估计梯度的不确定性，对每个坐标进行方差归一化；随后在归一化后的梯度上应用动量，从而实现动量-归一化的先后顺序，消除传统 Adam 中存在的动量与归一化的耦合。

Result: 在 Cifar-100 图像分类和 GPT 语言建模基准上，MVN-Grad 与 Adam、AdaBelief、LaProp 相比能够实现更平滑的训练曲线、更优的泛化性能，且不增加任何额外计算开销。

Conclusion: MVN-Grad 通过先对梯度进行基于方差的归一化，再施加动量，彻底解耦了动量与归一化之间的交互，使更新方差更小、对单个梯度尖峰的反应受限，并避免高方差环境下的符号崩塌，最终实现更稳健、更快的收敛，并在图像分类与语言建模等基准上匹配或超过 Adam、AdaBelief 与 LaProp。

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [59] [Neural Network Quantum Field Theory from Transformer Architectures](https://arxiv.org/abs/2602.10209)
*Dmitry S. Ageev,Yulia A. Ageeva*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\to\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an "independence-breaking" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.

</details>


### [60] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [61] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [62] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: LoRA 的秩-精度关系可用闭式公式描述；梯度流分析显示同步/顺序更新等价；秩 1 的 LoRA 在某些损失下可匹敌全参数方法。


<details>
  <summary>Details</summary>
Motivation: 前期实证已证明 LoRA（秩 1 更新）可与全参数方法取得相同精度，但缺乏关于准确度如何随秩变化的理论依据。本研究致力于填补这一空白。

Method: 对全秩与低秩情形下的梯度流进行严格推导，证明同步与顺序 LoRA 参数更新导致相同的动力学方程；利用这些方程得到 LoRA 秩 r 与准确度的显式关联；分别针对 trace‑squared 与 Frobenius‑norm 损失求解收敛关系。

Result: 获得了两种低秩近似损失下，LoRA 秩 r 与微调准确度的闭式关系；梯度流方程被严格推导并证明对不同更新策略等价；结果表明即使在极低秩时也可接近全参数精度。

Conclusion: 本研究从动力学系统视角阐明 LoRA 的更新秩与微调准确度之间的关系，推导出两种常用低秩近似损失（trace‑squared 与 Frobenius‑norm）的闭式公式，并证明无论采用同步还是顺序更新，梯度流方程保持一致，进一步说明即使在秩 1 的极限下 LoRA 也能实现与全参数方法相当的性能。

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [63] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [64] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: T3-Unlearning 通过退火+倾斜两步轻量化方法，在大规模生成模型的聚集型数据遗忘上提供理论保证，实验显著优于传统方案，速度快，参数少。


<details>
  <summary>Details</summary>
Motivation: 在大规模生成模型中实现机器遗忘面临挑战，传统的分类器指导方法难以在有限样本下对聚集型数据集有效遗忘。本研究致力于排除此类限制，提供可行与理论上可靠的遗忘策略。

Method: 提出 Temper-Then-Tilt Unlearning（T3-Unlearning）。首先将基础模型冻结，利用退火方式平滑高置信度峰值；随后训练轻量级分类器区分保留与遗忘样本，对平滑后的分布进行倾斜。

Result: 理论分析给出有限样本下的误差上界，将分类器风险与遗忘误差关联，并证明退火对聚集型分布必不可少。实验证明，在 TOFU 基准上，T3-Unlearning 的遗忘质量与生成效能均优于现有基线，仅训练模型少数参数，运行时间极低。

Conclusion: T3-Unlearning 在理论与实践上均证明了其在大生成模型中稳健、有效、低成本的遗忘能力，解决了密集数据分布遗忘的难题。

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [65] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: MEL 在 RLVR 基础上加入自蒸馏元经验，对比分析错误轨迹挖掘分叉点，将元经验内化为参数记忆，提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR 缺乏错误归因和经验内化，导致难以进行细粒度信用分配和知识复用，必须引入能从错误中提炼可复用知识的机制。

Method: 在 RLVR 基础上添加自我蒸馏元经验构造与对比分析：利用正确与错误轨迹进行对比，识别分叉点并抽象成元经验；再最小化负对数似然，将元经验写入参数记忆，实现奖励信号桥接。

Result: 对不同规模模型测试，MEL 在 Pass@1 方面平均提高 3.92%–4.73%，在多个基准上持续取得进步。

Conclusion: MEL通过利用LLM自我验证能力，生成可验证的元经验并将其内化，实现了细粒度错误归因和知识复用，在RLVR框架上提升了LLM的推理效率，显著提高了考试基准的 Pass@1 分数。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [66] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: 通过训练音频LM在内部使用帧级表示并辅以分类器和泊松过程损失，实现超过50倍的推理速度和更稳健的时间定位，超越传统token化解决方案


<details>
  <summary>Details</summary>
Motivation: 需要精准的时间定位，传统文本时序生成效率低且容易错误

Method: 训练音频LM使用内部帧级别表示，结合二元分类器和不均匀泊松过程损失预测时序事件

Result: 在词定位、说话人分离和事件定位上优于基于token的基线，推理速度提升>50倍，且在训练分布外的音频长度上保持高准确度

Conclusion: 帧级内部工具可有效实现高效、准确的时间定位，显著改善传统方法的计算成本与泛化性能

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [67] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: GRPO assigns one advantage to all tokens, hurting structured outputs. Blockwise Advantage Estimation gives each block its own advantage and uses outcomes‑conditioned baselines to avoid rollouts, achieving competitive results on math tasks.


<details>
  <summary>Details</summary>
Motivation: Single‑scalar advantages in GRPO couple unrelated rewards across segments, causing misattributed credit and difficulty scaling to multiple objectives.

Method: Introduce a family of GRPO-compatible methods that assign each objective its own block-level advantage, and use an Outcome‑Conditioned Baseline to estimate intermediate state values via within‑group statistics.

Result: On math problems with uncertainty estimation, the method mitigates reward interference, rivals state‑of‑the‑art reward‑designed models, and preserves confidence‑weighted ensembling benefits.

Conclusion: Blockwise Advantage Estimation effectively reduces objective interference in structured language generation, maintaining competitive performance while avoiding costly rollouts.

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [68] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 引入离群风险评估，再加权学习的 DP 合成方法，有效强化离群样本的隐私，实验验证其对成员推断防护的有效性。


<details>
  <summary>Details</summary>
Motivation: 当公开合成数据时，离群记录更易被识别并受到成员推断攻击，传统差分隐私的最坏情况保证不足以解决此问题。

Method: 首先用小隐私预算估计每条记录的“离群程度”，随后在DP学习过程中按逆风险分数对记录加权；在 Gaussian 机制下，缩小离群记录的影响即可实现更严格的个体隐私界限。

Result: 实验表明，风险权重化显著降低对高离群记录的成员推断成功率；实测在 Breast Cancer、Adult、German Credit 等真实数据集上，性能提升取决于评分器质量和合成管线的配合。

Conclusion: 通过风险均衡DP合成框架，针对高风险记录（如稀有疾病或异常交易），实现更紧凑的个体隐私保护，并在实验中显著降低了成员推断攻击成功率。

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [69] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [70] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: GLIMARK是把多核加性回归融入广义线性模型，用指数族形式处理多样化数据，实验验证了其在医疗影像预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 单核方法无法充分捕捉复杂异质数据特征；传统多核学习仅针对连续结果，需拓展至更广泛的数据分布。

Method: 在传统MKL的基础上结合广义线性模型，构造多核加性回归结构，以适应离散或连续的指数族响应。

Result: GLIMARK在模拟实验中能逼近真实数据生成机制，在真实COVID-19病例中预测ICU升级并可提取临床意义特征，展示了实用性。

Conclusion: GLIMARK扩展了多核学习框架，使其适用于指数族目标变量，并在COVID-19胸片数据上表现出可解释的预测效果。

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [71] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: LLM对线性高斯SCM的定量参数化挑战重，存在估计不稳定和误图易感，提供公共评测框架供进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在连续域中执行定量因果推理的能力，弥补其在估计功能关系效应大小方面的不足。

Method: 构建Linear-LLM-SCM框架，基于已知DAG拆分父子集合，调用LLM生成回归式结构方程，再聚合并与真实参数对比评估。

Result: 实验显示不同模型结果存在强随机性，DAG构造错误会导致误评，系数估计变化大，对结构和语义扰动敏感，凸显LLM当前的局限。

Conclusion: LLM在线性高斯结构因果模型中仍难以精确量化效应大小，表现出显著的系数估计波动和对图结构误差的易感性。

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [72] [Configuration-to-Performance Scaling Law with Neural Ansatz](https://arxiv.org/abs/2602.10300)
*Huaqing Zhang,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 构建神经配置至性能缩放法则，利用LLM拟合完整训练配置与性能映射，显著提升预测精度和调优可用性。


<details>
  <summary>Details</summary>
Motivation: 传统缩放法则假设超参数已最优选择，难以处理多样化和硬件受限的训练配置，导致预测不稳定。

Method: 利用开放源代码预训练日志构建训练配置与性能的映射，并用大型语言模型拟合该映射，形成神经配置至性能缩放法则。

Result: NCPL在多源公开日志上训练，准确预测最终预训练损失，预测误差比Chinchilla法则低20-40%，并能泛化到高达10倍计算量的未见配置；同时支持多超参数协同调优，性能接近传统缩放基线，并可扩展到损失曲线预测。

Conclusion: 本文提出并验证了基于大语言模型的配置至性能缩放法则（NCPL），显著提升了对大型预训练任务的预测准确性和可调性。

Abstract: Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.

</details>


### [73] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文自动从离线数据学习奖励塑造函数，基于因果贝尔曼方程得到最优值上界，用作 PBRS 势函数；在有混杂变量的连续控制任务中通过 SAC 展示出优良鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励塑造缺乏系统的设计原则，特别是在复杂连续控制问题中；现有方法对混杂变量敏感，缺少鲁棒性。本研究旨在从因果视角为奖励塑造提供理论与实践依据。

Method: 利用因果贝尔曼方程得到最优状态值的紧上界，再将其作为 Potential-Based Reward Shaping (PBRS) 框架中的势函数，在 Soft-Actor-Critic (SAC) 中应用；在离线数据上学习奖励塑造函数。

Result: 在多种常用连续控制基准上，所提奖励塑造算法在存在未观测混杂变量的情况下仍保持优秀表现，并在 Soft-Actor-Critic (SAC) 上实现强性能保证。

Conclusion: 本文提出一种基于因果贝尔曼方程的奖励塑造方法，能够在存在未观测混杂变量的离线数据中自动学习潜在偏移函数，并在连续控制任务中提供强性能保证。

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [74] [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)
*Lipai Huang,Kai Yin,Chia-Fu Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: 利用预训练大模型和检索式增强推理，R2RAG‑Flood在无需训练的情况下，接近监督式表格模型的准确性，且更具成本效益和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在洪灾后实践中，降低房产损失的实时预测需求迫切。传统监督式表格模型虽具一定准确度，但需要昂贵的训练与参数更新。R2RAG-Flood通过构建包含先验推理轨迹的知识库，力求无训练、无参数更新即可实现在大语言模型上的推理，再次提振推理效果与成本效率。

Method: 1）利用已有监督表格预测器生成基础预测及推理轨迹；2）构建包含结构预测、文本摘要和模型推理路径的知识库；3）推理时采用地理邻近和类原型检索来补充上下文，生成上下文增强Prompt；4）大语言模型在此基础上发出两阶段预测：先判定是否受损，再细化到三等级PDE；5）加入降级机制纠正过高预估。

Result: 在德州哈里斯县案例中，监督表格基线取得整体准确率0.714、严重程度分类准确率0.859；R2RAG-Flood在多种模型基底上可达整体准确率0.613–0.668，严重程度分类准确率0.757–0.896，接近监督基线，并能生成结构化推理理由。基于API与GPU成本的效益度量显示，轻量级R2RAG-Flood优于监督基线和大型语言模型，且不需要任务特定训练或微调。

Conclusion: R2RAG-Flood提供了一种检索增强、推理驱动的无训练预测框架，在洪灾后房产损失实时评估中既保持了高准确度，又实现了显著的成本与效率优势，同时为每个预测生成可解释性的推理路径。

Abstract: R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>


### [75] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PUMA改造MDMs的掩码流程，使训练和推理掩码同步，显著加速预训练（约2.5倍）并提升性能。


<details>
  <summary>Details</summary>
Motivation: MDMs在训练时需覆盖指数级别的掩码集合，导致计算成本高且训练-测试不匹配，迫切需要一种更高效且对推理友好的掩码策略。

Method: 在Masked Diffusion Models的前向掩码过程中增加渐进式还原掩码的步骤，使训练时使用的掩码逐步逼近推理时所需的结构化掩码。

Result: 在125M规模的预训练实验中，PUMA实现了约2.5倍的速度提升，并在与自回归初始化等常见技巧结合时展现了互补的性能优势。

Conclusion: PUMA通过在前向掩码过程中引入渐进式反掩码，使训练时的掩码模式与推理时的掩码模式保持一致，从而提高优化效率并减少预训练时间。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [76] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [77] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 论文阐述了大模型差异检测的缺口，提出三维评价标准并对比LLM与SAE方法。结果显示改进的LLM方法与SAE相当，但能揭示更抽象的差异。


<details>
  <summary>Details</summary>
Motivation: 现有大模型评估仅关注设计目标的能力，忽略了模型版本间隐藏的行为差异或新出现的不对齐倾向，需要一种系统方法来发现与比较这些差异。

Method: 提出三项核心评价维度（泛化性、趣味性、抽象程度）作为量化指标，随后使用这些指标对两类模型差异检测方法（LLM生成描述法与稀疏自编码器法）进行统一比较。

Result: 改进后的LLM基线在总体表现与SAE方法相当，但在发现更高层次、更具抽象性的行为差异方面表现更佳。

Conclusion: 提供的评价框架使得模型差异检测方法能被客观公正地比较，且LLM基线在抽象差异捕获方面具有明显优势，为后续模型对比研究奠定了基础。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [78] [Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs](https://arxiv.org/abs/2602.10377)
*Luoyang Sun,Jiwen Jiang,Yifeng Ding,Fengfa Li,Yan Song,Haifeng Zhang,Jian Ying,Lei Ren,Kun Zhan,Wei Chen,Yan Xie,Cheng Deng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>


### [79] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [80] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: LITT通过在相对时间线上对齐事件并关注时间信息，提升了精准医疗中时间序列事件发现和预测的性能


<details>
  <summary>Details</summary>
Motivation: 寻找在大规模时间序列数据中发现个性化顺序事件的重要性，以实现精准医疗的挑战

Method: 提出LITT时间变换器架构，在虚拟相对时间线上对序列事件进行暂时对齐，支持事件时间聚焦注意力并实现个性化解释

Result: 在真实EHR数据（3,276名乳腺癌患者）上验证解读能力，在公开数据集上优于基准和状态最先进的生存分析方法

Conclusion: LITT能有效捕捉个体化时间序列中的关键事件顺序，实现对心脏毒性诱导心脏疾病发生时间的精准预测

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [81] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 研究提出把图结构按照 Weisfeiler‑Lehman 类别映射为人类可解释的颜色词提示，直接注入LLM的自然语言输入，并在各种图任务实验中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLM在处理文本结构良好，但在图任务中面临结构、置换不变性及复杂关系的挑战，需要寻找更适合LLM的图编码方式。

Method: 采用WL相似性类别，将其映射为颜色文字提示，将图结构直接注入自然语言提示中；随后将该提示喂给LLM进行图到文本的翻译和推理。

Result: 在多种算法性与预测性的图任务上，包括合成与真实数据集，方法相较基线提升显著，尤其在需要全局结构推理的场景。

Conclusion: 本研究认为使用人类可解释的结构编码能显著提升LLM在图问题上的表现，并验证了其有效性。

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [82] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [83] [Affordances Enable Partial World Modeling with LLMs](https://arxiv.org/abs/2602.10390)
*Khimya Khetarpal,Gheorghe Comanici,Jonathan Richens,Jeremy Shar,Fei Xia,Laurent Orseau,Aleksandra Faust,Doina Precup*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents~\citep{khetarpal2020can}. Can we posit large models as partial world models? We provide a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, we introduce distribution-robust affordances and show that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that our affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

</details>


### [84] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: LoRA能在有限化学数据上高效微调大型语言模型，达到与完整微调相当的性能，同时更好地保持多任务能力并避免遗忘。


<details>
  <summary>Details</summary>
Motivation: 在化学和制药研发中，需要在广泛训练的“大模型”上对小型专门化数据集进行微调，传统完整微调在参数量和灾难性遗忘上存在挑战，需寻找更高效的替代方案。

Method: 使用LoRA将大型语言模型在有限的有机化学数据集上进行快速适配，比较其在正向预测、逆向合成和试剂预测等任务中的表现，并进行多任务泛化评估。

Result: LoRA在测试任务中与完整微调达成相近准确度，显著减轻灾难性遗忘，更好地保留多任务能力；在C‑H官能化任务中，LoRA与完整微调捕获的反应活性模式细微差别提示LoRA的更有效化学特定适配。

Conclusion: LoRA作为一种参数高效的适配方法，在有机反应预测任务中可与完整微调实现相近精度，同时更好地保持多任务性能并缓解灾难性遗忘。

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [85] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: TaperNorm 用一个全局门控的稀疏归一化替代标准化，训练稳定、推理加速，可实现近乎无归一化 Transformer。


<details>
  <summary>Details</summary>
Motivation: 重新审视预归一化 Transformer 中样本依赖归一化的必要性，削减计算开销并探索无归一化架构，同时明确输出归一化在消除径向梯度、阻止 logit 追逐方面的关键角色。

Method: 采用全局门控策略：在门控暖身期保持 g=1 以通过指数滑动平均EMA 估计缩放分支；随后采用余弦衰减把 g 逐步降到 0，使 per‑token 统计消失；最后可用固定目标辅助损失进一步提锚点并移除最终归一化层。

Result: TaperNorm 在与标准归一化基线相同设定下保持性能；折叠内部缩放后，在 last‑token logits 模式下推理吞吐量提升高达 1.22 倍。

Conclusion: TaperNorm 能在不影响性能的前提下，将样本相关的标准化层逐渐替换为固定的线性/仿射映射，并可在推理阶段把该变换折叠到相邻线性投影中，从而实现更高的吞吐量，显示出输出归一化在 Transformer 训练中的根本作用。

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [86] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: 软max注意力在长序列下失效，LUCID 通过键间预条件化解决聚焦难题，保持复杂度不变，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 softmax 注注意力在长上下文下易把概率扩散到无关 token，导致性能下降；降温会导致梯度消失，难以学习。需要一种既能聚焦又可学习的机制。

Method: 在标准注意力基础上，引入基于 key-key 相似度指数化的预条件器，在 RKHS 中最小化键之间的重叠，使查询在海量键中精准聚焦，且保持与传统注意力相同的计算复杂度。

Result: 在 1B 参数语言模型上，训练并评估至 128k token，LUCID 在长上下文检索任务上比标准注意力提高 14%–18%（举例：BABILong +18%，RULER +14%）。

Conclusion: LUCID Attention 通过预条件化软max注意力，显著提升了大规模上下文检索任务的性能，在 BABILong、RULER、SCROLLS 和 LongBench 上出现 14%–18% 的提升。

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [87] [LightGTS-Cov: Covariate-Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.10412)
*Yong Shang,Zhipeng Yao,Ning Jin,Xiangfei Qiu,Hui Zhang,Bin Yang*

Main category: cs.LG

TL;DR: LightGTS-Cov：在轻量级LightGTS上加套小MLP，显式融合过去/未来协变量，显著提升电价和新能源预测性能，实际部署表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统前置训练的时间序列基础模型常忽视或仅通过简单拼接方式处理协变量，限制了其在需要大量协变量信息的电价和可再生能源预测等应用中的效能。

Method: 在~1M参数的LightGTS主干上插入约0.1M参数的MLP插件，利用时间对齐的协变量对解码过程的输出进行残差校正，实现对过去和未来协变量的显式融合。

Result: 在电价和能源生成数据集的协变量感知基准测试中，LightGTS-Cov始终优于原始LightGTS和其他协变量感知基线；在两项真实案例（光伏功率预测和日间电价预测）中，模型同样实现了优异的预测精度和部署后稳定的运行。

Conclusion: LightGTS-Cov通过显式整合过去和未来已知协变量，不仅保持了轻量级的LightGTS架构，还在带协变量的能源价格和发电量预测任务中持续优于基线模型，并在实际工业应用中表现出高精度和稳定的运营性能。

Abstract: Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.

</details>


### [88] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [89] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: 提出DRPO：将惰性优化转化为乐观DRO，并证明硬过滤为解析解，实现高质量离线RL的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习在离线日志训练时受低质量数据支配，导致模型崩溃。

Method: 提出了对抗性优化的发散理论，并将问题重构为乐观的分布鲁棒优化DRO，最终得到Distributionally Robust Policy Optimization（DRPO），该方法通过严格过滤噪声实现最优恢复高质量行为。

Result: 在混合质量推荐基准上，DRPO实现了SOTA表现。

Conclusion: 通过严格识别并抛弃导致发散的噪声，DRPO能够在离线历史日志中有效优化高质量用户交互。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [90] [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)
*Kanghyun Noh,Jinheon Choi,Yulwha Kim*

Main category: cs.LG

TL;DR: QTALE通过训练时冗余保持和推理时灵活回冗余，解决量化+自适应执行的准确性裂痕，达成高效LLM部署。


<details>
  <summary>Details</summary>
Motivation: LLM需要巨额计算和存储资源，单靠自适应执行或量化均会削弱冗余导致精度下降。

Method: 先通过多路径多样化训练来保留模型冗余，再使用后训练机制在推理时动态调整执行比例，以在低精度量化条件下平衡性能与效率。

Result: 在CommonsenseQA基准上，QTALE与单量化模型的准确率差距低于0.5%，同时兼顾FLOPs和内存减省。

Conclusion: QTALE使代币自适应层执行与量化可无缝结合，在保持准确性的同时通过补偿冗余实现低能耗高效的LLM部署。

Abstract: Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing layers, and quantization, which lowers memory footprint by reducing weight precision. However, naively integrating these techniques leads to additional accuracy degradation due to reduced redundancy in token-adaptive models. We propose QTALE (Quantization-Robust Token-Adaptive Layer Execution for LLMs), a novel framework that enables seamless integration of token-adaptive execution with quantization while preserving accuracy. Conventional token-adaptive methods reduce redundancy in two ways: (1) by limiting the diversity of training paths explored during fine-tuning, and (2) by lowering the number of parameters actively involved in inference. To overcome these limitations, QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of the execution ratio at inference to reintroduce redundancy when needed. Experimental results show that QTALE enables seamless integration of token-adaptive layer execution with quantization, showing no noticeable accuracy difference, with the gap to quantization-only models kept below 0.5% on CommonsenseQA benchmarks. By combining token-adaptive execution for FLOPs reduction and quantization for memory savings, QTALE provides an effective solution for efficient LLM deployment.

</details>


### [91] [LakeMLB: Data Lake Machine Learning Benchmark](https://arxiv.org/abs/2602.10441)
*Feiyu Pan,Tianbin Zhang,Aoqian Zhang,Yu Sun,Zheng Wang,Lixing Chen,Li Pan,Jianhua Li*

Main category: cs.LG

TL;DR: 提出LakeMLB基准，聚焦Union/Join场景，涵盖多源数据集，支持三种集成策略，实验评估最新表格学习方法，释放数据和代码


<details>
  <summary>Details</summary>
Motivation: 缺乏针对数据湖环境下机器学习效果的标准化基准

Method: 设计LakeMLB基准，涵盖Union与Join两种多表场景，提供三条实际数据集，支持预训练、数据增强和特征增强三种集成策略，并利用最新表格学习方法进行实验评估

Result: 通过大量实验揭示不同表格学习模型在复杂数据湖多表场景下的性能差异，为后续研究提供洞察，并公开数据与代码以推动社区合作

Conclusion: 本文提出了LakeMLB，填补了数据湖机器学习评测的空白，展示了多表场景下的挑战与策略，并提供可复现的资源，促进该领域的系统研究

Abstract: Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performance in data lake environments remain scarce. To address this gap, we present LakeMLB (Data Lake Machine Learning Benchmark), designed for the most common multi-source, multi-table scenarios in data lakes. LakeMLB focuses on two representative multi-table scenarios, Union and Join, and provides three real-world datasets for each scenario, covering government open data, finance, Wikipedia, and online marketplaces. The benchmark supports three representative integration strategies: pre-training-based, data augmentation-based, and feature augmentation-based approaches. We conduct extensive experiments with state-of-the-art tabular learning methods, offering insights into their performance under complex data lake scenarios. We release both datasets and code to facilitate rigorous research on machine learning in data lake ecosystems; the benchmark is available at https://github.com/zhengwang100/LakeMLB.

</details>


### [92] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: 提出Chamfer-linkage，利用Chamfer距离衡量簇间相似度，时间复杂度与传统链接相同，且在多种数据集上取得更优聚类质量，可作为层次聚类的可靠替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统经典链接如单链、平均链和Ward方法在不同真实数据集上表现差异大，缺乏一致的高质量聚类，迫切需要更具代表性且性能稳健的链接函数。

Method: 本研究提出Chamfer-linkage，该链接函数使用Chamfer距离衡量簇间距离，并通过巧妙实现保持O(n^2)的时间复杂度。

Result: 实验结果显示，Chamfer-linkage在大量数据集上均能获得更高质量的层次聚类结果，优于平均链和Ward方法。

Conclusion: Chamfer-linkage不但在理论上实现了与传统链接同等O(n^2)的效率，而且在多样化实测数据集上稳定超越平均连结法和Ward方法，证明其可作为层次聚类中的实用替代链接函数。

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [93] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 制定了投影投影理论框架，阐明影响函数在随机投影下的保真条件；给出投影维度选择的实用准则。


<details>
  <summary>Details</summary>
Motivation: 在过参数化模型中直接操作或逆转大尺寸Hessian/ Fisher矩阵成本极高，随机投影被广泛用于加速计算，然而现有的Johnson–Lindenstrauss理论并不能解释投影如何与矩阵逆和正则化交互；缺少理论指导如何选取投影维度。

Method: 针对未正则化、正则化以及Kronecker准则的F矩阵，分析了投影矩阵P在不同空间投影下的行为，推导出有效维数和秩的关系，并给出了对非范围外梯度泄漏项的定量估计。

Result: 证明：1）未正则化时，精确保留等价于P在F范围内注入；2）正则化时误差取决于F的有效维数；3）Kronecker 结构下的分块投影亦保持保留；4）对超出范围的梯度，得到泄漏项评价，保障一般测试点的影响查询。

Conclusion: 本论文提供了一套统一理论，阐述何时投影可以保证对影响函数的近似保留，并给出选择样本缩放大小的原则。

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [94] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [95] [Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation](https://arxiv.org/abs/2602.10457)
*Brian Hyeongseok Kim,Jacqueline L. Mitchell,Chao Wang*

Main category: cs.LG

TL;DR: 通过对数据集标签的有限改动，快速检验模型公平性并定位影响预测的核心训练样本。



<details>
  <summary>Details</summary>
Motivation: 传统对抗解释聚焦输入空间，难以揭示标签偏差在训练过程中的作用；需一种更有效的方法直接评估标签偏差对预测的影响。

Method: 对原始数据集随机少量标签做有限修改，重新训练模型，观察指定测试样本预测变化，并通过启发式排名快速筛选最关键标签。

Result: 在7个主流公平性数据集上对1100+测试案例验证，方法仅需修改极少量标签即可改动预测，并成功识别出驱动预测变化的关键训练样本。

Conclusion: 该方法通过构造对抗性数据集，展示了训练标签的微小改变可显著影响模型预测，从而揭示标签偏差对公平性的影响，并定位关键训练样本。

Abstract: Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.

</details>


### [96] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: LatentRxnFlow将反应预测转化为连续潜在轨迹，兼具高精度与可解释性，支持对失效模式的诊断和不确定性的估计，适用于高通量发掘流程。


<details>
  <summary>Details</summary>
Motivation: 现有反应预测模型多为单步映射，缺乏对反应过程的洞察，且已有的分步方法依赖大量额外信息、离散操作和昂贵推理，难以提供完整的反应轨迹与不确定性评估。

Method: 基于Conditional Flow Matching的连续潜在轨迹学习，直接从反应物-产物对学习时间相关的潜在动力学，省去机制标注和离散符号编辑的需求。

Result: 在USPTO基准上达成了最先进的预测准确度，并通过轨迹层级诊断定位失误；利用轨迹的几何特性实现了对模型不确定性的内在评估，可识别可靠与可疑的预测结果。

Conclusion: 本文提出了LatentRxnFlow模型，能够以连续潜在轨迹的方式预测化学反应，并在标准USPTO数据集上实现了优异性能，同时通过轨迹分析提升了模型透明度、可诊断性和不确定性评估。

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [97] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [98] [Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks](https://arxiv.org/abs/2602.10496)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) stochastic gradient descent (SGD) exhibits approximately integrable dynamics when projected onto the execution subspace, with non-integrability confined to orthogonal staging directions, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.

</details>


### [99] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: MVGR-Net通过融合POI与时间模式，并使用LLM提示框架，解决了场景异质性和外部事件挑战，实现了最高预测精度


<details>
  <summary>Details</summary>
Motivation: ride-hailing forecasting面临地理异质性和外部事件敏感性的问题

Method: 提出MVGR-Net，两阶段:预训练阶段整合POI与时间移动模式，获取多视角地理表征；预测阶段通过提示框架微调大型语言模型并加入外部事件

Result: 在DiDi真实数据集上实现了业界最佳性能

Conclusion: 通过多模态地理表征和提示驱动的LLM，显著提升了动态出行需求预测精度

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [100] [Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation](https://arxiv.org/abs/2602.10506)
*Wei Chen,Xingyu Guo,Shuang Li,Yan Zhong,Zhao Zhang,Fuzhen Zhuang,Hongrui Liu,Libang Zhang,Guo Ye,Huimei He*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \textbf{DiffGDA}, a \textbf{Diff}usion-based \textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.

</details>


### [101] [Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving](https://arxiv.org/abs/2602.10512)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

</details>


### [102] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: RLTT将奖励沿潜在推理轨迹均匀分配，替代传统终点奖励，显著提升LoopLM在数学和非数学推理任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: LoopLM利用多步潜在推理提升推理性能，但传统RL（如GRPO）仅给最终潜在状态奖励，导致与模型内部计算不匹配，无法进一步提升。

Method: 提出RLTT（Reward Latent Thought Trajectories）框架，将奖励沿完整潜在推理轨迹分布，实现密集轨迹级信用分配，无需外部验证器，可直接替代GRPO，且开销可忽略。

Result: 在Ouro-2.6B‑Thinking模型上，统一训练/推理条件下，RLTT相较GRPO在数学推理基准上表现显著提升：MATH‑500 +14.4%，AIME24 +16.6%，BeyondAIME +10.0%。同时在非数学推理基准上亦有良好迁移。

Conclusion: RLTT通过轨迹级奖励分配解决了LoopLM RL中的信用分配不匹配问题，显著提升推理性能，并证明了该方法在跨域推理中的有效性。

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [103] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [104] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [105] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: 分离测量与语言生成，结合多速率采样与跨模态桥，构建首个保证临床测量精度的自动EEG报告系统，误报降低60%，速度提升50%。


<details>
  <summary>Details</summary>
Motivation: 传统LLM受限于上下文窗口、时间序列理解缺乏，压缩导致临床测量失真，误报频繁。

Method: 利用信号处理精确提取临床值，压缩前保持多速率采样，跨模态桥梁完成EEG-语言翻译，参数高效微调并在冻结槽附近约束解码。

Result: 在TUH与CHB-MIT数据集上，相比基准降低60%误报、检测速度提升50%，并实现亚临床测量精度。

Conclusion: 本系统将测量提取与文本生成分离，实现了临床级精度的自动EEG报告，显著减少误报并提高检测速度。

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [106] [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545)
*Yuxin Ma,Nan Chen,Mateo Díaz,Soufiane Hayou,Dmitriy Kunisky,Soledad Villar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>


### [107] [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565)
*Abhijeet Vyas,Brian Bullins*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose and study an online version of min-max optimization based on cumulative saddle points under a variety of performance measures beyond convex-concave settings. After first observing the incompatibility of (static) Nash equilibrium (SNE-Reg$_T$) with individual regrets even for strongly convex-strongly concave functions, we propose an alternate \emph{static} duality gap (SDual-Gap$_T$) inspired by the online convex optimization (OCO) framework. We provide algorithms that, using a reduction to classic OCO problems, achieve bounds for SDual-Gap$_T$~and a novel \emph{dynamic} saddle point regret (DSP-Reg$_T$), which we suggest naturally represents a min-max version of the dynamic regret in OCO. We derive our bounds for SDual-Gap$_T$~and DSP-Reg$_T$~under strong convexity-strong concavity and a min-max notion of exponential concavity (min-max EC), and in addition we establish a class of functions satisfying min-max EC~that captures a two-player variant of the classic portfolio selection problem. Finally, for a dynamic notion of regret compatible with individual regrets, we derive bounds under a two-sided Polyak-Łojasiewicz (PL) condition.

</details>


### [108] [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568)
*Lev McKinney,Anvith Thudi,Juhan Bae,Tara Rezaei,Nicolas Papernot,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Standard large language model training can create models that produce outputs their trainer deems unacceptable in deployment. The probability of these outputs can be reduced using methods such as LLM unlearning. However, unlearning a set of data (called the forget set) can degrade model performance on other distributions where the trainer wants to retain the model's behavior. To improve this trade-off, we demonstrate that using the forget set to compute only a few uphill Gauss-Newton steps provides a conceptually simple, state-of-the-art unlearning approach for LLMs. While Gauss-Newton steps adapt Newton's method to non-linear models, it is non-trivial to efficiently and accurately compute such steps for LLMs. Hence, our approach crucially relies on parametric Hessian approximations such as Kronecker-Factored Approximate Curvature (K-FAC). We call this combined approach K-FADE (K-FAC for Distribution Erasure). Our evaluation on the WMDP and ToFU benchmarks demonstrates that K-FADE suppresses outputs from the forget set and approximates, in output space, the results of retraining without the forget set. Critically, our method does this while altering the outputs on the retain set less than previous methods. This is because K-FADE transforms a constraint on the model's outputs across the entire retain set into a constraint on the model's weights, allowing the algorithm to minimally change the model's behavior on the retain set at each step. Moreover, the unlearning updates computed by K-FADE can be reapplied later if the model undergoes further training, allowing unlearning to be cheaply maintained.

</details>


### [109] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: PiT-PO 通过强化学习和双重约束，将 LLM 转为自适应方程生成器，既保证物理可行性又降低冗余，达到符号回归领先水平，并展示了在湍流建模等领域的优秀表现。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 仅作为静态生成器，缺乏基于搜索反馈的动态更新，易产生物理不一致或数学冗余的等式。

Method: 通过强化学习融合双重约束机制：层级物理有效性约束与细粒度 token 级惩罚，从而动态更新 LLM 内部表征并抑制冗余结构。

Result: 在标准基准任务上实现了最优性能，成功发现了新的湍流模型；小规模模型可超越闭源巨头，推动科研民主化。

Conclusion: PiT-PO 将 LLM 转化为自适应方程生成器，能够产生既科学一致又结构简洁的数学表达式，并实现了领先水平的符号回归。

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [110] [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.

</details>


### [111] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: 论文提出 TRACE 框架，将两模型风险变化分解为四个可解释项，实验证明其能准确诊断协变量偏移导致的性能变化，并提供安全的模型升级门控评分。


<details>
  <summary>Details</summary>
Motivation: 当用在迁移数据上训练的 ∃Q 替换原有源域模型 Q 时，源域性能可能出现不可预测的变化；现有方法缺乏能解释风险变化原因的可解释框架。

Method: 构建两模型风险变化 ΔR 的理论上界，并将其分解为两项泛化差距、模型改变惩罚和协变量偏移惩罚。协变量偏移惩罚通过高分位输入梯度的模型敏感因子与基于特征空间的最优输运（或MMD）实现；模型改变惩罚通过两模型在目标样本上的平均输出距离量化；泛化差距则在保留子集上估计。最终实现 TRACE 的全可计算诊断。

Result: 在理想线性回归实验和多种合成/视觉基准中，TRACE 上界能准确捕捉真实风险差的缩放；诊断指标与实际性能下降保持强单调关系；门控评分与 |ΔR|高度相关，AUROC/AUPRC 亦表现优异，证明其在安全标签高效替换中的实用性。

Conclusion: TRACE框架通过将两模型风险变化分解为四个可解释的上界项，为在协变量偏移下评估新模型在源域上的性能变化提供了系统的诊断工具。实验验证显示，该框架不仅在理论上与真实风险差距保持一致，而且在合成与视觉基准上呈现出高度一致的单调关系，为安全的模型升级提供了可衡量的门控评分。

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [112] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: RI-FedAvg 通过 Roughness Index 正则化抑制客户端漂移，在非 IID 场景中实现了对现有基线算法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，非 IID 数据导致客户端更新出现漂移，影响收敛速度与最终性能，需要一种能够自适应调节更新幅度的方法来解决此问题。

Method: 在本工作中，我们在 FL 的本地目标函数中加入一个由 Roughness Index 衡量的正则化项，该项根据本地损失曲面波动动态地对模型更新进行惩罚，从而抑制客户端漂移。

Result: 在 MNIST、CIFAR-10 与 CIFAR-100 数据集的非 IID 设定下，RI-FedAvg 相比 FedAvg、FedProx、FedDyn、SCAFFOLD 以及 DP‑FedAvg 等基线方法表现出更高的准确率和更快的收敛速度。

Conclusion: RI-FedAvg通过引入基于 Roughness Index 的正则化项，有效缓解了非 IID 环境下的客户端漂移，实现了稳健的优化和更快的收敛，能够提升联邦学习在异构数据场景下的鲁棒性与效率。

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [113] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [114] [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611)
*Nicolás Becerra-Zuniga,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: PINN受限于数据质量，只有高精度一致的数据才能突破一致性障碍，获得高精度解。


<details>
  <summary>Details</summary>
Motivation: 在实验或数值数据受噪声、离散化误差、建模假设等因素影响时，PINN的精度和收敛性尚不清楚。

Method: 通过在一维可压薄牛顿方程配合可控解析解，使用不同精度的数据集训练PINN，并量化由数据不一致产生的误差下限。

Result: 发现添加PDE残差可部分补偿低精度数据，但最终误差被一致性障碍限制；当使用高精度数值或解析数据时，该障碍消失，PINN表现与解析解相当。

Conclusion: 本文证明了数据与PDE不一致所导致的“一致性障碍”是PINN最终精度的内在下限。

Abstract: Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>


### [115] [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614)
*Nusaibah Farrukh,Malavika Pradeep,Akshay Sasi,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cognitive load, the mental effort required during working memory, is central to neuroscience, psychology, and human-computer interaction. Accurate assessment is vital for adaptive learning, clinical monitoring, and brain-computer interfaces. Physiological signals such as pupillometry and electroencephalography are established biomarkers of cognitive load, but their comparative utility and practical integration as lightweight, wearable monitoring solutions remain underexplored. EEG provides high temporal resolution of neural activity. Although non-invasive, it is technologically demanding and limited in wearability and cost due to its resource-intensive nature, whereas pupillometry is non-invasive, portable, and scalable. Existing studies often rely on deep learning models with limited interpretability and substantial computational expense. This study integrates feature-based and model-driven approaches to advance time-series analysis. Using the OpenNeuro 'Digit Span Task' dataset, this study investigates cognitive load classification from EEG and pupillometry. Feature-based approaches using Catch-22 features and classical machine learning models outperform deep learning in both binary and multiclass tasks. The findings demonstrate that pupillometry alone can compete with EEG, serving as a portable and practical proxy for real-world applications. These results challenge the assumption that EEG is necessary for load detection, showing that pupil dynamics combined with interpretable models and SHAP based feature analysis provide physiologically meaningful insights. This work supports the development of wearable, affordable cognitive monitoring systems for neuropsychiatry, education, and healthcare.

</details>


### [116] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: 合成多变量临床时间序列在大样本下能抵御已知隐私攻击，当前的 DP 方案会导致实用性下降；因此，对生成模型进行隐私审计仍是必不可少的。


<details>
  <summary>Details</summary>
Motivation: 合成医学数据能在不直接披露患者信息的前提下用于机器学习，但生成模型可能仍泄露身份信息；因此需要系统评估其隐私保护效果。

Method: 在公开的 MIMIC‑IV 数据集上训练医院时间序列生成模型，并通过一套成熟的隐私攻击对生成的数据进行评估；另外使用 eICU 数据对在 MIMIC‑IV 上训练的生成器进行攻击。

Result: 攻击对基于大型训练集生成的数据无效；引入现有 DP 机制虽然能提升隐私保证，却会显著降低机器学习预测任务的性能。

Conclusion: 对于足够大的训练集，使用已知隐私攻击对合成多变量临床时间序列无效；但差分隐私（DP）机制会显著降低模型的实用性，无法在不牺牲效能的前提下提升隐私。

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [117] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: 提出CG-BGs：在粗粒化坐标下通过流模型生成样本，并用从力匹配学习得到的PMF进行重加权，既保持了BG的准确性，又实现了大规模系统的可扩展采样。


<details>
  <summary>Details</summary>
Motivation: 传统BG在高维度难以扩展；粗粒化模型缺乏保证统计正确性的重加权；需要一种能在降低维度同时保持统计准确性的框架。

Method: 构建流模型对粗粒化坐标生成样本，利用从力匹配快速收敛得到的潜势平均力（PMF）对样本进行重权重；通过优势学习精确的重加权因子，实现保真采样并兼顾可扩展性。

Result: CG-BGs成功捕捉了显式溶剂介导的复杂相互作用，在高度简化的表示下实现无偏采样，验证了可扩展且准确的分子配置采样方法。

Conclusion: CG-BGs通过在粗粒化坐标空间使用学习到的PMF实现重要性重加权，能够准确且可扩展地采样大型分子系统，展示了在更高效的低维模型中保持布朗分布的可行性。

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [118] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: 提出多维度评估合成纵向患者数据时间保留的指标，揭示单指标局限，强调综合评估的重要性，助力模型改进与合成数据生成。


<details>
  <summary>Details</summary>
Motivation: 在合成纵向患者数据中缺乏对时间依赖性的全面评估工具，导致模型改进缺乏指导。

Method: 提出包含边缘、协方差、个体水平及测量结构四类的多维度指标体系，并探讨原始数据质量、测量频率以及预处理对时间结构保持的影响。

Result: 实验表明，单纯的边缘相似度可能掩盖协方差扭曲和个体轨迹破坏；稀疏、非规则测量时间降低了合成数据与原始数据的时序相似度；无单一指标能够全面衡量时序保留，需要组合多维度指标进行评估。

Conclusion: 该指标体系精准揭示时间结构的保持与退化机制，为生成模型的评估与改进提供了可靠依据，并推动了时序真实度高的合成纵向患者数据生成。

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [119] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: 用物理洞察进行坐标变换，能把高维耦合问题简化为一维主轴搜索，从而让BO在复杂光学系统里快速、鲁棒地找到最优解。


<details>
  <summary>Details</summary>
Motivation: 现有BO方法在高维紧耦合、极不对称的目标函数中表现不佳，难以应对稀疏奖励的“针尖陷阱”情形，需要更鲁棒的优化方案。

Method: 利用领域物理洞察对输入空间进行坐标变换，解耦特征并对齐主子空间；结合反向退火探索策略实施BO。

Result: 在12维、6晶体Split-and-Delay光学系统上，与标准BO、TuRBO及多目标BO相比，该方法通过坐标变换与反向退火实现了可靠的全局最优收敛。

Conclusion: 通过基于物理知识的坐标变换，BO在高维、耦合参数的光学系统中实现了可重复收敛至全局最优，显著提升了搜索效率。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [120] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: VESPO 通过变分方差缩减的序列级核，解决 RL 训练中的分布偏移问题，提升稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: RL 训练中策略陈旧、异步和训练推理引擎不匹配导致分布偏移，易导致训练崩溃。

Method: 通过变分推断对提议分布进行方差缩减，推导闭式序列级重构核，直接在序列重要性权重上操作，不做长度规范化。

Result: 在数学推理基准测试中，VESPO 在 up to 64× staleness 与完全异步执行下保持稳定训练，并在稠密及 Mixture-of-Experts 模型中持续提升。

Conclusion: VESPO 提升了大语言模型 RL 的训练稳定性，能在高 staleness 和异步执行下保持收敛与性能提升。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [121] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: ProtoGLAD 用正常原型图对照解释异常图，既保持高检测性能，又提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度 GLAD 方法虽表现良好，却缺乏可解释性；现有解释方法未引用具体正常图或仅使用抽象潜在向量，难以被人类直观理解。

Method: 利用点集核对数据集进行多次迭代，发现若干正常原型图及其簇；随后将距离所有这些正常簇都较远的图标记为异常。

Result: 在多组真实数据集上实验表明，ProtoGLAD 在异常检测准确率上与最前沿方法相当，同时提供了更具人类可解释性的原型图解释。

Conclusion: ProtoGLAD 通过与最近的正常原型图对比，为每个检测到的异常图提供可解释的异常检测结果，兼顾检测性能与可解释性。

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [122] [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718)
*Yifan Zhang,Zunhai Su,Shuhao Hu,Rui Yang,Wei Wu,Yulei Qian,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: SnapMLA通过RoPE-aware KV量化、PV流水线重构和数据流优化，解决FP8在MLA解码中的数值与尺度挑战，实验显示吞吐量提升1.91倍，保持高精度。


<details>
  <summary>Details</summary>
Motivation: FP8注意力在FlashAttention-3等创新中表现突出，但在DeepSeek Multi-head Latent Attention（MLA）解码阶段整合时面临数值异质性、RoPE位置编码分离导致的量化尺度不匹配以及系统级优化需求等挑战。

Method: SnapMLA通过三项硬件感知算法-核协同优化实现：1）RoPE-aware Per-Token KV量化——在保持RoPE高精度的同时，采用每个token粒度量化对齐自回归解码，提升量化精度；2）Quantized PV计算流水线重构——修复FP8 PV计算中的量化尺度不匹配，解决MLA KV缓存共享结构的问题；3）端到端数据流优化——使用专用核构建高效读写流程，保证数据流畅，提升整体性能。

Result: 在一系列先进MLA LLM上大量实验表明，SnapMLA可实现最大1.91倍的吞吐量提升，在长上下文的数学推理与代码生成等苛刻任务中几乎无性能退化风险。

Conclusion: SnapMLA demonstrates that judicious FP8 quantization coupled with hardware-aware algorithmic innovations can significantly accelerate MLA decoding while preserving accuracy, enabling efficient deployment on long-context workloads.

Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>


### [123] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: 提供了预算感知RMAB算法CURE-UCB，理论与实验上优于现有基线


<details>
  <summary>Details</summary>
Motivation: 说明RMAB框架在不同预算下最优策略变化较大，现有方法缺乏对时间预算的考量

Method: 提出CURE‑UCB，明确将预算信息融入累积奖励估计中，并给出理论上限

Result: 证明CURE‑UCB在线性后扁平等结构化环境下优于无预算信息算法，实验进一步验证其性能提升

Conclusion: 本工作填补了RMAB时间预算相关研究空白，为未来预算感知算法奠定基础

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [124] [Predicting integers from continuous parameters](https://arxiv.org/abs/2602.10751)
*Bas Maat,Peter Bloem*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.

</details>


### [125] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: 提出了$(t,K)$-阈值水印，确保仅大规模联盟能验证模型，实验验证其可检测性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，模型所有权需可验证，单客户端水印易被稀释或被窃取。

Method: 采用$(t,K)$-阈值水印，使用秘密共享技术派发水印密钥，仅当至少$t$个客户端联合时才能重构，白盒实现并在图像分类上验证。

Result: 在$K=128$规模下，水印可检测，精度损失极小，检测阈值$z\ge4$在包括自适应微调攻击在内的多种攻击下仍保持上限。

Conclusion: 等价可靠的水印验证机制能够防止单个或小规模联盟操控模型并维护版权归属。

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [126] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 发布SS-CDIL算法，利用少量目标专家演示与无标签轨迹，构建跨域损失与自适应权重，提升MuJoCo和Robosuite上的模仿学习效果。


<details>
  <summary>Details</summary>
Motivation: 在专家数据昂贵的场景下，跨域知识迁移能加速策略学习，但现有方法依赖完整对齐或易不稳定，缺乏半监督设置。

Method: 结合离线数据、跨域损失函数学习域间状态-动作映射，并设计自适应权重平衡源域与目标域知识，以处理域差异。

Result: 在MuJoCo与Robosuite环境实验中，该方法相较基线统一提升性能，显示出稳定性与数据效率。

Conclusion: 在半监督跨域模仿学习（SS-CDIL）框架下，提出的算法通过仅使用少量目标专家示例和无标签不完整轨迹，实现了稳定且数据高效的策略学习，显著优于现有监督及无监督方法。

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [127] [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794)
*Benjy Friedmann,Nadav Dym*

Main category: cs.LG

TL;DR: CycFlow用确定性点传输取代扩散模型的迭代边去噪，线性坐标动力学让TSP求解速度提升约1000倍，保持相近最优度。


<details>
  <summary>Details</summary>
Motivation: 弥补传统扩散模型在边评分导致的二次复杂度瓶颈，寻找更高效的神经组合优化方式。

Method: 学习实例条件向量场，连续将二维坐标传输至圆形排列，并通过角度排序恢复最优旅行商路径；采用数据相关流匹配，实现线性坐标动力学。

Result: 相较于最先进的扩散基线，CycFlow速度提升多达三阶，对应的最优性差距保持竞争水平。

Conclusion: 本文提出CycFlow框架，利用确定性点传输替代迭代边去噪，显著加速并保持竞争性最优性。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.

</details>


### [128] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: RePO 通过把离线知识重述为 on‑policy 轨迹，提升难样本利用并实现 SOTA。


<details>
  <summary>Details</summary>
Motivation: 对大型语言模型进行行业数据对齐是一大挑战。监督微调能注入领域知识，但往往降低模型的通用性；基于策略的强化学习保持通用性，却难以有效利用超出模型当前推理水平的难样本；近期的离线强化学习在提升难样本利用率的同时，因强行把分布朝离线知识偏移而导致训练不稳定。

Method: Rephrasing Policy Optimization (RePO) 方案：首先让策略模型理解离线知识，然后重新表述为符合自身风格和参数分布的轨迹，动态用这些高质量轨迹替换低奖励走向，确保训练仍保持 on‑policy 动态。

Result: 在多个基准上，RePO 能提升难样本利用率，优于现有基线，达到现有最优性能。

Conclusion: RePO 在保持 on‑policy 稳定性的同时，有效吸收离线知识，显著提升对难样本的利用，推动 LLM 在领域对齐任务的性能。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [129] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: ASC为差分隐私下的最差组优化提供自适应采样与裁剪方案，提升少数组表现并保证统一隐私。


<details>
  <summary>Details</summary>
Motivation: 传统平均损失优化导致少数组效果差。加权优先处理最差组的做法在差分隐私下会导致隐私不均，尤其对少数组不利。

Method: 通过自适应控制每个组的采样率与裁剪阈值，实现更高学习难度组的迁移采样，同时保持隐私一致。

Result: ASC在异方差梯度、隐私保证与最差组准确率方面均优于前沿工作，且未牺牲总体平均准确率。

Conclusion: ASC算法在保证各组隐私水平一致的同时，提高了最差组的准确率并降低梯度方差，优于现有方法。

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [130] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: SimuScene数据集包含7,659个物理场景，LLM平均通过率21.5%，通过视觉奖励RL提升能显著改善物理模拟和代码生成效果。


<details>
  <summary>Details</summary>
Motivation: 本文探讨大型语言模型（LLM）在通过代码模拟物理场景方面的能力，目前这方面研究不足；因此提出对多领域多概念的物理模拟进行系统评估。

Method: 构建自动化管线采集7,659个物理场景数据并通过人工验证；将334例作为测试集；对10款主流LLM进行评估；进一步设计基于视觉奖励的强化学习管线，利用视语言模型作为判定器训练文本模型。

Result: 即便是最强模型，其通过率仅为21.5%；采用RL方案后，模型在物理模拟代码生成及通用代码生成任务上均得到显著提升。

Conclusion: 物理模拟任务对LLM构成挑战，SimuScene数据集与RL框架为提升此类能力提供了有效路径。

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [131] [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847)
*Fanpu Cao,Lu Dai,Jindong Han,Hui Xiong*

Main category: cs.LG

TL;DR: 多变量预测模型受限于短期历史窗口，GTR 通过维护全局时间嵌入并动态检索全局周期片段，借助 2D 卷积+残差融合融合长周期信息，实验表明其在多数据集上优于现有方法，且开销低


<details>
  <summary>Details</summary>
Motivation: 解释多变量时间序列预测模型无法充分利用长周期全局周期模式

Method: 利用轻量级 Global Temporal Retriever (GTR) 在不改变主模型结构的前提下，维护全局时间嵌入，并通过 2D 卷积和残差融合动态检索、对齐输入序列对应的全局周期片段，从而融合短期观测与长期周期信息

Result: 在六个真实数据集上，GTR 在短期和长期预测任务中均实现了最先进的性能，且参数和计算开销极低

Conclusion: GTR 为多变量时间序列预测提供了一种高效、通用的全局周期建模方案，可显著提升现有模型的预测效果

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.

</details>


### [132] [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 四大TSFMs在长上下文下MASE约0.31，优于季节性Naive 47%；短上下文仍稳健优于Prophet；Chronos-2校准最佳。


<details>
  <summary>Details</summary>
Motivation: 验证TSFMs从训练场景到关键电力负荷预测任务的迁移性能，评估其零样本预测能力在实际应用中的有效性。

Method: 构建多维基准，利用ERCOT 2020-2024小时负荷数据评估四大TSFM（Chronos-Bolt、Chronos-2、Moirai-2、TinyTimeMixer）与行业基线Prophet及统计参考SARIMA、Seasonal Naive，并在无GPU硬件上进行四个维度实验：上下文长度敏感性、概率预测校准、分布漂移鲁棒性以及决策支持。

Result: 顶级TSFM在2048小时上下文下MASE≈0.31，相比季节性Naive下降47%；TSFM在短上下文仍保持稳定，Prophet在短窗时MASE>74；Chronos-2提供良好校准，Moirai-2与Prophet过度自信。

Conclusion: TSFMs在长时间上下文和分布漂移场景下显示出优越的预测精度与稳健性，能够为电力系统的运营决策提供更可靠的支持。

Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly affect grid operations--remains an open question. We present a multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, and TinyTimeMixer) alongside Prophet as an industry-standard baseline and two statistical references (SARIMA and Seasonal Naive), using ERCOT hourly load data from 2020 to 2024. All experiments run on consumer-grade hardware (AMD Ryzen 7, 16GB RAM, no GPU). The evaluation spans four axes: (1) context length sensitivity from 24 to 2048 hours, (2) probabilistic forecast calibration, (3) robustness under distribution shifts including COVID-19 lockdowns and Winter Storm Uri, and (4) prescriptive analytics for operational decision support. The top-performing foundation models achieve MASE values near 0.31 at long context lengths (C = 2048h, day-ahead horizon), a 47% reduction over the Seasonal Naive baseline. The inclusion of Prophet exposes a structural advantage of pre-trained models: Prophet fails when the fitting window is shorter than its seasonality period (MASE > 74 at 24-hour context), while TSFMs maintain stable accuracy even with minimal context because they recognise temporal patterns learned during pre-training rather than estimating them from scratch. Calibration varies substantially across models--Chronos-2 produces well-calibrated prediction intervals (95% empirical coverage at 90% nominal level) while both Moirai-2 and Prophet exhibit overconfidence (~70% coverage). We provide practical model selection guidelines and release the complete benchmark framework for reproducibility.

</details>


### [133] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: 仅用单比特反馈，学习多维CDF的样本复杂度几乎与维度无关，只受对数影响，进一步延伸至固定价格机制的最优回报分析。


<details>
  <summary>Details</summary>
Motivation: 在多维累计分布函数学习中，仅利用极简的单比特反馈，探究在误差ε下的样本复杂度。

Method: 先构建对Dither Coin (DKW)不等式的带反馈对偶版本，采用细网实现均匀逼近，分析其样本复杂度与维度的关系。

Result: 得到样本复杂度约为(1/ε^3)·(log(1/ε))^{O(n)}，维度n仅以对数形式出现；随后给出在小型市场固定价格机制中的紧致样本复杂度与新颖期望回报界。

Conclusion: 在单比特反馈场景下，均匀逼近多维CDF可达高效性，维度对样本需求影响极小，且可用于机制设计与极简性经济模型。

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [134] [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: FedPS通过分布式统计聚合与数据压缩实现高效联邦预处理，兼容水平与垂直联邦，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，数据预处理被忽视；隐私限制禁止集中原始数据，通信开销又制约分布式预处理；亟需一种既遵守隐私又能高效通信的预处理方案。

Method: 利用数据压缩技术生成局部数据的汇总统计，基于这些统计开发分布式特征缩放、编码、离散化、缺失值填充算法，并在水平与垂直联邦场景下扩展k‑Means、k‑最近邻、贝叶斯线性回归等模型。

Result: FedPS实现了低通信成本、跨站点一致的预处理管道，支持多种传统算法在联邦环境下的应用，提升整体模型性能与部署可行性。

Conclusion: FedPS为联邦学习提供了一套统一、可通信高效且一致的数据预处理框架，有效提升模型训练性能。

Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.

</details>


### [135] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [136] [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905)
*Deyi Kong,Zaiwei Chen,Shuzhong Zhang,Shancong Mou*

Main category: cs.LG

TL;DR: NHGD 通过经验 Fisher 矩阵近似 Hessian，降低二阶计算负担，理论与实验均显示其在双层优化中的高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中超梯度估计的计算瓶颈，即 Hessian 逆的计算或近似问题。

Method: 使用经验 Fisher 信息矩阵近似 Hessian 逆，在并行优化与近似框架中同步更新，充分利用梯度信息。

Result: 理论上提供高概率误差界和样本复杂度保证；实验上在代表性双层学习任务中表现出可扩展性与有效性。

Conclusion: 本文提出了天然超梯度下降（NHGD）方法，并证明其在高概率误差界和样本复杂度上与最先进方法相当，同时显著降低计算开销。

Abstract: In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.

</details>


### [137] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 为安全强化学习中的受限马尔可夫决策过程提出FlexDOME算法，结合可变安全边距和正则化，借助渐近支配与Lyapunov分析实现近常数强约束违规、次线性奖励退化及最终迭代收敛，解决了传统方法的违规累积与平均迭代问题。


<details>
  <summary>Details</summary>
Motivation: 现有的原始‑对偶方法在实现次线性强奖励退化的同时，往往会导致强约束违规量的无限增长，或仅能够得到平均迭代收敛；这种误差累加或振荡的行为限制了安全强化学习的实用性。

Method: 引入时间可变的安全边距和正则化项，将其嵌入到原始‑对偶框架中，并采用新的逐项渐近支配策略，严格调度安全边距以在渐近上主导优化和统计误差的功能衰减率，从而把累计违规量控制在近常数级；通过政策‑对偶Lyapunov论证，进一步得到非渐近的最终迭代收敛保证。

Result: FlexDOME是首个能在强约束违例度上实现$	ilde O(1)$、强奖励退化保持次线性、并提供非渐近最终迭代收敛保证的算法；实验结果进一步验证了理论分析。

Conclusion: FlexDOME算法实现了近乎常数级的强约束违规量$	ilde O(1)$，同时保持次线性强奖励退化和非渐近收敛的最终迭代收敛；这在先前基于原始-对偶方法的算法中是首次实现的。

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [138] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: 时序注意力存在对角注意力沉陷，导致首位偏置；本文通过敏感度分析揭示此现象，并提出正则化方法，实验验证有效削弱偏差。


<details>
  <summary>Details</summary>
Motivation: 抑制时空模型中的信息退化，检验时序注意力机制中潜在的首位偏置（过度压缩）。

Method: 通过推导时序注意力层Jacobian期望值的敏感度界限，分析了沿对角线与非对角线注意力分数的关系，揭示了对角注意力sink。进一步提出并实现了正则化策略以抑制偏差。

Result: 理论证明了对角注意力sink，并通过实验验证了所提出正则化方法在减少偏置方面的有效性。

Conclusion: 本文阐明了时序注意力矩阵存在‘对角注意力沉陷’现象，并证明其受序列长度影响，从而导致首位偏置。通过引入正则化方法，可显著缓解此偏差。

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [139] [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959)
*Feilong Liu*

Main category: cs.LG

TL;DR: 本文用信号处理视角重新定义 RoPE，推导低/高精度下限并验证模型实际表现，指明了精度与深度决定的长上下文可行区间


<details>
  <summary>Details</summary>
Motivation: 阐明在长上下文条件下，旋转位置嵌入(RoPE)的行为并提供可操作的设置指引

Method: 将RoPE视为复杂振荡器的相位调制，利用信号处理理论推导低阶别重叠/DC稳定性下界以及高阶深度层数上的相位累积上界，并结合有限浮点精度导出上界

Result: 得到一套双边可行域（保真与精度）并通过对大模型(LLaMA、Mistral、DeepSeek)的实证验证，验证了当模型参数越过稳定下界时会出现注意力崩塌；以及一条独立于架构的精度墙限制了可处理的最大长度约百万级

Conclusion: 确定了“金玉其味”区间，为设计长上下文 Transformer 提供理论最优参数取值与实现前的风险评估机制

Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.

</details>


### [140] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: MoEEdit通过专家空洞空间投影与块坐标下降，提供了在稀疏MoE LLM中高效且路由稳定的知识编辑方案，兼具性能与资源优势。


<details>
  <summary>Details</summary>
Motivation: 密集模型编辑方法不适用于普及的稀疏MoE模型；直接适配既昂贵又易导致路由不稳定，需要一种能兼顾计算效率与路由稳定的新框架。

Method: 采用每个专家的空洞空间投影重参数化专家更新，以保持路由器输入不变，抑制路由分布偏移，并通过块坐标下降求解块结构优化，实现高效训练。

Result: MoEEdit在多项实验中表现出最先进的有效性和泛化能力，同时保持高特异性与路由稳定，并在计算与内存方面优于现有方法。

Conclusion: 本研究提出了MoEEdit框架，成功实现了在稀疏Mixture-of-Experts LLM中实现路由稳定的知识编辑，显著提升了编辑效果、可推广性与资源效率。

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [141] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: 推导一种低内存开销、鲁棒的 HugGLB 算法，在多类异方差 GLB 环境下可达到近似最优 regret，理论上与已知特例兼容。


<details>
  <summary>Details</summary>
Motivation: 缺乏能够统一处理多种异方差GLB（如线性、逻辑、泊松）并对抗惰性攻击的算法；

Method: 结合在线镜像下降估计器与基于Hessian的置信权重，以获得自我共形链接函数下的鲁棒估计；

Result: 得到$\tilde{O}(d\sqrt{\sum_t g(τ_t)\dotμ_{t,\star}}+d^2 g_{\max} κ+d κC)$ 上界，并证明下界$\tildeΩ(d\sqrt{\sum_t g(τ_t)\dotμ_{t,\star}}+d C)$，实现近似实例最优；

Conclusion: 提出了一个理论上与实例相关的最优算法HCW-GLB-OMD，在考虑异方差GLB和对抗性干扰时实现了无锁定的 regret 的下限和上限匹配；

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [142] [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008)
*Ammar Ali,Baher Mohammad,Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: ROCKET通过背包分配和单步稀疏分解，实现20-50%压缩，保留90%性能，无需微调，进一步微调可恢复接近原始性能。


<details>
  <summary>Details</summary>
Motivation: 解决模型压缩中训练成本高、参数量大问题

Method: 采用多选背包分配权衡层级压缩，单步稀疏矩阵分解结合字典学习，避免迭代优化

Result: 在20-50%压缩率下超过现有压缩基线，30%压缩保持90%性能，无需微调；通过轻量微调，Qwen3-14B压缩至8B并用3千万token恢复几乎与原模型相同

Conclusion: ROCKET提供一种训练无关、全局优化、迭代-free的压缩方案，性能明显优于传统方法，可快速部署大模型

Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.

</details>


### [143] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: OSIL使用非优轨迹估计安全成本，改进离线安全模仿学习，在不失奖励的前提下实现更安全的策略。


<details>
  <summary>Details</summary>
Motivation: 在线环境学习存在安全风险，且难以准确指定安全成本；然而可以收集反映不安全行为的轨迹，利用其暗含的危险信息进行安全学习。

Method: 将安全策略学习视为受限马尔可夫决策过程，通过推导奖励最大化目标的下界以及学习一个成本模型估计非优轨迹发生的可能性，从而在无显式安全成本标签的情况下实现安全不可行性约束。

Result: 实验显示OSIL在满足成本约束的同时，保持甚至提升了奖励性能，优于多种基线。

Conclusion: OSIL能够从仅含非优缺轨迹的离线演示中学习到既安全又能最大化奖励的策略，在成本约束下不降低奖励性能，优于多种基线方法。

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [144] [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)
*Xinyu Yuan,Yan Qiao,Zonghui Wang,Wenzhi Chen*

Main category: cs.LG

TL;DR: Pram：利用多模态语言模型与多智能体强化学习，快速求解多商品流问题，保持高质量解、低运行时、强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统优化引擎在大规模分配系统中难以兼顾最优性和可扩展性，需要一种既能快速求解又能保持全局一致性的创新方法。

Method: 将原问题划分为局部子问题，使用MLM驱动的“Agent”快速求解子问题，再通过多智能体强化学习算法对齐全局一致性。

Result: 在真实数据和公开拓扑上，Pram的性能与线性规划求解器相当甚至更优，运行时间至少快10到100倍，且在链路失效或流量激增下性能下降不足10%。

Conclusion: Pram是一种基于多模态语言模型的高效多商品流问题求解器，能够在保持接近最优解的同时显著提升运行速度，并具备良好鲁棒性。

Abstract: The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

</details>


### [145] [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)
*Jialin Liu,Zhaorui Zhang,Ray C. C. Cheung*

Main category: cs.LG

TL;DR: MoToRec通过稀疏正则化RQ‑VAE生成可解释的离散语义码，并结合自适应稀有度放大与层次化图编码，显著提升冷启动推荐性能。


<details>
  <summary>Details</summary>
Motivation: GNN在推荐系统中表现优异，但在稀疏数据和新项冷启动场景下性能受限，现有多模态方法因噪声和数据稀疏导致表示不佳。

Method: 采用稀疏正则化的残差量化变分自编码器(RQ‑VAE)生成离散可解释的语义码；引入自适应稀有度放大机制优先学习冷启动项；以及层次化多源图编码器融合协作信号。

Result: 在三个大型数据集上实验，MoToRec在整体和冷启动场景下均超越最新方法。

Conclusion: 离散化语义标记能有效、可扩展地缓解长期存在的冷启动问题。

Abstract: Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.

</details>


### [146] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: 合成人体运动数据对可穿戴 HAR 预训练有益，规模+混合是关键，单纯大规模运动捕捉预训练效果有限。


<details>
  <summary>Details</summary>
Motivation: 本研究关注在真实数据稀缺且可扩展预训练方案需要的情境下，合成数据对全身人体运动（Human Activity Recognition, HAR）迁移性能的提升。

Method: 在运动捕捉得到的表示上生成合成运动时间序列，对运动时间序列模型进行预训练，并在多种下游 HAR 任务中评估其迁移效果。

Result: 实验显示，合成预训练在与真实数据混合或规模足够时能提升泛化；而仅靠大规模运动捕捉预训练由于与可穿戴信号的域不匹配，收益有限。

Conclusion: 合成运动数据在提升 HAR 表示方面具备潜力，但受域差距限制；通过规模扩大或与真实数据混合，可实现可迁移的表示。

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [147] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [148] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [149] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 通过将$f$‑divergence与Bellman残差约束关联，并构造可调的$f$‑divergence函数，本文提出一种更灵活的离线RL方法，实验验证其在有限探索和多来源数据上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统离线RL在数据稀疏、探索回报有限，且包含多行为策略时会出现过于保守或估值失误的问题，需要一种更灵活的约束机制来平衡目标与数据支持。

Method: 先从RL的线性规划形式和凸共轭推导出$f$‑divergence与Bellman残差优化约束的关系，然后构造可调的$f$‑divergence函数，将其嵌入到离线RL的约束优化目标中。

Result: 在MuJoCo、Fetch、AdroitHand等环境的实验中，改进后的LP形式与可调$f$‑divergence均在与兼容约束优化算法配合时提升了学习效果。

Conclusion: 提出了一种基于可调的$f$‑divergence约束的离线强化学习框架，能够在有限探索和多行为策略的数据集下实现更均衡的政策学习。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [150] [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](https://arxiv.org/abs/2602.11090)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.

</details>


### [151] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [152] [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097)
*David A. Barajas-Solano*

Main category: cs.LG

TL;DR: 本文将PINN参数估计转化为统计学习问题，发现其为Singular学习，并借助局部学习系数解析热方程样本的估计效果，为PINN的不确定性与外推能力提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 探究PINN训练机制与性能，从统计学习理论角度理解物理约束的作用，提升对预测不确定性与外推能力的把控。

Method: 采用硬初始边界约束，将PINN参数估计视作统计学习问题，利用KL散度衡量PINN残差分布与真生成分布的相似，将物理残差解释为无穷间接数据；对串行学习模型应用Singular学习理论中的局部学习系数，对热方程IBVP的随机优化估计进行严谨分析。

Result: 证明PINN训练属于Singular学习问题，并运用局部学习系数对热方程IBVP参数估计进行分析；结果为PINN不确定性量化与外推性能的理论评估提供了新的视角。

Conclusion: 从统计学习视角重新表述PINN的参数估计，揭示物理约束惩罚是无穷间接数据，非正则化；并通过Singular学习理论及局部学习系数分析热方程IBVP的参数估计，进一步为PINN的预测不确定性与外推能力提供理论依据。

Abstract: We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.

</details>


### [153] [From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123)
*Genmao Zhuang,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

</details>


### [154] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [155] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [156] [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130)
*Maximilian Plattner,Fabian Paischer,Johannes Brandstetter,Arturs Berzins*

Main category: cs.LG

TL;DR: 稀疏点云重建使用扩散变换器时易出现碎片化失效（Meltdown）。论文通过激活补丁定位问题源，提出以谱熵为控制信号的 PowerRemap 方案，在多模型、多数据集、多去噪策略上显著提升输出连通性。


<details>
  <summary>Details</summary>
Motivation: 提高稀疏点云表面重建的可靠性，避免因微小扰动导致输出碎片化的灾难性失效。

Method: 1) 对 3D 扩散变换器进行激活补丁实验定位故障源；
2) 通过跨注意力激活的奇异值谱推导出谱熵作为标量指标；
3) 将谱熵与逆扩散动力学的对称破裂分叉联系；
4) 设计 PowerRemap 的测试时控制，规范稀疏点云条件。

Result: 发现 Meltdown 在 WaLa、Make-a-Shape 等先进架构以及 GSO、SimJEB 数据集、DDPM、DDIM 去噪策略下普遍存在；PowerRemap 在测试集上实现了 98.3% 的稳定率提升。

Conclusion: 论文通过机制解释发现并修复了稀疏点云表面重建过程中出现的‘Meltdown’故障，提出的 PowerRemap 方案在多种模型、数据集和去噪策略上实现了高达 98.3% 的稳定性提升。

Abstract: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

</details>


### [157] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 提供一种训练自由的 token 级仅需迭代一次的停止机制，显著减少绝大多数 token 的去噪步骤，提升整体生成效率。


<details>
  <summary>Details</summary>
Motivation: Diffusion 语言模型在生成中需要多次迭代，但大多数 token 在最终去噪前就已稳定，导致计算开销不必要地高。

Method: 提出无训练、基于 token 的早停方法，利用模型预测与局部上下文的轻量信号，独立判断每个位置何时收敛并冻结，从而实现自适应 token 冻结。

Result: 在数学推理、通用问答与科学理解等多项基准上，显著减少 diffusion 步骤数，保持生成质量的同时获得业界领先的效率提升。

Conclusion: 通过动态判定 token 收敛，既提升了生成速度，又无需额外任务特定的微调，验证了无训练早停在 diffusion 语言模型中的有效性。

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [158] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: 把权重衰减从仅仅降低预训练损失的角度，转向提升模型微调可塑性的思路，比过去更能帮助找到更适合下游使用的模型。


<details>
  <summary>Details</summary>
Motivation: 常规的超参调优聚焦于预训练阶段的交叉熵损失，忽略了模型在下游任务中的适应能力。本文从模型可塑性角度重新审视权重衰减的作用。

Method: 对预训练过程中的权重衰减进行系统实验，评估不同衰减值下模型在多项下游任务微调后的性能提升，并通过分析表示的线性可分性、注意力矩阵的正则化及对训练集过拟合的抑制来探究其机制。

Result: 实验显示，较大权重衰减的模型在微调时能获得更大性能提升——即使预训练时表现更差。机制方面，权重衰减促进了线性可分表示、正则化了注意力矩阵并降低了对训练数据的过拟合。

Conclusion: 本研究表明，优化权重衰减时应关注模型在微调任务中的可塑性，而非仅仅是预训练阶段的验证损失。较大的权重衰减能够提升后续微调表现，尽管在预训练阶段表现略逊，说明单一超参数在塑造模型行为方面具有多重作用。

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [159] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


### [160] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: GENIUS提供生成流动智力评估框架，揭示现有视觉生成模型对即时推理的缺陷，并提出无训练注意力干预方案提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有评测侧重于知识回忆，忽视了模型在即时创生及约束推理方面的综合能力，需要评估真正的动态推理与灵活生成。

Method: 将生成流动智力定义为三大原语：隐式模式诱导、临时约束实现与上下文适应，构建相应评估任务并对12类模型进行系统测试。

Result: 系统评测显示模型在三类原语上普遍失效，诊断显示根源于对上下文的理解不足；提出的注意力干预能显著改善性能。

Conclusion: GENIUS通过引入晶体与流动智力评估，揭示现有UMMs在生成流动智力方面存在显著欠缺，并提供了基于注意力的无训练干预方案来提升模型在即时上下文推理中的表现。

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


### [161] [Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151)
*Sedigheh Eslami,Maksim Gaiduk,Markus Krimmel,Louis Milliken,Bo Wang,Denis Bykov*

Main category: cs.LG

TL;DR: 推出pplx-embed系列多语言检索嵌入模型，采用多阶段对比学习与扩散预训练，显著提升大规模检索效果，尤其是上下文化版本在ConTEB上刷新记录


<details>
  <summary>Details</summary>
Motivation: 开发高效多语言检索嵌入模型

Method: 在扩散预训练语言模型骨干上采用多阶段对比学习，结合双向注意力与延迟分块技术

Result: pplx-embed-v1在MTEB、MIRACL、BERGEN、ToolRet等基准上表现竞争力，pplx-embed-context-v1在ConTEB创下新纪录；在内部大规模检索测试中亦表现优异

Conclusion: pplx系列模型在大规模检索场景下兼具检索质量与效率，可直接用于生产环境

Abstract: In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.

</details>
